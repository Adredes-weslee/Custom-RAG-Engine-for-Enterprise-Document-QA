[{"content": "import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, FunctionTransformer from sklearn.pipeline import Pipeline # Ordinal feature processing def replace_education_values(df: pd.DataFrame) -> pd.DataFrame: \"\"\"This function takes reads a pandas dataframe and the following 'Survive': Changes all values to 0 and 1 (int) 'Smoke': Changes all values to No and Yes 'Ejection_Fraction': Changes all values to Low, Normal and High 'Age': Negative values are assumed to be entry errors Parameter: df: pandas dataframe Returns: df: pandas dataframe with all values replaced \"\"\" df['education'].replace({'Associates degree-occup /vocational': 'Associates degree', 'Associates degree-academic program':'Associates degree', 'Children': 'Some college but no degree'}, inplace=True) df['income_group'].replace({'- 50000.': 0, '50000+.':1, }, inplace=True) return df # Creating the ranking list for education def ordinal_encoding_education(df: pd.DataFrame) -> pd.DataFrame: \"\"\"This function ordinally encodes the education feature based on the education_ranking Input: df Output: df with education encoded\"\"\" education_ranking = [ 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', ] # Creating the object for education encoding education_encoder = OrdinalEncoder(categories= [education_ranking]) # Applying the encoding to the dataset df['education'] = education_encoder.fit_transform(df[['education']]) return df # Defining the after split functions def scaling_numeric_features(df: pd.DataFrame) -> pd.DataFrame: '''This function will take in Dataframe (df) and perform min max scaling on columns found in feature_to_scale Input: df Output: df with numeric features scaled ''' # defining the features to scale feature_to_scale = ['age','capital_gains', 'capital_losses','dividends_from_stocks', 'weeks_worked_in_year','num_persons_worked_for_employer'] # Creating the scaling object min_max_scaler = MinMaxScaler() # applying the scaling to respective columns in the scale df[feature_to_scale] = min_max_scaler.fit_transform(df[feature_to_scale]) return df def one_hot_encoding (df:pd.DataFrame) -> pd.DataFrame: '''This function will take in Dataframe (df) and perform one-hot-encoding on columns found in feature_to_ohc Input: df Output: df with numeric features scaled ''' # Defining the features to one-hot-encode features_to_ohc = ['tax_filer_stat','major_industry_code', 'major_occupation_code', 'class_of_worker',] # Creating the dummies encoding_df = pd.get_dummies(df[features_to_ohc], drop_first=True, dtype=float) # Combing the dummies with the original df df = pd.concat([df,encoding_df], axis = 1) # dropping the original features df = df.drop(columns =features_to_ohc) return df # Defining the pipelines # pipeline before split before_split_pipeline= Pipeline([ ('value_replacement', FunctionTransformer(replace_education_values)), ('ordinal_encoding', FunctionTransformer(ordinal_encoding_education)) ]) # after split pipeline after_split_pipeline = Pipeline([ # This is for numeric data preprocessing ('min_max_scaling', FunctionTransformer(scaling_numeric_features)), # This is for nominal data preprocessing ('one_hot_encoding', FunctionTransformer(one_hot_encoding)) ]) def transform(data_path): \"\"\" This function will take in data from data_path and preprocesses the data according to the functions. Steps involved 1) Reads .csv into pandas DataFrame (df) 2) Filtering the df for features to be used 3) Apply pre-split processing 4) split data 5) Apply post-split processing Params: data_path: file path to stored .csv file Returns: 4 datasets (X_train, y_train, X_test, y_test) ready for model training. \"\"\" #1 This code block reads the csv file using the pd.read_csv function #1 and converts it into a pandas dataframe df = pd.read_csv(data_path) #checkpoint for troubleshoot - data", "source": "datapipeline.py"}, {"content": "path should be correct print(f'Data_loaded from ' + data_path) # applying the pipeline df = before_split_pipeline.fit_transform(df) # Checkpoint for troubleshooting - should be numbers instead of strings print(df['education'].unique()) # Defining X as input X = df[['education', 'age','capital_gains', 'capital_losses','dividends_from_stocks', 'weeks_worked_in_year','num_persons_worked_for_employer', 'tax_filer_stat','major_industry_code', 'major_occupation_code', 'class_of_worker', ]] # Defining y as output y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Checkpoint for troubleshooting - should be 11/ print (X_train.shape, X_test.shape, y_train.shape, y_test.shape) # Applying the pipeline to X_train and X_test X_train_scaled = after_split_pipeline.fit_transform(X_train) X_test_scaled = after_split_pipeline.transform(X_test) return X_train_scaled, X_test_scaled, y_train, y_test raise NotImplementedError # Code block to run this python script # data_path = '/Users/guanling/all-assignments/assignment1/data/raw/data.csv' # X_train, X_test, y_train, y_test = transform(data_path) # print(f'After Transformation shape is ', X_train.shape, X_test.shape, y_train.shape, y_test.shape) # how to check # print(df.shape) # print (X_train.shape)", "source": "datapipeline.py"}, {"content": "from sklearn.tree import DecisionTreeClassifier import numpy as np import pandas as pd class DecisionTree: def __init__(self) -> None: # initiatizes tree structure with the class self.tree = {} pass # This function works for section 2.3 only def _gini(self, list1:list, list2:list): '''This function takes in 2 list containing 0s and 1s and returns a gini score Inputs: list1 = a list of 1 and 0 list2 = a list of 1 and 0 Output: gini score the underscore prefix is meant to denote underscore ''' # 2 list of number will come in. # i need to know 3 values. # total number of values # number of 1 # number of 0 # nest the list together incoming_list = [list1, list2] # For loop to go through each list for list in incoming_list: # defining variable to store number of 0 and 1 count_of_1 =0 count_of_0 =0 count_total = len(list) # go through the list to count the number of 0 and 1 for value in list: if value == 0: count_of_0 += 1 elif value == 1: count_of_1 += 1 # Proportion of 0 ratio_0 = count_of_0 / count_total # Proportion of 1 ratio_1 = count_of_1 / count_total # calculate the gini impurity based on formula provided gini = 1 - ratio_0**2 - ratio_1**2 return gini # This function will be meant for counting list with any different unique values def gini(self, list:list): '''This function takes in a list and returns a gini score Inputs: list: a listing containing values. Output: gini score ''' # incoming list # i need to know this values: # total number of values # occurrences of each unique value for # dictionary to store counts of unique values (key: value)(unique value: count) label_counts = {} for value in list: label_counts[value] = label_counts.get(value, 0) + 1 # Calculate gini impurity # gini starts at 1 and subtracts away proportion of each unique value gini = 1 # total number of observations total_sample = len(list) # iterate through the dictionary's values for unique_value, count in label_counts.items(): #calculating the proportion proportion = count / total_sample calculated_gini -= proportion**2 pass # # nest the list together # incoming_list = [list1, list2] # # For loop to go through each list # for list in incoming_list: # # defining variable to store number of 0 and 1 # count_of_1 =0 # count_of_0 =0 # count_total = len(list) # # go through the list to count the number of 0 and 1 # for value in list: # if value == 0: # count_of_0 += 1 # elif value == 1: # count_of_1 += 1 # # Proportion of 0 # ratio_0 = count_of_0 / count_total # # Proportion of 1 # ratio_1 = count_of_1 / count_total # # calculate the gini impurity based on formula provided # gini = 1 - ratio_0**2 - ratio_1**2 pass def fit(self, X: pd.DataFrame, y:list): '''This function takes in 2 numpy arrays. Feature_array = M observations and N features Target_array = M observation and target It will then Inputs: X = M observations and", "source": "decision_tree.py"}, {"content": "N features y = M observation and target Output: Fit model ''' # Calling the fit function will tree an internal function _build_tree self.tree = self._build_tree(X,y,0) pass def _build_tree(self): pass def _find_best_split(self): # defining 3 variables best_gain = -np.inf best_feature = None best_threshold = None for feature in range(X.shape[1]): thresholds = np.unique(X[:, feature]) for threshold in thresholds: gain = information_gain(X, y, feature, threshold) if gain > best_gain: best_gain = gain best_feature = feature best_threshold = threshold return best_feature, best_threshold def check_purity(data): # At a leaf node (no outgoing arrows) label_column = data[:, -1] unique_classes = np.unique(label_column) if len(unique_classes) == 1: return True else: return False # To understand how to build the tree. A typical dataframe consist # what happens within a node. # incoming array of data, check for purity of data, if", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import cross_val_score, KFold, RandomizedSearchCV from sklearn.metrics import f1_score, make_scorer class Model: def __init__(self): self.model = RandomForestClassifier() def train(self, model_params, X_train, y_train): '''This fuction serves to fit model with training data Parameter: model_params: pass in a dictionary with model parameters as keys e.g {'max_depth': 4) will generate a RandomForestClassifier(max_depth = 4) X_train: Features of Training Data y_train: Outcomes of Training Data Returns: Model trained with X_Train and y_train ''' # sets the parameters for the model self.model.set_params(**model_params) # fit model with training data return self.model.fit(X_train, y_train) # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it # pass def evaluate(self, X_test, y_test): '''This function will provide the recall score when prediction of the model is compared to the ground truth in testing data Parameter X_test: features of testing data (used to obtain predictions) y_test: outcomes in testing data to compare against predictions Returns: f1_score ''' # save predictions made from passing the test data y_pred = self.model.predict(X_test) # saves the f1 score from the predictions f1 = f1_score(y_test, y_pred) return print(f'The F1 score is :', f1) def get_default_params(self): \"\"\"This function allows the end user to obtain the parameters of the model itself. Inputs: Nil Returns: parameters of the model\"\"\" # setting the default parameter used default_params = {'max_depth': 4, 'max_features': 4, 'n_estimators': 100 } return default_params # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model # pass def validate_model(self,X_data, y_data, folds): '''This function will enable a KFold cross validation in the desired metric Parameter: self.model: The trained model (estimator) X_data: the feature data y_dat: the outcome data folds: number of folds to be used for cross validation score: the scoring metric of choice Returns: An average score of the results from n_folds ''' metric = make_scorer(f1_score) kf = KFold(n_splits = folds, shuffle = True) cv_results = cross_val_score(self.model, X_data, y_data, cv=kf, scoring=metric) print('The average score of ' + str(folds) + ' fold cross validation is ' + str(cv_results.mean()))", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import pandas as pd from sklearn.preprocessing import LabelEncoder, StandardScaler from sklearn.model_selection import train_test_split from imblearn.over_sampling import SMOTE def transform_data(df:pd.DataFrame) -> tuple: \"\"\" Transform the input DataFrame by encoding the 'Class' column using LabelEncoder, splitting the data into training, validation, and test sets, applying SMOTE to the training set, and returning them. Parameters: df (pd.DataFrame): Input DataFrame containing the data Returns: tuple: X_train_resampled, X_val, y_train_resampled, y_val, X_test, y_test after splitting the data \"\"\" l_enc = LabelEncoder() df['Class'] = l_enc.fit_transform(df['Class']) # Split the features and targets X = df.drop(columns=['Class']) y = df['Class'] # Split the data into training, validation, and test sets X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42) X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.25, random_state=42) scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_val = scaler.transform(X_val) X_test = scaler.transform(X_test) # Apply SMOTE to the training set smoter = SMOTE() X_train_resampled, y_train_resampled = smoter.fit_resample(X_train, y_train) print (f'SMOTED-Training shape',X_train_resampled.shape, y_train_resampled.shape) print (f'Validation shape',X_val.shape, y_val.shape) print (f'Testing shape',X_test.shape, y_test.shape) return X_train_resampled, X_val, y_train_resampled, y_val, X_test, y_test", "source": "a4p1_preprocess.py"}, {"content": "'''This script is for training a model on a4p1 dataset.''' # Importing libraries import pandas as pd from a4p1_preprocess import transform_data # load in data df = pd.read_csv('assignment4/data/raw/data.csv', index_col=0) X_train, X_val, y_train, y_val, X_test, y_test = transform_data(df)", "source": "a4p1_train_model.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # Initialzing weight and bias # 4 inputs from features to each of 16 neurons (4 lines to each of 16 neurons) self.w1 = np.random.rand(4,16) # 16 biases, 1 per neuron in hidden layer self.b1 = np.random.rand(1,16) # 3 inputs from 16 to each of the 3 neurons (3 lines to each of 3 neuons) self.w2 = np.random.rand(16,3) # bias at output layer 3 neurons to have a bias each neuron self.b2 = np.random.rand(1,3) # raise NotImplementedError def init_params(self): \"\"\"Returns the initialized params\"\"\" return print(self.w1,self.b1,self.w2,self.b2) def scratch_RELU(self, X): \"\"\"Apply Rectified Linear Unit (ReLU) activation function element-wise to the input array. Parameters: X (numpy.ndarray): Input array. Returns: numpy.ndarray: Output array after applying ReLU activation function. \"\"\" X = np.maximum(0,X) return X def scratch_softmax(self, X): \"\"\" Calculate the softmax function for the input array X. Parameters: - X: numpy array, input array Returns: - numpy array, softmax output of the input array X \"\"\" X = np.exp(X) / np.sum(np.exp(X)) return X def forward(self, X_array): \"\"\" Perform forward pass through the neural network model. Parameters: X_array (numpy.ndarray): Input array for the forward pass. Returns: numpy.ndarray: Output array after passing through the neural network layers. \"\"\" # input signal from input layer to hidden layer (features:a0 = 1,4) (w1 = 4, 16) + b1 (16, 1) z1 = X_array.dot(self.w1) + self.b1 # activation of a1 = passing in input z1 though Relu function a1 = self.scratch_RELU(z1) # input signal from hidden layer to output layer (a1 = 16,1) (w2 = 16, 3) + b2 (3, 1) z2 = a1.dot(self.w2) + self.b2 a2 = self.scratch_softmax(z2) # initializing the parameters within the class self.z1 = z1 self.a1 = a1 self.z2 = z2 self.a2 = a2 # a2 = pass z2 thru softmax (change the 3,1 into probabilities) print(f'Prediction: {a2}') return a2 def encode_label(self,y_label:np.array): ''' Encode the label using one-hot encoding. Parameters: y_label (np.array): The target label to be encoded. Returns: np.array: An array with one-hot encoded label. ''' # creates and array of 3 zeros label_array = np.zeros(3) # converts the target from numpy array into a variable to use as element number element = y_label.astype(int) # update element in array with 1 label_array[element] = 1 return label_array def loss(self, predictions:np.array, label): \"\"\" Calculate the training loss based on the predictions and label. Parameters: predictions (np.array): Array of predicted values. label: The target label for comparison. Returns: float: The calculated training loss. \"\"\" # convert the target into an array label_array = self.encode_label(label) # loss = -sum(y_true * log(y_pred)) loss = -np.sum(label_array* np.log(predictions)) return loss def reverse_RELU(self, Z): return (Z>0).astype(int) def backward(self, y_labels): \"\"\" Adjusts the internal weights/biases \"\"\" # defining m as length of y labels m = y_labels.size # one-hot-encoding y_true y_pred_array = self.encode_label(y_labels) # change in z2 = A2 - prediction (1x3) (error between prediction and labels) dz2 = self.a2 - y_pred_array # change in w2 = reciprocal of the change in z2 and the activation from previous", "source": "mlp.py"}, {"content": "layer 16 # need to transpose to enable matrix multiplication # dw2 = 1/m * dz2.dot(self.a1.T) # change b2 = reciprocal of sum of dz2,2 db2 = 1/m * np.sum(dz2,2) dz1 = self.w2.T.dot(dz2) * self.reverse_RELU(self.z1) dw1 = 1/m * dz1.dot(self.input_size.T) # change b2 = reciprocal of sum of dz2,2 db1 = 1/m * np.sum(dz1,2) return dw1,db1,dw2,db2 def update_params(self, learning_rate, dw1,db1,dw2,db2 ): self.w1 = self.w1 - (learning_rate * dw1) # 16 biases, 1 per neuron in hidden layer self.b1 = self.b1 - (learning_rate * db1) # 3 inputs from 16 to each of the 3 neurons (3 lines to each of 3 neuons) self.w2 = self.w2 - (learning_rate * dw2) # bias at output layer 3 neurons to have a bias each neuron self.b2 = self.b2 - (learning_rate * db2) #defining the ReLU function", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, datapath:str): self.datapath = datapath def get_data(self) -> pd.DataFrame: \"\"\" Reads a CSV file located at self.datapath and returns the data as a pandas DataFrame. Returns: pd.DataFrame: A pandas DataFrame containing the data from the CSV file. \"\"\" df = pd.read_csv(self.datapath, index_col=0) print(f'Data loaded!') return df def transform(self, df:pd.DataFrame) -> np.array: \"\"\" Transforms the input DataFrame by dropping the 'id' column, splitting the data into features (X_array) and targets (y_array), and standardizing the features using StandardScaler. Parameters: df (pd.DataFrame): Input DataFrame containing the data. Returns: tuple: A tuple containing the standardized features (X_array) and the targets (y_array) as numpy arrays. \"\"\" # drop irrelevant columns df = df.drop(columns=['id']) # split features and targets and into numpy arrays X_array = df.drop(columns=['y']).values y_array = df['y'].values.reshape(-1,1) # Scales the features data scaler = StandardScaler() X_array = scaler.fit_transform(X_array) print(f'Data transformed!') return X_array , y_array", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127,-618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.model_selection import TimeSeriesSplit # Step 1: Load the data class DataPipeline: def __init__(self, csv_path:str): # saves variable csv_path to the instance variable self self.csv_path = csv_path # loads the data from the csv into pandas dataframe def load_data(self) -> pd.DataFrame: df = pd.read_csv(self.csv_path) return df def run_data_pipeline(self, df:pd.DataFrame) -> pd.DataFrame: # Create and set datetime index df = self._create_datetime_index(df) # Drops columns that are not needed # Fill in missing value in target df = self._fill_missing_values(df) # encode wind direction df = self._ohc_wind_direction(df) # Handling of outliers. df = self._remove_outliers(df) # Introduce lag Features df = self._create_lag_features(df, 'TEMP', 3) df = self._create_lag_features(df, 'PRES', 3) df = self._drop_columns(df) df = self._dropna(df) return df def feature_engineering(self, df:pd.DataFrame) -> pd.DataFrame: df = self._create_lag_features(df, 'TEMP', 3) df = self._create_lag_features(df, 'PRES', 3) return df def _create_datetime_index(self, df:pd.DataFrame) -> pd.DataFrame: df['Hourly']=pd.to_datetime(df[['year','month','day','hour']]) df.set_index('Hourly', inplace=True) return df def _drop_columns(self, df:pd.DataFrame) -> pd.DataFrame: df.drop(columns=['year','month','day','hour','No','Iws','Is','Ir','cbwd','DEWP'], inplace=True) return df def _fill_missing_values(self, df:pd.DataFrame) -> pd.DataFrame: df['pm2.5'].fillna(df['pm2.5'].mean(),inplace=True) return df def _ohc_wind_direction(self, df:pd.DataFrame) -> pd.DataFrame: wind_dir_map = {'NW': 0, 'NE': 1, 'SE': 2, 'cv': 3} df['cbwd'] = df['cbwd'].map(wind_dir_map) return df def _remove_outliers(self, df:pd.DataFrame) -> pd.DataFrame: q1 = df['pm2.5'].quantile(0.25) q3 = df['pm2.5'].quantile(0.75) iqr = q3 - q1 lower_bound = q1 - (1.5 * iqr) upper_bound = q3 + (1.5 * iqr) df = df[(df['pm2.5'] >= lower_bound) & (df['pm2.5'] <= upper_bound)] return df def normalize_values(self, df:pd.DataFrame) -> pd.DataFrame: # Create a standard scaler object and a list of features to scale scaler = StandardScaler() feature_to_scale = ['DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir'] df = scaler.fit_transform(df[feature_to_scale]) return df def _create_lag_features(self, df:pd.DataFrame, feature:str, no_of_lags:int) -> pd.DataFrame: for i in range(1,no_of_lags+1): df[f'{feature}_lag_{i}'] = df[feature].shift(i) return df def _dropna(self, df:pd.DataFrame) -> pd.DataFrame: df.dropna(inplace=True) return df def _split_data(self, df:pd.DataFrame): # create the spltting class tss = TimeSeriesSplit(n_splits=5) # split the data X = df.drop(columns=['pm2.5']) y = df['pm2.5'] # iterate over the splits for train_index, test_index in tss.split(X): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y.iloc[train_index], y.iloc[test_index] return X_train, X_test, y_train, y_test", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import root_mean_squared_error class ForecastModel: def __init__(self): self.model = RandomForestRegressor() def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = root_mean_squared_error(y_train, y_train_pred) y_test_pred = model.predict(X_test) test_error = root_mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # init_params upon instantiation of class and creates windows based on # the 3 arguments passed in. self.data = data self.lookback = lookback self.lookahead = lookahead self.X = [] self.y = [] self.create_window() # for i in range(len(data) - lookback - lookahead + 1): # self.X.append(data[i:i + lookback]) # self.y.append(data[i + lookback + lookahead - 1]) # # self.X = np.array(self.X) # # self.y = np.array(self.y) def create_window(self): \"\"\" Generates windows of data for time series prediction. This method iterates through the dataset and creates windows of data based on the specified lookback and lookahead periods. The windows are stored in the instance variables `self.X` and `self.y`. `self.X` will contain the input sequences, each of length `lookback`. `self.y` will contain the corresponding target values, each located `lookahead` steps ahead of the end of the input sequence. The resulting lists are converted to numpy arrays for efficient numerical operations. Parameters: None Returns: None \"\"\" # loops through data, # looks for points with enough data for lookback and lookahead. # len(data) = the entire data set for i in range(len(self.data) - self.lookback - self.lookahead + 1): self.X.append(self.data[i:i + self.lookback]) self.y.append(self.data[i + self.lookback + self.lookahead - 1]) # The list itself is converted into a numpy array self.X = np.array(self.X) self.y = np.array(self.y) def get_tensors(self): \"\"\" Converts the attributes X and y to PyTorch tensors. Returns: tuple: A tuple containing two PyTorch tensors: X_tensor - tensor form of X. y_tensor - tensor form of y. \"\"\" X_tensor = torch.tensor(self.X, dtype=torch.float32) y_tensor = torch.tensor(self.y, dtype=torch.float32) return X_tensor, y_tensor def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 1, 1, 1, 1] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "# Basic Import import numpy as np import pandas as pd # For processing data from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from scipy.sparse import issparse def transform(data_path): #try: \"\"\" Description of the function. :param data_path: ...... :return: ...... \"\"\" # Load in the data df = pd.read_csv(data_path) # Define X as input features and y as the outcome variable X = df.drop(columns=['income_group'],axis=1) y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Seperate out the columns with numerical, ordinal and categorical features num_features = list(X.select_dtypes(exclude=\"object\").columns) ord_features = ['education','financial_savvy'] cat_features = list(X.select_dtypes(include=\"object\").drop(columns=ord_features).columns) # Build a preprocessing step for ordinal features. Ranking by order ratings_order_education = ['Children', 'High School or Less', 'Associates Degree or College dropout', 'Bachelors Degree', 'Masters Degree','Doctoral Degree'] ratings_order_fsavvy = ['Low','High'] # To use OneHotEncoder() as the transformer for Categorical features, OrdinalEncoder() for Ordinal features and StandardScaler() as the transformer for Numerical features num_transformer = StandardScaler() cat_transformer = OneHotEncoder(sparse_output=False) ord_transformer = OrdinalEncoder(categories=[ratings_order_education, ratings_order_fsavvy] ) # Put all transformer into a preprocessor preprocessor = ColumnTransformer( [(\"OneHotEncoder\", cat_transformer, cat_features), (\"OrdinalEncoder\", ord_transformer, ord_features), (\"StandardScaler\", num_transformer, num_features) ]) X_train = preprocessor.fit_transform(X_train) X_test = preprocessor.transform(X_test) \"\"\" # Convert the sparse matrix to a dense format if necessary if issparse(X_train): X_train = X_train.todense() if issparse(X_test): X_test = X_test.todense() # Convert the transformed data to numpy arrays X_train = np.array(X_train) X_test = np.array(X_test) y_train = np.array(y_train) y_test = np.array(y_test) \"\"\" return (X_train, X_test, y_train, y_test) #except Exception as e: # raise e", "source": "datapipeline.py"}, {"content": "# Import the libraries needed import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier # For Evaluation from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score #for accuracy_score from sklearn.metrics import precision_recall_curve, roc_curve, roc_auc_score from sklearn.model_selection import KFold, StratifiedKFold #for K-fold cross validation from sklearn.model_selection import cross_val_score #score evaluation from sklearn.model_selection import cross_val_predict #prediction from sklearn.model_selection import GridSearchCV from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay #for confusion matrix from sklearn.metrics import classification_report class Model: def __init__(self): # init your model here self.model = RandomForestClassifier(random_state=42) def train(self, params, X_train, y_train): \"\"\" Description of the function. :param params: ...... :param X_train: ...... :param y_train: ...... :return: ...... \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it. Implementing this in evaluate. # Set model parameters if any if params: self.model.set_params(**params) # Train the model self.model.fit(X_train, y_train) # Predict on the training data y_train_pred = self.model.predict(X_train) # Calculate the F1 score f1_train = f1_score(y_train, y_train_pred, average='weighted') return f1_train def evaluate(self, X_test, y_test): \"\"\" Description of the function. :param X_test: ...... :param y_test: ...... :return: ...... \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score # Predict on the test data y_pred = self.model.predict(X_test) # Calculate the F1 score f1_test = f1_score(y_test, y_pred, average='weighted') return (f1_test, y_pred) def hyperparameter_tuning(self, model, param_grid, X_train, X_test, y_train, y_test): # Define the GridSearchCV object grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='accuracy') # Fit the GridSearchCV object grid_search.fit(X_train, X_test) # Retrieve the best parameters and best score best_params = grid_search.best_params_ best_score = grid_search.best_score_ print(best_params) print(best_score) # Set model parameters to the best_params self.model.set_params(**best_params) # Train the model with the best_params fitted_model = self.model.fit(X_train, X_test) # Predict on the test data y_pred = fitted_model.predict(y_train) # Classification report model_classification_report = classification_report(y_test, y_pred) print(model_classification_report) return (best_params, best_score, model_classification_report) def get_default_params(self): \"\"\" Description of the function. :return: ...... \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # edit this function to return the optimized parameters for your model default_params = { 'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'random_state': 42 } return default_params", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "# Basic Libraries import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import os from sklearn.preprocessing import RobustScaler from sklearn.model_selection import train_test_split class Datapipeline(): def transform_data(self, df): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" # Split to X and y X = df.drop('Class', axis=1) y = df['Class'] # Split to train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Further split the training data to get validation data X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train) # Standardize the data rob_scaler = RobustScaler() X_train = rob_scaler.fit_transform(X_train) X_val = rob_scaler.transform(X_val) X_test = rob_scaler.transform(X_test) return X_train, X_val, X_test, y_train, y_val, y_test", "source": "datapipeline.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd import numpy as np import os from typing import List, Optional, Dict, Tuple from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.compose import ColumnTransformer class DataPipeline: def __init__(self): # Your code here pass # Load data from a csv file. If the csv file contains columns 'year', 'month', 'day', and 'hour', create a datetime index. def load_data(self, file_path: str) -> pd.DataFrame: df = pd.read_csv(file_path) # Convert the 'date' column to datetime format df['datetime'] = pd.to_datetime(df['datetime']) df.set_index('datetime', inplace=True) df.sort_index(inplace=True) # Ensure the data is sorted by date return df def combine_datetime(self, df): # Combine year, month, day, and hour into a datetime index df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) df.drop(columns=['year', 'month', 'day', 'hour', 'No'], inplace=True) return df # Apply back fill to the null values in the 'pm2.5' column def handle_null_values(self, df: pd.DataFrame, na_col: str, method: str) -> pd.DataFrame: if method == 'drop': df[na_col] = df[na_col].dropna() elif method == 'ffill': df[na_col] = df[na_col].ffill() elif method == 'bfill': df[na_col] = df[na_col].bfill() elif method == 'interpolate': df[na_col] = df[na_col].interpolate(method='polynomial', order=2) elif method == 'moving_average': df[na_col] = df[na_col].fillna(df.rolling(window=24*7, min_periods=1).mean()) else: raise ValueError(f'Invalid method: {method}') return df # Feature engineering: Create a new column 'wind_speed_cat' that categorizes the wind speed as 'above_<wind_speed>' if the wind speed is greater than a given threshold and 'below_<wind_speed>' otherwise. def wind_speed_category(self, df: pd.DataFrame, wind_speed: int) -> pd.DataFrame: # Create the wind_speed_cat column df['wind_speed_cat'] = np.where(df['Iws'] > wind_speed, f'above_{wind_speed}', f'below_{wind_speed}') return df def save_csv(self, df: pd.DataFrame, filename: str): # Define the directory where you want to save the model save_dir = 'data' os.makedirs(save_dir, exist_ok=True) # Create the directory if it doesn't exist # Save the DataFrame to the specified directory file_path = os.path.join(save_dir, filename) df.to_csv(file_path, index=False) return file_path def run_data_pipeline(self, csv_path: str): df = self.load_data(csv_path) df = self.combine_datetime(df) df = self.handle_null_values(df, 'pm2.5', 'bfill') df = self.wind_speed_category(df, 120) return df def create_lagged_features(self, df, columns, lags): for col in columns: for lag in range(1, lags + 1): df[f'{col}_lag_{lag}'] = df[col].shift(lag) df.dropna(inplace=True) return df def create_target_variable(self, df, lags): df[f'target_{lags}'] = df['pm2.5'].shift(-lags) return df def split_time_series_data(self, df, target_column, split_date): X = df.drop(columns=[target_column]) y = df[target_column] X_train, X_test = X[:split_date], X[split_date:] y_train, y_test = y[:split_date], y[split_date:] return X, X_train, y_train, X_test, y_test def create_preprocessor(self, X): # Seperate out the columns with numerical, ordinal and categorical features num_features = list(X.select_dtypes(exclude=\"object\").columns) cat_features = list(X.select_dtypes(include=\"object\").columns) # To use OneHotEncoder() as the transformer for Categorical features and StandardScaler() as the transformer for Numerical features num_transformer = StandardScaler() cat_transformer = OneHotEncoder(sparse_output=False) preprocessor = ColumnTransformer( [(\"cat\", cat_transformer, cat_features), (\"num\", num_transformer, num_features) ] ) return preprocessor", "source": "datapipeline.py"}, {"content": "import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stats # Check for missing values def nan_values(df): nan_count = df.isna().sum() value_count = df.isna().count() nan_percentage = round(nan_count / value_count * 100, 2) nan_count_df = pd.DataFrame({\"count\": nan_count, \"percentage\": nan_percentage}) return nan_count_df # Check duplicates and remove them (if any) def duplicate_values(df): print(\"Duplicate check...\") # subset=None: Indicates that all columns should be considered when checking for duplicates. # keep='first': Specifies that the first occurrence of a duplicate row should be marked as False, while subsequent occurrences will be marked as True num_duplicates = df.duplicated(subset=None, keep='first').sum() if num_duplicates > 0: print(\"There are\", num_duplicates, \"duplicated observations in the dataset.\") df.drop_duplicates(keep='first', inplace=True) print(num_duplicates, \"duplicates were dropped.\") print(\"There are no more duplicate rows!\") else: print(\"There are no duplicated observations in the dataset.\")", "source": "eda.py"}, {"content": "import pandas as pd import matplotlib.pyplot as plt from sklearn.model_selection import TimeSeriesSplit import numpy as np from src.datapipeline import DataPipeline from src.ml_model import ForecastModel def run_experiment(data_path, features_to_lag, lags): # Read data pipeline = DataPipeline() # Load the data from the CSV file df = pipeline.load_data(data_path) metrics_list = [] for lag in lags: df_lagged = pipeline.create_lagged_features(df, features_to_lag, lag) # Split the data split_date = '2013-12-31 23:59:59' target_column = 'pm2.5' X, X_train, y_train, X_test, y_test_ = pipeline.split_time_series_data(df_lagged, target_column, split_date) preprocessor = pipeline.create_preprocessor(X) # Initialize TimeSeriesSplit tscv = TimeSeriesSplit(n_splits=5) # Initialize lists to store evaluation results train_rmse = [] val_rmse = [] # Perform time series cross-validation for train_index, val_index in tscv.split(X_train): X_train_split, X_val = X_train.iloc[train_index], X_train.iloc[val_index] y_train_split, y_val = y_train.iloc[train_index], y_train.iloc[val_index] X_train_scaled = preprocessor.fit_transform(X_train_split) X_val_scaled = preprocessor.transform(X_val) model = ForecastModel() model.fit(X_train_scaled, y_train_split) # Evaluate on training and validation sets train_error, val_error = model.evaluate(X_train_scaled, y_train_split, X_val_scaled, y_val) train_rmse.append(train_error) val_rmse.append(val_error) # Calculate and print the average RMSE average_train_rmse = np.mean(train_rmse) average_val_rmse = np.mean(val_rmse) metrics_list.append({ 'lag': lag, 'train_rmse': average_train_rmse, 'val_rmse': average_val_rmse }) # Create a DataFrame from the metrics metrics_df = pd.DataFrame(metrics_list) return metrics_df", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import root_mean_squared_error, r2_score class ForecastModel: def __init__(self): self.model = RandomForestRegressor() self.models_features = {} def fit(self, X, y): self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): y_train_pred = self.predict(X_train) train_error = root_mean_squared_error(y_train, y_train_pred) y_test_pred = self.predict(X_test) test_error = root_mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X) def get_feature_importance(self, feature_names): return dict(zip(feature_names, self.model.feature_importances_))", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import scipy.stats as stats import pickle from collections import Counter from typing import Optional, List import nltk from nltk.stem import PorterStemmer import re from nltk.stem import WordNetLemmatizer # Download necessary resources for nltk (only needs to be done once) nltk.download(\"punkt\") # For tokenizing sentences nltk.download(\"wordnet\") # WordNet dictionary for lemmatization nltk.download(\"omw-1.4\") # Additional wordnet data # Define a function to perform lemmatization lemmatizer = WordNetLemmatizer() # Initialize the PorterStemmer stemmer = PorterStemmer() # Load data from a csv file. def load_data(file_path: str) -> pd.DataFrame: df = pd.read_csv(file_path) return df def save_csv(df: pd.DataFrame, filename: str): # Define the directory where you want to save the model save_dir = 'data/processed' os.makedirs(save_dir, exist_ok=True) # Create the directory if it doesn't exist # Save the DataFrame to the specified directory save_path = os.path.join(save_dir, filename) df.to_csv(save_path, index=False) return save_path # Check for missing values def nan_values(df: pd.DataFrame) -> pd.DataFrame: nan_count = df.isna().sum() value_count = df.isna().count() nan_percentage = round(nan_count / value_count * 100, 2) nan_count_df = pd.DataFrame({\"count\": nan_count, \"percentage\": nan_percentage}) return nan_count_df # Check duplicates def check_duplicate_values(df: pd.DataFrame): print(\"Duplicate check...\") # subset=None: Indicates that all columns should be considered when checking for duplicates. # keep='first': Specifies that the first occurrence of a duplicate row should be marked as False, while subsequent occurrences will be marked as True num_duplicates = df.duplicated(subset=None, keep='first').sum() if num_duplicates > 0: print(\"There are\", num_duplicates, \"duplicated observations in the dataset.\") else: print(\"There are no duplicated observations in the dataset.\") # Print duplicates def print_duplicate_values(df: pd.DataFrame, columns: Optional[List[str]] = None) -> pd.DataFrame: if columns: for column_name in columns: if column_name not in df.columns: print(f\"Column '{column_name}' does not exist in the DataFrame.\") return pd.DataFrame() # Check for duplicates in the specified columns summary_data = { 'Column': [], 'Value': [], 'Row Indices': [] } for column_name in columns: column_values = df[column_name] element_counts = Counter(column_values) duplicates = {item: count for item, count in element_counts.items() if count > 1} for item in duplicates: row_indices = df.index[df[column_name] == item].tolist() summary_data['Column'].append(column_name) summary_data['Value'].append(item) summary_data['Row Indices'].append(row_indices) if summary_data['Column']: summary_df = pd.DataFrame(summary_data) return summary_df else: print(f\"No duplicate values found in the specified columns.\") return pd.DataFrame() else: # Check for duplicates in all columns summary_data = { 'Column': [], 'Value': [], 'Row Indices': [] } for col in df.columns: column_values = df[col] element_counts = Counter(column_values) duplicates = {item: count for item, count in element_counts.items() if count > 1} for item in duplicates: row_indices = df.index[df[col] == item].tolist() summary_data['Column'].append(col) summary_data['Value'].append(item) summary_data['Row Indices'].append(row_indices) if summary_data['Column']: summary_df = pd.DataFrame(summary_data) return summary_df else: print(\"No duplicate values found in the DataFrame.\") return pd.DataFrame() # Drop duplicates def drop_duplicates(df: pd.DataFrame): num_duplicates = df.duplicated(subset=None, keep='first').sum() df.drop_duplicates(keep='first', inplace=True) print(num_duplicates, \"duplicates were dropped.\") return df def check_for_artifacts(df: pd.DataFrame, column_name: str) -> pd.DataFrame: if column_name not in df.columns: print(f\"Column '{column_name}' does not exist in the DataFrame.\") return pd.DataFrame() # Initialize a list to store the summary of artifacts summary_data = { 'Artifact Type': [], 'Artifact': [], 'Count': [], 'Row Indices': [] } # Define the artifact patterns artifact_patterns = { 'html_tags': r'<[^>]+>', 'backslashes': r'\\\\', 'double_hyphens': r'--' } #", "source": "eda.py"}, {"content": "Check for each type of artifact for artifact_type, artifact in artifact_patterns.items(): row_indices = df[df[column_name].astype(str).str.contains(artifact, regex=True)].index.tolist() if row_indices: summary_data['Artifact Type'].append(artifact_type) summary_data['Artifact'].append(artifact) summary_data['Count'].append(len(row_indices)) summary_data['Row Indices'].append(row_indices) # Convert the summary data to a DataFrame summary_df = pd.DataFrame(summary_data) return summary_df def text_cleaning(df: pd.DataFrame, column_name: str) -> pd.DataFrame: if column_name not in df.columns: print(f\"Column '{column_name}' does not exist in the DataFrame.\") return df # Function to remove HTML tags, non-alphanumeric characters, multiple dashes and lower case it using regex def clean_text(text): # Remove HTML tags text = re.sub(r'<.*?>', \"\", text) # Remove non-alphanumeric characters except spaces (e.g., punctuation, special symbols) text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Replace multiple dashes with a single space text = re.sub(r\"-+\", \" \", text) # Replace multiple spaces with a single space text = re.sub(r\"\\s+\", \" \", text) # Convert to lowercase text = text.lower() return text # Apply the function to the specified column df[column_name+'_clean'] = df[column_name].astype(str).apply(clean_text) return df # Function to remove HTML tags, non-alphanumeric characters, multiple dashes and lower case it using regex def clean_text(text): # Remove HTML tags text = re.sub(r'<.*?>', \"\", text) # Remove non-alphanumeric characters except spaces (e.g., punctuation, special symbols) text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text) # Replace multiple dashes with a single space text = re.sub(r\"-+\", \" \", text) # Replace multiple spaces with a single space text = re.sub(r\"\\s+\", \" \", text) # Convert to lowercase text = text.lower() return text def lemmatize_text(text): # Tokenize the text by splitting on spaces words = text.split() # Apply the lemmatizer to each word lemmatized_words = [lemmatizer.lemmatize(word) for word in words] # Join the lemmatized words back into a single string return \" \".join(lemmatized_words) def stem_text(text): # Tokenize the text by splitting on spaces words = text.split() # Apply the Porter Stemmer to each word stemmed_words = [stemmer.stem(word) for word in words] # Join the stemmed words back into a single string return \" \".join(stemmed_words) def save_artifact(artifact, filename): # Define the folder path folder_path = 'data/artifacts' # Create the folder if it doesn't exist os.makedirs(folder_path, exist_ok=True) # Full path to save the file full_path = os.path.join(folder_path, filename) # Save the model as a pickle file with open(full_path, 'wb') as file: pickle.dump(artifact, file) print(f\"Artifact saved as {full_path}\") def load_artifact(filename, file_path): # Full path to load the file full_path = os.path.join(file_path, filename) with open(full_path, 'rb') as file: artifact = pickle.load(file) print(f\"Artifact loaded from {full_path}\") return artifact", "source": "eda.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import OrdinalEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import StandardScaler # Categorical Dtype from pandas.api.types import CategoricalDtype import warnings warnings.filterwarnings(\"ignore\") ###### Helper Functions ######' ### Assignment ### Nominal Features def education_grouping(df): \"\"\" Groups the education column into broader categories. :param df: The input DataFrame that contains the 'education' column. :return: DataFrame with the 'education' column grouped into predefined categories. \"\"\" grade_grouping = { 'No Formal Education': [ 'Children', 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade' ], 'Some High School Education': [ '9th grade', '10th grade', '11th grade', '12th grade no diploma' ], 'High School Graduate': [ 'High school graduate' ], 'Some College Education': [ 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program' ], 'Undergraduate Degree': [ 'Bachelors degree(BA AB BS)' ], 'Graduate Education': [ 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)' ] } def map_category(value, grouping): for key, values in grouping.items(): if value in values: return key return value # In case the value doesn't match any key df['education'] = df['education'].apply(lambda x: map_category(x, grade_grouping)) return df def marital_status_grouping(df): \"\"\" Groups various marital statuses into broader categories. :param df: The input DataFrame that contains the 'marital_stat' column. :return: DataFrame with 'marital_stat' column grouped. \"\"\" df['marital_stat'] = df['marital_stat'].replace('Married-civilian spouse present', 'Married') df['marital_stat'] = df['marital_stat'].replace('Married-A F spouse present', 'Married') return df def reason_for_unemployment_grouping(df): \"\"\" Groups the reasons for unemployment into broader categories. :param df: The input DataFrame that contains the 'reason_for_unemployment' column. :return: DataFrame with 'reason_for_unemployment' column grouped. \"\"\" df['reason_for_unemployment'] = df['reason_for_unemployment'].replace('Other job loser', 'Job Loser') df['reason_for_unemployment'] = df['reason_for_unemployment'].replace('Job loser - on layoff', 'Job Loser') return df def household_family_grouping(df): \"\"\" Groups the household and family relationship status into broader categories. :param df: The input DataFrame that contains 'detailed_household_summary_in_household' column. :return: DataFrame with grouped family relationship statuses. \"\"\" household_family_grouping_map = { 'Householder': 'Householder', 'Spouse of householder': 'Spouse/Partner', 'Child under 18 never married': 'Children', 'Child 18 or older': 'Children', 'Child under 18 ever married': 'Children', 'Other relative of householder': 'Other Relatives', 'Nonrelative of householder': 'Nonrelatives', 'Group Quarters- Secondary individual': 'Nonrelatives' } df['detailed_household_summary_in_household'] = df['detailed_household_summary_in_household'].map(household_family_grouping_map) return df def reclassify_reason_for_unemployment(df): \"\"\" Groups and reclassifies the reasons for unemployment. :param df: The input DataFrame containing the 'reason_for_unemployment' column. :return: DataFrame with the reclassified 'reason_for_unemployment' column. \"\"\" df['reason_for_unemployment'] = df['reason_for_unemployment'].replace('Other job loser', 'Job Loser') df['reason_for_unemployment'] = df['reason_for_unemployment'].replace('Job loser - on layoff', 'Job Loser') return df ### Ordinal Features def update_columns_after_drop(to_drop, *column_lists): \"\"\" Removes columns from the provided lists of columns. :param to_drop: List of columns to drop :param column_lists: Lists of columns (numerical, nominal, ordinal) to update \"\"\" for col in to_drop: for col_list in column_lists: if col in col_list: col_list.remove(col) def order_and_rank_ordinal_columns(df, ordinal_cols, predefined_orders=None): \"\"\" Function to order and rank ordinal columns in a DataFrame. :param df: The DataFrame that contains the columns to be ordered. :param ordinal_cols: List of ordinal columns to process. :param predefined_orders: Dictionary where the key is the column", "source": "datapipeline.py"}, {"content": "name and the value is the predefined order (list). If not provided, columns will be ordered based on their natural order (e.g., alphabetical or numerical). :return: Updated DataFrame with the ordinal columns converted to ordered categorical and ranked. \"\"\" for col in ordinal_cols: # If there's a predefined order for the column, use it; otherwise, infer the natural order if predefined_orders and col in predefined_orders: order = predefined_orders[col] else: order = sorted(df[col].dropna().unique()) # Sort the unique values naturally (lexicographically or numerically) # Convert to a categorical type with an order cat_type = CategoricalDtype(categories=order, ordered=True) df[col] = df[col].astype(cat_type) return df ### Numerical Features def equity(df,numerical_cols): \"\"\" Calculates equity and removes related columns. :param df: The DataFrame that contains 'capital_gains', 'capital_losses', and 'dividends_from_stocks' columns. :param numerical_cols: List of numerical columns in the DataFrame. :return: Updated DataFrame with an 'equity' column and related columns dropped. \"\"\" # capital_gains - capital_losses + dividends_from_stocks df['equity'] = df['capital_gains'] - df['capital_losses'] + df['dividends_from_stocks'] df = df.drop(columns=['capital_gains', 'capital_losses', 'dividends_from_stocks']) df['equity'] = df['equity'].astype(float) # update numerical columns update_columns_after_drop(['capital_gains', 'capital_losses', 'dividends_from_stocks'], numerical_cols) numerical_cols.append('equity') return df ############ Main Function ############ def transform(data_path): \"\"\" Transforms the input dataset by processing categorical, ordinal, and numerical features, applying feature engineering. :param data_path: The path to the input CSV data file. :return: Transformed train and test datasets: X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed. \"\"\" # Load the data print(f\"==> Loading the data from {data_path}\") try: df = pd.read_csv(data_path) except Exception as e: print(f\"Error loading the data: {e}\") return None # Catergorize the columns numerical_cols = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'num_persons_worked_for_employer', 'weeks_worked_in_year'] nominal_cols = [ 'id', 'detailed_industry_recode', 'detailed_occupation_recode', 'year', 'detailed_household_and_family_stat', 'marital_stat', 'race', 'sex', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'detailed_household_summary_in_household', 'family_members_under_18', 'own_business_or_self_employed', 'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt','age', 'enroll_in_edu_inst_last_wk'] ordinal_cols = ['education','veterans_benefits', 'income_group'] assert set(nominal_cols + ordinal_cols + numerical_cols) == set(df.columns) # Drop Columns print(\"==> Dropping columns\") to_drop = ['id', # Not useful 'detailed_industry_recode', # same as major_industry_code 'detailed_occupation_recode', # same as major_occupation_code 'year', # census year, not useful 'detailed_household_and_family_stat', # replaced by detailed_household_summary_in_household 'migration_code_move_within_reg', # most Not in universe, replaced by live_in_this_house_1_year_ago 'migration_code_change_in_reg', # most Not in universe, replaced by live_in_this_house_1_year_ago 'migration_prev_res_in_sunbelt', # most Not in universe, replaced by live_in_this_house_1_year_ago 'state_of_previous_residence'] # too much Not in universe, replaced by region_of_previous_residence df = df.drop(columns=to_drop) update_columns_after_drop(to_drop, nominal_cols, ordinal_cols, numerical_cols) # Drop duplicates print(\"==> Dropping duplicates\") df.drop_duplicates(inplace=True) # Handle Missing Values & Not Applicable print(\"==> Handling missing values\") df = df.replace('?', np.nan) df = df.replace(np.nan, 'No Response') df = df.replace('Not in universe', 'Not Applicable') df['country_of_birth_father'] = df['country_of_birth_father'].fillna('No Response') df['country_of_birth_mother'] = df['country_of_birth_mother'].fillna('No Response') df['country_of_birth_self'] = df['country_of_birth_self'].fillna('No Response') df['hispanic_origin'] = df['hispanic_origin'].fillna('No Response') df['hispanic_origin'] = df['hispanic_origin'].replace('Do not know', 'No Response') # Assign to float or objects df[numerical_cols] = df[numerical_cols].astype(float) # as category type df[nominal_cols] = df[nominal_cols].astype('category') # For now df[ordinal_cols] = df[ordinal_cols].astype(str) # Feature Engineering print(\"==> Feature Engineering\") # Nominal Features df = education_grouping(df) df = marital_status_grouping(df) df = reason_for_unemployment_grouping(df) df = household_family_grouping(df) # Ordinal Features predefined_orders = { 'education': ['No Formal Education', 'Some High School Education', 'High School Graduate', 'Some College Education', 'Undergraduate Degree', 'Graduate Education'], 'income_group': ['- 50000.', '50000+.'], 'veterans_benefits': ['0', '1', '2'] } df", "source": "datapipeline.py"}, {"content": "= order_and_rank_ordinal_columns(df, ordinal_cols, predefined_orders) # Numerical Features df = equity(df, numerical_cols) # Define X as input features and y as the outcome variable X = df.drop(columns=['income_group']) update_columns_after_drop(['income_group'], ordinal_cols) y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Define the preprocessing for the numerical features numeric_transformer = Pipeline(steps=[ ('scaler', StandardScaler()) ]) # Define the preprocessing for the categorical features categorical_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(drop = 'first', handle_unknown='ignore')) ]) # Define the preprocessing for the ordinal features ordinal_transformer = Pipeline(steps=[ ('ordinal', OrdinalEncoder()) ]) # Define the column transformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numerical_cols), ('cat', categorical_transformer, nominal_cols), ('ord', ordinal_transformer, ordinal_cols) ]) # Fit the preprocessor preprocessor.fit(X_train) # Transform the training and test data X_train_transformed = preprocessor.transform(X_train).todense() X_test_transformed = preprocessor.transform(X_test).todense() # Get the column names numerical_feature_names = numerical_cols categorical_feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(nominal_cols) ordinal_feature_names = ordinal_cols column_names = list(numerical_feature_names) + list(categorical_feature_names) + list(ordinal_feature_names) # Convert the transformed data back to a DataFrame X_train_transformed = pd.DataFrame(X_train_transformed, columns=column_names) X_test_transformed = pd.DataFrame(X_test_transformed, columns=column_names) y_train_transformed = y_train.replace(['- 50000.', '50000+.'], [0, 1]) y_test_transformed = y_test.replace(['- 50000.', '50000+.'], [0, 1]) return X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd class Node: \"\"\" Represents a single node in the decision tree. Nodes vs Leaves: - Nodes have children (left and right nodes) - Leaves have no children and contain the output value \"\"\" def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None): self.feature_index = feature_index # Index of the feature used for splitting self.threshold = threshold # Threshold value for splitting self.left = left # Left child node self.right = right # Right child node self.value = value # Value for leaf nodes (most common class) class DecisionTree(): def __init__(self, max_depth= float('inf') , min_sample_split=2): self.max_depth = max_depth # maximum depth of the tree self.min_samples_split = min_sample_split # minimum number of samples required to split an internal node self.criterion = \"gini\" self.tree = None def get_params(self): params = { \"max_depth\": self.max_depth, \"criterion\": self.criterion, \"min_samples_split\": self.min_samples_split } return params def fit(self, X, y, feature_indices=None): \"\"\" Takes in 2 NumPy (m*n) arrays with m training examples of 'n'features and a m*1 array of labels. :param X: NumPy array of shape (m, n) with features :param y: NumPy array of shape (m, ) with labels :return: None \"\"\" self.tree = self._build_tree(X, y, 0, feature_indices) def _build_tree(self, X, y, depth=0, feature_indices=None): \"\"\" Recursively builds the decision tree. Algorithm: 1. Check termination criteria 2. Find the best split 3. Split the data 4. Recursively build the tree on the left and right nodes :param X: NumPy array of shape (m, n) with features :param y: NumPy array of shape (m, ) with labels :param depth: Current depth of the :param feature_indices: List of feature indices to consider for splitting :return: The root node of the decision \"\"\" num_samples, num_features = X.shape # Check termination criteria if num_samples >= self.min_samples_split and depth <= self.max_depth: best_split = self._find_best_split(X, y, feature_indices) if best_split: # If the best split is not None, happens if there are no splits left_node = self._build_tree(X[best_split[\"y_left\"]], best_split[\"y_left\"], depth+1, feature_indices) right_node = self._build_tree(X[best_split[\"y_right\"]], best_split[\"y_right\"], depth+1, feature_indices) return Node(best_split[\"feature_idx\"], best_split[\"threshold\"], left_node, right_node) # If termination criteria is met, return a leaf node leaf_value = self._most_common_label(y) return Node(value=leaf_value) def predict(self, X): \"\"\" Predict the class for each sample in X. :param X: NumPy array of shape (m, n) with features. :return: NumPy array of predicted labels of shape (m, ). \"\"\" return np.array([self._predict_single(sample, self.tree) for sample in X]) def _predict_single(self, sample, node): \"\"\" Traverse the tree to predict the label for a single sample. :param sample: A single data point (1D array). :param node: The current node in the decision tree. :return: Predicted label. \"\"\" # If we are at a leaf node if node.value is not None: return node.value # Traverse left or right based on the feature threshold if sample[node.feature_index] <= node.threshold: return self._predict_single(sample, node.left) else: return self._predict_single(sample, node.right) def _find_best_split(self, X, y, feature_indices = None): \"\"\" Find the best split for a node in the decision tree. Algorithm: 1. Calculate the Gini impurity for the parent node. 2. For each feature, calculate the Gini impurity for each split. 3. Select the feature with the smallest Gini impurity. 4. Return the best", "source": "decision_tree.py"}, {"content": "feature and the corresponding threshold. :param X: NumPy array of shape (m, n) with features :param y: NumPy array of shape (m, ) with labels :param feature_indices: List of feature indices to consider for splitting :return: Dictionary with the best split (feature index and threshold) and the information \"\"\" if feature_indices is None: # If feature indices are not provided, consider all features feature_indices = range(X.shape[1]) best_split = {} max_info_gain = 0 # Initialize the information gain for feature_idx in feature_indices: # For each feature feature_values = X[:, feature_idx] # Get the feature values unique_values = np.unique(feature_values) # Get the unique values, these are the potential thresholds for threshold in unique_values: left_indices = np.where(feature_values <= threshold)[0] # Get the indices of samples in the left node right_indices = np.where(feature_values > threshold)[0] # Get the indices of samples in the right node if len(left_indices) == 0 or len(right_indices) == 0: continue # Skip this iteration if there are no samples in the left or right node y_left, y_right = y[left_indices], y[right_indices] # Get the labels in the left and right node x_left, x_right = X[left_indices], X[right_indices] # Get the features in the left and right node # Calculate the information gain info_gain = self._information_gain(y, y_left, y_right) if info_gain > max_info_gain: best_split = { \"feature_idx\": feature_idx, \"threshold\": threshold, \"x_left\": x_left, \"x_right\": x_right, \"y_left\": y_left, \"y_right\": y_right } max_info_gain = info_gain return best_split if best_split else None def _information_gain(self, y, y_left, y_right): \"\"\" Calculate the information gain based on Gini impurity. :param y: The original labels before the split. :param y_left: Labels in the left node after the split. :param y_right: Labels in the right node after the split. :return: Information gain. \"\"\" # Gini impurity of Parent gini_parent = self._gini_single_node(y) # After split, calculate the weighted Gini impurity of children weighted_gini = self.gini(y_left, y_right) # Information gain information_gain = gini_parent - weighted_gini return information_gain # gini def gini(self, y_left, y_right): \"\"\" Calculate the weighted Gini impurity for a split with two sets of labels. :param y_left: List or array of class labels in the left node. :param y_right: List or array of class labels in the right node. :return: Weighted Gini impurity for the split. \"\"\" # Calculate the number of samples in both left and right nodes n_left = len(y_left) n_right = len(y_right) n_total = n_left + n_right # Handle edge case where there are no samples in one of the nodes if n_total == 0: return 0 # Gini impurity for the left node gini_left = self._gini_single_node(y_left) # Gini impurity for the right node gini_right = self._gini_single_node(y_right) # Weighted Gini impurity weighted_gini = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right return weighted_gini def _gini_single_node(self, y): \"\"\" Calculate the Gini impurity for a single node (list of labels). :param y: List or array of class labels in a node. :return: Gini impurity of the node. \"\"\" total_samples = len(y) if total_samples == 0: return 0 # No samples, no impurity # Count the occurrences of each class unique_classes, counts = np.unique(y, return_counts=True) # Calculate the probability for each class", "source": "decision_tree.py"}, {"content": "probabilities = counts / total_samples # Calculate the Gini impurity gini_impurity = 1 - np.sum(probabilities ** 2) return gini_impurity def _most_common_label(self, y): \"\"\" Find the most common label in a list of labels. :param y: List or array of class labels. :return: The most common label. \"\"\" unique_classes, counts = np.unique(y, return_counts=True) most_common = unique_classes[np.argmax(counts)] return most_common", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score class Model: MODEL_MAP = { 'random_forest': RandomForestClassifier, 'logistic_regression': LogisticRegression } METRIC_MAP = { 'f1': f1_score, 'f1_weighted': lambda y_true, y_pred: f1_score(y_true, y_pred, average='weighted') } def __init__(self, model_type = 'random_forest', metric = 'f1_weighted'): if model_type not in self.MODEL_MAP: raise ValueError(f\"Invalid model type. Model type must be one of {list(self.MODEL_MAP.keys())}\") self.model = None self.model_type = model_type self.metric = self.METRIC_MAP[metric] def train(self, params, X_train, y_train): \"\"\" Trains the initialized model using the provided training data and parameters. :param params: A dictionary of model parameters to be used during the training process. These parameters are model-specific and are passed to the model initialization. :param X_train: Training data features (input variables), typically a Pandas DataFrame or NumPy array. :param y_train: Training data labels (target variable), typically a NumPy array or Pandas Series. :return: The training F1 score as a float, which is used to evaluate the model's performance on the training set. \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.model = self.MODEL_MAP[self.model_type](**params) self.model.fit(X_train, y_train) y_pred = self.model.predict(X_train) score = self.metric(y_train, y_pred) return score def evaluate(self, X_test, y_test): \"\"\" Evaluates the trained model on the test data and returns the evaluation score. :param X_test: Test data features (input variables), typically a Pandas DataFrame or NumPy array. :param y_test: Test data labels (target variable), typically a NumPy array or Pandas Series. :return: The F1 score on the test data as a float, which is used to evaluate the model's performance. \"\"\" y_pred = self.model.predict(X_test) score = self.metric(y_test, y_pred) return score def get_default_params(self): \"\"\" Returns the default parameters for training the model from scratch. :return: A dictionary containing the default parameters for the selected model type. These parameters should match the params used in the train function. The parameters returned are pre-optimized for the selected model. \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model if self.model_type == 'random_forest': return { 'max_depth': 20, 'n_estimators': 100, 'random_state': 2024 } elif self.model_type == 'logistic_regression': return { 'penalty': 'l2', 'C': 1.0, 'random_state': 0 } else: raise ValueError(f\"Invalid model type. Model type must be one of {list(self.MODEL_MAP.keys())}\")", "source": "model.py"}, {"content": "import pandas as pd import numpy as np from .decision_tree import DecisionTree class RandomForest: \"\"\" RandomForest is an ensemble of Decision Trees. This class implements a RandomForest with 5 trees using bootstrapping. \"\"\" def __init__(self, n_trees=5, max_depth = float('inf'), min_sample_split = 2, subsample_size = 1.0, sample_with_replacement = True, feature_proportion = 1.0): \"\"\" Initialize the RandomForest model with parameters. :param n_trees: Number of trees to grow. :param max_depth: Maximum depth of the tree. :param min_sample_split: Minimum number of samples required to split a node. :param feature_proportion: Proportion of features to consider for each tree. :param subsample_size: Proportion of the dataset to use for each tree (between 0 and 1). :param sample_with_replacement: Boolean to indicate if sampling should be with replacement. \"\"\" # To ensure that subsample_size is not greater than 1.0 if sampling without replacement if subsample_size > 1.0 and not sample_with_replacement: print(f\"Warning: subsample_size is greater than 1.0, automatically setting sample_with_replacement=True\") sample_with_replacement = True self.n_trees = n_trees # Number of trees in the forest self.max_depth = max_depth # Maximum depth of the tree self.min_sample_split = min_sample_split # Minimum number of samples required to split an internal self.feature_proportion = feature_proportion # Proportion of features to consider for each tree self.subsample_size = subsample_size # Fraction of the samples to use for fitting the decision tree self.sample_with_replacement = sample_with_replacement self.trees = [] # List to store the trees def _bootstrap_samples(self, X, y): \"\"\" Create a random subsample of the dataset with replacement. :param X: Feature array (m, n) :param y: Label array (m,) :return: Subsample of X and y of size (m, n) and (m,) \"\"\" n_samples = X.shape[0] subsample_size = int(self.subsample_size * n_samples) idxs = np.random.choice(n_samples, size=subsample_size, replace=self.sample_with_replacement) return X[idxs], y[idxs] def _get_subsample_features(self,X): \"\"\" Get a random subset of features to consider at each split. :param X: Feature array (m, n) :return: Subsampled feature indices \"\"\" n_features = X.shape[1] splitting_features = int(self.feature_proportion * n_features) if splitting_features == 0: raise ValueError(\"Number of features to consider is 0. Please increase the feature_proportion value.\") if splitting_features == n_features: # return all feature indices return np.arange(n_features) feature_idxs = np.random.choice(n_features, splitting_features, replace=False) # Randomly select max_features return feature_idxs def fit(self, X, y): \"\"\" Train the Random Forest on the input data. :param X: Feature array (m, n) :param y: Label array (m,) :return: None \"\"\" self.trees = [] for _ in range(self.n_trees): # For each tree X_sample, y_sample = self._bootstrap_samples(X, y) tree = DecisionTree(max_depth=self.max_depth, min_sample_split=self.min_sample_split) feature_indices = self._get_subsample_features(X_sample) tree.fit(X_sample, y_sample, feature_indices) self.trees.append(tree) def predict(self, X): \"\"\" Predict the class label for input data. :param X: Feature array (m, n) :return: Predicted labels \"\"\" # Make predictions from all trees tree_preds = np.array([tree.predict(X) for tree in self.trees]) # (n_trees, m) tree_preds = np.transpose(tree_preds) # (m, n_trees) # For each sample get the most frequent class label majority_votes = np.apply_along_axis(lambda x: np.argmax(np.bincount(x)), axis=1, arr=tree_preds) return majority_votes", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "# Configuration file for the Sphinx documentation builder. # # This file only contains a selection of the most common options. For a full # list see the documentation: # https://www.sphinx-doc.org/en/master/usage/configuration.html # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys sys.path.insert(0, os.path.abspath(\"../src\")) # -- Project information ----------------------------------------------------- project = \"AIAP DSP MLOps\" copyright = \"2024, hcy\" author = \"hcy\" # The full version, including alpha/beta/rc tags release = \"v1.0.0\" # -- General configuration --------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ \"sphinx.ext.napoleon\", \"sphinx.ext.autodoc\" ] # Add any paths that contain templates here, relative to this directory. templates_path = [\"_templates\"] # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path. exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"] # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = \"alabaster\" # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named \"default.css\" will overwrite the builtin \"default.css\". html_static_path = [\"static\"]", "source": "conf.py"}, {"content": "# models/base_model.py from abc import ABC, abstractmethod class BaseModel(ABC): @abstractmethod def build_model(self): pass @abstractmethod def compile_model(self): pass @abstractmethod def train(self, x_train, y_train, epochs, batch_size, validation_data, callbacks): pass @abstractmethod def evaluate(self, x_test, y_test): pass @abstractmethod def save_model(self, filepath): pass @abstractmethod def save_history(self, filepath): pass", "source": "base_model.py"}, {"content": "\"\"\"Script to conduct batch inferencing. \"\"\" import os import datetime import logging import glob import hydra import jsonlines import aiap_dsp_mlops as amlo # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"batch_infer.yaml\") def main(args): \"\"\"This main function does the following: - load logging config - gets list of files to be loaded for inferencing - loads trained model - conducts inferencing on data - outputs prediction results to a jsonline file - returns an error if there are no files/data specified to be inferred \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") logger_config_path = os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) amlo.general_utils.setup_logging(logger_config_path) logger.info(\"Loading the model...\") loaded_model = amlo.modeling.utils.load_model() glob_expr = os.path.join( args[\"input_data_dir\"], args[\"file_check_glob\"] ) logger.info(\"Conducting inferencing on text files...\") jsonl_path = os.path.join( os.getcwd(), args[\"output_path\"] ) has_text = False # Checking if for loop is running for text_file in glob.glob(glob_expr): has_text = True text = open(text_file).read() pred_str = loaded_model.predict(text) curr_time = datetime.datetime.now(datetime.timezone.utc).strftime( \"%Y-%m-%dT%H:%M:%S%z\" ) curr_res_jsonl = { \"time\": curr_time, \"text_filepath\": text_file, \"prediction\": pred_str, } with jsonlines.open(jsonl_path, mode=\"a\") as writer: writer.write(curr_res_jsonl) writer.close() if not has_text: e = \"Folder {} has no files to infer according to the given glob '{}'.\".format( os.path.abspath(args[\"input_data_dir\"]), args[\"file_check_glob\"] ) logger.error(e) raise FileNotFoundError(e) logger.info(\"Batch inferencing has completed.\") logger.info(\"Output result location: %s\", jsonl_path) if __name__ == \"__main__\": main()", "source": "batch_infer.py"}, {"content": "from sqlalchemy import create_engine import pandas as pd class DataLoader: def __init__(self, server, database, username, password, driver): \"\"\" Initialize the DataLoader with connection details. :param server: Server address. :param database: Database name. :param username: Username for the database. :param password: Password for the database. :param driver: ODBC driver name. \"\"\" self.connection_url = f\"mssql+pyodbc://{username}:{password}@{server}:1433/{database}?driver={driver.replace(' ', '+')}\" self.engine = None def connect(self): \"\"\" Establish a connection to the database. \"\"\" try: self.engine = create_engine(self.connection_url) self.engine.connect() print(\"Connection established successfully.\") except Exception as e: print(f\"An error occurred: {e}\") def load_data(self, query): \"\"\" Fetch data from the database using a SQL query. :param query: SQL query to fetch the data. :return: DataFrame with the fetched data. \"\"\" if self.engine is None: raise RuntimeError(\"Connection has not been established. Call connect() first.\") try: with self.engine.connect() as conn: df = pd.read_sql(query, conn) print(\"Data fetched successfully.\") return df except Exception as e: print(f\"An error occurred: {e}\") return None", "source": "dataloader.py"}, {"content": "import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline class Datapipeline: def __init__(self, target_column): \"\"\" Initialize the Datapipeline with the target column. :param target_column: Name of the target variable in the dataset. \"\"\" self.target_column = target_column self.pipeline = None self.feature_names = None def data_preprocessing_pipeline(self,data): if not isinstance(data, pd.DataFrame): raise ValueError(\"The input data should be a pandas DataFrame.\") # Split features and target X = data.drop(columns=[self.target_column]) y = data[self.target_column] # Identify numerical columns (since there are no categorical columns) numerical_features = X.select_dtypes(include=['int64', 'float64']).columns # Numerical transformer: impute missing values and scale numerical_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='mean')), # Fill missing values with the mean ('scaler', StandardScaler()) # Standardize features ]) # Combine numerical transformers preprocessor = ColumnTransformer( transformers=[ ('num', numerical_transformer, numerical_features) ] ) # Preprocessing pipeline: combine transformations pipeline = Pipeline(steps=[ ('preprocessor', preprocessor) ]) # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42 ) # Fit the pipeline on the training data pipeline.fit(X_train) # Since there are no categorical features, feature names are just numerical features feature_names = list(numerical_features) # Transform the data X_train_processed = pipeline.transform(X_train) X_test_processed = pipeline.transform(X_test) # Convert processed arrays back into DataFrames with feature names X_train_processed = pd.DataFrame(X_train_processed, columns=feature_names) X_test_processed = pd.DataFrame(X_test_processed, columns=feature_names) return X_train_processed, X_test_processed, y_train, y_test, feature_names", "source": "datapipeline.py"}, {"content": "\"\"\"Script for testing of logging to MLflow server. \"\"\" import sys import time import random import mlflow def main(): \"\"\"Main function for testing of logging to MLflow server.\"\"\" mlflow.set_tracking_uri(sys.argv[1]) mlflow.set_experiment(sys.argv[2]) dummy_param_1 = 64 dummy_param_2 = 1.0 tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(sys.argv[2]) print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) with mlflow.start_run(): print(\"Logging parameters...\") mlflow.log_param(\"dummy_param_1\", dummy_param_1) mlflow.log_param(\"dummy_param_2\", dummy_param_2) print(\"Logging metrics...\") for step in range(1, 5): mlflow.log_metric(\"dummy_metric_1\", time.time(), step=step) mlflow.log_metric(\"dummy_metric_2\", random.uniform(0.1, 0.5), step=step) time.sleep(2) dummy_text_content = \"This text content should be uploaded to the bucket.\" with open(\"text_artifact.txt\", \"w\", encoding=\"utf-8\") as file: file.write(dummy_text_content) mlflow.log_artifact(\"text_artifact.txt\") artifact_uri = mlflow.get_artifact_uri() print(\"Current artifact URI: \", artifact_uri) if __name__ == \"__main__\": main()", "source": "mlflow_test.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "\"\"\" This script is a template for processing data. \"\"\" \"\"\" Docker Build: docker build \\ -t asia-southeast1-docker.pkg.dev/aiap-17-ds/aiap-17-ds/hoo-chi-yang/cpu:0.1.0 \\ -f docker/aiap-dsp-mlops-cpu.Dockerfile \\ --platform linux/amd64 . Run at Assignment4 working directory: khull kaniko --context $(pwd) \\ --dockerfile $(pwd)/docker/aiap-dsp-mlops-cpu.Dockerfile \\ --destination asia-southeast1-docker.pkg.dev/aiap-17-ds/aiap-17-ds/hoo-chi-yang/cpu:0.1.0 \\ --gcp \\ -v pvc-data:/pvc-data Run Ai: runai submit \\ --job-name-prefix hoo-chi-yang-data-prep\\ -i asia-southeast1-docker.pkg.dev/aiap-17-ds/aiap-17-ds/hoo-chi-yang/cpu:0.1.0 \\ --existing-pvc claimname=pvc-data,path=/pvc-data \\ --cpu 2 --cpu-limit 2 --memory 4G --memory-limit 4G --backoff-limit 1 \"\"\" import os import shutil import logging import hydra import aiap_dsp_mlops as amlo from dataloader import DataLoader from datapipeline import Datapipeline # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"process_data.yaml\") def main(args) -> None: \"\"\"This function is a template to process data. Parameters ---------- args: omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") amlo.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"assignment4\", \"conf\", \"logging.yaml\" ) ) # Initialize Dataloader with connection details data_loader = DataLoader( server = args['server'], database = args['database'], username = args['username'], password = args['password'], driver = args['driver'] ) # Connect to database data_loader.connect() # Load from database data = data_loader.load_data(\"SELECT * FROM creditcard\") logger.info(\"Data loaded successfully.\") print(data.head()) # Optional: view the first few rows processed_data_dir_path = args[\"processed_data_dir_path\"] os.makedirs(args[\"processed_data_dir_path\"], exist_ok=True) # Initialize Datapipeline with the target column pipeline = Datapipeline(target_column='Class') X_train, X_test, y_train, y_test, feature_names = pipeline.data_preprocessing_pipeline(data) logger.info(\"Data preprocessing completed.\") logger.info(f\"Training set size: {X_train.shape}, Test set size: {X_test.shape}\") logger.info(f\"Features: {feature_names}\") # If needed, save processed data (optional step) processed_data_dir_path = args[\"processed_data_dir_path\"] os.makedirs(processed_data_dir_path, exist_ok=True) X_train.to_csv(os.path.join(processed_data_dir_path, 'X_train.csv'), index=False) X_test.to_csv(os.path.join(processed_data_dir_path, 'X_test.csv'), index=False) y_train.to_csv(os.path.join(processed_data_dir_path, 'y_train.csv'), index=False) y_test.to_csv(os.path.join(processed_data_dir_path, 'y_test.csv'), index=False) logger.info(\"Processed data saved successfully.\") if __name__ == \"__main__\": main()", "source": "process_data.py"}, {"content": "# src/pytorch_model.py import torch import torch.nn as nn import torch.optim as optim import json from base_model import BaseModel from sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score class PyTorchModel(BaseModel): def __init__( self, input_dim=29, output_units=1, output_activation='sigmoid', # Handle in the forward method loss_function='BCELoss', optimizer='Adam', optimizer_params={'lr': 0.001}, metrics=['Recall'], hidden_layers=[64, 128, 64, 32] ): self.input_dim = input_dim self.output_units = output_units self.output_activation = output_activation self.loss_function = loss_function self.optimizer_name = optimizer self.optimizer_params = optimizer_params self.metrics = metrics self.hidden_layers = hidden_layers self.model = None self.criterion = None self.optimizer = None self.history = {'loss': [], 'val_loss': []} # Extend as needed def build_model(self): layers = [] in_features = self.input_dim for units in self.hidden_layers: layers.append(nn.Linear(in_features, units)) layers.append(nn.ReLU()) in_features = units layers.append(nn.Linear(in_features, self.output_units)) if self.output_activation == 'sigmoid': layers.append(nn.Sigmoid()) elif self.output_activation == 'softmax': layers.append(nn.Softmax(dim=1)) # Add other activations as needed self.model = nn.Sequential(*layers) print(\"PyTorch model built successfully.\") def compile_model(self): if self.model is None: raise ValueError(\"Model not built. Call 'build_model()' first.\") # Initialize loss function if self.loss_function == 'BCELoss': self.criterion = nn.BCELoss() elif self.loss_function == 'CrossEntropyLoss': self.criterion = nn.CrossEntropyLoss() else: raise ValueError(f\"Unsupported loss function: {self.loss_function}\") # Initialize optimizer optimizer_class = getattr(optim, self.optimizer_name) self.optimizer = optimizer_class(self.model.parameters(), **self.optimizer_params) print(\"PyTorch model compiled successfully.\") def train(self, x_train, y_train, epochs, batch_size, validation_data, callbacks): if self.model is None or self.criterion is None or self.optimizer is None: raise ValueError(\"Model not compiled. Call 'compile_model()' first.\") device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') self.model.to(device) # Convert data to PyTorch tensors X_train_tensor = torch.tensor(x_train.values, dtype=torch.float32).to(device) y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device) X_val, y_val = validation_data X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32).to(device) y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device) dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor) dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True) for epoch in range(epochs): self.model.train() epoch_loss = 0.0 for inputs, targets in dataloader: self.optimizer.zero_grad() outputs = self.model(inputs) loss = self.criterion(outputs, targets) loss.backward() self.optimizer.step() epoch_loss += loss.item() avg_loss = epoch_loss / len(dataloader) self.history['loss'].append(avg_loss) # Validation self.model.eval() with torch.no_grad(): val_outputs = self.model(X_val_tensor) val_loss = self.criterion(val_outputs, y_val_tensor).item() self.history['val_loss'].append(val_loss) # Compute additional metrics if specified if 'Recall' in self.metrics or 'Precision' in self.metrics or 'F1Score' in self.metrics or 'Accuracy' in self.metrics: preds = (val_outputs.cpu().numpy() > 0.5).astype(int) true = y_val_tensor.cpu().numpy().astype(int) metrics_results = {} if 'Recall' in self.metrics: metrics_results['Recall'] = recall_score(true, preds, average='binary', zero_division=0) if 'Precision' in self.metrics: metrics_results['Precision'] = precision_score(true, preds, average='binary', zero_division=0) if 'F1Score' in self.metrics: metrics_results['F1Score'] = f1_score(true, preds, average='binary', zero_division=0) if 'Accuracy' in self.metrics: metrics_results['Accuracy'] = accuracy_score(true, preds) # Append to history or handle as needed for key, value in metrics_results.items(): self.history.setdefault(key.lower(), []).append(value) print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, Val Metrics: {metrics_results if 'metrics_results' in locals() else {}}\") return self.history def evaluate(self, x_test, y_test): device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') self.model.to(device) self.model.eval() X_test_tensor = torch.tensor(x_test.values, dtype=torch.float32).to(device) y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device) with torch.no_grad(): outputs = self.model(X_test_tensor) loss = self.criterion(outputs, y_test_tensor).item() preds = (outputs.cpu().numpy() > 0.5).astype(int) true = y_test_tensor.cpu().numpy().astype(int) metrics_results = {} if 'Recall' in self.metrics: metrics_results['Recall'] = recall_score(true, preds, average='binary', zero_division=0) if 'Precision' in self.metrics: metrics_results['Precision'] = precision_score(true, preds, average='binary', zero_division=0) if 'F1Score' in self.metrics: metrics_results['F1Score'] = f1_score(true, preds, average='binary', zero_division=0) if 'Accuracy' in self.metrics: metrics_results['Accuracy'] = accuracy_score(true, preds) results = {'loss': loss} results.update(metrics_results) print(f\"Evaluation results: {results}\") return results def save_history(self, filepath): if self.history: with open(filepath, 'w') as f: json.dump(self.history, f) print(f\"Training history saved to {filepath}\")", "source": "pytorch_model.py"}, {"content": "else: print(\"No training history to save.\") def save_model(self, filepath): if self.model: torch.save(self.model.state_dict(), filepath) print(f\"PyTorch model saved to {filepath}\") else: print(\"No model to save.\")", "source": "pytorch_model.py"}, {"content": "import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense class TF_Model: def __init__( self, input_dim=29, # Default number of input features, adjust as needed output_units=1, # Default to 1 output unit (e.g., for binary classification) output_activation='sigmoid', # Activation for binary classification; change to 'linear' or 'softmax' as needed loss_function='binary_crossentropy', # Default loss for binary classification; adjust as needed optimizer='adam', # Optimizer choice, can be string or optimizer object metrics=['accuracy', 'f1'] # List of metrics to track during training ): self.input_dim = input_dim self.output_units = output_units self.output_activation = output_activation self.loss_function = loss_function # 'binary_crossentropy' , 'binary_focal_cross_entropy' self.optimizer = optimizer self.metrics = metrics self.model = None def build_model(self): # Define the model architecture self.model = Sequential([ Dense(64, activation='relu', input_shape=(self.input_dim,)), # Input layer Dense(128, activation='relu'), # First hidden layer Dense(64, activation='relu'), # Second hidden layer Dense(32, activation='relu'), # Third hidden layer Dense(self.output_units, activation=self.output_activation) # Output layer ]) print(\"Model built successfully.\") def compile_model(self): if self.model is None: raise ValueError(\"Model not built. Call 'build_model()' first.\") # Compile the model self.model.compile( optimizer=self.optimizer, # Common choice for optimization loss=self.loss_function, # Adjust loss based on problem type metrics=self.metrics # Metrics to evaluate during training ) print(\"Model compiled successfully.\") def summary(self): if self.model is None: raise ValueError(\"Model not built. Call 'build_model()' first.\") # Print model summary self.model.summary() def train( self, x_train, y_train, epochs=10, batch_size=32, validation_data=None, callbacks=None ): if self.model is None: raise ValueError(\"Model not compiled. Call 'compile_model()' first.\") # Train the model history = self.model.fit( x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, callbacks=callbacks ) return history def evaluate(self, x_test, y_test): if self.model is None: raise ValueError(\"Model not compiled. Call 'compile_model()' first.\") # Evaluate the model results = self.model.evaluate(x_test, y_test) print(f\"Test Loss: {results[0]}, Test Accuracy: {results[1]}\") return results", "source": "tf_model temp.py"}, {"content": "import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense from omegaconf import OmegaConf import json from base_model import BaseModel class TF_Model(BaseModel): \"\"\" A class to create, compile, train, evaluate, and save a sequential neural network model. Attributes: input_dim (int): Number of input features. output_units (int): Number of output units (e.g., 1 for binary classification). output_activation (str): Activation function for the output layer. loss_function (str): Loss function used during training. optimizer_name (str): Name of the optimizer (e.g., 'Adam'). optimizer_params (dict): Parameters for the optimizer (e.g., {'learning_rate': 0.001}). metrics (list): List of metrics to evaluate during training (e.g., ['accuracy']). hidden_layers (list): List specifying the number of units in each hidden layer. model (Sequential): The constructed Sequential model (initially None). history (dict): Training history (initially None). \"\"\" def __init__( self, input_dim=29, # Number of input features output_units=1, # Number of output units (1 for binary classification) output_activation='sigmoid', # Activation function for the output layer loss_function='binary_crossentropy', # Loss function optimizer='Adam', # Optimizer (string name) optimizer_params={}, # Optimizer parameters, e.g., {'learning_rate': 0.001} metrics=['Recall'], # List of metrics hidden_layers=[64, 128, 64, 32] # List specifying the units in hidden layers ): \"\"\" Initializes the TF_Model class with the specified parameters. Args: input_dim (int): Number of input features. output_units (int): Number of output units. output_activation (str): Activation function for the output layer. loss_function (str): Loss function to use during training. optimizer (str): Name of the optimizer. optimizer_params (dict): Parameters for the optimizer. metrics (list): List of metrics to evaluate during training. hidden_layers (list): List specifying the units in each hidden layer. \"\"\" self.input_dim = input_dim self.output_units = output_units self.output_activation = output_activation self.loss_function = loss_function self.optimizer_name = optimizer self.optimizer_params = optimizer_params self.metrics = metrics self.hidden_layers = hidden_layers self.model = None self.history = None def build_model(self): \"\"\" Builds a sequential neural network model using the specified architecture. This function constructs a neural network by defining an input layer, hidden layers, and an output layer based on the parameters provided within the class instance. Attributes: self.hidden_layers (list): A list of integers specifying the number of units in each hidden layer, including the input layer as the first element. self.input_dim (int): The dimensionality of the input data. self.output_units (int): The number of units in the output layer. self.output_activation (str): The activation function used in the output layer. Model Architecture: - Input Layer: Defined by `self.hidden_layers[0]` units, 'relu' activation, and input shape as `self.input_dim`. - Hidden Layers: Defined by the remaining elements in `self.hidden_layers`, each with 'relu' activation. - Output Layer: Defined by `self.output_units` units with activation specified by `self.output_activation`. Side Effects: - Sets `self.model` to the constructed Sequential model. - Prints a confirmation message upon successful model construction. Example: >>> model_instance = YourModelClass() >>> model_instance.build_model() Model built successfully. \"\"\" # Define the model architecture layers = [] # Input layer layers.append(Dense(self.hidden_layers[0], activation='relu', input_shape=(self.input_dim,))) # Hidden layers for units in self.hidden_layers[1:]: layers.append(Dense(units, activation='relu')) # Output layer layers.append(Dense(self.output_units, activation=self.output_activation)) self.model = Sequential(layers) print(\"Model built successfully.\") def compile_model(self): \"\"\" Compiles the neural network model using the specified optimizer, loss function, and metrics. Raises: ValueError: If the model has not been built yet. Side", "source": "tf_model.py"}, {"content": "Effects: - Compiles `self.model` with the specified optimizer, loss function, and metrics. - Prints a confirmation message upon successful model compilation. \"\"\" if self.model is None: raise ValueError(\"Model not built. Call 'build_model()' first.\") # Set up the optimizer with parameters optimizer_class = getattr(tf.keras.optimizers, self.optimizer_name) optimizer = optimizer_class(**self.optimizer_params) compiled_metrics = [] for metric in self.metrics: compiled_metrics.append(metric) # Compile the model self.model.compile( optimizer=optimizer, loss=self.loss_function, metrics=compiled_metrics ) print(\"Model compiled successfully.\") def summary(self): if self.model is None: raise ValueError(\"Model not built. Call 'build_model()' first.\") self.model.summary() def train( self, x_train, y_train, epochs=10, batch_size=32, validation_data=None, callbacks=None, ): \"\"\" Trains the compiled model on the given training data. Args: x_train: Training data inputs. y_train: Training data targets. epochs (int): Number of training epochs. batch_size (int): Batch size for training. validation_data (tuple): Tuple of validation data (x_val, y_val). callbacks (list): List of callbacks to use during training. Returns: History object containing details about the training process. Raises: ValueError: If the model has not been compiled yet. \"\"\" if self.model is None: raise ValueError(\"Model not compiled. Call 'compile_model()' first.\") # Train the model history = self.model.fit( x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=validation_data, callbacks=callbacks, ) self.history = history.history # Save training history return history def evaluate(self, x_test, y_test): \"\"\" Evaluates the model on the given test data. Args: x_test: Test data inputs. y_test: Test data targets. Returns: dict: Dictionary of evaluation metrics and their corresponding values. Raises: ValueError: If the model has not been compiled yet. \"\"\" if self.model is None: raise ValueError(\"Model not compiled. Call 'compile_model()' first.\") # Evaluate the model results = self.model.evaluate(x_test, y_test) metrics_names = self.model.metrics_names results_dict = dict(zip(metrics_names, results)) print(f\"Evaluation results: {results_dict}\") return results_dict def save_history(self, filepath): \"\"\" Saves the training history to a JSON file. Args: filepath (str): The path to save the training history. Side Effects: - Writes the training history to a file in JSON format. \"\"\" if self.history is not None: with open(filepath, 'w') as f: json.dump(self.history, f) print(f\"Training history saved to {filepath}\") else: print(\"No training history to save.\") def save_model(self, filepath): \"\"\" Saves the model to the specified filepath. Args: filepath (str): The path to save the model. Side Effects: - Saves the model to the specified filepath. \"\"\" if self.model is not None: self.model.save(filepath) print(f\"Model saved to {filepath}\") else: print(\"No model to save.\")", "source": "tf_model.py"}, {"content": "# src/train_model.py \"\"\" This script trains models (TensorFlow or PyTorch) on a dataset using hyperparameter tuning with Optuna and configuration management with Hydra. \"\"\" import os import logging import hydra import mlflow import pandas as pd import numpy as np from omegaconf import DictConfig, OmegaConf from hydra.utils import to_absolute_path import importlib import sys sys.path.append(os.path.join(os.path.dirname(__file__), '..')) # Ensure src/ is in PYTHONPATH import aiap_dsp_mlops as amlo # Ensure this module is accessible # Suppress TensorFlow warnings (optional) os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(cfg: DictConfig): \"\"\"Main function for training the model with hyperparameter tuning.\"\"\" logger = logging.getLogger(__name__) logger.info(\"Starting training process.\") # Print the loaded configuration print(OmegaConf.to_yaml(cfg)) # Set up logging amlo.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"assignment4\", \"conf\", \"logging.yaml\" ) ) # Initialize MLflow mlflow_init_status, mlflow_run = amlo.general_utils.mlflow_init( cfg, setup_mlflow=cfg.setup_mlflow, autolog=cfg.mlflow_autolog ) # Load Data logger.info(\"Loading data...\") X_train = pd.read_csv(to_absolute_path(cfg.data_paths.X_train)) X_test = pd.read_csv(to_absolute_path(cfg.data_paths.X_test)) y_train = pd.read_csv(to_absolute_path(cfg.data_paths.y_train)) y_test = pd.read_csv(to_absolute_path(cfg.data_paths.y_test)) # Convert y_train and y_test to numpy arrays y_train = y_train.values.ravel().reshape(-1,1) y_test = y_test.values.ravel().reshape(-1,1) # Dynamically import the model class model_module = importlib.import_module(cfg.model.module) # e.g., 'tf_model' model_class = getattr(model_module, cfg.model.class_name) # e.g., 'TF_Model' model = model_class(**cfg.model.params) logger.info(f\"Initialized model with module: {model_module.__name__}, class: {cfg.model.class_name}, parameters: {cfg.model.params}\") # Determine the model type (e.g., 'tf_model' or 'pytorch_model') model_type = 'tf_model' if cfg.model.module == 'tf_model' else 'pytorch_model' # Define a function to validate parameters def validate_parameters(model_type, params): \"\"\" Validates that the selected hyperparameters are compatible with the model type. \"\"\" if model_type == 'tf_model': supported_loss = ['binary_crossentropy', 'binary_focal_crossentropy'] if params.get('loss_function') not in supported_loss: raise ValueError(f\"Unsupported loss function '{params.get('loss_function')}' for TensorFlow model.\") # Add more TensorFlow-specific validations if needed elif model_type == 'pytorch_model': supported_loss = ['BCELoss', 'CrossEntropyLoss'] if params.get('loss_function') not in supported_loss: raise ValueError(f\"Unsupported loss function '{params.get('loss_function')}' for PyTorch model.\") # Add more PyTorch-specific validations if needed else: raise ValueError(f\"Unsupported model type: {model_type}\") # Define the objective function for Optuna def objective(trial): with mlflow.start_run(nested=True): # Fetch the appropriate hyperparameter search space search_space = cfg.sweeper_config.params.get(model_type, {}) if not search_space: logger.error(f\"No hyperparameter search space defined for model type: {model_type}\") raise ValueError(f\"No hyperparameter search space defined for model type: {model_type}\") # Suggest hyperparameters based on the model type suggested_params = {} for param, options in search_space.items(): if 'min' in options and 'max' in options: if options.get('distribution') == 'loguniform': suggested_params[param] = trial.suggest_loguniform(param, options['min'], options['max']) else: # Assume integer parameters if min and max are present without distribution suggested_params[param] = trial.suggest_int(param, options['min'], options['max']) elif 'choices' in options: suggested_params[param] = trial.suggest_categorical(param, options['choices']) else: logger.error(f\"Unsupported hyperparameter configuration for parameter: {param}\") raise ValueError(f\"Unsupported hyperparameter configuration for parameter: {param}\") # Validate hyperparameters validate_parameters(model_type, suggested_params) # Log hyperparameters to MLflow mlflow.log_params(suggested_params) # Update model parameters with hyperparameters for param, value in suggested_params.items(): setattr(model, param, value) # Build and compile the model model.build_model() model.compile_model() # Train the model history = model.train( X_train, y_train, epochs=suggested_params.get('epochs', cfg.model.params.get('epochs', 10)), batch_size=suggested_params.get('batch_size', cfg.model.params.get('batch_size', 32)), validation_data=(X_test, y_test), callbacks=None # Add callbacks if needed ) # Evaluate the model results = model.evaluate(X_test, y_test) mlflow.log_metrics(results) # Determine the appropriate file extension based on the framework if model_type == 'tf_model': model_file = \"model.h5\" elif model_type == 'pytorch_model': model_file = \"model.pth\" else: model_file = \"model\" # Default or handle other frameworks accordingly", "source": "train_model.py"}, {"content": "# Save model and history artifact_dir = os.path.join(to_absolute_path(cfg.artifact_dir_path), f\"trial_{trial.number}\") os.makedirs(artifact_dir, exist_ok=True) model.save_model(os.path.join(artifact_dir, model_file)) model.save_history(os.path.join(artifact_dir, \"history.json\")) mlflow.log_artifacts(artifact_dir) # Objective is to minimize loss return results['loss'] # Run Optuna optimization import optuna study = optuna.create_study(direction=cfg.sweeper_config.direction, study_name=cfg.sweeper_config.study_name) study.optimize(objective, n_trials=cfg.sweeper_config.n_trials) # Log best trial logger.info(f\"Best trial: {study.best_trial.number}\") logger.info(f\"Best value (loss): {study.best_trial.value}\") logger.info(f\"Best hyperparameters: {study.best_trial.params}\") # Log best trial parameters and metrics to MLflow mlflow.log_params(study.best_trial.params) mlflow.log_metric('best_loss', study.best_trial.value) # Finish MLflow run if mlflow_init_status: mlflow.end_run() logger.info(\"Training process completed.\") if __name__ == \"__main__\": main()", "source": "train_model.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities.3 Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "\"\"\"This package contains modules pertaining to differing parts of the end-to-end workflow, excluding the source for serving the model through a REST API.\"\"\" from . import data_prep from . import modeling from . import general_utils", "source": "__init__.py"}, {"content": "\"\"\"Dataset classes for defining how datasets are to be loaded. \"\"\" import os import shutil class DummyDataset(): \"\"\"Dummy dataset class.\"\"\" def __init__(self, data_dir_path) -> None: if not os.path.isdir(data_dir_path): e = \"Path is not a directory, or does not exist: {}\".format(data_dir_path) raise ValueError(e) self.data_dir_path = data_dir_path def save_data(self, processed_data_dir_path) -> None: shutil.copytree( self.data_dir_path, processed_data_dir_path, dirs_exist_ok=True )", "source": "datasets.py"}, {"content": "\"\"\"Definition of transforms sequence for data preparation. \"\"\"", "source": "transforms.py"}, {"content": "\"\"\"This `data_prep` module includes module(s) which contains utilities for processing, cleaning, or loading data.\"\"\" from . import datasets from . import transforms", "source": "__init__.py"}, {"content": "\"\"\"Module for containing architectures/definition of models.\"\"\" class DummyModel(): \"\"\"Dummy model that returns the input.\"\"\" def __init__(self): pass def predict(self, x): \"\"\"'Prediction' of the dummy model. Parameters ---------- x : str Input string. Returns ------- str Output string. \"\"\" return x", "source": "models.py"}, {"content": "\"\"\"Utilities for model training and experimentation workflows. \"\"\" import aiap_dsp_mlops as amlo def load_model(): \"\"\"Load dummy model. A sample utility function to be used. Returns ------- loaded_model : Object containing dummy model. \"\"\" loaded_model = amlo.modeling.models.DummyModel() return loaded_model", "source": "utils.py"}, {"content": "\"\"\"This `modeling` module includes module(s) relevant for the model training pipeline.\"\"\" from . import models from . import utils", "source": "__init__.py"}, {"content": "\"\"\"Configuration module for the FastAPI application.\"\"\" import pydantic_settings class Settings(pydantic_settings.BaseSettings): \"\"\"Settings for the FastAPI application.\"\"\" API_NAME: str = \"AIAP DSP MLOps - Fastapi\" API_V1_STR: str = \"/api/v1\" LOGGER_CONFIG_PATH: str = \"../conf/logging.yaml\" MODEL_UUID: str = \"change-this\" SETTINGS = Settings()", "source": "config.py"}, {"content": "\"\"\"FastAPI dependencies and global variables.\"\"\" import aiap_dsp_mlops as amlo import aiap_dsp_mlops_fastapi as amlo_fapi PRED_MODEL = amlo.modeling.utils.load_model()", "source": "deps.py"}, {"content": "\"\"\"Main module for initialising and defining the FastAPI application.\"\"\" import logging import fastapi from fastapi.middleware.cors import CORSMiddleware import aiap_dsp_mlops as amlo import aiap_dsp_mlops_fastapi as amlo_fapi LOGGER = logging.getLogger(__name__) LOGGER.info(\"Setting up logging configuration.\") amlo.general_utils.setup_logging( logging_config_path=amlo_fapi.config.SETTINGS.LOGGER_CONFIG_PATH ) API_V1_STR = amlo_fapi.config.SETTINGS.API_V1_STR APP = fastapi.FastAPI( title=amlo_fapi.config.SETTINGS.API_NAME, openapi_url=f\"{API_V1_STR}/openapi.json\" ) API_ROUTER = fastapi.APIRouter() API_ROUTER.include_router( amlo_fapi.v1.routers.model.ROUTER, prefix=\"/model\", tags=[\"model\"] ) APP.include_router(API_ROUTER, prefix=amlo_fapi.config.SETTINGS.API_V1_STR) ORIGINS = [\"*\"] APP.add_middleware( CORSMiddleware, allow_origins=ORIGINS, allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], )", "source": "main.py"}, {"content": "\"\"\"Package for FastAPI application.\"\"\" from . import config from . import deps from . import v1", "source": "__init__.py"}, {"content": "\"\"\"This package contains the v1 API submodules.\"\"\" from . import routers", "source": "__init__.py"}, {"content": "\"\"\"Module containing definitions and workflows for FastAPI's application endpoints.\"\"\" import os import logging import fastapi import aiap_dsp_mlops_fastapi as amlo_fapi logger = logging.getLogger(__name__) ROUTER = fastapi.APIRouter() PRED_MODEL = amlo_fapi.deps.PRED_MODEL @ROUTER.post(\"/predict\", status_code=fastapi.status.HTTP_200_OK) def predict(data: str = fastapi.Body()): \"\"\"Endpoint that returns the input as-is. Parameters ---------- data : str Input text. Returns ------- result_dict : dict Dictionary containing the input string. Raises ------ fastapi.HTTPException A 500 status error is returned if the prediction steps encounters any errors. \"\"\" result_dict = {\"data\": []} try: logger.info(\"Copying input...\") pred_str = PRED_MODEL.predict(data) result_dict[\"data\"].append( {\"input\": pred_str} ) logger.info(\"Input: %s\", data) except Exception as error: logger.error(error) raise fastapi.HTTPException(status_code=500, detail=\"Internal server error.\") return result_dict @ROUTER.get(\"/version\", status_code=fastapi.status.HTTP_200_OK) def model_version(): \"\"\"Get sample version used for the API. Returns ------- dict Dictionary containing the sample version. \"\"\" return {\"data\": {\"model_uuid\": amlo_fapi.config.SETTINGS.MODEL_UUID}}", "source": "model.py"}, {"content": "\"\"\"This package contains the v1 endpoints for the FastAPI application.\"\"\" from . import model", "source": "__init__.py"}, {"content": "def test_dummy(): \"\"\"A test dummy such that the test CI/CD doesn't fail.\"\"\" assert True == True", "source": "test_dummy.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 1, 1, 1], [ 1, 1, 1], [ 1, 1, 1], ] # Replace below with your response matrix_2 = [ [ 1, 1, 1], [ 1, 1, 1], [ 1, 1, 1], ] # Replace below with your response matrix_3 = [ [ 1, 1, 1], [ 1, 1, 1], [ 1, 1, 1], ]", "source": "convolved_matrices.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "# to run datapipeline.py and also logging.", "source": "preprocess_model.py"}, {"content": "# src/train.py import tensorflow as tf from tensorflow.keras.models import Model from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout class ModelTrainer: def __init__(self, base_model_cfg, model_cfg, training_cfg, class_labels): \"\"\" Initializes the ModelTrainer with configuration parameters. Args: base_model_cfg (Dict): Configuration for the base model. model_cfg (Dict): Configuration for additional model layers. training_cfg (Dict): Configuration for training parameters. class_labels (List): List of class labels for classification. \"\"\" self.base_model_cfg = base_model_cfg self.model_cfg = model_cfg self.training_cfg = training_cfg self.class_labels = class_labels self.model = self.build_model() def build_model(self): \"\"\" Builds the TensorFlow model based on the base model and additional layers. Returns: tensorflow.keras.models.Model: Compiled TensorFlow model. \"\"\" # Dynamically instantiate the base model using the configuration base_model = tf.keras.applications.ResNet50(**self.base_model_cfg) # Default to ResNet50 if not specified if 'VGG16' in self.base_model_cfg.get('name', ''): base_model = tf.keras.applications.VGG16(**self.base_model_cfg) elif 'InceptionV3' in self.base_model_cfg.get('name', ''): base_model = tf.keras.applications.InceptionV3(**self.base_model_cfg) base_model.trainable = False # Freeze the base model # Add custom layers on top of the base model x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(self.model_cfg['dense_units_1'], activation='relu')(x) x = Dropout(self.model_cfg['dropout_rate_1'])(x) x = Dense(self.model_cfg['dense_units_2'], activation='relu')(x) x = Dropout(self.model_cfg['dropout_rate_2'])(x) predictions = Dense(len(self.class_labels), activation='softmax')(x) model = Model(inputs=base_model.input, outputs=predictions) return model def compile_model(self): \"\"\" Compiles the TensorFlow model with the specified optimizer, loss, and metrics. \"\"\" optimizer = tf.keras.optimizers.Adam(learning_rate=self.training_cfg['learning_rate']) self.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy']) def train(self, train_data, val_data): \"\"\" Trains the TensorFlow model using the provided training and validation data. Args: train_data (tf.data.Dataset): Training dataset. val_data (tf.data.Dataset): Validation dataset. Returns: tensorflow.keras.callbacks.History: Training history. \"\"\" self.compile_model() history = self.model.fit( train_data, validation_data=val_data, epochs=self.training_cfg['epochs'], callbacks=[ tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=self.training_cfg['early_stopping_patience'] ) ] ) return history def evaluate(self, test_data): \"\"\" Evaluates the TensorFlow model using the provided test data. Args: test_data (tf.data.Dataset): Test dataset. Returns: Tuple[float, float]: Test loss and test accuracy. \"\"\" test_loss, test_accuracy = self.model.evaluate(test_data) return test_loss, test_accuracy", "source": "train.py"}, {"content": "import os import logging import omegaconf import hydra import mlflow import torch import tensorflow as tf import general_utils as gu import random import numpy as np from tensorflow.keras.preprocessing import image_dataset_from_directory from train import ModelTrainer from omegaconf import DictConfig, OmegaConf # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(args): \"\"\"This is the main function for 'training' the model. Parameters ---------- args : omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") gu.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) logger.info(\"Starting the training pipeline.\") mlflow_init_status, mlflow_run = gu.mlflow_init( args, setup_mlflow=args[\"setup_mlflow\"], autolog=args[\"mlflow_autolog\"] ) if mlflow_init_status: logger.info(\"MLflow run started with run ID: %s\", mlflow_run.info.run_id) tar_path = args.tar_path data_path = args.data_path # Get Class Labels try: class_labels = os.listdir(data_path) class_labels = [f for f in class_labels if not f.startswith(\".\")] logger.info(f\"Found class labels: {class_labels}\") except Exception as e: logger.error(f\"Failed to list class labels in {data_path}: {e}\") raise e # Set random seeds for reproducibility seed = args.seed tf.random.set_seed(seed) random.seed(seed) np.random.seed(seed) logger.info(f\"Random seeds set to {seed}\") # Data augmentation data_augmentation = tf.keras.Sequential([ tf.keras.layers.RandomFlip(args.augmentation.random_flip), tf.keras.layers.RandomRotation(args.augmentation.random_rotation), ]) logger.info(\"Data augmentation layers configured.\") # Prepare the datasets try: train_data, val_test_data = image_dataset_from_directory( directory=data_path, labels=\"inferred\", label_mode=\"categorical\", class_names=class_labels, color_mode=\"rgb\", batch_size=args.batch_size, image_size=(args.base_model._base_model.input_shape[0], args.base_model._base_model.input_shape[1]), shuffle=args.shuffle_value, seed=seed, validation_split=args.validation_split, subset=\"both\", interpolation='bilinear', follow_links=False, crop_to_aspect_ratio=False, pad_to_aspect_ratio=False, data_format=\"channels_last\", verbose=1 ) logger.info(\"Datasets loaded successfully.\") except Exception as e: logger.error(f\"Failed to load datasets: {e}\") raise e # Split validation into validation and test sets try: val_batches = tf.data.experimental.cardinality(val_test_data) test_data = val_test_data.take((2 * val_batches) // 3) val_data = val_test_data.skip((2 * val_batches) // 3) logger.info(f\"Training batches: {tf.data.experimental.cardinality(train_data).numpy()}\") logger.info(f\"Validation batches: {tf.data.experimental.cardinality(val_data).numpy()}\") logger.info(f\"Test batches: {tf.data.experimental.cardinality(test_data).numpy()}\") except Exception as e: logger.error(f\"Failed to split validation and test datasets: {e}\") raise e # Instantiate the ModelTrainer try: model_trainer = ModelTrainer( base_model_cfg=args.base_model._base_model, model_cfg=args.model, training_cfg=args.training, class_labels=class_labels ) logger.info(\"ModelTrainer instantiated successfully.\") except Exception as e: logger.error(f\"Failed to instantiate ModelTrainer: {e}\") raise e # Train the model try: history = model_trainer.train(train_data, val_data) logger.info(\"Model training completed.\") except Exception as e: logger.error(f\"Model training failed: {e}\") raise e # Evaluate the model on the test data try: test_loss, test_accuracy = model_trainer.evaluate(test_data) logger.info(f\"Test Accuracy: {test_accuracy:.4f}\") except Exception as e: logger.error(f\"Model evaluation failed: {e}\") raise e if mlflow_init_status: try: gu.mlflow_log( mlflow_init_status, \"log_params\", params={ \"learning_rate\": args.training.learning_rate, \"dense_units_1\": args.model.dense_units_1, \"dense_units_2\": args.model.dense_units_2, \"dropout_rate_1\": args.model.dropout_rate_1, \"dropout_rate_2\": args.model.dropout_rate_2, \"epochs\": args.training.epochs, \"batch_size\": args.batch_size, \"base_model\": args.base_model.name }, ) gu.mlflow_log( mlflow_init_status, \"log_metric\", key=\"test_accuracy\", value=test_accuracy ) # Log the configuration as a JSON artifact config_path = os.path.join(args.artifact_dir_path, \"train_config.json\") with open(config_path, \"w\") as f: f.write(OmegaConf.to_yaml(args)) gu.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=config_path, artifact_path=\"config\" ) logger.info(\"Parameters and metrics logged to MLflow.\") except Exception as e: logger.error(f\"Failed to log to MLflow: {e}\") # Log additional artifacts if needed try: os.makedirs(args.artifact_dir_path, exist_ok=True) artifact_path = os.path.join(args.artifact_dir_path, \"training_log.txt\") with open(artifact_path, \"w\") as f: f.write(f\"Test Loss: {test_loss}\\nTest Accuracy: {test_accuracy}\\n\") gu.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=artifact_path, artifact_path=\"outputs\" ) logger.info(\"Training artifacts logged to MLflow.\") except Exception as e: logger.error(f\"Failed to log training artifacts: {e}\") # Finalize MLflow run if mlflow_init_status: try: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) gu.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() except Exception as e: logger.error(f\"Failed to finalize MLflow run: {e}\") else: logger.info(\"Model training has completed without MLflow", "source": "train_model.py"}, {"content": "logging.\") # Outputs for conf/train.yaml for hydra.sweeper.direction return test_accuracy # You can adjust based on optimization direction if __name__ == \"__main__\": main()", "source": "train_model.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "\"\"\"This package contains modules pertaining to differing parts of the end-to-end workflow, excluding the source for serving the model through a REST API.\"\"\" from . import data_prep from . import modeling from . import general_utils", "source": "__init__.py"}, {"content": "\"\"\"Dataset classes for defining how datasets are to be loaded. \"\"\" import os import shutil class DummyDataset(): \"\"\"Dummy dataset class.\"\"\" def __init__(self, data_dir_path) -> None: if not os.path.isdir(data_dir_path): e = \"Path is not a directory, or does not exist: {}\".format(data_dir_path) raise ValueError(e) self.data_dir_path = data_dir_path def save_data(self, processed_data_dir_path) -> None: shutil.copytree( self.data_dir_path, processed_data_dir_path, dirs_exist_ok=True )", "source": "datasets.py"}, {"content": "\"\"\"Definition of transforms sequence for data preparation. \"\"\"", "source": "transforms.py"}, {"content": "\"\"\"This `data_prep` module includes module(s) which contains utilities for processing, cleaning, or loading data.\"\"\" from . import datasets from . import transforms", "source": "__init__.py"}, {"content": "\"\"\"Module for containing architectures/definition of models.\"\"\" class DummyModel(): \"\"\"Dummy model that returns the input.\"\"\" def __init__(self): pass def predict(self, x): \"\"\"'Prediction' of the dummy model. Parameters ---------- x : str Input string. Returns ------- str Output string. \"\"\" return x", "source": "models.py"}, {"content": "\"\"\"Utilities for model training and experimentation workflows. \"\"\" import srcpackagename as srcpacakagenameshort def load_model(): \"\"\"Load dummy model. A sample utility function to be used. Returns ------- loaded_model : Object containing dummy model. \"\"\" loaded_model = srcpacakagenameshort.modeling.models.DummyModel() return loaded_model", "source": "utils.py"}, {"content": "\"\"\"This `modeling` module includes module(s) relevant for the model training pipeline.\"\"\" from . import models from . import utils", "source": "__init__.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "# src/data_pipeline.py import pandas as pd from sklearn.base import BaseEstimator from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.metrics import root_mean_squared_error from typing import Optional, List, Dict, Tuple import numpy as np from src.imputer import Imputer from src.ml_model import ForecastModel # import time series split from sklearn.model_selection import TimeSeriesSplit class DataPipeline: \"\"\" DataPipeline Class for Time Series Data Processing. This class handles the preprocessing of time series data, including missing value imputation, differencing, and lagged feature creation, making the data ready for modeling. Parameters for Hyperparameter Tuning: - max_lag: Maximum number of lagged time steps to create (e.g., tuning between 12, 24, 48). - diff_periods: Number of periods to use for differencing (e.g., tuning between 1, 2, 3). - ma_window: Window size for moving average imputation (e.g., tuning between 12, 24, 48). - feature_strategy: Strategy for feature engineering ('default', 'difference', 'both'). General Parameters: - file_path: Path to the CSV data file. - target: Name of the target variable. - num_cols: List of numerical feature column names. - cat_cols: List of categorical feature column names. Usage: ```python from sklearn.ensemble import RandomForestRegressor # Initialize DataPipeline pipeline = DataPipeline( file_path='data.csv', target='target_column', num_cols=['feature1', 'feature2', 'feature3'], cat_cols=['categorical_feature'], max_lag=24, diff_periods=1, ma_window=24, feature_strategy='both', imputation_model=RandomForestRegressor(n_estimators=100, random_state=0), target_lag=1, time_series_split=3, test_size=0.2 ) # Run data preprocessing and get processed data processed_data = pipeline.get_processed_data() print(processed_data.head()) ``` \"\"\" def __init__( self, file_path: str, target: str, num_cols: List[str], cat_cols: List[str], max_lag: int = 24, diff_periods: int = 1, ma_window: int = 24, feature_strategy: str = 'both', # Options: 'default', 'difference', 'both' imputation_model: Optional[BaseEstimator] = None, target_lag: int = 1, time_series_split: int = 3, test_size: float = 0.2 ): \"\"\" Initialize the DataPipeline with file path and column information. Parameters: ---------- file_path : str Path to the CSV data file. target : str Name of the target variable. num_cols : List[str] List of numerical feature column names. cat_cols : List[str] List of categorical feature column names. max_lag : int, optional Maximum number of lagged time steps to create. Default is 24. diff_periods : int, optional Number of periods to use for differencing. Default is 1. ma_window : int, optional Window size for the moving average imputation. Default is 24. feature_strategy : str, optional Strategy for feature engineering ('default', 'difference', 'both'). Default is 'both'. imputation_model : Optional[BaseEstimator], optional A scikit-learn estimator used for model-based imputation of missing values. target_lag : int, optional The forecast horizon for the shifted target. Default is 1. test_size : float, optional The proportion of the data to be used as the test set. Default is 0.2. \"\"\" self.file_path = file_path self.target = target self.num_cols = num_cols self.cat_cols = cat_cols self.max_lag = max_lag self.diff_periods = diff_periods self.ma_window = ma_window self.feature_strategy = feature_strategy.lower() # Convert to lowercase self.data = None self._validate_parameters() self._prepare_data() self.imputation_model = imputation_model self.target_lag = target_lag self.time_series_split = time_series_split self.test_size = test_size def _validate_parameters(self) -> None: \"\"\" Validate the initialization parameters. Raises: ------- ValueError: If `feature_strategy` is not among the valid options. \"\"\" valid_strategies = ['default', 'difference', 'both'] if self.feature_strategy not in valid_strategies: raise ValueError(f\"feature_strategy must be one of {valid_strategies}, got '{self.feature_strategy}'.\") def _prepare_data(self) -> None:", "source": "data_pipeline.py"}, {"content": "\"\"\" Load data from the CSV file, create a datetime index, and clean the DataFrame. \"\"\" # Load data self.data = pd.read_csv(self.file_path) # Include only relevant columns all_cols = self.num_cols + self.cat_cols + ['year', 'month', 'day', 'hour', self.target] self.data = self.data[all_cols] # Create datetime column self.data['date_time'] = pd.to_datetime( self.data['year'].astype(str) + '-' + self.data['month'].astype(str) + '-' + self.data['day'].astype(str) + ' ' + self.data['hour'].astype(str) + ':00:00' ) self.data.set_index('date_time', inplace=True) # Drop original temporal columns self.data.drop(['year', 'month', 'day', 'hour'], axis=1, inplace=True) def impute_missing_values(self) -> None: \"\"\" Handle missing values using the Imputer class. This method performs: 1. Moving average imputation on the target variable. 2. Machine learning model-based imputation for remaining missing target values. 3. Forward fill imputation for other numerical features. 4. Drops any remaining missing values. \"\"\" imputer = Imputer( data=self.data, target=self.target, num_cols=self.num_cols, ma_window=self.ma_window, imputation_model=self.imputation_model ) imputer.run_imputation() self.data = imputer.get_data() def create_differenced_features(self) -> None: \"\"\" Create differenced features to capture short-term changes and trends. Applies differencing based on the specified feature_strategy. \"\"\" if self.feature_strategy in ['difference', 'both']: differenced_data = self.data[self.num_cols].diff(periods=self.diff_periods) differenced_data.rename(columns=lambda x: f\"{x}_diff_{self.diff_periods}\", inplace=True) self.data = pd.concat([self.data, differenced_data], axis=1) self.data.dropna(inplace=True) def create_lagged_features(self) -> None: \"\"\" Create lagged features based on the feature_strategy. - 'default': Only original numerical features are lagged. - 'difference': Only differenced numerical features are lagged. - 'both': Both original and differenced numerical features are lagged. \"\"\" lagged_columns = {} if self.feature_strategy == 'default': feature_columns = self.num_cols elif self.feature_strategy == 'difference': feature_columns = [f\"{col}_diff_{self.diff_periods}\" for col in self.num_cols] elif self.feature_strategy == 'both': feature_columns = self.num_cols + [f\"{col}_diff_{self.diff_periods}\" for col in self.num_cols] for col in feature_columns: for lag in range(1, self.max_lag + 1): lagged_columns[f'{col}_lag_{lag}'] = self.data[col].shift(lag) # Use pd.concat to add all lagged columns at once lagged_data = pd.concat([self.data, pd.DataFrame(lagged_columns, index=self.data.index)], axis=1) lagged_data.dropna(inplace=True) self.data = lagged_data def get_processed_data(self) -> pd.DataFrame: \"\"\" Execute the complete data preprocessing pipeline and return the processed data. Returns: ------- pd.DataFrame The fully processed DataFrame ready for modeling. \"\"\" self.impute_missing_values() self.create_differenced_features() self.create_lagged_features() return self.data def create_sklearn_pipeline(self, model: BaseEstimator) -> Pipeline: \"\"\" Create a scikit-learn pipeline to handle preprocessing and modeling. Parameters: ---------- model : BaseEstimator A scikit-learn-like estimator (e.g., RandomForestRegressor). Returns: ------- Pipeline A scikit-learn Pipeline object encapsulating preprocessing and the model. \"\"\" from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder # Define numerical and categorical features after preprocessing numerical_features = self.num_cols categorical_features = self.cat_cols # Create transformers for numeric and categorical features numeric_transformer = Pipeline(steps=[ ('scaler', StandardScaler()) ]) categorical_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Combine transformers into a preprocessor preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numerical_features), ('cat', categorical_transformer, categorical_features) ] ) # Create the complete pipeline pipeline = Pipeline(steps=[ ('preprocessor', preprocessor), ('model', model) ]) return pipeline def create_shifted_target(self) -> pd.DataFrame: \"\"\" Create a shifted target column for a given forecast horizon. Parameters: ---------- data : pd.DataFrame The preprocessed data. target : str The name of the target variable. target_lag : int The forecast horizon (number of time steps ahead). Returns: ------- pd.DataFrame DataFrame with shifted target. \"\"\" shifted_data = self.data.copy() shifted_data[f'{self.target}_lag_{self.target_lag}'] = shifted_data[self.target].shift(-self.target_lag) shifted_data.dropna(inplace=True) self.data = shifted_data return self.data # Function to split data def train_validation_test_split(self) -> Tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Split", "source": "data_pipeline.py"}, {"content": "the data into train-validation and test sets. Parameters: ---------- data : pd.DataFrame The input data to be split. Returns: ------- Tuple[pd.DataFrame, pd.DataFrame] A tuple containing (train_validation_data, test_data). \"\"\" test_index = int(len(self.data) * (1 - self.test_size)) # Index to split test set train_validation_data = self.data.iloc[:test_index] # Train-Validation set, first (1 - test_size) test_data = self.data.iloc[test_index:] # Test set, last test_size print(f'Train-Validation size: {len(train_validation_data)}, Test size: {len(test_data)}') return train_validation_data, test_data def _train_with_cross_validation( self, X: pd.DataFrame, y: pd.Series, model: BaseEstimator ) -> Dict[str, float]: \"\"\" Train the model using TimeSeriesSplit cross-validation and evaluate performance. Parameters: ---------- X : pd.DataFrame Feature matrix for training. y : pd.Series Target vector for training. model : BaseEstimator The machine learning model to be trained. n_splits : int, optional Number of splits for TimeSeriesSplit. Default is 5. Returns: ------- Dict[str, float] A dictionary containing average MAE, RMSE, and R2 scores across folds. \"\"\" tscv = TimeSeriesSplit(n_splits=self.time_series_split) # Initialize lists to store metrics mae_scores = [] rmse_scores = [] r2_scores = [] # Initialize ForecastModel with the pipeline sklearn_pipeline = self.create_sklearn_pipeline(model) forecast_model = ForecastModel(model=sklearn_pipeline) # Perform cross-validation for fold, (train_index, val_index) in enumerate(tscv.split(X)): print(f\"Fold {fold + 1}\") X_train, X_val = X.iloc[train_index], X.iloc[val_index] y_train, y_val = y.iloc[train_index], y.iloc[val_index] # Fit the ForecastModel (which includes the pipeline) forecast_model.fit(X_train, y_train) # Predict on validation set y_pred = forecast_model.predict(X_val) # Evaluate evaluation = forecast_model.evaluate(y_val, y_pred, metrics=['mae', 'rmse', 'r2']) print(f\"MAE: {evaluation['MAE']:.4f}, RMSE: {evaluation['RMSE']:.4f}, R2: {evaluation['R2']:.4f}\\n\") # Store metrics mae_scores.append(evaluation['MAE']) rmse_scores.append(evaluation['RMSE']) r2_scores.append(evaluation['R2']) # Calculate average metrics avg_metrics = { 'Average MAE': np.mean(mae_scores), 'Average RMSE': np.mean(rmse_scores), 'Average R2': np.mean(r2_scores) } print(f\"Average MAE: {avg_metrics['Average MAE']:.4f}\") print(f\"Average RMSE: {avg_metrics['Average RMSE']:.4f}\") print(f\"Average R2: {avg_metrics['Average R2']:.4f}\\n\") return avg_metrics def run_pipeline(self, model: BaseEstimator) -> Dict[str, float]: \"\"\" Run the complete data processing and modeling pipeline, including preprocessing, cross-validation, training on full train-validation set, and evaluation on test set. Parameters: ---------- model : BaseEstimator The machine learning model to be used in the pipeline. test_size : float, optional The proportion of the data to be used as the test set. Default is 0.2. n_splits : int, optional Number of splits for TimeSeriesSplit cross-validation. Default is 5. Returns: ------- Dict[str, float] A dictionary containing average cross-validation metrics and test set metrics. \"\"\" # Preprocess the data print(\"Starting data preprocessing...\") processed_data = self.get_processed_data() # Shift the target for forecasting print(\"Creating shifted target for forecasting...\") shifted_data = self.create_shifted_target() # Update processed_data with shifted target processed_data = shifted_data # Split the data into train-validation and test sets print(\"Splitting data into train-validation and test sets...\") train_validation_data, test_data = self.train_validation_test_split() # Define target and features target = f\"{self.target}_lag_{self.target_lag}\" features = [col for col in processed_data.columns if col != target] X = train_validation_data[features] y = train_validation_data[target] X_test = test_data[features] y_test = test_data[target] # Perform cross-validation training print(\"Starting cross-validation training...\") cv_metrics = self._train_with_cross_validation(X, y, model) # Fit the ForecastModel on the entire train-validation set print(\"Fitting the model on the entire train-validation set...\") sklearn_pipeline = self.create_sklearn_pipeline(model) forecast_model = ForecastModel(model=sklearn_pipeline) forecast_model.fit(X, y) # Predict on the test set print(\"Predicting on the test set...\") y_test_pred = forecast_model.predict(X_test) # Evaluate on the test set print(\"Evaluating on the test set...\") test_evaluation = forecast_model.evaluate(y_test, y_test_pred, metrics=['mae', 'rmse', 'r2'])", "source": "data_pipeline.py"}, {"content": "print(\"\\nTest Set Evaluation:\") print(f\"MAE: {test_evaluation['MAE']:.4f}\") print(f\"RMSE: {test_evaluation['RMSE']:.4f}\") print(f\"R2: {test_evaluation['R2']:.4f}\") # Aggregate all metrics all_metrics = { 'Average CV MAE': cv_metrics['Average MAE'], 'Average CV RMSE': cv_metrics['Average RMSE'], 'Average CV R2': cv_metrics['Average R2'], 'Test MAE': test_evaluation['MAE'], 'Test RMSE': test_evaluation['RMSE'], 'Test R2': test_evaluation['R2'] } # Optionally, save the trained model # joblib.dump(forecast_model, 'forecast_model.pkl') output_dict = { 'all_metrics': all_metrics, 'forecast_model': forecast_model, 'X': X, 'y': y, 'X_test': X_test, 'y_test': y_test, 'y_test_pred': y_test_pred } return output_dict", "source": "data_pipeline.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import pandas as pd from sklearn.base import BaseEstimator from typing import Optional class Imputer: \"\"\" Imputer Class for Handling Missing Values in Time Series Data This class provides methods to impute missing values in a time series dataset using various techniques. The class supports moving average imputation as well as imputation with a machine learning model. \"\"\" def __init__(self, data: pd.DataFrame, target: str, num_cols: list[str], ma_window: int = 24, imputation_model: Optional[BaseEstimator] = None): \"\"\" Initialize the Imputer with data and parameters for missing value handling. Parameters: - data (pd.DataFrame): The dataset containing missing values to be imputed. - target (str): Name of the target variable. - num_cols (list[str]): List of numerical feature column names. - ma_window (int): Window size for the moving average imputation. - imputation_model (BaseEstimator, optional): Regression model for imputing missing values. If None, default to RandomForestRegressor. \"\"\" self.data = data self.target = target self.num_cols = num_cols self.ma_window = ma_window self.imputation_model = imputation_model def moving_average_imputation(self): \"\"\" Impute missing values in the target column using a backward-looking moving average. \"\"\" self.data[self.target] = self.data[self.target].fillna( self.data[self.target].shift().rolling(window=self.ma_window, min_periods=1).mean() ) def model_based_imputation(self, model: Optional[BaseEstimator] = None): \"\"\" Impute missing values in the target column using a specified machine learning model. Parameters: - model (BaseEstimator, optional): Regression model for imputing missing values. If None, use the provided imputation_model attribute. \"\"\" model = model if model is not None else self.imputation_model if model is None: from sklearn.ensemble import RandomForestRegressor model = RandomForestRegressor(n_estimators=100, random_state=0) # Identify remaining missing values in the target remaining_na = self.data[self.target].isnull() if remaining_na.sum() > 0: # Prepare data for imputation predictors = [col for col in self.num_cols if col != self.target] train_data = self.data[~remaining_na & self.data[predictors].notnull().all(axis=1)] predict_data = self.data[remaining_na & self.data[predictors].notnull().all(axis=1)] X_train = train_data[predictors] y_train = train_data[self.target] X_predict = predict_data[predictors] # Fit the imputation model model.fit(X_train, y_train) predicted_values = model.predict(X_predict) # Fill the missing values self.data.loc[predict_data.index, self.target] = predicted_values def forward_fill_numerical(self): \"\"\" Impute missing values in numerical columns using forward fill. \"\"\" for col in self.num_cols: if self.data[col].isnull().sum() > 0: self.data[col] = self.data[col].fillna(method='ffill') def drop_remaining_nas(self): \"\"\" Drop any remaining rows with missing values. \"\"\" self.data.dropna(inplace=True) def run_imputation(self): \"\"\" Execute the full imputation process, including moving average, model-based imputation, forward fill for numerical features, and dropping any remaining rows with missing values. \"\"\" self.moving_average_imputation() self.model_based_imputation() self.forward_fill_numerical() self.drop_remaining_nas() def get_data(self) -> pd.DataFrame: \"\"\" Get the imputed dataset. Returns: - pd.DataFrame: The imputed dataset. \"\"\" return self.data", "source": "imputer.py"}, {"content": "# src/ml_experiment.py import pandas as pd from typing import List, Dict, Tuple from src.data_pipeline import DataPipeline from sklearn.ensemble import RandomForestRegressor def run_experiment( data_path: str, target: str = 'pm2.5', target_lags: List[int] = [1, 3, 6, 12], num_cols: List[str] = [\"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"], cat_cols: List[str] = [\"cbwd\"], max_lag: int = 24, diff_periods: int = 1, ma_window: int = 24, feature_strategy: str = 'both', imputation_model: RandomForestRegressor = RandomForestRegressor(n_estimators=100, random_state=0), time_series_split: int = 3, test_size: float = 0.2 ) -> Tuple[Dict[int, any], Dict[str, Dict[int, float]]]: \"\"\" Run a forecasting experiment for multiple forecast horizons (lags). Parameters: ---------- data_path : str Path to the CSV data file. target : str, optional Name of the target variable. Default is 'pm2.5'. target_lags : List[int], optional List of forecast horizons (in time steps) to evaluate. Default is [1, 3, 6, 12]. num_cols : List[str], optional List of numerical feature column names. Default is [\"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"]. cat_cols : List[str], optional List of categorical feature column names. Default is [\"cbwd\"]. max_lag : int, optional Maximum number of lagged time steps to create. Default is 24. diff_periods : int, optional Number of periods to use for differencing. Default is 1. ma_window : int, optional Window size for moving average imputation. Default is 24. feature_strategy : str, optional Strategy for feature engineering ('default', 'difference', 'both'). Default is 'both'. imputation_model : RandomForestRegressor, optional A scikit-learn estimator used for model-based imputation of missing values. time_series_split : int, optional Number of splits for TimeSeriesSplit cross-validation. Default is 3. test_size : float, optional The proportion of the data to be used as the test set. Default is 0.2. Returns: ------- Tuple[Dict[int, any], Dict[str, Dict[int, float]]] - A dictionary mapping each lag to its trained ForecastModel. - A dictionary containing evaluation metrics for each lag. \"\"\" # Initialize dictionaries to store models and metrics models_dict: Dict[int, any] = {} metrics_dict: Dict[str, Dict[int, float]] = { 'Average CV MAE': {}, 'Average CV RMSE': {}, 'Average CV R2': {}, 'Test MAE': {}, 'Test RMSE': {}, 'Test R2': {} } # Iterate over each lag for lag in target_lags: print(f\"\\n=== Forecast Horizon: {lag} Time Step(s) Ahead ===\\n\") # Initialize DataPipeline with the current target_lag pipeline = DataPipeline( file_path=data_path, target=target, num_cols=num_cols, cat_cols=cat_cols, max_lag=max_lag, diff_periods=diff_periods, ma_window=ma_window, feature_strategy=feature_strategy, imputation_model=imputation_model, target_lag=lag, time_series_split=time_series_split, test_size=test_size ) # Run the pipeline output = pipeline.run_pipeline(model=RandomForestRegressor(n_estimators=100, random_state=0)) # Extract metrics all_metrics = output['all_metrics'] models_dict[lag] = output['forecast_model'] # Store metrics metrics_dict['Average CV MAE'][lag] = all_metrics['Average CV MAE'] metrics_dict['Average CV RMSE'][lag] = all_metrics['Average CV RMSE'] metrics_dict['Average CV R2'][lag] = all_metrics['Average CV R2'] metrics_dict['Test MAE'][lag] = all_metrics['Test MAE'] metrics_dict['Test RMSE'][lag] = all_metrics['Test RMSE'] metrics_dict['Test R2'][lag] = all_metrics['Test R2'] return models_dict, metrics_dict # Example usage if __name__ == \"__main__\": data_file_path = 'data/beijing.csv' target_variable = 'pm2.5' forecast_lags = [1, 3, 6, 12] # Example lags models, metrics = run_experiment( data_path=data_file_path, target=target_variable, target_lags=forecast_lags, num_cols=[\"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"], cat_cols=[\"cbwd\"], max_lag=24, diff_periods=1, ma_window=24, feature_strategy='both', imputation_model=RandomForestRegressor(n_estimators=100, random_state=0), time_series_split=3, test_size=0.2 ) # Display metrics for metric_name, lag_metrics in metrics.items(): print(f\"\\n=== {metric_name} ===\") for lag, value in lag_metrics.items(): print(f\"Lag {lag}: {value:.4f}\")", "source": "ml_experiment.py"}, {"content": "# src/ml_model.py import numpy as np import pandas as pd from sklearn.base import BaseEstimator from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error from typing import Optional, Union, Dict class ForecastModel: \"\"\" ForecastModel Class for Training, Predicting, and Evaluating Machine Learning Models. This class provides a standardized interface for training machine learning models on time series data, making predictions, and evaluating model performance using various metrics. Parameters: ---------- model : BaseEstimator A scikit-learn-like estimator (e.g., RandomForestRegressor, LinearRegression). This allows for flexibility in choosing different forecasting models. Attributes: ---------- model : BaseEstimator The machine learning model used for forecasting. trained : bool Flag indicating whether the model has been trained. \"\"\" def __init__(self, model: BaseEstimator): \"\"\" Initialize the ForecastModel with a specified machine learning estimator. Parameters: ---------- model : BaseEstimator A scikit-learn-like estimator for forecasting. \"\"\" self.model = model self.trained = False def fit(self, X: pd.DataFrame, y: pd.Series) -> None: \"\"\" Train the machine learning model using the provided training data. Parameters: ---------- X : pd.DataFrame Training features. y : pd.Series Training target variable. Returns: ------- None \"\"\" self.model.fit(X, y) self.trained = True print(f\"Model {self.model.__class__.__name__} trained successfully.\") def predict(self, X: pd.DataFrame) -> np.ndarray: \"\"\" Make predictions using the trained machine learning model. Parameters: ---------- X : pd.DataFrame Features for which to make predictions. Returns: ------- np.ndarray Predicted values. Raises: ------ ValueError: If the model has not been trained yet. \"\"\" if not self.trained: raise ValueError(\"Model must be trained before making predictions.\") predictions = self.model.predict(X) return predictions def evaluate( self, y_true: pd.Series, y_pred: np.ndarray, metrics: Optional[list[str]] = None ) -> Dict[str, float]: \"\"\" Evaluate the model's predictions against the true values using specified metrics. Parameters: ---------- y_true : pd.Series Actual target values. y_pred : np.ndarray Predicted target values. metrics : list of str, optional List of evaluation metrics to compute. Supported metrics are: 'mae' - Mean Absolute Error 'mse' - Mean Squared Error 'rmse' - Root Mean Squared Error 'r2' - R-squared If None, all supported metrics will be computed. Returns: ------- Dict[str, float] Dictionary containing the computed evaluation metrics. Raises: ------ ValueError: If an unsupported metric is provided. \"\"\" if metrics is None: metrics = ['mae', 'mse', 'rmse', 'r2'] supported_metrics = ['mae', 'mse', 'rmse', 'r2'] for metric in metrics: if metric not in supported_metrics: raise ValueError(f\"Unsupported metric '{metric}'. Supported metrics are {supported_metrics}.\") evaluation_results = {} if 'mae' in metrics: evaluation_results['MAE'] = mean_absolute_error(y_true, y_pred) if 'mse' in metrics: evaluation_results['MSE'] = root_mean_squared_error(y_true, y_pred) ** 2 if 'rmse' in metrics: evaluation_results['RMSE'] = root_mean_squared_error(y_true, y_pred) if 'r2' in metrics: evaluation_results['R2'] = r2_score(y_true, y_pred) return evaluation_results def get_model(self) -> BaseEstimator: \"\"\" Retrieve the underlying machine learning model. Returns: ------- BaseEstimator The machine learning model instance. \"\"\" return self.model", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "# src/train_model.py # to run : python -m src.train_model \"\"\" This script is for training a Random Forest model for time series forecasting. \"\"\" import os import logging from omegaconf import DictConfig from src.data_pipeline import DataPipeline # Directly import DataPipeline import hydra import mlflow from sklearn.ensemble import RandomForestRegressor import src.general_utils # Disable pylint warning for missing parameters (since Hydra injects them) # pylint: disable=no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(args: DictConfig): \"\"\" Main function for training the Random Forest model. Parameters ---------- args : omegaconf.DictConfig Configuration parameters from Hydra. \"\"\" # Initialize logger logger = logging.getLogger(__name__) # Setup logging using general_utils general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) logger.info(\"Logging configuration has been set up.\") # Initialize MLflow using general_utils logger.info(\"Initializing MLflow.\") mlflow_init_status, mlflow_run = general_utils.mlflow_init( args, setup_mlflow=args.setup_mlflow, autolog=args.mlflow_autolog ) if mlflow_init_status: logger.info(\"MLflow has been initialized successfully.\") else: logger.warning(\"MLflow initialization failed. Proceeding without MLflow.\") # Initialize DataPipeline with configuration parameters logger.info(\"Initializing DataPipeline.\") data_pipeline = DataPipeline( file_path=args.data_dir_path, target=args.general.target, num_cols=args.general.num_cols, cat_cols=args.general.cat_cols, max_lag=args.params.max_lag if 'max_lag' in args.params else 24, # Default to 24 if not swept diff_periods=args.params.diff_periods if 'diff_periods' in args.params else 1, ma_window=args.params.ma_window if 'ma_window' in args.params else 24, feature_strategy=args.params.feature_strategy if 'feature_strategy' in args.params else 'both', imputation_model=RandomForestRegressor(n_estimators=args.params.model__n_estimators if 'model__n_estimators' in args.params else 100, max_depth=args.params.model__max_depth if 'model__max_depth' in args.params else None, random_state=0), target_lag=args.params.general.target_lag if 'general.target_lag' in args.params else 1, time_series_split=args.general.time_series_split, test_size=args.general.test_size ) logger.info(\"DataPipeline has been initialized.\") # Run the pipeline logger.info(\"Running the data processing and modeling pipeline.\") output = data_pipeline.run_pipeline(model=RandomForestRegressor( n_estimators=args.params.model__n_estimators if 'model__n_estimators' in args.params else 100, max_depth=args.params.model__max_depth if 'model__max_depth' in args.params else None, random_state=0 )) # Extract metrics and trained model all_metrics = output['all_metrics'] trained_model = output['forecast_model'] # Log metrics to MLflow if mlflow_init_status: for metric_name, value in all_metrics.items(): # Log each metric with a unique name if necessary general_utils.mlflow_log( mlflow_init_status, \"log_metric\", key=metric_name, value=value ) logger.info(\"Metrics have been logged to MLflow.\") # Log hyperparameters as parameters if mlflow_init_status: # Flatten the configuration dictionary for logging hyperparameters = { key: value for key, value in args.items() if key not in ['general', 'hydra'] } def flatten_dict(d, parent_key='', sep='__'): \"\"\" Recursively flattens a nested dictionary. Parameters: ---------- d : dict The dictionary to flatten. parent_key : str, optional The base key string for recursion. sep : str, optional The separator between parent and child keys. Returns: ------- dict A flattened dictionary. \"\"\" items = [] for k, v in d.items(): new_key = f\"{parent_key}{sep}{k}\" if parent_key else k if isinstance(v, DictConfig): items.extend(flatten_dict(v, new_key, sep=sep).items()) else: items.append((new_key, v)) return dict(items) hyperparameters_flat = flatten_dict(hyperparameters) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=hyperparameters_flat ) logger.info(\"Hyperparameters have been logged to MLflow.\") # Log the entire configuration as an artifact if mlflow_init_status: # Convert DictConfig to a standard dictionary config_dict = hydra.utils.to_absolute_path(args) config_path = os.path.join(args.artifact_dir_path, \"train_model_config.yaml\") os.makedirs(args.artifact_dir_path, exist_ok=True) with open(config_path, \"w\") as f: import yaml yaml.dump(hydra.utils.to_container(args, resolve=True), f) # Log the configuration file as an artifact in MLflow general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=config_path, artifact_path=\"config\" ) logger.info(\"Configuration file has been logged as an artifact.\") # Save and log the trained model as an artifact if mlflow_init_status: model_path = os.path.join(args.artifact_dir_path, f\"trained_model_target_lag_{data_pipeline.target_lag}.pkl\") import joblib # Serialize and save the model joblib.dump(trained_model, model_path) # Log the serialized model as an artifact in MLflow general_utils.mlflow_log(", "source": "train_model.py"}, {"content": "mlflow_init_status, \"log_artifact\", local_path=model_path, artifact_path=f\"models/target_lag_{data_pipeline.target_lag}\" ) logger.info(f\"Trained model for target_lag {data_pipeline.target_lag} has been logged as an artifact.\") # Finalize MLflow run if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed without MLflow.\") # Optionally, log metrics to console for metric_name, value in all_metrics.items(): logger.info(f\"{metric_name}: {value:.4f}\") # Since you prefer not to return undefined parameters, we omit the return statement # If necessary, you can return `all_metrics` or other relevant information # return all_metrics if __name__ == \"__main__\": main()", "source": "train_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "import pytest import pandas as pd from sklearn.ensemble import RandomForestRegressor from src.datapipeline import DataPipeline # Import your DataPipeline class here @pytest.fixture def processor(): # Replace 'test_data.csv' with the path to a small test dataset file_path = 'test_data.csv' target = 'pm2.5' num_cols = [\"pm2.5\", \"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"] cat_cols = [\"cbwd\"] processor = DataPipeline( file_path=file_path, target=target, num_cols=num_cols, cat_cols=cat_cols, max_lag=3, diff_periods=1, n_splits=3, imputation_model=RandomForestRegressor(n_estimators=10, random_state=0), ma_window=2, feature_strategy='both' ) processor.impute_missing_values() processor.create_differenced_features() processor.create_lagged_features() processor.process_categorical() return processor def test_lagged_values(processor): # Check if the lagged values are correctly lagged for both difference and original values data = processor.get_processed_data() for lag in range(1, processor.max_lag + 1): for col in processor.num_cols: # Check original lagged columns lagged_col = f'{col}_lag_{lag}' if lagged_col in data.columns: assert (data[col].shift(lag) == data[lagged_col]).all(), f\"{lagged_col} is not correctly lagged\" # Check differenced lagged columns diff_col = f'{col}_diff_{processor.diff_periods}' diff_lagged_col = f'{diff_col}_lag_{lag}' if diff_lagged_col in data.columns: assert (data[diff_col].shift(lag) == data[diff_lagged_col]).all(), f\"{diff_lagged_col} is not correctly lagged\" def test_no_missing_values(processor): # Ensure there are no missing values in the final dataset data = processor.get_processed_data() assert data.isnull().sum().sum() == 0, \"There are missing values in the processed data\" def test_column_names(processor): # Check if the expected columns are present in the final dataset data = processor.get_processed_data() expected_columns = set() # Add original and differenced columns based on the strategy for col in processor.num_cols: if processor.feature_strategy in ['default', 'both']: expected_columns.update({f'{col}_lag_{lag}' for lag in range(1, processor.max_lag + 1)}) if processor.feature_strategy in ['difference', 'both']: diff_col = f'{col}_diff_{processor.diff_periods}' expected_columns.add(diff_col) expected_columns.update({f'{diff_col}_lag_{lag}' for lag in range(1, processor.max_lag + 1)}) # Add one-hot encoded columns for categorical variables for cat_col in processor.cat_cols: expected_columns.update({col for col in data.columns if cat_col in col}) # Verify the expected columns actual_columns = set(data.columns) assert expected_columns.issubset(actual_columns), \"The expected columns are not present in the processed data\" def test_feature_strategy(processor): # Verify that feature strategy 'both' creates both lagged and differenced columns data = processor.get_processed_data() for col in processor.num_cols: for lag in range(1, processor.max_lag + 1): original_lag = f'{col}_lag_{lag}' diff_lag = f'{col}_diff_{processor.diff_periods}_lag_{lag}' assert original_lag in data.columns, f\"Missing {original_lag} in 'both' strategy\" assert diff_lag in data.columns, f\"Missing {diff_lag} in 'both' strategy\" def test_time_series_split(processor): # Ensure that the time series split returns the correct number of splits and sizes splits = processor.time_series_split() assert len(splits) == processor.n_splits, f\"Expected {processor.n_splits} splits but got {len(splits)}\" # Check that each split's training data is before its test data (to maintain temporal order) for train_data, test_data in splits: assert train_data.index.max() < test_data.index.min(), \"Train data must be before test data in time series split\"", "source": "test_dataprocessor.py"}, {"content": "import pandas as pd # Path to the original dataset file_path = 'data/beijing.csv' # Read the full dataset df = pd.read_csv(file_path) # Sample a subset of the data (e.g., 5% of the data or the first 500 rows) # Option 1: Sample 5% of the data randomly subset = df.sample(frac=0.05, random_state=42) # Option 2: Take the first 500 rows as a simple test set # subset = df.head(500) # Save the sampled subset to a new CSV file subset_file_path = 'data/test_data.csv' subset.to_csv(subset_file_path, index=False) print(f\"Sampled subset saved to {subset_file_path}\")", "source": "test_data_extractor.py"}, {"content": "import os import logging import omegaconf import hydra import mlflow import src.general_utils as general_utils from src.data_loader import DataLoader from src.A7P1.text_processor import TextProcessor from src.preprocessing_utils import check_and_remove_artifacts, train_validation_test_splitting from src.A7P1.pipeline_utils import ModifiedPipeline from src.A7P1.evaluation import evaluate_model from sklearn.linear_model import LogisticRegression import pandas as pd # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(args): \"\"\"This is the main function for training the sentiment analysis model. Parameters ---------- args : omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" print(f\"Original working directory: {hydra.utils.get_original_cwd()}\") logging_conf = os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\") print(f\"logging_conf:{logging_conf}\") logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) # Initialize MLflow if enabled mlflow_init_status, mlflow_run = general_utils.mlflow_init( args, setup_mlflow=args[\"setup_mlflow\"], autolog=args[\"mlflow_autolog\"] ) # Log sweeper params general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"model\": args[\"model\"][\"name\"], \"C\": args[\"model\"][\"C\"], \"penalty\": args[\"model\"][\"penalty\"], }, ) # Load and preprocess data logger.info(\"Loading and preprocessing data.\") data_file_path = os.path.join(hydra.utils.get_original_cwd(), args[\"data\"][\"path\"]) if os.path.exists(data_file_path): logger.info(f\"Data file found at {data_file_path}. Skipping data loading from server.\") # Load data from the file directly data = pd.read_csv(data_file_path) else: # Load and preprocess data using DataLoader logger.info(\"Data file not found. Loading data from server.\") data_loader = DataLoader( server=args[\"data\"][\"server\"], database=args[\"data\"][\"database\"], username=args[\"data\"][\"username\"], password=args[\"data\"][\"password\"], driver=args[\"data\"][\"driver\"] ) data = data_loader.load_data() data, _ = check_and_remove_artifacts(data) X_train_validation, X_test, y_train_validation, y_test = train_validation_test_splitting( data, test_size=args[\"preprocessing\"][\"train_test_split\"], random_state=args[\"training\"][\"random_seed\"] ) # Define the model (Logistic Regression) model = LogisticRegression( penalty=args[\"model\"][\"penalty\"], tol=args[\"model\"][\"tol\"], C=args[\"model\"][\"C\"], fit_intercept=args[\"model\"][\"fit_intercept\"], solver=args[\"model\"][\"solver\"], random_state=args[\"training\"][\"random_seed\"] ) # Instantiate the text processor text_processor = TextProcessor( stop_words=args[\"preprocessing\"][\"stop_words\"], use_tfidf=args[\"preprocessing\"][\"use_tf_idf\"], max_features=args[\"preprocessing\"][\"max_features\"], ngrams=tuple(args[\"preprocessing\"][\"ngrams\"]), transformer=args[\"preprocessing\"][\"transformer\"], apply_svd=args[\"preprocessing\"][\"apply_svd\"], n_components=args[\"preprocessing\"][\"n_components\"] ) # Instantiate the pipeline pipeline = ModifiedPipeline(processor=text_processor, model=model) scoring_metric = args[\"training\"][\"scoring_metric\"] # Evaluate the model metrics = evaluate_model( pipeline=pipeline, X_train=X_train_validation, y_train=y_train_validation, X_test=X_test, y_test=y_test, cv=args[\"training\"][\"k_folds\"], scoring_metric=scoring_metric ) # Log metrics general_utils.mlflow_log( mlflow_init_status, \"log_metrics\", metrics={ \"cv_mean_score\": metrics[\"mean_cv_score\"], \"cv_std_score\": metrics[\"std_cv_score\"], \"test_accuracy\": metrics[\"accuracy\"], \"test_precision\": metrics[\"precision\"], \"test_recall\": metrics[\"recall\"], \"test_f1\": metrics[\"f1\"], \"test_roc_auc\": metrics[\"roc_auc\"], \"test_log_loss\": metrics[\"log_loss\"], } ) # Save the model if needed if args[\"model_checkpoint_interval\"] > 0: # Ensure the checkpoint directory exists model_checkpoint_dir = os.path.dirname(os.path.join(args[\"model_checkpoint_path\"], \"model.pkl\")) os.makedirs(model_checkpoint_dir, exist_ok=True) model_checkpoint_path = os.path.join(args[\"model_checkpoint_path\"], \"model.pkl\") pipeline.save(model_checkpoint_path) logger.info(\"Model checkpoint saved at %s\", model_checkpoint_path) general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model_checkpoint_path, artifact_path=\"model\" ) # Log the configuration used for the run general_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) # Finalize the MLflow run if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return metrics[\"log_loss\"], metrics[scoring_metric] if __name__ == \"__main__\": main()", "source": "A7P1_train_model.py"}, {"content": "import numpy as np class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Given encoder hidden states and decoder hidden state S0 = np.array([0.3, 0.11, 0.9, 0.5]) S1 = np.array([0.8, 0.3, 0.7, 0.1]) S2 = np.array([0.5, 0.3, 0.4, 0.8]) T1 = np.array([0.2, 0.7, 0.9, 0.3]) # Compute alignment scores alignment_scores = np.array([np.dot(S0, T1), np.dot(S1, T1), np.dot(S2, T1)]) # Compute attention weights using softmax attention_weights = np.exp(alignment_scores) / np.sum(np.exp(alignment_scores)) # Compute the context vector c1 c1 = np.dot(attention_weights, np.array([S0, S1, S2])) # Round the context vector to 2 decimal places c1_rounded = np.round(c1, 2) # Write the context vector as a list vector = [c1_rounded.tolist()] # Print the context vector for verification print(vector)", "source": "context_vector.py"}, {"content": "import os from dotenv import dotenv_values import pyodbc import pandas as pd class DataLoader: def __init__(self, server, database, username, password, driver): self.server = server self.database = database self.username = username self.password = password self.driver = driver def load_data(self): # Connection string connection_string = f'DRIVER={self.driver};SERVER={self.server};DATABASE={self.database};UID={self.username};PWD={self.password}' # Establish connection conn = None data = [] try: conn = pyodbc.connect(connection_string) print(\"Connection successful!\") # Create a cursor object cursor = conn.cursor() # Execute query query = \"SELECT * FROM imdb\" cursor.execute(query) # Fetch the result rows = cursor.fetchall() # Store the result in a dataframe for row in rows: data.append({ 'text': row.text, 'label': row.label }) except pyodbc.Error as e: print(f\"Error connecting to the database: {e}\") finally: # Close the connection if conn: conn.close() print(\"Connection closed.\") data = pd.DataFrame(data) return data def main(): # Load environment variables from .venv file config = dotenv_values(\".env\") # Retrieve parameters from environment variables server = config.get('server') database = config.get('database') username = config.get('username') password = config.get('password') driver = config.get('driver') # Create an instance of DataLoader data_loader = DataLoader(server, database, username, password, driver) # Load data data = data_loader.load_data() # save data to csv data.to_csv('data/data.csv', index=False) print(f\"Total number of records: {len(data)} loaded and saved successfully!\") if __name__ == \"__main__\": main()", "source": "data_loader.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(f\"MLflow initialisation has failed. {e}\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import pandas as pd import re from sklearn.model_selection import train_test_split from typing import Tuple, Union import random import csv import torch from torch.utils.data import Dataset class CustomDataset(Dataset): def __init__(self, texts, labels, tokenizer, max_length): self.texts = texts self.labels = labels self.tokenizer = tokenizer self.max_length = max_length def __len__(self): return len(self.texts) def __getitem__(self, idx): text = self.texts[idx] label = self.labels[idx] encoding = self.tokenizer.encode_plus( text, add_special_tokens=True, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt' ) return { 'input_ids': encoding['input_ids'].squeeze(), 'attention_mask': encoding['attention_mask'].squeeze(), 'labels': torch.tensor(label, dtype=torch.long) } def sample_data(input_data: Union[str, pd.DataFrame], output_file: str = \"./data/sample_data.csv\", sample_fraction: float = 0.1, random_seed: int = 42, chunk_size: int = 10000) -> pd.DataFrame: \"\"\" Efficiently samples a portion of the data from a large CSV file or a Pandas DataFrame and saves it to a new CSV file. Args: input_data (Union[str, pd.DataFrame]): The path to the input CSV file or a Pandas DataFrame. output_file (str): The path where the sampled data will be saved. sample_fraction (float): The fraction of data to sample (default is 10%). random_seed (int): The random seed for reproducibility (default is 42). chunk_size (int): The number of rows per chunk to read from the CSV file (default is 10,000), used only if input_data is a file path. Returns: pd.DataFrame: The sampled data. \"\"\" random.seed(random_seed) sampled_rows = [] if isinstance(input_data, pd.DataFrame): # If a DataFrame is passed, sample directly sampled_data = input_data.sample(frac=sample_fraction, random_state=random_seed) else: # Process the file in chunks if a file path is provided for chunk in pd.read_csv(input_data, chunksize=chunk_size): sampled_chunk = chunk.sample(frac=sample_fraction, random_state=random_seed) sampled_rows.append(sampled_chunk) # Concatenate all sampled chunks sampled_data = pd.concat(sampled_rows) # Save the sampled data to CSV sampled_data.to_csv(output_file, index=False) print(f\"Sampled data saved to {output_file}\") return sampled_data def check_and_remove_artifacts( df, column='text', flag_column='html_flag', new_column='modified_text', html_regex_exp=\"<(\\\"[^\\\"]*\\\"|'[^']*'|[^'\\\">])*>\" ): \"\"\" Checks for HTML tags, backslashes, and double dashes in the specified column of the DataFrame. Removes these artifacts if detected, and creates a new cleaned column. Parameters: - df (pd.DataFrame): The DataFrame containing the text data. - column (str): The column name to check and clean. - flag_column (str): The name of the flag column to indicate artifact presence. - new_column (str): The name of the new column to store cleaned text. - html_regex_exp (str): Regex pattern for HTML tags. - backslash_regex_exp (str): Regex pattern for backslashes. - double_dash_regex_exp (str): Regex pattern for double dashes. Returns: - tuple: A tuple containing: - pd.DataFrame: The original DataFrame with artifacts removed. - pd.DataFrame: A DataFrame containing rows where artifacts were still detected after cleaning. - int: The count of rows where artifacts remain even after cleaning. \"\"\" # Compile the combined regex pattern html_pattern = re.compile(html_regex_exp, re.IGNORECASE) # Step 1: Create the flag column to indicate rows with artifacts df[flag_column] = df[column].apply(lambda x: 1 if html_pattern.search(x) else 0) # Step 2: Create the cleaned text column by only processing rows with artifacts def clean_text_if_needed(row): if row[flag_column] == 1: cleaned_text = html_pattern.sub('', row[column]) # Ensure that no artifacts remain in the cleaned text if html_pattern.search(cleaned_text): # Artifacts remain after cleaning, log this for reporting artifacts_remaining.append({ \"original_text\": row[column], \"cleaned_text\": cleaned_text, \"index\": row.name }) return cleaned_text return row[column] # List to store artifacts that remain after cleaning artifacts_remaining =", "source": "preprocessing_utils.py"}, {"content": "[] # Apply the cleaning function conditionally df[new_column] = df.apply(clean_text_if_needed, axis=1) # Create a DataFrame of rows where artifacts remain after cleaning artifacts_remaining_df = pd.DataFrame(artifacts_remaining) # Log the count of texts with remaining artifacts remaining_count = len(artifacts_remaining_df) print(f\"Number of texts that still have artifacts after cleaning: {remaining_count}\") # Replace the original DataFrame with the cleaned version df = df.drop(columns=[column, flag_column]) df = df.rename(columns={new_column: column}) return df, artifacts_remaining_df def train_validation_test_splitting(data: pd.DataFrame, test_size: float = 0.2, random_state: int = 42, stratify: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: \"\"\" Splits the data into training, validation, and testing sets. \"\"\" if stratify: stratify_val = data[\"label\"] else: stratify_val = None train_validation_data, test_data = train_test_split(data, test_size=test_size, random_state=random_state, stratify=stratify_val) X_train_validation_data = train_validation_data['text'] y_train_validation_data = train_validation_data['label'] X_test_data = test_data['text'] y_test_data = test_data['label'] return X_train_validation_data, X_test_data, y_train_validation_data, y_test_data", "source": "preprocessing_utils.py"}, {"content": "from sklearn.model_selection import cross_val_score from sklearn.metrics import ( accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, log_loss, confusion_matrix ) from typing import Any, Dict, Tuple import numpy as np from src.A7P1.pipeline_utils import ModifiedPipeline import pandas as pd def evaluate_model( pipeline: ModifiedPipeline, X_train: pd.Series, y_train: pd.Series, X_test: pd.Series, y_test: pd.Series, cv: int = 5, scoring_metric: str = 'accuracy' ) -> Dict[str, Any]: \"\"\" Evaluate the model using cross-validation and test data evaluation. Parameters ---------- pipeline : ModifiedPipeline The pipeline to train and evaluate. X_train : pd.Series Training data. y_train : pd.Series Training labels. X_test : pd.Series Test data. y_test : pd.Series Test labels. cv : int, optional Number of cross-validation folds, by default 5. scoring_metric : str, optional The metric to optimize during cross-validation, by default 'accuracy'. Returns ------- Dict[str, Any] A dictionary containing cross-validation scores, test accuracy, and other metrics. \"\"\" print(f\"Starting cross-validation with scoring: {scoring_metric}...\") # Convert string labels to numerical values y_train = y_train.replace({'neg': 0, 'pos': 1}) y_test = y_test.replace({'neg': 0, 'pos': 1}) # Extract the text processor and classifier from the pipeline text_processor = pipeline.pipeline.named_steps['text_processor'] model = pipeline.pipeline.named_steps['classifier'] # Transform the training data for cross-validation X_train_transformed = text_processor.fit_transform(X_train) # Perform cross-validation with the selected scoring metric cv_scores = cross_val_score(model, X_train_transformed, y_train, cv=cv, scoring=scoring_metric) print(f\"Cross-validation scores ({scoring_metric}): {cv_scores}\") print(f\"Mean cross-validation score ({scoring_metric}): {np.mean(cv_scores):.2f}\") print(f\"Standard deviation of cross-validation scores ({scoring_metric}): {np.std(cv_scores):.2f}\") # Fit the entire pipeline on the training data print(\"Training the model on the entire training set...\") pipeline.fit(X_train, y_train) # Predict on the test set print(\"Evaluating the model on the test set...\") y_pred = pipeline.predict(X_test) y_pred_proba = pipeline.predict_proba(X_test)[:, 1] # Calculate various metrics metrics = { 'accuracy': accuracy_score(y_test, y_pred), 'precision': precision_score(y_test, y_pred), 'recall': recall_score(y_test, y_pred), 'f1': f1_score(y_test, y_pred), 'roc_auc': roc_auc_score(y_test, y_pred_proba), 'log_loss': log_loss(y_test, y_pred_proba), 'confusion_matrix': confusion_matrix(y_test, y_pred) } # Print out the metrics for better readability print(\"\\nTest Metrics:\") for metric_name, metric_value in metrics.items(): if metric_name == 'confusion_matrix': print(f\"{metric_name}:\\n{metric_value}\") else: print(f\"{metric_name}: {metric_value:.4f}\") # Add cross-validation results to the metrics dictionary metrics['cv_scores'] = cv_scores metrics['mean_cv_score'] = np.mean(cv_scores) metrics['std_cv_score'] = np.std(cv_scores) return metrics", "source": "evaluation.py"}, {"content": "import joblib import pandas as pd from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin import shap from typing import Any, List, Tuple import numpy as np # Defines a transformer that applies a text processor so that it can be used in a pipeline class TextProcessorTransformer(BaseEstimator, TransformerMixin): \"\"\" A custom transformer to apply the text processor in a scikit-learn pipeline. Parameters ---------- processor : object A text processor instance with `fit_transform` and `transform` methods. \"\"\" def __init__(self, processor: Any): self.processor = processor self.feature_names_ = None def fit(self, X, y=None): X_transformed, feature_names = self.processor.fit_transform(X) self.feature_names_ = feature_names return self def transform(self, X): return self.processor.transform(X) class ModifiedPipeline: \"\"\" A class to encapsulate the creation, training, evaluation, and saving of a text processing pipeline. \"\"\" def __init__(self, processor: Any, model: Any): \"\"\" Initializes the ModifiedPipeline with a text processor and a classification model. Parameters ---------- processor : object A text processor instance with `fit_transform` and `transform` methods. model : object A scikit-learn compatible classifier. \"\"\" self.pipeline = Pipeline([ ('text_processor', TextProcessorTransformer(processor)), ('classifier', model) ]) self.feature_names_ = None def fit(self, X_train, y_train) -> None: \"\"\" Fits the pipeline on the training data. Parameters ---------- X_train : pd.Series Training data. y_train : pd.Series Training labels. \"\"\" self.pipeline.fit(X_train, y_train) self.feature_names_ = self.pipeline.named_steps['text_processor'].feature_names_ def predict(self, X_test) -> List: \"\"\" Predicts labels for the test data using the trained pipeline. Parameters ---------- X_test : pd.Series Test data. Returns ------- List Predicted labels for the test data. \"\"\" return self.pipeline.predict(X_test) def predict_proba(self, X_test) -> np.ndarray: \"\"\" Predicts probabilities for the test data using the trained pipeline. Parameters ---------- X_test : pd.Series Test data. Returns ------- np.ndarray Predicted probabilities for each class for the test data. \"\"\" # Check if the classifier supports predict_proba if hasattr(self.pipeline.named_steps['classifier'], 'predict_proba'): return self.pipeline.predict_proba(X_test) else: raise AttributeError(\"The classifier does not support probability predictions (predict_proba).\") def save(self, path: str) -> None: \"\"\" Saves the pipeline to a specified path using joblib. Parameters ---------- path : str Path to save the pipeline. \"\"\" joblib.dump(self.pipeline, path) print(f\"Pipeline saved to {path}.\") def load(self, path: str) -> None: \"\"\" Loads a pipeline from a specified path using joblib. Parameters ---------- path : str Path to load the pipeline from. \"\"\" self.pipeline = joblib.load(path) self.feature_names_ = self.pipeline.named_steps['text_processor'].feature_names_ print(f\"Pipeline loaded from {path}.\") def explain_with_shap(self, X_sample: pd.Series, max_display: int = 10) -> Tuple[shap.Explanation, pd.DataFrame]: \"\"\" Generates SHAP values for the model using the provided samples. Parameters ---------- X_sample : pd.Series Sample texts to analyze with SHAP. max_display : int, optional Maximum number of features to display in SHAP plots, by default 10. Returns ------- Tuple[shap.Explanation, pd.DataFrame] SHAP values explanation and a DataFrame of transformed features. \"\"\" # Get the classifier from the pipeline model = self.pipeline.named_steps['classifier'] # Get transformed features for SHAP analysis sample_features = self.pipeline.named_steps['text_processor'].transform(X_sample) sample_features = sample_features.todense() feature_df = pd.DataFrame(sample_features, columns=self.feature_names_) # Initialize the SHAP explainer and calculate values explainer = shap.Explainer(model, feature_df) shap_values = explainer(feature_df) # Plot SHAP values shap.plots.waterfall(shap_values[0], max_display=max_display) shap.summary_plot(shap_values, feature_df, plot_type='bar') return shap_values, feature_df", "source": "pipeline_utils.py"}, {"content": "import nltk import pandas as pd from nltk.tokenize import word_tokenize from nltk.corpus import stopwords from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer from sklearn.decomposition import TruncatedSVD from sklearn.feature_selection import SelectKBest, chi2 from nltk.stem import PorterStemmer, WordNetLemmatizer, LancasterStemmer, SnowballStemmer from typing import List, Tuple, Union from scipy.sparse import spmatrix import numpy as np # Ensure necessary NLTK resources are downloaded nltk.download('punkt') nltk.download('wordnet') nltk.download('omw-1.4') nltk.download('stopwords') class TextProcessor: def __init__(self, stop_words=None, use_tfidf=True, max_features=1000, ngrams=(1, 1), transformer=None, apply_svd=False, n_components=300): \"\"\" Initializes the TextProcessor with various parameters for text preprocessing. Parameters ---------- stop_words : list or None List of stopwords to remove. Uses NLTK's stopwords if None. use_tfidf : bool Whether to use TF-IDF (True) or CountVectorizer (False). max_features : int Maximum number of features (words) to retain. ngrams : tuple The range of n-grams to include (e.g., (1, 2) for unigrams and bigrams). transformer : object or None An instance of a stemmer or lemmatizer from NLTK (e.g., PorterStemmer(), WordNetLemmatizer()). apply_svd : bool Whether to apply Truncated SVD for dimensionality reduction. n_components : int Number of components for SVD if apply_svd is True. \"\"\" self.stop_words = stop_words if stop_words else set(stopwords.words('english')) self.use_tfidf = use_tfidf self.max_features = max_features self.ngrams = ngrams self.apply_svd = apply_svd self.n_components = n_components self.vectorizer = None self.svd = None # Store the transformer (stemmer or lemmatizer) self.transformer = transformer def tokenize(self, text): \"\"\" Tokenizes, stems/lemmatizes, and removes stopwords from the input text. Parameters ---------- text : str A single string of text. Returns ------- str A cleaned and processed version of the text. \"\"\" # Tokenize the cleaned text into words tokens = word_tokenize(text) # Remove stopwords tokens = [token for token in tokens if token not in self.stop_words] # Apply the specified stemmer or lemmatizer if self.transformer: if hasattr(self.transformer, 'lemmatize'): tokens = [self.transformer.lemmatize(token) for token in tokens] elif hasattr(self.transformer, 'stem'): tokens = [self.transformer.stem(token) for token in tokens] else: raise ValueError( \"The provided transformer must have a 'lemmatize' or 'stem' method.\" f\" Received {self.transformer} which has no such method.\" ) # Join the tokens to form the cleaned text processed_text = ' '.join(tokens) return processed_text def fit_transform(self, documents: List[str] ) -> Tuple[spmatrix, List[str]]: \"\"\" Fits the vectorizer on the given documents and applies optional dimensionality reduction or feature selection. Parameters ---------- documents : list of str A list of text documents. Returns ------- sparse_matrix Transformed feature matrix. \"\"\" # Choose the vectorizer: TF-IDF or CountVectorizer if self.use_tfidf: self.vectorizer = TfidfVectorizer(max_features=self.max_features, ngram_range=self.ngrams) else: self.vectorizer = CountVectorizer(max_features=self.max_features, ngram_range=self.ngrams) # Fit and transform the documents X = self.vectorizer.fit_transform(documents) feature_names = self.vectorizer.get_feature_names_out() # Apply SVD for dimensionality reduction if specified if self.apply_svd: self.svd = TruncatedSVD(n_components=self.n_components) X = self.svd.fit_transform(X) feature_names = [f\"component_{i}\" for i in range(self.n_components)] return X, feature_names def transform(self, documents): \"\"\" Transforms new documents using the already fitted vectorizer. Parameters ---------- documents : list of str A list of text documents. Returns ------- sparse_matrix Transformed feature matrix for the new documents. \"\"\" X = self.vectorizer.transform(documents) if self.apply_svd and self.svd: X = self.svd.transform(X) return X", "source": "text_processor.py"}, {"content": "import nltk from nltk.translate.bleu_score import sentence_bleu from nltk.translate.meteor_score import meteor_score from collections import Counter import math import numpy as np # 1. Perplexity Calculation def calculate_perplexity(probabilities: list) -> float: \"\"\" Calculate the perplexity of a language model based on the probabilities of predicted tokens. Args: probabilities (list): A list of probabilities corresponding to the predicted tokens. Returns: float: The perplexity score (lower is better). \"\"\" n = len(probabilities) log_sum = sum(math.log(p) for p in probabilities) perplexity = math.exp(-log_sum / n) print(f\"Perplexity: {perplexity}\") return perplexity # 2. BLEU Score Calculation def calculate_bleu(reference: list, candidate: list, weights=(0.25, 0.25, 0.25, 0.25)) -> float: \"\"\" Calculate BLEU score between a reference and candidate sentence using n-gram overlap. Args: reference (list): List of reference text. candidate (list): List of generated text. weights (tuple): Weights for BLEU n-gram precision, default is (0.25, 0.25, 0.25, 0.25). Returns: float: BLEU score (higher is better). \"\"\" score = sentence_bleu([reference], candidate, weights) print(f\"BLEU Score: {score}\") return score # 3. ROUGE Calculation (ROUGE-N for example) def calculate_rouge_n(reference: list, candidate: list, n: int = 2) -> float: \"\"\" Calculate ROUGE-N score by measuring n-gram overlap between reference and candidate text. Args: reference (list): List of reference text. candidate (list): List of generated text. n (int): The length of n-grams to use, default is 2 (bigram). Returns: float: ROUGE-N score (higher is better). \"\"\" ref_ngrams = list(nltk.ngrams(reference, n)) cand_ngrams = list(nltk.ngrams(candidate, n)) ref_count = Counter(ref_ngrams) cand_count = Counter(cand_ngrams) overlap = sum((ref_count & cand_count).values()) rouge_n = overlap / max(len(ref_ngrams), 1) print(f\"ROUGE-{n} Score: {rouge_n}\") return rouge_n # 4. METEOR Score Calculation def calculate_meteor(reference: str, candidate: str) -> float: \"\"\" Calculate METEOR score between a reference and candidate sentence considering exact matches, stemming, synonyms. Args: reference (str): Reference text as a string. candidate (str): Generated text as a string. Returns: float: METEOR score (higher is better). \"\"\" score = meteor_score([reference], candidate) print(f\"METEOR Score: {score}\") return score # 5. CIDEr Score Calculation def calculate_cider(reference: list, candidate: list) -> float: \"\"\" Calculate CIDEr score by comparing the frequency of common phrases between the generated and reference captions. Args: reference (list): List of reference captions. candidate (list): List of generated captions. Returns: float: CIDEr score (higher is better). \"\"\" ref_count = Counter(reference) cand_count = Counter(candidate) ref_tf = {k: v / len(reference) for k, v in ref_count.items()} cand_tf = {k: v / len(candidate) for k, v in cand_count.items()} score = sum(min(ref_tf.get(word, 0), cand_tf.get(word, 0)) for word in cand_tf) print(f\"CIDEr Score: {score}\") return score # 6. Distinct-1 and Distinct-2 Calculation def calculate_distinct_1_2(candidate: list) -> dict: \"\"\" Calculate Distinct-1 and Distinct-2 scores, measuring diversity by ratio of unique unigrams and bigrams. Args: candidate (list): Generated text as a list of words. Returns: dict: Dictionary containing Distinct-1 and Distinct-2 scores. \"\"\" unigrams = list(nltk.ngrams(candidate, 1)) bigrams = list(nltk.ngrams(candidate, 2)) distinct_1 = len(set(unigrams)) / len(unigrams) if unigrams else 0 distinct_2 = len(set(bigrams)) / len(bigrams) if bigrams else 0 result = {'Distinct-1': distinct_1, 'Distinct-2': distinct_2} print(f\"Distinct-1: {distinct_1}, Distinct-2: {distinct_2}\") return result # 7. Human Evaluation Metrics def evaluate_human_metrics(fluency: float, coherence: float, relevance: float, creativity: float) -> dict: \"\"\" Human evaluation metrics such as fluency, coherence, relevance, and", "source": "text_gen_metrics.py"}, {"content": "creativity. Args: fluency (float): Fluency score (0-1). coherence (float): Coherence score (0-1). relevance (float): Relevance score (0-1). creativity (float): Creativity score (0-1). Returns: dict: Dictionary containing the evaluation scores. \"\"\" metrics = { 'Fluency': fluency, 'Coherence': coherence, 'Relevance': relevance, 'Creativity': creativity } print(f\"Human Evaluation Metrics: {metrics}\") return metrics", "source": "text_gen_metrics.py"}, {"content": "import torch from torch.utils.data import DataLoader, Dataset, random_split from transformers import Trainer, TrainingArguments, AutoTokenizer import torch.nn as nn import torch.optim as optim def train_pytorch_model(dataloader, model, optimizer, criterion, device): model.train() for batch in dataloader: input_ids = batch['input_ids'].to(device) attention_mask = batch['attention_mask'].to(device) labels = batch['labels'].to(device) optimizer.zero_grad() # Forward pass outputs = model(input_ids, attention_mask) loss = criterion(outputs, labels) # Backward pass and optimization loss.backward() optimizer.step() print(f\"Loss: {loss.item()}\") def evaluate_pytorch_model(dataloader, model, criterion, device): model.eval() total_loss = 0 with torch.no_grad(): for batch in dataloader: input_ids = batch['input_ids'].to(device) attention_mask = batch['attention_mask'].to(device) labels = batch['labels'].to(device) # Forward pass outputs = model(input_ids, attention_mask) loss = criterion(outputs, labels) total_loss += loss.item() avg_loss = total_loss / len(dataloader) print(f\"Average Test Loss: {avg_loss}\")", "source": "train_utils.py"}, {"content": "import pytest import pandas as pd from src.preprocessing_utils import check_and_remove_artifacts # Sample input data for testing test_data = { \"text\": [ \"This is a clean text.\", # No artifacts \"This text contains a <br> tag.\", # HTML tag \"Backslashes\\\\ are tricky!\", # Backslash \"Multiple -- dashes\", # Double dashes \"Mixed <b>HTML</b> and \\\\ backslashes -- together\" # Mixed artifacts ], \"label\": [\"neutral\", \"neutral\", \"neutral\", \"neutral\", \"neutral\"] } @pytest.fixture def test_dataframe(): return pd.DataFrame(test_data) def test_no_remaining_artifacts(test_dataframe): # Run the combined artifact check and removal function cleaned_df, remaining_artifacts = check_and_remove_artifacts( df=test_dataframe.copy(), column='text', flag_column='html_flag', new_column='modified_text' ) remaining_count = remaining_artifacts.shape[0] # Assert that no artifacts remain in the cleaned text assert remaining_count == 0, f\"Expected no remaining artifacts, but found {remaining_count}\" assert remaining_artifacts.empty, \"There should be no rows with remaining artifacts after cleaning\" if __name__ == \"__main__\": pytest.main()", "source": "test_artifact_functions.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder from sklearn.pipeline import Pipeline def clean(df): \"\"\" Cleans the census dataset :param df: Dataframe representing the original dataset :return: Cleaned DataFrame, ready for further processing and analysis \"\"\" # Data cleaning and transformation # 1. Convert some numerical columns into categorical objects features_to_convert = ['detailed_industry_recode', 'detailed_occupation_recode', 'own_business_or_self_employed', 'year'] for feature in features_to_convert: df[feature] = df[feature].astype(object) # 2. Convert y-label into numerical feature df['income_group'] = df['income_group'].map({ '- 50000.': 0, '50000+.': 1 }) # 3. Replace NaN values df['hispanic_origin'] = df['hispanic_origin'].replace(np.nan, \"Do not know\") # 4. Drop unnecessary columns columns_to_drop = ['id', 'wage_per_hour', 'capital_losses', 'capital_gains', 'dividends_from_stocks', 'enroll_in_edu_inst_last_wk', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'detailed_industry_recode', 'detailed_occupation_recode', 'hispanic_origin', 'member_of_a_labor_union', 'reason_for_unemployment', 'family_members_under_18', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'migration_prev_res_in_sunbelt'] df = df.drop(columns=columns_to_drop) return df def transform(data_path): \"\"\" Transforms the original dataset by applying preprocessing steps and splitting the data into training and test sets :param data_path: file path to the CSV dataset :return: Preprocessed training and test sets \"\"\" # Load in the data df = pd.read_csv(data_path) df = clean(df) # Define X as input features and y as the outcome variable X = df.drop(columns=['income_group']) y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Separate numeric, nominal, and ordinal features numeric_features = ['age', 'num_persons_worked_for_employer', 'weeks_worked_in_year', 'veterans_benefits'] nominal_features = ['marital_stat', 'race', 'sex', 'citizenship','class_of_worker', 'major_industry_code', 'major_occupation_code', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'own_business_or_self_employed', 'tax_filer_stat', 'live_in_this_house_1_year_ago'] ordinal_features = ['education', 'year'] # Build a preprocessing step for numeric features numeric_transformer = Pipeline(steps=[ ('scaler', StandardScaler()) ]) # Build a preprocessing step for nominal features nominal_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Build a preprocessing step for ordinal features ordinal_transformer = Pipeline(steps=[ ('ordinal', OrdinalEncoder()) ]) # Combine transformations using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('nom', nominal_transformer, nominal_features), ('ord', ordinal_transformer, ordinal_features) ], remainder='passthrough' # Pass through any columns not transformed ) # Apply preprocessing to the data X_train_preprocessed = preprocessor.fit_transform(X_train) X_test_preprocessed = preprocessor.transform(X_test) return X_train_preprocessed, X_test_preprocessed, y_train, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np from collections import Counter from src.treenode import TreeNode from graphviz import Digraph class DecisionTree(): def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, num_of_features_splitting=None) -> None: self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.num_of_features_splitting = num_of_features_splitting def _gini(self, left, right): # Combine the left and right lists to get the total labels total = left + right total_size = len(total) # If the node is empty, return a Gini impurity of 0 if total_size == 0: return 0 # Function to calculate the Gini for a single group def gini_group(group): size = len(group) if size == 0: return 0 score = 0.0 # Count occurrences of each label in the group labels = set(group) for label in labels: proportion = group.count(label) / size score += proportion ** 2 # Gini impurity for the group return 1 - score # Calculate Gini impurity for left and right groups left_gini = gini_group(left) right_gini = gini_group(right) # Weighted Gini impurity for the combined groups weighted_gini = (len(left) / total_size) * left_gini + (len(right) / total_size) * right_gini return weighted_gini def _calculate_feature_importance(self, node): \"\"\"Calculates the feature importance by visiting each node in the tree recursively.\"\"\" if node is not None and node.feature_idx is not None: # The feature importance is proportional to the Gini decrease (or impurity decrease) self.feature_importances[node.feature_idx] += node.impurity_decrease # Recur for the left and right subtrees self._calculate_feature_importance(node.left) self._calculate_feature_importance(node.right) def _create_tree(self, data: np.array, current_depth: int) -> TreeNode: if current_depth > self.max_depth: return None # Find best split using Gini impurity split_1_data, split_2_data, split_feature_idx, split_feature_val, split_gini = self._find_best_split(data) # Find label probs for the node label_probabilities = self._find_label_probs(data) # Calculate Gini impurity for the current node (parent) node_gini = self._gini(list(data[:, -1]), []) # Gini decrease (similar to information gain but for Gini) gini_decrease = node_gini - split_gini # Create node node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, gini_decrease) if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]: return node elif gini_decrease < self.min_information_gain: return node current_depth += 1 node.left = self._create_tree(split_1_data, current_depth) node.right = self._create_tree(split_2_data, current_depth) return node def _find_best_split(self, data: np.array) -> tuple: \"\"\" Finds the best split (with the lowest Gini impurity) given the data. \"\"\" min_gini = float('inf') # Use a large number initially feature_idx_to_use = self._select_features_to_use(data) for idx in feature_idx_to_use: # splits the idx feature at the three percentile points feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25)) for feature_val in feature_vals: # potential left and right childs g1, g2 = self._split(data, idx, feature_val) # only labels required for gini impurity calculation (deselects all the X variables) gini_impurity = self._gini(list(g1[:, -1]), list(g2[:, -1])) if gini_impurity < min_gini: min_gini = gini_impurity min_gini_feature_idx = idx min_gini_feature_val = feature_val g1_min, g2_min = g1, g2 return g1_min, g2_min, min_gini_feature_idx, min_gini_feature_val, min_gini def _select_features_to_use(self, data: np.array) -> list: feature_idx = list(range(data.shape[1]-1)) if self.num_of_features_splitting == None: feature_idx_to_use = feature_idx elif self.num_of_features_splitting == \"sqrt\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx)))) elif self.num_of_features_splitting == \"log\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx)))) elif isinstance(self.num_of_features_splitting, int): feature_idx_to_use = np.random.choice(feature_idx, size=self.num_of_features_splitting) return feature_idx_to_use def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple: mask_below_threshold = data[:, feature_idx] < feature_val group1 = data[mask_below_threshold] group2 = data[~mask_below_threshold] return group1,", "source": "decision_tree.py"}, {"content": "group2 def _find_label_probs(self, data: np.array) -> np.array: labels_as_integers = data[:,-1].astype(int) total_labels = len(labels_as_integers) # array same length as number of unique labels label_probabilities = np.zeros(len(self.labels_in_train), dtype=float) # for each label for i, _ in enumerate(self.labels_in_train): label_index = np.where(labels_as_integers == i)[0] if len(label_index) > 0: # Percentage of samples that belong to a particular class label_probabilities[i] = len(label_index) / total_labels return label_probabilities def fit(self, X_train, y_train) -> None: \"\"\" Trains the model with given X and Y datasets \"\"\" # Concat features and labels self.labels_in_train = np.unique(y_train) train_data = np.concatenate((X_train, np.reshape(y_train, (-1, 1))), axis=1) # print(\"train_data shape:\", train_data.shape) # Start creating the tree self.tree = self._create_tree(data=train_data, current_depth=0) # Calculate feature importance self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0) self._calculate_feature_importance(self.tree) # Normalize the feature importance values self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()} def predict(self, X_set: np.array) -> np.array: pred_probs = self.predict_proba(X_set) preds = np.argmax(pred_probs, axis=1) return preds def predict_proba(self, X_set: np.array) -> np.array: \"\"\"Returns the predicted probabilities for a given dataset (X_set).\"\"\" # Initialize an empty list to hold the predictions for each sample predictions = [] # Traverse the decision tree for each sample in X_set for x in X_set: node = self.tree # Start at the root node while node.left or node.right: # Traverse until a leaf node is reached if x[node.feature_idx] < node.feature_val: node = node.left # Go to the left child else: node = node.right # Go to the right child # When a leaf node is reached, store the prediction probabilities predictions.append(node.prediction_probs) return np.array(predictions) def print_tree(self, node, depth=0): indent = \"->| \" * depth # Create an indent for readability based on the depth of the node if node is not None: if node.left or node.right: # Non-leaf node feature_name = self.all_feature_names[node.feature_idx] print(f'{indent}Node:') print(f'{indent} Split on feature: \"{feature_name}\"') print(f'{indent} Split value: {node.feature_val}') # Recursively print the left and right children self.print_tree(node.left, depth + 1) self.print_tree(node.right, depth + 1) else: # Leaf node predicted_class = np.argmax(node.prediction_probs) print(f'{indent}Leaf:') print(f'{indent} Class probabilities: {node.prediction_probs}') print(f'{indent} Predicted class: {predicted_class}') def graph_tree(self, node, dot=None, depth=0, node_size='0.5', font_size='10', edge_font_size='8', graph_size='10,10'): \"\"\" Graphs the decision tree using graphviz with customizable size and font settings. Parameters: - node_size: The size of the nodes (width and height). - font_size: The font size used for node labels. - edge_font_size: The font size used for edge labels. - graph_size: The overall size of the graph (width, height). \"\"\" if dot is None: dot = Digraph(comment='Decision Tree') # Set graph attributes to customize size and fonts dot.attr(size=graph_size) # Overall size of the image dot.attr('node', shape='box', fontsize=font_size, width=node_size, height=node_size) # Node font size and shape dot.attr('edge', fontsize=edge_font_size) # Edge font size if node is not None: # Create unique node ID for Graphviz node_id = str(id(node)) # Non-leaf node if node.left or node.right: feature_name = self.all_feature_names[node.feature_idx] label = f'Split on {feature_name}\\nValue: {node.feature_val}' dot.node(node_id, label) if node.left: left_id = str(id(node.left)) dot.node(left_id) dot.edge(node_id, left_id, label='True') self.graph_tree(node.left, dot, depth + 1, node_size, font_size, edge_font_size, graph_size) if node.right: right_id = str(id(node.right)) dot.node(right_id) dot.edge(node_id, right_id, label='False') self.graph_tree(node.right, dot, depth + 1, node_size, font_size, edge_font_size, graph_size) else: # Leaf node label = f'Class", "source": "decision_tree.py"}, {"content": "probs: {node.prediction_probs}\\nPred class: {np.argmax(node.prediction_probs)}' dot.node(node_id, label) return dot", "source": "decision_tree.py"}, {"content": "class Model: def __init__(self): # init your model here pass def train(self, params, X_train, y_train): \"\"\" Description of the function. :param params: ...... :param X_train: ...... :param y_train: ...... :return: ...... \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it pass def evaluate(self, X_test, y_test): \"\"\" Description of the function. :param X_test: ...... :param y_test: ...... :return: ...... \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score pass def get_default_params(self): \"\"\" Description of the function. :return: ...... \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model pass", "source": "model.py"}, {"content": "import numpy as np import pandas as pd from src.decision_tree import DecisionTree class RandomForest(): \"\"\" Random Forest Classifier Training: Use \"train\" function with train set features and labels Predicting: Use \"predict\" function with test set features \"\"\" def __init__(self, n_trees=5, subsample_size=None, feature_proportion=None): self.n_trees = n_trees self.subsample_size = subsample_size self.feature_proportion = feature_proportion def _create_bootstrap_samples(self, X, Y) -> tuple: \"\"\" Creates bootstrap samples for each base learner \"\"\" bootstrap_samples_X = [] bootstrap_samples_Y = [] for i in range(self.n_trees): if not self.subsample_size: self.bootstrap_sample_size = X.shape[0] else: self.bootstrap_sample_size = int(X.shape[0] * self.subsample_size) sampled_idx = np.random.choice(X.shape[0], size=self.bootstrap_sample_size, replace=True) # print(sampled_idx) bootstrap_samples_X.append(X[sampled_idx]) # print(bootstrap_samples_X) # Use .iloc if Y is a Pandas DataFrame or Series --> Y is a pandas object. if isinstance(Y, (pd.DataFrame, pd.Series)): bootstrap_samples_Y.append(Y.iloc[sampled_idx]) else: bootstrap_samples_Y.append(Y[sampled_idx]) return bootstrap_samples_X, bootstrap_samples_Y def fit(self, X_train, y_train): \"\"\"Trains the model with the given X and Y datasets\"\"\" if self.feature_proportion: self.num_of_features_splitting = int(self.feature_proportion * X_train.shape[1]) else: self.num_of_features_splitting = None bootstrap_samples_X, bootstrap_samples_Y = self._create_bootstrap_samples(X_train, y_train) # list of all estimators (weak decision tree learners) self.base_learner_list = [] for base_learner_idx in range(self.n_trees): base_learner = DecisionTree(num_of_features_splitting=self.num_of_features_splitting) base_learner.fit(bootstrap_samples_X[base_learner_idx], bootstrap_samples_Y[base_learner_idx]) self.base_learner_list.append(base_learner) def predict(self, X_train: np.array) -> np.array: \"\"\" Creates list of predictions for all base learners \"\"\" # list of predictions by individual dts (np arrays) pred_prob_list = [] for base_learner in self.base_learner_list: pred_prob_list.append(base_learner.predict_proba(X_train)) # Stack the list of prediction arrays along a new axis (axis=0 by default) stacked_predictions = np.stack(pred_prob_list, axis=0) # Compute the mean along the stacked dimension (axis=0), resulting in the average prediction avg_predictions = np.mean(stacked_predictions, axis=0) # Convert probabilities to class labels by taking the argmax (class with the highest probability) predicted_labels = np.argmax(avg_predictions, axis=1) return predicted_labels # shape (n_samples,)", "source": "random_forest.py"}, {"content": "import numpy as np class TreeNode(): def __init__(self, data, feature_idx, feature_val, prediction_probs, impurity_decrease) -> None: # subset of data that reaches this particular node (incl both features and labels) self.data = data # index of the feature used to split the data at this node self.feature_idx = feature_idx # value of split point (e.g. age = 50) self.feature_val = feature_val # holds the predicted probabilities of the classes at this node. If it\u2019s a leaf node, # it represents the probability distribution of the classes based on the labels of the samples that reach this node. self.prediction_probs = prediction_probs # decrease in impurity (for Gini impurity or entropy) when splitting the data at this node self.impurity_decrease = impurity_decrease # Renamed from information_gain # the total decrease in impurity caused by splits on that feature across the entire tree self.feature_importance = self.data.shape[0] * self.impurity_decrease # child nodes self.left = None self.right = None def node_def(self) -> str: if self.left or self.right: return f\"NODE | Impurity Decrease = {self.impurity_decrease} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\" else: unique_values, value_counts = np.unique(self.data[:, -1], return_counts=True) output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)]) return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"", "source": "treenode.py"}, {"content": "import numpy as np from collections import Counter from src.treenode import TreeNode class DecisionTree(): def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, numb_of_features_splitting=None, amount_of_say=None) -> None: self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.numb_of_features_splitting = numb_of_features_splitting self.amount_of_say = amount_of_say # The Gini method remains unchanged def _gini(self, left, right): total = left + right total_size = len(total) if total_size == 0: return 0 def gini_group(group): size = len(group) if size == 0: return 0 score = 0.0 labels = set(group) for label in labels: proportion = group.count(label) / size score += proportion ** 2 return 1 - score left_gini = gini_group(left) right_gini = gini_group(right) weighted_gini = (len(left) / total_size) * left_gini + (len(right) / total_size) * right_gini return weighted_gini def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple: mask_below_threshold = data[:, feature_idx] < feature_val group1 = data[mask_below_threshold] group2 = data[~mask_below_threshold] return group1, group2 def _select_features_to_use(self, data: np.array) -> list: feature_idx = list(range(data.shape[1]-1)) if self.numb_of_features_splitting == \"sqrt\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx)))) elif self.numb_of_features_splitting == \"log\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx)))) else: feature_idx_to_use = feature_idx return feature_idx_to_use def _find_best_split(self, data: np.array) -> tuple: \"\"\" Finds the best split (with the lowest Gini impurity) given the data. \"\"\" min_gini = float('inf') # Use a large number initially feature_idx_to_use = self._select_features_to_use(data) for idx in feature_idx_to_use: feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25)) for feature_val in feature_vals: g1, g2 = self._split(data, idx, feature_val) gini_impurity = self._gini(list(g1[:, -1]), list(g2[:, -1])) if gini_impurity < min_gini: min_gini = gini_impurity min_gini_feature_idx = idx min_gini_feature_val = feature_val g1_min, g2_min = g1, g2 return g1_min, g2_min, min_gini_feature_idx, min_gini_feature_val, min_gini def _find_label_probs(self, data: np.array) -> np.array: labels_as_integers = data[:,-1].astype(int) total_labels = len(labels_as_integers) label_probabilities = np.zeros(len(self.labels_in_train), dtype=float) for i, label in enumerate(self.labels_in_train): label_index = np.where(labels_as_integers == i)[0] if len(label_index) > 0: label_probabilities[i] = len(label_index) / total_labels return label_probabilities def _create_tree(self, data: np.array, current_depth: int) -> TreeNode: if current_depth > self.max_depth: return None # Find best split using Gini impurity split_1_data, split_2_data, split_feature_idx, split_feature_val, split_gini = self._find_best_split(data) # Find label probs for the node label_probabilities = self._find_label_probs(data) # Calculate Gini impurity for the current node (parent) node_gini = self._gini(list(data[:, -1]), []) # Gini decrease (similar to information gain but for Gini) gini_decrease = node_gini - split_gini # Create node node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, gini_decrease) if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]: return node elif gini_decrease < self.min_information_gain: return node current_depth += 1 node.left = self._create_tree(split_1_data, current_depth) node.right = self._create_tree(split_2_data, current_depth) return node def _calculate_feature_importance(self, node): \"\"\"Calculates the feature importance by visiting each node in the tree recursively.\"\"\" if node is not None and node.feature_idx is not None: # The feature importance is proportional to the Gini decrease (or impurity decrease) self.feature_importances[node.feature_idx] += node.information_gain # Recur for the left and right subtrees self._calculate_feature_importance(node.left) self._calculate_feature_importance(node.right) def fit(self, X_train: np.array, Y_train: np.array) -> None: self.labels_in_train = np.unique(Y_train) X_train_dense = X_train.toarray() # Convert sparse matrix to dense Y_train_reshaped = np.reshape(Y_train, (-1, 1)) train_data = np.concatenate((X_train_dense, Y_train_reshaped), axis=1) self.tree = self._create_tree(data=train_data, current_depth=0) self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0) self._calculate_feature_importance(self.tree) self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()} def predict(self, X_set: np.array) -> np.array: pred_probs", "source": "decision_tree-gini-version.py"}, {"content": "= self.predict_proba(X_set) preds = np.argmax(pred_probs, axis=1) return preds", "source": "decision_tree-gini-version.py"}, {"content": "import numpy as np from collections import Counter from src.treenode import TreeNode class DecisionTree(): \"\"\" Decision Tree Classifier Training: Use \"train\" function with train set features and labels Predicting: Use \"predict\" function with test set features \"\"\" def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, numb_of_features_splitting=None, amount_of_say=None) -> None: \"\"\" Setting the class with hyperparameters max_depth: (int) -> max depth of the tree min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible min_information_gain: (float) -> min information gain required to make the splitting possible num_of_features_splitting: (str) -> when splitting if sqrt then sqrt(# of features) features considered, if log then log(# of features) features considered else all features are considered amount_of_say: (float) -> used for Adaboost algorithm \"\"\" self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.numb_of_features_splitting = numb_of_features_splitting self.amount_of_say = amount_of_say def _entropy(self, class_probabilities: list) -> float: return sum([-p * np.log2(p) for p in class_probabilities if p>0]) def _class_probabilities(self, labels: list) -> list: total_count = len(labels) return [label_count / total_count for label_count in Counter(labels).values()] def _data_entropy(self, labels: list) -> float: return self._entropy(self._class_probabilities(labels)) def _partition_entropy(self, subsets: list) -> float: \"\"\"subsets = list of label lists (EX: [[1,0,0], [1,1,1])\"\"\" total_count = sum([len(subset) for subset in subsets]) return sum([self._data_entropy(subset) * (len(subset) / total_count) for subset in subsets]) def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple: mask_below_threshold = data[:, feature_idx] < feature_val group1 = data[mask_below_threshold] group2 = data[~mask_below_threshold] return group1, group2 def _select_features_to_use(self, data: np.array) -> list: \"\"\" Randomly selects the features to use while splitting w.r.t. hyperparameter numb_of_features_splitting \"\"\" feature_idx = list(range(data.shape[1]-1)) if self.numb_of_features_splitting == \"sqrt\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx)))) elif self.numb_of_features_splitting == \"log\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx)))) else: feature_idx_to_use = feature_idx return feature_idx_to_use def _find_best_split(self, data: np.array) -> tuple: \"\"\" Finds the best split (with the lowest entropy) given data Returns 2 splitted groups and split information \"\"\" min_part_entropy = 1e9 feature_idx_to_use = self._select_features_to_use(data) for idx in feature_idx_to_use: feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25)) for feature_val in feature_vals: g1, g2, = self._split(data, idx, feature_val) part_entropy = self._partition_entropy([g1[:, -1], g2[:, -1]]) if part_entropy < min_part_entropy: min_part_entropy = part_entropy min_entropy_feature_idx = idx min_entropy_feature_val = feature_val g1_min, g2_min = g1, g2 return g1_min, g2_min, min_entropy_feature_idx, min_entropy_feature_val, min_part_entropy def _find_label_probs(self, data: np.array) -> np.array: labels_as_integers = data[:,-1].astype(int) # Calculate the total number of labels total_labels = len(labels_as_integers) # Calculate the ratios (probabilities) for each label label_probabilities = np.zeros(len(self.labels_in_train), dtype=float) # Populate the label_probabilities array based on the specific labels for i, label in enumerate(self.labels_in_train): label_index = np.where(labels_as_integers == i)[0] if len(label_index) > 0: label_probabilities[i] = len(label_index) / total_labels return label_probabilities def _create_tree(self, data: np.array, current_depth: int) -> TreeNode: \"\"\" Recursive, depth first tree creation algorithm \"\"\" # Check if the max depth has been reached (stopping criteria) if current_depth > self.max_depth: return None # Find best split split_1_data, split_2_data, split_feature_idx, split_feature_val, split_entropy = self._find_best_split(data) # Find label probs for the node label_probabilities = self._find_label_probs(data) # Calculate information gain node_entropy = self._entropy(label_probabilities) information_gain = node_entropy - split_entropy # Create node node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain) # Check if the min_samples_leaf has been satisfied (stopping criteria) if", "source": "decision_tree_git.py"}, {"content": "self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]: return node # Check if the min_information_gain has been satisfied (stopping criteria) elif information_gain < self.min_information_gain: return node current_depth += 1 node.left = self._create_tree(split_1_data, current_depth) node.right = self._create_tree(split_2_data, current_depth) return node def _predict_one_sample(self, X: np.array) -> np.array: \"\"\"Returns prediction for 1 dim array\"\"\" node = self.tree # Finds the leaf which X belongs while node: pred_probs = node.prediction_probs if X[node.feature_idx] < node.feature_val: node = node.left else: node = node.right return pred_probs def train(self, X_train: np.array, Y_train: np.array) -> None: \"\"\" Trains the model with given X and Y datasets \"\"\" # Concat features and labels self.labels_in_train = np.unique(Y_train) train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1) # Start creating the tree self.tree = self._create_tree(data=train_data, current_depth=0) # Calculate feature importance self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0) self._calculate_feature_importance(self.tree) # Normalize the feature importance values self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()} def predict_proba(self, X_set: np.array) -> np.array: \"\"\"Returns the predicted probs for a given data set\"\"\" pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set) return pred_probs def predict(self, X_set: np.array) -> np.array: \"\"\"Returns the predicted labels for a given data set\"\"\" pred_probs = self.predict_proba(X_set) preds = np.argmax(pred_probs, axis=1) return preds def _print_recursive(self, node: TreeNode, level=0) -> None: if node != None: self._print_recursive(node.left, level + 1) print(' ' * 4 * level + '-> ' + node.node_def()) self._print_recursive(node.right, level + 1) def print_tree(self) -> None: self._print_recursive(node=self.tree) def _calculate_feature_importance(self, node): \"\"\"Calculates the feature importance by visiting each node in the tree recursively\"\"\" if node != None: self.feature_importances[node.feature_idx] += node.feature_importance self._calculate_feature_importance(node.left) self._calculate_feature_importance(node.right)", "source": "decision_tree_git.py"}, {"content": "import numpy as np from src.decision_tree import DecisionTree class RandomForestClassifier(): \"\"\" Random Forest Classifier Training: Use \"train\" function with train set features and labels Predicting: Use \"predict\" function with test set features \"\"\" def __init__(self, n_base_learner=10, max_depth=5, min_samples_leaf=1, min_information_gain=0.0, \\ numb_of_features_splitting=None, bootstrap_sample_size=None) -> None: self.n_base_learner = n_base_learner self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.numb_of_features_splitting = numb_of_features_splitting self.bootstrap_sample_size = bootstrap_sample_size def _create_bootstrap_samples(self, X, Y) -> tuple: \"\"\" Creates bootstrap samples for each base learner \"\"\" bootstrap_samples_X = [] bootstrap_samples_Y = [] for i in range(self.n_base_learner): if not self.bootstrap_sample_size: self.bootstrap_sample_size = X.shape[0] sampled_idx = np.random.choice(X.shape[0], size=self.bootstrap_sample_size, replace=True) bootstrap_samples_X.append(X[sampled_idx]) bootstrap_samples_Y.append(Y[sampled_idx]) return bootstrap_samples_X, bootstrap_samples_Y def train(self, X_train: np.array, Y_train: np.array) -> None: \"\"\"Trains the model with given X and Y datasets\"\"\" bootstrap_samples_X, bootstrap_samples_Y = self._create_bootstrap_samples(X_train, Y_train) self.base_learner_list = [] for base_learner_idx in range(self.n_base_learner): base_learner = DecisionTree(max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, \\ min_information_gain=self.min_information_gain, numb_of_features_splitting=self.numb_of_features_splitting) base_learner.train(bootstrap_samples_X[base_learner_idx], bootstrap_samples_Y[base_learner_idx]) self.base_learner_list.append(base_learner) # NOT IMPLEMENTED YET # Calculate feature importance self.feature_importances = self._calculate_rf_feature_importance(self.base_learner_list) def _predict_proba_w_base_learners(self, X_set: np.array) -> list: \"\"\" Creates list of predictions for all base learners \"\"\" pred_prob_list = [] for base_learner in self.base_learner_list: pred_prob_list.append(base_learner.predict_proba(X_set)) return pred_prob_list def predict_proba(self, X_set: np.array) -> list: \"\"\"Returns the predicted probs for a given data set\"\"\" pred_probs = [] base_learners_pred_probs = self._predict_proba_w_base_learners(X_set) # Average the predicted probabilities of base learners for obs in range(X_set.shape[0]): base_learner_probs_for_obs = [a[obs] for a in base_learners_pred_probs] # Calculate the average for each index obs_average_pred_probs = np.mean(base_learner_probs_for_obs, axis=0) pred_probs.append(obs_average_pred_probs) return pred_probs def predict(self, X_set: np.array) -> np.array: \"\"\"Returns the predicted labels for a given data set\"\"\" pred_probs = self.predict_proba(X_set) preds = np.argmax(pred_probs, axis=1) return preds def _calculate_rf_feature_importance(self, base_learners): \"\"\"Calcalates the average feature importance of the base learners\"\"\" feature_importance_dict_list = [] for base_learner in base_learners: feature_importance_dict_list.append(base_learner.feature_importances) feature_importance_list = [list(x.values()) for x in feature_importance_dict_list] average_feature_importance = np.mean(feature_importance_list, axis=0) return average_feature_importance", "source": "random_forest_git.py"}, {"content": "import numpy as np class TreeNode(): def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None: self.data = data self.feature_idx = feature_idx self.feature_val = feature_val self.prediction_probs = prediction_probs self.information_gain = information_gain self.feature_importance = self.data.shape[0] * self.information_gain self.left = None self.right = None def node_def(self) -> str: if (self.left or self.right): return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\" else: unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True) output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)]) return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\"", "source": "treenode_git.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\") from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10, learning_rate=1e-3): \"\"\" Initializes weights and biases for a two-layer neural network. Args: - input_size (int): Size of the input layer. - hidden_size (int): Size of the hidden layer. - output_size (int): Size of the output layer. - learning_rate (float): Learning rate for weight updates. \"\"\" self.learning_rate = learning_rate # Initialize weights and biases self.W1 = np.random.randn(input_size, hidden_size) * 0.01 # Input to hidden self.b1 = np.zeros((hidden_size,)) self.W2 = np.random.randn(hidden_size, output_size) * 0.01 # Hidden to output self.b2 = np.zeros((output_size,)) # Placeholders for intermediate variables self.z1 = None self.a1 = None self.z2 = None self.a2 = None self.x = None # Input features self.y = None # True label def forward(self, features): \"\"\" Performs the forward pass of the neural network. Args: - features (numpy.ndarray): Input features of shape (input_size,). Returns: - predictions (numpy.ndarray): Output predictions (probabilities). \"\"\" self.x = features # Store input for backpropagation # First layer computations self.z1 = np.dot(self.x, self.W1) + self.b1 # Linear transformation z1 = x \u22c5 W1 + b1 self.a1 = self.relu(self.z1) # Activation function a1 = ReLU(z1) # Second layer computations self.z2 = np.dot(self.a1, self.W2) + self.b2 # Linear transformation z2 = a1 \u22c5 W2 + b2 self.a2 = self.softmax(self.z2) # Output probabilities a2 = softmax(z2) return self.a2 def loss(self, predictions, label): \"\"\" Computes the cross-entropy loss between predictions and the true label. Args: - predictions (numpy.ndarray): Predicted probabilities. - label (int): True class label. Returns: - loss (float): Computed loss value. \"\"\" self.y = label # Store true label for backpropagation # Convert label to one-hot encoding y_one_hot = np.zeros_like(predictions) y_one_hot[label] = 1 # Compute cross-entropy loss loss = -np.sum(y_one_hot * np.log(predictions + 1e-15)) # Add epsilon to avoid log(0) loss = -\u2211(y_true * log(predictions)) return loss def backward(self): \"\"\" Performs backpropagation to compute gradients and update weights and biases. \"\"\" # Convert label to one-hot encoding y_one_hot = np.zeros_like(self.a2) y_one_hot[self.y] = 1 # Gradient of loss w.r.t z2 (output layer linear transform) dz2 = self.a2 - y_one_hot # Shape: (output_size,) gradient of loss w.r.t. z2 # Gradients for W2 and b2 dW2 = np.outer(self.a1, dz2) # Shape: (hidden_size, output_size) db2 = dz2 # Shape: (output_size,) # Backpropagate to hidden layer da1 = np.dot(self.W2, dz2) # Shape: (hidden_size,) dz1 = da1 * self.relu_derivative(self.z1) # Shape: (hidden_size,) # Gradients for W1 and b1 dW1 = np.outer(self.x, dz1) # Shape: (input_size, hidden_size) db1 = dz1 # Shape: (hidden_size,) # Update weights and biases self.W2 -= self.learning_rate * dW2 self.b2 -= self.learning_rate * db2 self.W1 -= self.learning_rate * dW1 self.b1 -= self.learning_rate * db1 @staticmethod def relu(z): \"\"\"Applies the ReLU activation function.\"\"\" return np.maximum(0, z) @staticmethod def relu_derivative(z): \"\"\"Computes the derivative of ReLU.\"\"\" return (z > 0).astype(float) @staticmethod def softmax(z): \"\"\"Computes the softmax function.\"\"\" e_z = np.exp(z - np.max(z)) # Stability improvement return e_z / np.sum(e_z)", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, raw_path, target_column, columns_to_drop): \"\"\" Initialize the data pipeline with the raw path and the target column name. Args: raw_path (str): Path to the raw data file (e.g., CSV). target_column (str): The name of the target column in the dataset. \"\"\" self.raw_path = raw_path self.target_column = target_column if isinstance(target_column, list) else [target_column] self.columns_to_drop = columns_to_drop if isinstance(columns_to_drop, list) else [columns_to_drop] self.scaler = StandardScaler() def transform(self): \"\"\" Reads the data from the raw path, separates the features (X) and target (y), and returns them as NumPy arrays. Returns: X_array (numpy.ndarray): Feature array. y_array (numpy.ndarray): Target array. \"\"\" # Load the data using pandas df = pd.read_csv(self.raw_path) self.columns_to_drop = self.columns_to_drop + self.target_column # Split the data into features and target X = df.drop(columns=self.columns_to_drop) X_scaled = self.scaler.fit_transform(X) y = df[self.target_column] # Convert the Series to NumPy arrays y_array = y.to_numpy() return X_scaled, y_array", "source": "mlp_datapipeline.py"}, {"content": "import argparse import sys import time import random import mlflow import mlflow.keras # MLflow's Keras integration import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import classification_report, confusion_matrix from tensorflow.keras.models import Sequential from tensorflow.keras.callbacks import EarlyStopping, Callback from tensorflow.keras.layers import Dense, Dropout, Input from preprocessing import load_data, preprocess_data def plot_confusion_matrix(conf_matrix, epoch=None): \"\"\" Generates and saves a visual representation of the confusion matrix. This function creates a plot of the confusion matrix with colored backgrounds indicating correct and incorrect predictions. The resulting image is saved to disk with a filename that includes the epoch number if provided. Parameters ---------- conf_matrix : numpy.ndarray A 2D array representing the confusion matrix. epoch : int, optional The epoch number to include in the filename, by default None. Returns ------- str The filename of the saved confusion matrix image. \"\"\" # Create the figure and axis fig, ax = plt.subplots(figsize=(6, 4)) # Define colors for correct and incorrect predictions correct_color = 'lightgreen' incorrect_color = 'lightcoral' # Plot the matrix as rectangles with colored backgrounds for i in range(conf_matrix.shape[0]): for j in range(conf_matrix.shape[1]): if i == j: # Correct predictions (diagonal) - fill with green ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=correct_color)) else: # Incorrect predictions (off-diagonal) - fill with red ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=incorrect_color)) # Add the text inside the boxes ax.text(j + 0.5, i + 0.5, f'{conf_matrix[i, j]}', ha='center', va='center', color='black', fontsize=12) # Setting labels and title ax.set_xticks(np.arange(conf_matrix.shape[1]) + 0.5) ax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5) ax.set_xticklabels(['False', 'True']) ax.set_yticklabels(['False', 'True']) # Label axes ax.set_ylabel('Actual') ax.set_xlabel('Predicted') ax.set_title('Confusion Matrix') # Set gridlines ax.set_xticks(np.arange(0, conf_matrix.shape[1]), minor=True) ax.set_yticks(np.arange(0, conf_matrix.shape[0]), minor=True) ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2) # Adjust limits and aspect ax.set_xlim(0, conf_matrix.shape[1]) ax.set_ylim(conf_matrix.shape[0], 0) ax.set_aspect('equal') # Hide major tick marks ax.tick_params(left=False, bottom=False) # Save the figure filename = 'confusion_matrix.png' if epoch is None else f'confusion_matrix_epoch_{epoch}.png' plt.savefig(filename) plt.close(fig) return filename class MLflowCallback(Callback): \"\"\" A custom Keras callback for integrating MLflow tracking into the training process. The MLflowCallback class handles the logging of training and validation metrics at the end of each epoch and logs a confusion matrix artifact upon training completion. It supports different model types, enabling appropriate handling of predictions. Attributes ---------- conf_matrix_fn : callable A function to generate and save the confusion matrix plot. model_type : str The type of model being trained ('deeper' or 'datret'), determining prediction handling. last_epoch : int or None Stores the number of the last completed epoch. \"\"\" def __init__(self, conf_matrix_fn, model_type): super(MLflowCallback, self).__init__() self.conf_matrix_fn = conf_matrix_fn self.model_type = model_type self.last_epoch = None def on_epoch_end(self, epoch, logs=None): \"\"\" Called at the end of each epoch to log metrics to MLflow. Parameters ---------- epoch : int The current epoch number. logs : dict, optional A dictionary of metrics from the epoch, by default None. \"\"\" self.last_epoch = epoch # Update the last epoch number # Log metrics for key, value in logs.items(): mlflow.log_metric(key, value, step=epoch) def on_train_end(self, logs=None): \"\"\" Called at the end of training to log the confusion matrix and print the classification report. Parameters ---------- logs : dict, optional A dictionary of final metrics, by default None. \"\"\" # After training, log the confusion matrix", "source": "model_training.py"}, {"content": "and classification report X_train, X_test, y_train, y_test = preprocess_data(load_data(), one_hot=(self.model_type == 'datret')) if self.model_type == 'deeper': y_pred = (self.model.predict(X_test) > 0.5).astype(\"int32\").flatten() y_true = y_test.to_numpy().flatten() elif self.model_type == 'datret': y_pred = self.model.predict(X_test).argmax(axis=1) y_true = y_test.argmax(axis=1) print(classification_report(y_true, y_pred)) conf_matrix = confusion_matrix(y_true, y_pred) cm_filename = self.conf_matrix_fn(conf_matrix, epoch=self.last_epoch) mlflow.log_artifact(cm_filename) def create_deeper_model(input_dim): \"\"\" Constructs the second model in A4P1 Parameters ---------- input_dim : int The dimensionality of the input features. Returns ------- tensorflow.keras.models.Sequential The compiled Keras Sequential model. \"\"\" # Define the model model = Sequential() # Input layer + First hidden layer model.add(Dense(128, input_dim=input_dim, activation='relu')) model.add(Dense(128, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(16, activation='relu')) # Output layer: Sigmoid for binary classification model.add(Dense(1, activation='sigmoid')) # Compile the model model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) return model def create_datret_model(input_dim): \"\"\" Constructs 3rd model in A4P1 \"\"\" # Define the model model = Sequential() # Input layer model.add(Input(shape=(input_dim,))) # Add dense layers with ReLU activation and dropout in between model.add(Dense(500, activation='relu')) model.add(Dropout(0.5)) # Dropout layer with 50% dropout rate model.add(Dense(250, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(125, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(62, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(31, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(15, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(7, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(3, activation='relu')) # Output layer (2 neurons for binary classification with softmax) model.add(Dense(2, activation='softmax')) # Compile the model model.compile(optimizer='adamw', loss='categorical_crossentropy', # Because of softmax and 2 output classes metrics=['accuracy']) return model def train_model(args, model_type='deeper'): \"\"\" Trains a specified Keras model type and logs relevant parameters, metrics, and artifacts to MLflow. Depending on the `model_type`, either a 'deeper' or 'datret' model is created, trained, and its performance metrics are logged. The function also handles MLflow run management and logging of environment details. Parameters ---------- args : argparse.Namespace Parsed command-line arguments containing training configurations. model_type : str, optional The type of model to train ('deeper' or 'datret'), by default 'deeper'. \"\"\" # Check if a run is already active if mlflow.active_run() is not None: print(\"An MLflow run is already active. Starting a nested run.\") nested = True else: nested = False # Start MLflow run with mlflow.start_run(nested=nested): # Log the model type as a parameter mlflow.log_param(\"model_type\", model_type) if model_type == 'deeper': X_train, X_test, y_train, y_test = preprocess_data(load_data()) input_dim = X_train.shape[1] model = create_deeper_model(input_dim) mlflow.log_param(\"architecture\", \"deeper\") elif model_type == 'datret': X_train, X_test, y_train, y_test = preprocess_data(load_data(), one_hot=True) input_dim = X_train.shape[1] model = create_datret_model(input_dim) mlflow.log_param(\"architecture\", \"datret\") model.summary() # Log model summary as a text artifact with open(\"model_summary.txt\", \"w\") as f: model.summary(print_fn=lambda x: f.write(x + '\\n')) mlflow.log_artifact(\"model_summary.txt\") early_stopping = EarlyStopping(monitor='val_loss', patience=args.early_stopping_patience, restore_best_weights=True) # Initialize MLflow callback mlflow_callback = MLflowCallback(conf_matrix_fn=plot_confusion_matrix, model_type=model_type) # Log hyperparameters mlflow.log_param(\"optimizer\", \"adam\" if model_type == 'deeper' else \"adamw\") mlflow.log_param(\"loss\", \"binary_crossentropy\" if model_type == 'deeper' else \"categorical_crossentropy\") mlflow.log_param(\"metrics\", \"accuracy\") mlflow.log_param(\"epochs\", args.epochs) mlflow.log_param(\"batch_size\", args.batch_size) mlflow.log_param(\"early_stopping_patience\", args.early_stopping_patience) history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=args.epochs, batch_size=args.batch_size, callbacks=[early_stopping, mlflow_callback]) # # Prepare input example and signature # input_example = X_test[:1] # example_output = model.predict(input_example) # signature = infer_signature(input_example, example_output) # # Log the trained model with signature and input example # mlflow.keras.log_model( # model, # \"model\", # signature=signature, # input_example=input_example # # Optionally, log the entire training history for key, values in history.history.items(): for epoch, value in enumerate(values): mlflow.log_metric(key, value, step=epoch) # After the run, print a confirmation print(f\"Training", "source": "model_training.py"}, {"content": "completed and logged to MLflow for model type: {model_type}\") def main(): \"\"\" The main function orchestrates the setup of MLflow and initiates the training of multiple models. It parses command-line arguments, configures MLflow based on user inputs, logs environment details and Git commit hash, and trains both 'deeper' and 'datret' models using the `train_model` function. \"\"\" parser = argparse.ArgumentParser(description=\"Training Script with MLflow Integration\") # Define expected arguments # parser.add_argument('--data_dir_path', type=str, required=True, help='Path to the processed data directory') parser.add_argument('--setup_mlflow', type=str, default='false', help='Whether to set up MLflow') parser.add_argument('--mlflow_tracking_uri', type=str, required=True, help='MLflow Tracking URI') parser.add_argument('--mlflow_exp_name', type=str, required=True, help='MLflow Experiment Name') # parser.add_argument('--model_checkpoint_dir_path', type=str, required=True, help='Path to save model checkpoints') parser.add_argument('--epochs', type=int, default=50, help='Number of training epochs') parser.add_argument('--batch_size', type=int, default=4096, help='Batch size for training') parser.add_argument('--early_stopping_patience', type=int, default=5, help='Patience for early stopping') # Optional: Add any other arguments as needed args = parser.parse_args() # Set up MLflow if requested if args.setup_mlflow.lower() == 'true': mlflow.set_tracking_uri(args.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow_exp_name) # Optionally, log environment details try: import pkg_resources with open(\"requirements.txt\", \"w\") as f: for dist in pkg_resources.working_set: f.write(f\"{dist.project_name}=={dist.version}\\n\") mlflow.log_artifact(\"requirements.txt\") except Exception as e: print(f\"Failed to log requirements.txt: {e}\") # Optionally, log Git commit hash try: import subprocess git_commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).strip().decode('utf-8') mlflow.log_param(\"git_commit\", git_commit) except Exception as e: print(f\"Could not log git commit: {e}\") # Train both models train_model(args, model_type='deeper') train_model(args, model_type='datret') if __name__ == \"__main__\": main()", "source": "model_training.py"}, {"content": "################################# # TODO: # 2. Add ml-flow ################################# import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import classification_report, confusion_matrix from tensorflow.keras.models import Sequential from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.layers import Dense, Dropout, Input from preprocessing import load_data, preprocess_data def plot_confusion_matrix(conf_matrix): # Create the figure and axis fig, ax = plt.subplots(figsize=(6, 4)) # Define colors for correct and incorrect predictions correct_color = 'lightgreen' incorrect_color = 'lightcoral' # Plot the matrix as rectangles with colored backgrounds for i in range(conf_matrix.shape[0]): for j in range(conf_matrix.shape[1]): if i == j: # Correct predictions (diagonal) - fill with green ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=correct_color)) else: # Incorrect predictions (off-diagonal) - fill with red ax.add_patch(plt.Rectangle((j, i), 1, 1, fill=True, color=incorrect_color)) # Add the text inside the boxes ax.text(j + 0.5, i + 0.5, f'{conf_matrix[i, j]}', ha='center', va='center', color='black', fontsize=12) # Setting labels and title ax.set_xticks(np.arange(conf_matrix.shape[1]) + 0.5) ax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5) ax.set_xticklabels(['False', 'True']) ax.set_yticklabels(['False', 'True']) # Label axes ax.set_ylabel('Actual') ax.set_xlabel('Predicted') ax.set_title('Confusion Matrix') # Set gridlines ax.set_xticks(np.arange(0, conf_matrix.shape[1]), minor=True) ax.set_yticks(np.arange(0, conf_matrix.shape[0]), minor=True) ax.grid(which=\"minor\", color=\"black\", linestyle='-', linewidth=2) # Adjust limits and aspect ax.set_xlim(0, conf_matrix.shape[1]) ax.set_ylim(conf_matrix.shape[0], 0) ax.set_aspect('equal') # Hide major tick marks ax.tick_params(left=False, bottom=False) plt.show() def create_deeper_model(input_dim): # Define the model model = Sequential() # Input layer + First hidden layer model.add(Dense(128, input_dim=input_dim, activation='relu')) model.add(Dense(128, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(64, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(32, activation='relu')) model.add(Dense(16, activation='relu')) # Output layer: Sigmoid for binary classification model.add(Dense(1, activation='sigmoid')) # Compile the model model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) return model def create_datret_model(input_dim): # Define the model model = Sequential() # Input layer model.add(Input(shape=(input_dim,))) # Add dense layers with ReLU activation and dropout in between model.add(Dense(500, activation='relu')) model.add(Dropout(0.5)) # Dropout layer with 50% dropout rate model.add(Dense(250, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(125, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(62, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(31, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(15, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(7, activation='relu')) model.add(Dropout(0.5)) model.add(Dense(3, activation='relu')) # Output layer (2 neurons for binary classification with softmax) model.add(Dense(2, activation='softmax')) # Compile the model model.compile(optimizer='adamw', loss='categorical_crossentropy', # Because of softmax and 2 output classes metrics=['accuracy']) return model def train_model(model_type='deeper'): if model_type == 'deeper': X_train, X_test, y_train, y_test = preprocess_data(load_data()) input_dim = X_train.shape[1] model = create_deeper_model(input_dim) elif model_type == 'datret': X_train, X_test, y_train, y_test = preprocess_data(load_data(), one_hot=True) input_dim = X_train.shape[1] model = create_datret_model(input_dim) model.summary() early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=4096, callbacks=[early_stopping]) if model_type == 'deeper': # Evaluate the model on the test set y_pred = (model.predict(X_test) > 0.5).astype(\"int32\") print(classification_report(y_test, y_pred)) conf_matrix = confusion_matrix(y_test, y_pred) elif model_type == 'datret': # Predict with softmax output (2 neurons) and select the class with the highest probability y_pred= model.predict(X_test).argmax(axis=1) # Convert y_test from one-hot encoding to class indices y_test = y_test.argmax(axis=1) print(classification_report(y_test, y_pred)) conf_matrix = confusion_matrix(y_test, y_pred) return conf_matrix train_model(model_type='deeper') train_model(model_type='datret')", "source": "model_training_no_mlflow.py"}, {"content": "import pandas as pd from imblearn.over_sampling import SMOTE from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from tensorflow.keras.utils import to_categorical def load_data(data_dir_path='assignment4/data/raw/data.csv'): \"\"\" Load data from a CSV file. Parameters: - data_dir_path (str): Path to the CSV file. Returns: - DataFrame: A pandas DataFrame containing the loaded data. \"\"\" print(f\"Loading data from {data_dir_path}\") data = pd.read_csv(data_dir_path) return data def preprocess_data(df, y_label='Class', sample=True, one_hot=False): \"\"\" Preprocess the data by applying SMOTE to balance classes and scaling features. Parameters: - df (DataFrame): The input data. - y_label (str): The column name for the target variable. - sample (bool): Whether to apply SMOTE to balance the data (default=True). Returns: - X_train (ndarray): Scaled training features. - X_test (ndarray): Scaled testing features. - y_train (Series): Resampled training labels (if SMOTE applied). - y_test (Series): Testing labels. \"\"\" X = df.drop(columns=[y_label]) y = df[y_label] # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) if sample: # Apply SMOTE to the training set smote = SMOTE(random_state=42) X_train, y_train = smote.fit_resample(X_train, y_train) # Define the scaler scaler = StandardScaler() # Combine both transformers into a ColumnTransformer X_train = scaler.fit_transform(X_train) X_test = scaler.transform(X_test) # Assuming y_train contains binary labels (0, 1) if one_hot: y_train = to_categorical(y_train, num_classes=2) y_test = to_categorical(y_test, num_classes=2) return X_train, X_test, y_train, y_test", "source": "preprocessing.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader class CNNModel(torch.nn.Module): def __init__(self, input_size, output_size, device='cpu'): \"\"\" Initializes the CNNModel for time series forecasting. Args: input_size (int): Number of input features. output_size (int): Number of output features (e.g., 1 for univariate prediction). device (str, optional): Device to run the model on ('cpu' or 'cuda'). Defaults to 'cpu'. \"\"\" super().__init__() self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Convolutional layers self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3) self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3) # Adaptive pooling to reduce the temporal dimension to 1 self.adaptive_pool = nn.AdaptiveAvgPool1d(output_size=1) # Fully connected layer self.fc = nn.Linear(128, output_size) # Activation function self.relu = nn.ReLU() # Move model to the specified device self.to(self.device) def forward(self, x): # Your code here \"\"\" Defines the forward pass of the model. Args: x (torch.Tensor): Input tensor of shape (batch_size, lookback, input_size). Returns: torch.Tensor: Output tensor of shape (batch_size, output_size). \"\"\" # Move input to the device x = x.to(self.device) # Shape: (batch_size, lookback, input_size) # Permute dimensions to match Conv1d input: (batch_size, input_channels, seq_length) x = x.permute(0, 2, 1) # Shape: (batch_size, input_size, lookback) # Apply convolutional layers x = self.conv1(x) # Shape: (batch_size, 64, L1) x = self.relu(x) x = self.conv2(x) # Shape: (batch_size, 128, L2) x = self.relu(x) # Adaptive pooling to reduce temporal dimension to 1 x = self.adaptive_pool(x) # Shape: (batch_size, 128, 1) # Flatten the output x = x.view(x.size(0), -1) # Shape: (batch_size, 128) # Fully connected layer x = self.fc(x) # Shape: (batch_size, output_size) return x def fit(self, dataloader, epochs=20, learning_rate=0.001, verbose=True): # Your code here \"\"\" Trains the model using the provided DataLoader. Args: dataloader (DataLoader): DataLoader for training data. epochs (int, optional): Number of training epochs. Defaults to 20. learning_rate (float, optional): Learning rate for the optimizer. Defaults to 0.001. verbose (bool, optional): If True, prints training progress. Defaults to True. \"\"\" # Define loss function and optimizer criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) self.train() # Set model to training mode for epoch in range(1, epochs + 1): epoch_loss = 0.0 for batch_idx, (features, labels) in enumerate(dataloader): # Move data to the device features = features.to(self.device) labels = labels.to(self.device) # Zero the parameter gradients optimizer.zero_grad() # Forward pass outputs = self.forward(features) # Compute loss loss = criterion(outputs, labels) # Backward pass and optimization loss.backward() optimizer.step() epoch_loss += loss.item() average_loss = epoch_loss / len(dataloader) if verbose: print(f\"Epoch [{epoch}/{epochs}], Loss: {average_loss:.4f}\") def predict(self, dataloader): # Your code here \"\"\" Generates predictions using the trained model. Args: dataloader (DataLoader): DataLoader for input data. Returns: torch.Tensor: Predictions of shape (num_samples, output_size). \"\"\" self.eval() # Set model to evaluation mode predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(self.device) outputs = self.forward(features) predictions.append(outputs.cpu()) predictions = torch.cat(predictions, dim=0) return predictions def evaluate(self, dataloader): \"\"\" Evaluates the model on the provided DataLoader and computes MSE. Args: dataloader (DataLoader): DataLoader for evaluation data. Returns: float: Mean Squared Error (MSE) on the evaluation data. \"\"\" self.eval() # Set model to evaluation mode criterion = nn.MSELoss() total_loss = 0.0 total_samples = 0 with torch.no_grad(): for features,", "source": "cnn_model.py"}, {"content": "labels in dataloader: features = features.to(self.device) labels = labels.to(self.device) outputs = self.forward(features) loss = criterion(outputs, labels) batch_size = features.size(0) total_loss += loss.item() * batch_size total_samples += batch_size mse = total_loss / total_samples return mse", "source": "cnn_model.py"}, {"content": "# src/datapipeline.py import pandas as pd class DataPipeline: def __init__(self, file_path): self.file_path = file_path self.df = None def load_data(self): \"\"\"Load the CSV file into a DataFrame.\"\"\" self.df = pd.read_csv(self.file_path) return self.df def handle_missing_data(self, method='ffill'): \"\"\" Handle missing data in the 'pm2.5' column. Options for method: - 'ffill': Forward fill - 'bfill': Backward fill - 'moving_avg': Moving average (with a window of 3) \"\"\" if method == 'ffill': self.df['pm2.5'] = self.df['pm2.5'].ffill() elif method == 'bfill': self.df['pm2.5'] = self.df['pm2.5'].bfill() elif method == 'moving_avg': self.df['pm2.5'] = self.df['pm2.5'].fillna(self.df['pm2.5'].rolling(window=3, min_periods=1).mean()) return self.df def set_datetime_index(self): \"\"\"Convert the 'year', 'month', 'day', 'hour' columns to a datetime index.\"\"\" # Create a datetime column from the year, month, day, and hour columns self.df['date'] = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']]) # Drop the original columns since 'date' now contains that information self.df = self.df.drop(columns=['year', 'month', 'day', 'hour', 'No']) # Set the 'date' column as the index self.df.set_index('date', inplace=True) return self.df def downsample(self, freq='10D'): \"\"\"Downsample the data to a specified frequency (e.g., 10 days), handling both numeric and categorical columns.\"\"\" # Select numeric and categorical columns separately numeric_columns = self.df.select_dtypes(include=['float64', 'int64']).columns categorical_columns = self.df.select_dtypes(include=['object', 'category']).columns # Downsample numeric columns using mean df_numeric = self.df[numeric_columns].resample(freq).mean() # Downsample categorical columns using mode (most frequent value) df_categorical = self.df[categorical_columns].resample(freq).apply(lambda x: x.mode()[0] if not x.mode().empty else None) # Combine the numeric and categorical dataframes self.df = pd.concat([df_numeric, df_categorical], axis=1) return self.df def upsample(self, freq='H', method='ffill'): \"\"\"Upsample the data to a higher frequency (e.g., hourly) and fill missing values.\"\"\" self.df = self.df.resample(freq).ffill() # Default method is forward fill return self.df def one_hot_encoding(self, feature): \"\"\"Perform one-hot encoding on a specified feature of the DataFrame.\"\"\" self.df = pd.get_dummies(self.df, columns=[feature]) feature_one_hot_cols = [col for col in self.df.columns if col.startswith(f'{feature}_')] # Map False to 0 and True to 1 self.df[feature_one_hot_cols] = self.df[feature_one_hot_cols].astype(int) return self.df def create_target_column(self, target, shift=-1): \"\"\"Create a target column for forecasting by shifting the target variable.\"\"\" # Shift the target variable 'pm2.5' by -1 to represent the next hour self.df[f'{target}_target'] = self.df[target].shift(shift) # Drop the last row as it will have NaN for the target self.df.dropna(inplace=True) return self.df def run_data_pipeline(self, handle_missing_method='ffill', downsample_freq=None, upsample_freq=None): \"\"\"Run the full pipeline and return the processed DataFrame.\"\"\" # Step 1: Load the data self.load_data() # Step 2: Handle missing data self.handle_missing_data(method=handle_missing_method) # Step 3: Set the datetime index self.set_datetime_index() # Step 4: Downsample the data if downsample_freq: self.downsample(freq=downsample_freq) # Step 5: Optional upsampling (if upsample_freq is specified) if upsample_freq: self.upsample(freq=upsample_freq) # Part2: Feature engineering # one hot encoding for categorial variable self.one_hot_encoding(feature='cbwd') # create target variable with lag of 1 self.create_target_column(target='pm2.5', shift=-1) # Return the processed DataFrame return self.df", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "# src/ml_model.py import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import root_mean_squared_error import random class ForecastModel: def __init__(self, n_estimators=100, random_state=42, **kwargs): \"\"\" Initialize the ForecastModel with a Random Forest Regressor. Parameters: - n_estimators (int): Number of trees in the forest. - random_state (int): Controls the randomness of the estimator. - **kwargs: Additional keyword arguments for RandomForestRegressor. \"\"\" self.model = RandomForestRegressor( n_estimators=n_estimators, random_state=random_state, **kwargs ) def fit(self, X, y): \"\"\" Fit the Random Forest model to the training data. Parameters: - X (DataFrame or ndarray): Feature matrix for training. - y (Series or ndarray): Target vector for training. \"\"\" self.model.fit(X, y) def train_with_sliding_window(self, X, y, window_size, test_ratio=0.2, step_size=1, random_state=None): \"\"\" Train and evaluate the model using a sliding window approach, with entire windows randomly assigned to train or test batches. Parameters: - X (DataFrame): Feature matrix. - y (Series): Target vector. - window_size (int): Number of samples in each window (batch). - step_size (int): Step size for the sliding window (default is 1). - test_ratio (float): Proportion of the windows to be used for testing (e.g., 0.2 for 20%). - random_state (int): Seed for random number generator for reproducibility. Returns: - rmse_dict (dict): Dictionary containing RMSE values for training and testing. - predictions (list): List of predictions on the test batches. - actuals (list): List of actual target values from the test batches. \"\"\" n_samples = len(X) rmse_dict = { 'train': [], 'test': [], } predictions = [] actuals = [] # Generate indices for windows window_indices = [ (start, start + window_size) for start in range(0, n_samples - window_size + 1, step_size) ] # Set random seed for reproducibility if random_state is not None: random.seed(random_state) # Shuffle the window indices randomly random.shuffle(window_indices) # Split window indices into training and testing sets based on test_ratio n_test_windows = int(len(window_indices) * test_ratio) test_window_indices = window_indices[:n_test_windows] train_window_indices = window_indices[n_test_windows:] # Training phase using training windows for start, end in train_window_indices: X_train = X.iloc[start:end] y_train = y.iloc[start:end] # Fit the model self.fit(X_train, y_train) # Evaluate on the training window train_rmse = self.evaluate_single_window(X_train, y_train) rmse_dict['train'].append(train_rmse) # Testing phase using testing windows for start, end in test_window_indices: X_test = X.iloc[start:end] y_test = y.iloc[start:end] # Predict on the test window y_pred = self.predict(X_test) # Evaluate on the test window test_rmse = root_mean_squared_error(y_test, y_pred) rmse_dict['test'].append(test_rmse) # Store predictions and actuals predictions.extend(y_pred) actuals.extend(y_test.values) return rmse_dict, predictions, actuals def evaluate_single_window(self, X, y): \"\"\" Evaluate the model's performance on a single window. Parameters: - X (DataFrame or ndarray): Feature matrix. - y (Series or ndarray): Target vector. Returns: - rmse (float): Root Mean Squared Error on the window. \"\"\" y_pred = self.predict(X) rmse = root_mean_squared_error(y, y_pred) return rmse def evaluate(self, X_train, y_train, X_test, y_test): \"\"\" Evaluate the model's performance on training and testing data. Parameters: - X_train (DataFrame or ndarray): Feature matrix for training. - y_train (Series or ndarray): Target vector for training. - X_test (DataFrame or ndarray): Feature matrix for testing. - y_test (Series or ndarray): Target vector for testing. Returns: - train_metrics (dict): Dictionary containing training metrics. - test_metrics (dict): Dictionary containing testing metrics. \"\"\" y_train_pred = self.model.predict(X_train) train_error =", "source": "ml_model.py"}, {"content": "root_mean_squared_error(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = root_mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): \"\"\" Generate predictions using the trained model. Parameters: - X (DataFrame or ndarray): Feature matrix for prediction. Returns: - y_pred (ndarray): Predicted target values. \"\"\" return self.model.predict(X)", "source": "ml_model.py"}, {"content": "# src/ml_model.py import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel2: def __init__(self, n_estimators=100, random_state=42, **kwargs): \"\"\" Initialize the ForecastModel with a Random Forest Regressor. Parameters: - n_estimators (int): Number of trees in the forest. - random_state (int): Controls the randomness of the estimator. - **kwargs: Additional keyword arguments for RandomForestRegressor. \"\"\" self.model = RandomForestRegressor( n_estimators=n_estimators, random_state=random_state, **kwargs ) self.feature_names = None # To store feature names self.feature_importances_list = [] # To store feature importances def fit(self, X, y): \"\"\" Fit the Random Forest model to the training data. Parameters: - X (DataFrame or ndarray): Feature matrix for training. - y (Series or ndarray): Target vector for training. \"\"\" self.model.fit(X, y) self.feature_names = X.columns # Store feature names def evaluate(self, y_true, y_pred): \"\"\" Evaluate the model's performance using RMSE. Parameters: - y_true (array-like): True target values. - y_pred (array-like): Predicted target values. Returns: - rmse (float): Root Mean Squared Error. \"\"\" # Convert to array-like if they are scalars if np.isscalar(y_true): y_true = [y_true] if np.isscalar(y_pred): y_pred = [y_pred] mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) return rmse def predict(self, X): \"\"\" Generate predictions using the trained model. Parameters: - X (DataFrame or ndarray): Feature matrix for prediction. Returns: - y_pred (ndarray): Predicted target values. \"\"\" return self.model.predict(X) def train_with_sliding_window(self, X, y, window_size, step_size=1): \"\"\" Train and evaluate the model using a sliding window approach. Parameters: - X (DataFrame): Feature matrix. - y (Series): Target vector (pm2.5_target). - window_size (int): Number of samples in the training window. - step_size (int): Step size for the sliding window (default is 1). Returns: - rmse_list (list): List of RMSE values for each window. - predictions (list): List of predictions. - actuals (list): List of actual target values. \"\"\" n_samples = len(X) rmse_list = [] predictions = [] actuals = [] # Initialize list to store feature importances self.feature_importances_list = [] for start in range(0, n_samples - window_size, step_size): end = start + window_size train_X = X.iloc[start:end] train_y = y.iloc[start:end] # The target is already shifted; the next value is at 'end' test_X = X.iloc[end:end+1] test_y = y.iloc[end:end+1] # Check if test set has data if test_X.empty or test_y.empty: continue # Train the model self.fit(train_X, train_y) # Store feature importances self.feature_importances_list.append(self.model.feature_importances_) # Predict y_pred = self.predict(test_X)[0] y_true = test_y.values[0] # Evaluate rmse = self.evaluate(y_true, y_pred) # Store results rmse_list.append(rmse) predictions.append(y_pred) actuals.append(y_true) # Optional: Print progress every 10,000 splits if (start + 1) % 10000 == 0: print(f'Processed {start + 1} samples out of {n_samples}') return rmse_list, predictions, actuals def get_mean_feature_importances(self): \"\"\" Calculate and return the mean feature importances across all windows. Returns: - mean_importances (Series): Mean feature importances sorted in descending order. \"\"\" if not self.feature_importances_list: raise ValueError(\"Feature importances have not been computed. Please run train_with_sliding_window first.\") # Convert list of arrays to DataFrame feature_importances_df = pd.DataFrame( self.feature_importances_list, columns=self.feature_names ) # Calculate the mean importance for each feature mean_importances = feature_importances_df.mean().sort_values(ascending=False) return mean_importances", "source": "ml_model_w_feature_impt.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader class RNNModel(nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size, dropout=0.2, device='cpu'): super().__init__() self.device = torch.device(device if torch.cuda.is_available() else 'cpu') # LSTM layer with dropout self.lstm = nn.LSTM(input_size=input_size, hidden_size=num_rnn, num_layers=num_layers, batch_first=True, dropout=dropout) # Fully connected layer self.fc = nn.Linear(num_rnn, output_size) # Move model to the specified device self.to(self.device) def forward(self, x): x = x.to(self.device) # Initialize hidden and cell states with zeros h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(self.device) c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(self.device) # Forward propagate LSTM out, _ = self.lstm(x, (h0, c0)) # out: (batch_size, lookback, num_rnn) # Take the output from the last time step out = out[:, -1, :] # (batch_size, num_rnn) # Pass through the fully connected layer out = self.fc(out) # (batch_size, output_size) return out def fit(self, dataloader, epochs=20, learning_rate=0.001, verbose=True): criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) self.train() for epoch in range(1, epochs + 1): epoch_loss = 0.0 for batch_idx, (features, labels) in enumerate(dataloader): features = features.to(self.device) labels = labels.to(self.device) # Shape: (batch_size, 1) optimizer.zero_grad() outputs = self.forward(features) # Shape: (batch_size, 1) loss = criterion(outputs, labels) loss.backward() optimizer.step() epoch_loss += loss.item() average_loss = epoch_loss / len(dataloader) if verbose: print(f\"Epoch [{epoch}/{epochs}], Loss: {average_loss:.4f}\") def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(self.device) outputs = self.forward(features) predictions.append(outputs.cpu()) predictions = torch.cat(predictions, dim=0) return predictions def evaluate(self, dataloader): self.eval() criterion = nn.MSELoss() total_loss = 0.0 total_samples = 0 with torch.no_grad(): for features, labels in dataloader: features = features.to(self.device) labels = labels.to(self.device) # Shape: (batch_size, 1) outputs = self.forward(features) loss = criterion(outputs, labels) batch_size = features.size(0) total_loss += loss.item() * batch_size total_samples += batch_size mse = total_loss / total_samples return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, features, labels, lookback, lookahead): \"\"\" Initializes the CustomWindowGenerator dataset for supervised learning. Args: features (pd.DataFrame or np.ndarray): Input feature data. labels (pd.Series or np.ndarray): Target labels. lookback (int): Number of past time steps to use as input. lookahead (int): Number of time steps ahead to predict. \"\"\" # Convert features to NumPy array if it's a DataFrame if isinstance(features, pd.DataFrame): self.features = features.values elif isinstance(features, np.ndarray): self.features = features else: raise TypeError(\"Features should be a NumPy array or a Pandas DataFrame.\") # Convert labels to NumPy array if it's a Series if isinstance(labels, pd.Series): self.labels = labels.values elif isinstance(labels, np.ndarray): self.labels = labels else: raise TypeError(\"Labels should be a NumPy array or a Pandas Series.\") self.lookback = lookback self.lookahead = lookahead # Ensure that features and labels have compatible lengths if len(self.features) != len(self.labels): raise ValueError(\"Features and labels must have the same number of samples.\") # Calculate the total number of windows self.length = len(self.features) - self.lookback - self.lookahead + 1 if self.length < 1: raise ValueError(\"Not enough data to create even one window. \" \"Consider reducing lookback or lookahead.\") self.num_features = self.features.shape[1] def __len__(self): \"\"\" Returns the total number of windows. \"\"\" return self.length def __getitem__(self, idx): \"\"\" Retrieves the input features and label for a given window index. Args: idx (int): The index of the window. Returns: tuple: (features, labels) - features: Tensor of shape (lookback, num_features) - labels: Tensor of shape (output_size,) \"\"\" if idx < 0 or idx >= self.length: raise IndexError(\"Index out of range.\") # Define the start and end indices for the input features start = idx end = idx + self.lookback # Input features: from start to end (exclusive) features_window = self.features[start:end] # Label: the data point lookahead steps after the end of features label_idx = end + self.lookahead - 1 label = self.labels[label_idx] # Convert to tensors features_tensor = torch.tensor(features_window, dtype=torch.float32) label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0) # Shape: (1,) return features_tensor, label_tensor", "source": "windowing.py"}, {"content": "def hello(): \"\"\" Returns a greeting message. Returns: str: A greeting message \"Hello, World!\". \"\"\" return \"Hello, World!\"", "source": "__init__.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import warnings from typing import Tuple import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from category_encoders.count import CountEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder def transform(data_path) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: \"\"\" Description of the function. :param data_path: file path to the data file :return: \"\"\" df = pd.read_csv(data_path) id_var = [\"id\"] # variables to be considered as categorical regroup_vars = [ \"detailed_industry_recode\", \"detailed_occupation_recode\", \"year\", \"own_business_or_self_employed\", \"veterans_benefits\", ] categorical_vars = [ var for var in df.select_dtypes(include=\"object\").columns.tolist() + regroup_vars if var != \"income_group\" ] # convert to categorical for var in regroup_vars: df[var] = pd.Categorical(df[var]) numeric_vars = [ var for var in df.select_dtypes(include=\"int64\").columns.tolist() if var not in (id_var + regroup_vars) ] # Define X as input features and y as the outcome variable X = df[numeric_vars + categorical_vars] y = df[[\"income_group\"]] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42, stratify=df[\"income_group\"] ) # Build a preprocessing step for numeric features prep_num = make_pipeline(MinMaxScaler()) # Build a preprocessing step for nominal features ordinal_vars = [\"year\"] categorical_vars = [var for var in categorical_vars if var not in ordinal_vars] prep_cat = make_pipeline( SimpleImputer(strategy=\"constant\", fill_value=\"?\"), # HashingEncoder(n_components=10) CountEncoder( handle_unknown=\"value\", # handle_missing=\"value\" ), ) # Build a preprocessing step for ordinal features prep_ord = make_pipeline(OrdinalEncoder()) # If using sklearn's pipelines, use sklearn.compose.ColumnTransformer to combine these pipelines, # taking care to apply each of them to the correct set of columns. Any feature not modified by # any of the pipelines can be passed through to the output dataframe (if needed). warnings.simplefilter(action=\"ignore\", category=FutureWarning) ct = ColumnTransformer( [ (\"numeric_preprocess\", prep_num, numeric_vars), (\"categorical_preprocess\", prep_cat, categorical_vars), (\"ordinal_preprocess\", prep_ord, ordinal_vars), ], remainder=\"passthrough\", verbose_feature_names_out=False, ) X_train = ct.fit_transform(X_train) X_test = ct.transform(X_test) # Encode y y_enc = OrdinalEncoder(categories=[[\"- 50000.\", \"50000+.\"]]) y_train = y_enc.fit_transform(y_train).reshape(-1) y_test = y_enc.transform(y_test).reshape(-1) return X_train, X_test, y_train, y_test", "source": "datapipeline.py"}, {"content": "from typing import Self, Tuple import numpy as np class Node: def __init__( self, X: np.ndarray, y: np.ndarray, var_idx: int = None, threshold: float = None, left: Self = None, right: Self = None, value: int = None, ): self.value = value self.var_idx = var_idx self.X = X self.y = y if X is not None: self.gini = self.calculate_gini((y == 0) * 1, y) assert X.shape[0] == len(y) self.threshold = threshold self.left = left self.right = right def calculate_gini(self, a: list, b: list) -> float: \"\"\" Calculates the gini coefficient Params: ------- a: list row indexes of the dataset with class 0 labels b: list row indexes of the dataset with class 1 labels Returns: -------- float gini impurity score of a node \"\"\" return 1 - ((np.sum(a) / len(a)) ** 2) - ((np.sum(b) / len(b)) ** 2) def split_data( self, var_idx, threshold ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: \"\"\" Find the data row indexes of left and right child node. Params: ------- threshold: float threshold to partition the value space of a variable Returns: -------- (np.ndarray, np.ndarray, np.ndarray, np.ndarray) X and y data subsets partitioned at the `threshold` - X_left: subset of X by filtering X on x <= `threshold` - X_right: subset of X by filtering X on x > `threshold` - y_left: subset of y corresponding to X_left - y_right: subset of y corresponding to X_right \"\"\" left_idx = np.where(self.X[:, var_idx] <= threshold) right_idx = np.where(self.X[:, var_idx] > threshold) X_left = self.X[left_idx] X_right = self.X[right_idx] y_left = self.y[left_idx[0]] y_right = self.y[right_idx[0]] return X_left, X_right, y_left, y_right def calculate_gini_delta(self, var_idx: int, threshold: float) -> float: \"\"\" Calculate the change in gini given a split. Params: ------- var_idx: int column index of the variable to split on threshold: float threshold to partition the value space of a variable Returns: -------- float change in gini from making a split on `var_idx` at given `threshold` \"\"\" X_left, X_right, y_left, y_right = self.split_data(var_idx, threshold) left_gini = self.calculate_gini((y_left == 0) * 1, y_left) right_gini = self.calculate_gini((y_right == 0) * 1, y_right) delta = ( self.gini - (len(y_left) / len(self.y)) * left_gini - (len(y_right) / len(self.y)) * right_gini ) return delta def find_best_split_x(self, var_idx) -> Tuple[float, float]: \"\"\" Find the best split for a given variable at a node. Params: ------- var_idx: int column index of the variable to find the best split on Returns: -------- (float, float) - delta_best: biggest change in gini coefficient - threshold_best: best value to partition the variable on (as indicated by `var_idx`) \"\"\" delta_best = 0 threshold_best = None X_var = np.sort(self.X[:, var_idx]) for i in range(1, len(X_var)): if X_var[i - 1] != X_var[i]: threshold = (X_var[i - 1] + X_var[i]) / 2 delta = self.calculate_gini_delta(var_idx, threshold) if delta > delta_best: delta_best = delta threshold_best = threshold return delta_best, threshold_best def find_best_split(self) -> Tuple[int, float]: \"\"\" Find the best split considering all variables. Params: ------- None Returns: -------- (int, float) - var_best (int): column index of the best variable to split on - threshold_best (float): best value to partition the best variable on \"\"\" delta_best = 0 threshold_best = np.nan", "source": "decision_tree.py"}, {"content": "var_best = np.nan for col_idx in range(self.X.shape[1]): delta, threshold = self.find_best_split_x(col_idx) if delta > delta_best: delta_best = delta threshold_best = threshold var_best = col_idx if not np.isnan(var_best): self.threshold = threshold_best self.var_idx = var_best return var_best, threshold_best class DecisionTree: def __init__(self, max_depth: int = 5, min_samples_split: int = 2): self.depth = None self.max_depth = max_depth self.min_samples_split = min_samples_split self.tree_root = None def gini(self, a: list, b: list) -> float: \"\"\" Calculates the gini coefficient Params: ------- a: list row indexes of the dataset with class 0 labels b: list row indexes of the dataset with class 1 labels Returns: -------- float gini impurity score of a node \"\"\" return 1 - ((np.sum(a) / len(a)) ** 2) - ((np.sum(b) / len(b)) ** 2) def get_most_common_label(self, y): return np.bincount(y.astype(int)).argmax() def fit(self, X: np.ndarray, y: np.ndarray): \"\"\" Builds a decision tree. Params: ------- X: np.ndarray (m x n) Feature matrix Y: np.ndarray (m x 1) Label array Returns: -------- None \"\"\" self.tree_root = self.build_tree(X, y) def build_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0): \"\"\" Builds a decision tree on a given dataset. Params: ------- X: np.ndarray Feature matrix y: np.ndarray Label array depth: int tracker for current depth of the tree Returns: -------- Node class object Either a terminal node or a parent node with 2 child nodes \"\"\" # Checks if terminal node is reached and not carry out any further split. # - Number of data observations is fewer than a set minimum given by # `min_samples_split` # - Homogeneous classes (i.e all 0s or 1s) # - Depth of the tree has reached the `max_depth` set if ( (X.shape[0] < self.min_samples_split) or (len(np.unique(y)) == 1) or (depth >= self.max_depth) ): # Terminal node return Node(X=None, y=None, value=self.get_most_common_label(y)) else: node = Node(X, y) var_idx, threshold = node.find_best_split() # Check if a valid split was found if var_idx is np.nan or np.isnan(threshold): # Terminal node return Node(X=None, y=None, value=self.get_most_common_label(y)) X_left, X_right, y_left, y_right = node.split_data(var_idx, threshold) node.left = self.build_tree(X_left, y_left, depth=depth + 1) node.right = self.build_tree(X_right, y_right, depth=depth + 1) return node def predict_x(self, x: np.ndarray) -> int: \"\"\" Predict the class label given an instance of the training data. Params: ------- x: np.ndarray (1 x N) Single instance of the feature matrix Returns: -------- int Predicted class i.e. 0 or 1 \"\"\" node = self.tree_root while node.value == None: if x[node.var_idx] <= node.threshold: node = node.left else: node = node.right return node.value def predict(self, X: np.ndarray) -> np.ndarray: \"\"\" Predict the class labels given a feature matrix Params: ------- X: np.ndarray(N x M) Feature matrix e.g. test dataset Returns: -------- np.ndarray (N) Class labels corresponding to each instance of the feature matrix \"\"\" return np.array([self.predict_x(x) for x in X])", "source": "decision_tree.py"}, {"content": "import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score from sklearn.model_selection import StratifiedKFold, cross_val_score class Model: def __init__(self): # init your model here self.model = RandomForestClassifier(random_state=42) def train(self, params: dict, X_train: pd.DataFrame, y_train: np.ndarray) -> float: \"\"\" Description of the function. :param params: dict model parameters :param X_train: :param y_train: ...... :return: ...... \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.model.set_params(**params) self.model.fit(X_train, y_train.reshape(-1)) skf = StratifiedKFold(n_splits=3) scores = cross_val_score( self.model, X_train, y_train, cv=skf, scoring=\"f1_macro\" ) return np.mean(scores) def evaluate(self, X_test, y_test) -> float: \"\"\" Description of the function. :param X_test: features of the test dataset :param y_test: labels of test dataset :return: float f1-score \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.model.predict(X_test) return f1_score(y_test, y_pred) def get_default_params(self, is_tuned: bool = False) -> dict: \"\"\" Description of the function. :return: dict parameters \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model # Parameters obtained from tuning the model with Grid Search self.params_tuned = { \"max_depth\": 10, \"min_weight_fraction_leaf\": 0.02, \"n_estimators\": 100, } if is_tuned: return self.params_tuned else: return self.model.get_params()", "source": "model.py"}, {"content": "import numpy as np from scipy.stats import mode from src.decision_tree import DecisionTree class RandomForest: def __init__( self, n_trees: int = 5, subsample_size: float = 1, sample_with_replacement: bool = True, feature_proportion: float = 1, random_state: int = 42, ): self.n_trees = n_trees self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.trees = [DecisionTree() for i in range(n_trees)] self.feature_proportion = feature_proportion self.random_state = random_state def fit(self, X: np.ndarray, y: np.ndarray): \"\"\" Fits multiple decision trees to the given data. Params: ------- X: np.ndarray (N x M) Feature matrix y: np.ndarray (N) Labels matrix Returns: -------- None \"\"\" # set seed np.random.seed(self.random_state) nrows = X.shape[0] ncols = X.shape[1] for tree in self.trees: # bootstrapped row indexes of the dataset X bootstrap_idx = np.random.choice( range(nrows), size=int(nrows * self.subsample_size), replace=self.sample_with_replacement, ) X_fit = X[bootstrap_idx] y_fit = y[bootstrap_idx] # feature subsample if self.feature_proportion < 1: feature_idx = np.random.choice( range(ncols), size=int(ncols * self.feature_proportion) ) X_fit = X_fit[:, feature_idx] # Fit to data tree.fit(X_fit, y_fit) def predict(self, X: np.ndarray) -> np.ndarray: \"\"\" Predicts the class labels for a given feature matrix, by conducting a majority voting among all the decision trees. Params: ------- X: np.ndarray (N x M) Feature matrix Returns: -------- np.ndarray (N) 1D array of 0 or 1 class labels \"\"\" return mode(np.stack([tree.predict(X) for tree in self.trees], axis=0), axis=0)[ 0 ]", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from dataclasses import dataclass, field from hydra.core.config_store import ConfigStore @dataclass class config_dataloader: batch_size_train: int = 256 batch_size_val: int = 256 batch_size_test: int = 256 random_state: int = 42 @dataclass class Config: dataloader: config_dataloader = field(default_factory=config_dataloader) cs = ConfigStore.instance() cs.store(name=\"base_config\", node=Config)", "source": "config.py"}, {"content": "from typing import Tuple import numpy as np import pandas as pd from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler from src.metrics import get_class_ratio class Datapipeline: def __init__(self): self.data_cleaning() self.train_val_test_split() self.set_preprocessor() def data_cleaning(self, data_path=\"data/raw/A1.csv\") -> pd.DataFrame: df = pd.read_csv(data_path) df = df.drop_duplicates() # Drop features which are highly collinear # df = df.drop([\"V13\", \"V21\"], axis=1) self.df = df def train_val_test_split(self): \"\"\" Splits the data into train, validation and test sets. Uses the class as the stratify parameter to ensure that the class distribution is the same in the train, validation and test sets. The test size is 0.33 and the validation size is 0.2 of the train set. The data is saved in the data/train, data/val and data/test directories as train_data.csv, val_data.csv and test_data.csv respectively. Parameters ---------- None Returns ------- None \"\"\" data_train, data_test = train_test_split( self.df, test_size=0.33, random_state=42, stratify=self.df[\"Class\"], ) data_train, data_val = train_test_split( data_train, test_size=0.2, random_state=42, stratify=data_train[\"Class\"] ) data_train.to_csv(\"data/train/data_train_A1.csv\", index=False) data_val.to_csv(\"data/val/data_val_A1.csv\", index=False) data_test.to_csv(\"data/test/data_test_A1.csv\", index=False) def set_preprocessor(self) -> None: \"\"\" Defines the preprocessor and stores as an attribute. Parameters ---------- None Returns ------- None \"\"\" self.preprocessor = ColumnTransformer( [(\"numeric_preprocess\", make_pipeline(StandardScaler()), [\"Amount\"])], remainder=\"passthrough\", verbose_feature_names_out=False, ) def transform_train_data( self, train_data_path=\"data/train/data_train_A1.csv\" ) -> Tuple[np.ndarray, np.ndarray]: \"\"\" Preprocesses the train data. Parameters ---------- train_data_path : str Path to the train data. Returns ------- X_train : np.ndarray Feature matrix. y_train : np.ndarray Target vector. \"\"\" data_train = pd.read_csv(train_data_path) X_train = data_train.drop(\"Class\", axis=1) X_train = self.preprocessor.fit_transform(X_train) y_train = data_train[\"Class\"].astype(\"float32\").to_numpy().reshape(-1) return X_train, y_train def transform_test_data(self, test_data_path=\"data/test/data_test_A1.csv\"): \"\"\" Preprocesses the test data. Parameters ---------- test_data_path : str Path to the test data. Returns ------- X_test : np.ndarray Feature matrix. y_test : np.ndarray Target vector. \"\"\" data_test = pd.read_csv(test_data_path) X_test = data_test.drop(\"Class\", axis=1) X_test = self.preprocessor.transform(X_test) y_test = data_test[\"Class\"].astype(\"float32\").to_numpy().reshape(-1) return X_test, y_test @staticmethod def resample_data( X_train, y_train, do_undersample=True, do_smote=True, undersample_ratio=\"auto\", smote_ratio=\"auto\", verbose=True, ) -> Tuple[np.ndarray, np.ndarray]: if verbose: print(\"Shape of X_train before any resampling:\", X_train.shape) print(\"Shape of y_train before any resampling:\", y_train.shape) print( \"Class counts before any resampling:\", np.unique(y_train, return_counts=True), ) print( f\"Ratio of minority to majority class before any resampling: {get_class_ratio(y_train):.4g}\" ) print(\"\\t\") if do_undersample: # Under sample the majority class # https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html rus = RandomUnderSampler( sampling_strategy=undersample_ratio, random_state=42 ) X_train, y_train = rus.fit_resample(X_train, y_train) print(\"Shape of X_train after undersampling:\", X_train.shape) print(\"Shape of y_train after undersampling:\", y_train.shape) print( \"Class counts after undersampling:\", np.unique(y_train, return_counts=True), ) print( \"Ratio of minority to majority class after undersampling:\", get_class_ratio(y_train), ) print(\"\\t\") if do_smote: # Oversample the minority class # https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html sm = SMOTE(sampling_strategy=smote_ratio, random_state=42) X_train, y_train = sm.fit_resample(X_train, y_train) print(\"Shape of X_train after SMOTE:\", X_train.shape) print(\"Shape of y_train after SMOTE:\", y_train.shape) print( \"Class counts after SMOTE:\", np.unique(y_train, return_counts=True), ) print( f\"Ratio of minority to majority class after SMOTE: {get_class_ratio(y_train):.4g}\" ) print(\"\\t\") return X_train, y_train", "source": "datapipeline.py"}, {"content": "import numpy as np import torch from omegaconf import DictConfig from torch.utils.data import DataLoader, Dataset class myDataset(Dataset): def __init__(self, X, y): self.X = torch.from_numpy(X.astype(np.float32)) self.y = torch.from_numpy(y.astype(np.float32)).reshape(-1, 1) def __len__(self): return self.X.shape[0] def __getitem__(self, index): return self.X[index], self.y[index] class myDataModule: def __init__( self, dataset_train: myDataset, dataset_val: myDataset, dataset_test: myDataset, cfg: DictConfig, ): self.batch_size_train = cfg.dataloader.batch_size_train self.batch_size_val = cfg.dataloader.batch_size_val self.batch_size_test = cfg.dataloader.batch_size_test self.dataset_train = dataset_train self.dataset_val = dataset_val self.dataset_test = dataset_test self.setup_dataloaders() def setup_dataloaders(self): torch.manual_seed(42) self.dataloader_train = DataLoader( self.dataset_train, batch_size=self.batch_size_train, shuffle=True ) self.dataloader_val = DataLoader( self.dataset_val, batch_size=self.batch_size_val, shuffle=False ) self.dataloader_test = DataLoader( self.dataset_test, batch_size=self.batch_size_test, shuffle=False )", "source": "dataset.py"}, {"content": "from typing import Tuple import keras import matplotlib.pyplot as plt import numpy as np from sklearn.metrics import ( confusion_matrix, f1_score, fbeta_score, precision_recall_curve, precision_score, recall_score, ) def plot_loss_curves(history: keras.callbacks.History, figsize: Tuple = (8, 5)) -> None: # Extract loss values \"\"\" Plot the training and validation loss over time. Parameters ---------- history : keras.callbacks.History The history of the training process figsize : tuple The size of the figure to create Returns ------- None \"\"\" train_loss = history.history[\"loss\"] val_loss = history.history[\"val_loss\"] # Get number of epochs epochs = range(1, len(train_loss) + 1) # Create the plot plt.figure(figsize=figsize) plt.plot(epochs, train_loss, \"b-\", label=\"Training Loss\") plt.plot(epochs, val_loss, \"r-\", label=\"Validation Loss\") # Add title and labels plt.title(\"Training and Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() # Show the plot plt.grid(True) plt.show() def f2_score(precision: float, recall: float) -> float: \"\"\" Calculate the F2-score given precision and recall. The F2-score is the weighted average of precision and recall, giving twice as much importance to recall as to precision. Parameters ---------- precision : float The precision score. recall : float The recall score. Returns ------- f2_score : float The F2-score. \"\"\" score = (5 * precision * recall) / (4 * precision + recall) return score def get_threshold_max_f2( y_test: np.ndarray, y_pred_proba: np.ndarray ) -> Tuple[float, float]: \"\"\" Get the threshold that maximises F2-score. Parameters ---------- y_test : np.ndarray The true labels y_pred_proba : np.ndarray The model predicted probabilities of the positive class Returns ------- float The threshold that maximises F2-score. float The maximum F2-score \"\"\" precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba) scores = list(map(f2_score, precision, recall)) return thresholds[np.argmax(scores)], np.max(scores) def plot_precision_recall_curves( y_test: np.ndarray, y_pred_proba: np.ndarray, figsize: Tuple = (12, 5) ) -> float: \"\"\" Plot precision and recall vs. threshold, as well as the precision-recall curve. Parameters ---------- y_test : np.ndarray Ground truth target values. y_pred_proba : np.ndarray Estimated probabilities. figsize : tuple (default is (12, 5)) Figure size. Returns ------- optimal_threshold : float Threshold that maximizes the F2-score. \"\"\" precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba) optimal_threshold, max_f2 = get_threshold_max_f2(y_test, y_pred_proba) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize) ax1.plot(thresholds, precision[:-1], \"b--\", label=\"Precision\") ax1.plot(thresholds, recall[:-1], \"g-\", label=\"Recall\") ax1.axvline( x=optimal_threshold, color=\"r\", linestyle=\"--\", label=f\"Max F2-score={max_f2:.2f} at threshold={optimal_threshold:.2f}\", ) ax1.set_xlabel(\"Threshold\") ax1.set_ylabel(\"Score\") ax1.set_title(\"Precision and Recall vs. Threshold\") ax1.legend(loc=\"best\") # Find the recall value corresponding to the optimal threshold optimal_recall = recall[np.where(thresholds == optimal_threshold)[0][0]] # Precision-Recall curve ax2.plot(recall, precision, \"b-\", label=\"Precision-Recall curve\") ax2.axvline( x=optimal_recall, color=\"r\", linestyle=\"--\", label=f\"Recall={optimal_recall:.2f} at threshold={optimal_threshold:.2f}\", ) ax2.set_xlabel(\"Recall\") ax2.set_ylabel(\"Precision\") ax2.set_title(\"Precision-Recall Curve\") ax2.legend(loc=\"best\") plt.tight_layout() plt.show() return optimal_threshold def generate_scores( y_test: np.ndarray, y_pred_proba: np.ndarray, threshold: float ) -> None: # convert predicted probabilities to binary values \"\"\" Generates various scores given the ground truth labels and predicted probabilities. Parameters ---------- y_test : np.ndarray The ground truth labels y_pred : np.ndarray The predicted probabilities threshold : float The threshold above which the predicted probabilities are considered positive Returns ------- None Notes ----- The function prints out the confusion matrix, precision score, recall score, F1 score, and F2 score. \"\"\" y_pred = np.where(y_pred_proba >= threshold, 1, 0) print(\"Confusion matrix:\") cm = confusion_matrix(y_test, y_pred) print(cm) print(\"\\t\") print(f\"Precision score: {precision_score(y_test, y_pred):.4f}\") print(f\"Recall score: {recall_score(y_test, y_pred):.4f}\") print(f\"F1 score: {f1_score(y_test, y_pred):.4f}\") # We track", "source": "metrics.py"}, {"content": "the F2 score, assuming that we prioritize recall as twice as important as precision print(f\"F2 score: {fbeta_score(y_test, y_pred, beta=2):.4f}\") def get_class_ratio(y: np.ndarray) -> float: \"\"\" Calculate the class ratio of the given labels. The class ratio is defined as the number of minority class samples over the number of majority class samples. The minority class is assumed to be the class with the label 1, and the majority class is assumed to be the class with the label 0. Parameters ---------- y : np.ndarray The labels Returns ------- float The class ratio \"\"\" minority_cnt = np.sum(np.where(np.array(y) == 1, 1, 0)) majority_cnt = np.sum(np.where(np.array(y) == 0, 1, 0)) return minority_cnt / majority_cnt", "source": "metrics.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.w1, self.b1 = self.initialise_weights((input_size, hidden_size)) self.w2, self.b2 = self.initialise_weights((hidden_size, output_size)) self.learning_rate = 1e-3 def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" h1, self.h1_cache = self.linear_forward( features.reshape(-1, self.w1.shape[0]), self.w1, self.b1, ) h1, self.h1_cache_sigmoid = self.sigmoid_forward(h1) h2, self.h2_cache = self.linear_forward(h1, self.w2, self.b2) self.h2 = self.softmax(h2) return self.h2 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the cross-entropy loss \"\"\" loss = -np.mean(np.log(predictions[np.arange(predictions.shape[0]), label])) self.label = label return loss def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" N, _ = self.h2.shape dh2 = self.h2.copy() dh2[np.arange(N), self.label] -= 1 dh2 /= N dh1, dw2, db2 = self.linear_backward(dh2, self.h2_cache) dh1 = self.sigmoid_backward(dh1, self.h1_cache_sigmoid) dx, dw1, db1 = self.linear_backward(dh1, self.h1_cache) # Step to update weights self.w2 = self.w2 - self.learning_rate * dw2 self.b2 = self.b2 - self.learning_rate * db2 self.w1 = self.w1 - self.learning_rate * dw1 self.b1 = self.b1 - self.learning_rate * db1 def initialise_weights(self, dim: tuple): weights = np.random.normal(0, 1 / np.sqrt(dim[0]), dim) bias = np.zeros((1, dim[1])) return weights, bias def softmax(self, x: np.ndarray) -> np.ndarray: x_softmax = np.exp(x) / np.sum(np.exp(x), axis=1).reshape(-1, 1) np.testing.assert_array_almost_equal( np.sum(x_softmax, axis=1), np.ones(x_softmax.shape[0]) ) return x_softmax def linear_forward(self, x, w, b): out = np.dot(x, w) + b cache = (x, w, b) return out, cache def linear_backward(self, dout, cache): x, w, b = cache N, D = x.shape D, M = w.shape dx = dout.dot(w.T) dw = x.T.dot(dout) db = np.ones(N).dot(dout).reshape(1, M) return dx, dw, db def sigmoid(self, x): out = 1 / (1 + np.exp(-x)) return out def sigmoid_forward(self, x: np.ndarray) -> np.ndarray: cache = x.copy() out = self.sigmoid(x) return out, cache def sigmoid_backward(self, dout, cache): out = self.sigmoid(cache) dx = dout * out * (1 - out) return dx if __name__ == \"__main__\": model = MLPTwoLayers(input_size=4, hidden_size=16, output_size=3) preds = model.forward(np.array([5.1, 3.5, 1.4, 0.2])) print(preds) train_loss = model.loss(preds, 0) model.backward()", "source": "mlp.py"}, {"content": "import pandas as pd from sklearn.model_selection import train_test_split class Datapipeline: def __init__(self): pass def train_val_test_split(self, df): data_train, data_test = train_test_split( df, test_size=0.33, random_state=42, stratify=df[\"y\"], ) data_train, data_val = train_test_split( data_train, test_size=0.2, random_state=42, stratify=data_train[\"y\"] ) return data_train, data_val, data_test def transform(self, data_path): df = pd.read_csv(data_path) X, y = ( df.drop([\"id\", \"y\"], axis=1).to_numpy(), df[\"y\"].to_numpy(), ) return X, y", "source": "mlp_datapipeline.py"}, {"content": "import torch import torch.nn as nn class Block(nn.Module): def __init__(self, size_input, size_ouput): super(Block, self).__init__() self.block_layers = nn.Sequential( nn.Linear(size_input, size_ouput), nn.BatchNorm1d(num_features=size_ouput, momentum=0.99), nn.LeakyReLU(negative_slope=0.1), nn.Dropout(p=0.3), ) def forward(self, x): x = self.block_layers(x) return x class Model(nn.Module): def __init__(self): super(Model, self).__init__() torch.manual_seed(42) self.block1 = Block(29, 32) self.block2 = Block(32, 16) self.block3 = Block(16, 8) self.block4 = nn.Sequential( nn.Linear(8, 4), nn.BatchNorm1d(num_features=4, momentum=0.99), nn.LeakyReLU(negative_slope=0.1), nn.Linear(4, 1), ) def forward(self, x): x = self.block1(x) x = self.block2(x) x = self.block3(x) x = self.block4(x) return x", "source": "models.py"}, {"content": "import matplotlib.pyplot as plt import torch import torch.nn as nn from torch.utils.data import DataLoader from tqdm import tqdm, trange from src.dataset import myDataModule class Trainer: def __init__( self, model: nn.Module, optimizer: torch.optim, data_module: myDataModule, random_state=42, ): self.model = model self.optimizer = optimizer self.data_module = data_module self.random_state = random_state self.loss_train = [] self.loss_val = [] def train_one_epoch( self, dataloader: DataLoader, is_train: bool = True, ): self.model.train() if is_train else self.model.eval() running_loss = 0 num_instances = 0 loss_fn = nn.BCEWithLogitsLoss() for batch_idx, (X, y) in enumerate(dataloader): if is_train: self.optimizer.zero_grad() logits = self.model(X) loss = loss_fn(logits, y) loss.backward() self.optimizer.step() else: with torch.no_grad(): logits = self.model(X) loss = loss_fn(logits, y) running_loss += loss.item() num_instances += X.size(0) return running_loss / num_instances def train(self, num_epochs): torch.manual_seed(self.random_state) for epoch in trange( num_epochs, desc=\"Training loop\", position=0, total=num_epochs ): loss_train_epoch = self.train_one_epoch( self.data_module.dataloader_train, is_train=True ) loss_val_epoch = self.train_one_epoch( self.data_module.dataloader_val, is_train=False ) self.loss_train.append(loss_train_epoch) self.loss_val.append(loss_val_epoch) def predict(self): self.model.eval() y_logits = None for batch_idx, (X_test, y_test) in tqdm( enumerate(self.data_module.dataloader_test), desc=\"Predicting, cycling through test dataloader\", ): y_lgt = self.model(X_test) y_logits = y_lgt if y_logits is None else torch.cat((y_logits, y_lgt)) y_proba = torch.sigmoid(y_logits) return y_proba.detach().numpy() def plot_loss_curves(self, figsize=(8, 5)): # Create the plot plt.figure(figsize=figsize) epochs = range(1, len(self.loss_train) + 1) plt.plot(epochs, self.loss_train, \"b-\", label=\"Training Loss\") plt.plot(epochs, self.loss_val, \"r-\", label=\"Validation Loss\") # Add title and labels plt.title(\"Training and Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() # Show the plot plt.grid(True) plt.show()", "source": "train.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A4.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "from dataclasses import dataclass, field from hydra.core.config_store import ConfigStore from conf.config_datamodule import * from conf.config_mlflow import * from conf.config_model import * @dataclass class Config: datamodule: config_datamodule = field(default_factory=config_datamodule) model: config_model = field(default_factory=config_model) mlflow: config_mlflow = field(default_factory=config_mlflow) optimizer: config_optimizer = field(default_factory=config_optimizer) checkpoint_path: str = \"checkpoint\" cs = ConfigStore.instance() cs.store(name=\"base_config\", node=Config)", "source": "config.py"}, {"content": "from dataclasses import dataclass, field from typing import Any import torch import torchvision from torchvision.transforms import v2 class config_datamodule_: def __init__( self, batch_size_train: int = 128, batch_size_val: int = 128, batch_size_test: int = 128, random_state: int = 42, do_augment: bool = True, augment_prop: float = 0.5, ): self.batch_size_train = batch_size_train self.batch_size_val = batch_size_val self.batch_size_test = batch_size_test self.random_state = random_state self.transform_train = v2.Compose( [ v2.PILToTensor(), v2.ToDtype(torch.float32, scale=True), # image net normalization statistics # v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # -------------------------------------------------------------- # Normalising stats for 2 class tensor food of curry puff and tau huay ## limitation is that it is calculated on training set so ## size is dependent on train-test split v2.Normalize([0.8967, 0.5826, 0.2022], [1.0705, 1.1498, 1.3385]), v2.Resize((150, 150)), v2.CenterCrop((128, 128)), ] ) self.transform_test = v2.Compose( [ v2.PILToTensor(), v2.ToDtype(torch.float32, scale=True), # image net normalization statistics # v2.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]), # -------------------------------------------------------------- v2.Normalize([0.8967, 0.5826, 0.2022], [1.0705, 1.1498, 1.3385]), v2.Resize((128, 128)), ] ) self.do_augment = do_augment self.augment_transform = v2.Compose( [ v2.RandomHorizontalFlip(p=0.65), v2.RandomRotation(degrees=[-45, 45]), # v2.RandomPerspective(distortion_scale=0.25, p=0.5), # v2.ColorJitter( # brightness=0.25, contrast=0.25, saturation=0.25, hue=0.25 # ), ] ) self.augment_prop = augment_prop @dataclass class config_datamodule: _target_: str = \"conf.config_datamodule.config_datamodule_\"", "source": "config_datamodule.py"}, {"content": "from dataclasses import dataclass, field @dataclass class config_mlflow: mlflow_exp_name: str = \"assignment5\" mlflow_run_name: str = \"A5P2_train\" setup_mlflow: bool = True mlflow_autolog: bool = False mlflow_tracking_uri: str = \"./mlruns\" # mlflow_tracking_uri: str = \"https://mlflow.aiap17.aisingapore.net/\" @dataclass class config_optimizer: # _target_: str = \"torch.optim.RMSprop\" # lr: float = 1e-3 # alpha: float = 0.9 _target_: str = \"torch.optim.Adam\" lr: float = 3e-5 # lr: float = 3e-4", "source": "config_mlflow.py"}, {"content": "from dataclasses import dataclass, field import torch.nn as nn @dataclass class config_model: # _target_: str = \"src.models.TL_efficientnet_v2_s\" # dropout_prob: float = 0.3 _target_: str = \"src.models.BinaryClassifier\" # _target_: str = \"src.models.MulticlassClassifier\"", "source": "config_model.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import numpy as np import torch from omegaconf import DictConfig from sklearn.model_selection import train_test_split from torch.utils.data import ConcatDataset, DataLoader, Dataset, RandomSampler, Subset from torchvision.datasets import ImageFolder class aug_dataset(Dataset): def __init__(self, X, y, transform=None, target_transform=None): self.X = X self.y = y self.transform = transform self.target_transform = target_transform def __len__(self): return self.X.shape[0] def __getitem__(self, index): img = self.X[index] label = self.y[index] if self.transform: img = self.transform(img) if self.target_transform: label = self.target_transform(label) return img, label class DataModule: def __init__( self, config: DictConfig, root=\"data/img-classification/tensorfood\", ): torch.manual_seed(42) self.config = config self.dataset_transform_train = ImageFolder( root=root, transform=getattr(config, \"transform_train\", None), target_transform=getattr(config, \"target_transform\", None), ) self.dataset_transform_test = ImageFolder( root=root, transform=getattr(config, \"transform_test\", None), target_transform=getattr(config, \"target_transform\", None), ) idx_train_val, idx_test = train_test_split( np.arange(len(self.dataset_transform_train.targets)), test_size=0.2, random_state=42, shuffle=True, stratify=self.dataset_transform_train.targets, ) idx_train, idx_val = train_test_split( idx_train_val, test_size=0.2, random_state=42, shuffle=True, stratify=np.array(self.dataset_transform_train.targets)[idx_train_val], ) self.dataset_train = Subset(self.dataset_transform_train, idx_train) self.dataset_val = Subset(self.dataset_transform_test, idx_val) self.dataset_test = Subset(self.dataset_transform_test, idx_test) # Augment if config.do_augment: self.dataset_train_aug = self.create_augument_dataset() self.dataset_train = ConcatDataset( [self.dataset_train, self.dataset_train_aug] ) self.setup_dataloaders() def setup_dataloaders(self): self.dataloader_train = DataLoader( self.dataset_train, batch_size=self.config.batch_size_train, shuffle=False, drop_last=True, sampler=RandomSampler(self.dataset_train), ) self.dataloader_val = DataLoader( self.dataset_val, batch_size=self.config.batch_size_val, shuffle=False ) self.dataloader_test = DataLoader( self.dataset_test, batch_size=self.config.batch_size_test, shuffle=False ) def create_augument_dataset(self) -> Dataset: # Looping over the existing train dataset to collect # - images (torch.tensor) and # - labels (list) imgs, labels = [], [] for i in range(len(self.dataset_train)): imgs.append(self.dataset_train[i][0]) labels.append(self.dataset_train[i][1]) imgs = torch.stack(imgs, dim=0) # labels = torch.tensor(labels) imgs_aug = [] labels_aug = [] for cls in np.unique(labels): # indexes of the images of the current class idx = torch.where(torch.tensor(labels) == cls)[0] idx = idx[torch.randperm(len(idx))] # shuffle indexes # Select a subset of the indexes, images and labels idx_selected = idx[: round(len(idx) * self.config.augment_prop)] imgs_aug.append(imgs[idx_selected]) labels_aug += [labels[i] for i in idx_selected] augmented_dataset = aug_dataset( torch.cat(imgs_aug), labels_aug, transform=getattr(self.config, \"augment_transform\", None), ) return augmented_dataset def one_batch_dataloader(self): return DataLoader( Subset(self.dataset_transform_train, range(self.config.batch_size_train)), batch_size=self.config.batch_size_train, shuffle=False, )", "source": "datamodule.py"}, {"content": "import numpy as np from keras.utils import image_dataset_from_directory from sklearn.model_selection import train_test_split from tensorflow.keras.utils import to_categorical class Datapipeline: def __init__(self, data_path=\"data/img-classification/tensorfood\"): self.dataset = image_dataset_from_directory( directory=data_path, validation_split=None, image_size=(260, 260), seed=123, batch_size=64, shuffle=False, ) def train_val_test_split(self): images = [] labels = [] for img_batch, label_batch in self.dataset.as_numpy_iterator(): images.append(img_batch) labels.append(label_batch) images = np.concatenate(images) labels = np.concatenate(labels) # train/val test split X_train_val, X_test, y_train_val, y_test = train_test_split( images, labels, test_size=0.2, stratify=labels, random_state=42 ) # train val split X_train, X_val, y_train, y_val = train_test_split( X_train_val, y_train_val, test_size=0.2, stratify=y_train_val, random_state=42, ) # convert to categorical format y_train = to_categorical(y_train, num_classes=12) y_val = to_categorical(y_val, num_classes=12) y_test = to_categorical(y_test, num_classes=12) return X_train, X_val, X_test, y_train, y_val, y_test", "source": "datapipeline.py"}, {"content": "import os import pprint import random import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns import torch import torch.nn as nn import torchvision.transforms as transforms from omegaconf import DictConfig from PIL import Image from torch.utils.data import DataLoader, Dataset from torchvision.utils import make_grid def collect_image_info(directory) -> dict: # Dictionary to store the count of images in each subfolder image_counts = {} # Iterate over each subfolder in the main folder for subfolder in os.listdir(directory): subfolder_path = os.path.join(directory, subfolder) # Check if the path is a directory if os.path.isdir(subfolder_path): # Count the number of files in the subfolder images_paths = [ os.path.join(subfolder_path, file) for file in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, file)) and (file != \".DS_Store\") ] num_images = len(images_paths) image_counts[subfolder] = {} image_counts[subfolder][\"Number of images\"] = 0 image_counts[subfolder][\"File extension\"] = {} image_counts[subfolder][\"Shape\"] = {} image_counts[subfolder][\"Number of images\"] = num_images for img in images_paths: file_ext = os.path.splitext(img)[1] image_counts[subfolder][\"File extension\"][file_ext] = ( image_counts[subfolder][\"File extension\"].get(file_ext, 0) + 1 ) with Image.open(img) as f: shape = (*f.size, len(f.getbands())) # W, H, C image_counts[subfolder][\"Shape\"][shape] = ( image_counts[subfolder][\"Shape\"].get(shape, 0) + 1 ) return image_counts def plot_class_counts(classes, class_counts): sorted_data = sorted(zip(class_counts, classes)) sorted_class_counts, sorted_classes = zip(*sorted_data) plt.bar(sorted_classes, sorted_class_counts, color=\"skyblue\", edgecolor=\"black\") # Add labels and title plt.xlabel(\"Classes\") plt.ylabel(\"Number of images\") plt.title(\"Number of images per class\") # Add gridlines plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7) # Rotate x-axis labels plt.xticks(rotation=45, ha=\"right\") # Show the plot plt.show() def plot_file_extension_counts(classes: list, ext_counts: list[dict]): # Combine jpg and jpeg keys new_ext_counts = [] for dic in ext_counts: new_dic = {} for k, v in dic.items(): if k in [\".jpg\", \".jpeg\"]: new_dic[\".jpg\"] = new_dic.get(\".jpg\", 0) + v else: new_dic[k] = v new_ext_counts.append(new_dic) # Prepare the data for tabling data = [] for cls, new_ext_counts in zip(classes, new_ext_counts): for ext, count in new_ext_counts.items(): data.append( { \"class\": cls, \"extension\": ext, \"count\": count, } ) # Create DataFrame df = pd.DataFrame(data) # Aggregate counts by class class_counts = df.groupby(\"class\")[\"count\"].sum().reset_index() # Sort classes by total count sorted_classes = class_counts.sort_values(by=\"count\")[\"class\"] # Reorder the original DataFrame based on sorted classes df[\"class\"] = pd.Categorical(df[\"class\"], categories=sorted_classes, ordered=True) df = df.sort_values(\"class\") # Plotting plt.figure(figsize=(10, 6)) sns.barplot(x=\"class\", y=\"count\", hue=\"extension\", data=df, errorbar=None) # Rotate x-axis labels plt.xticks(rotation=45, ha=\"right\") # Add labels and title plt.xlabel(\"Class\") plt.ylabel(\"Number of Images\") plt.title(\"Number of Images by File Extension and Class\") # Show the plot plt.show() def plot_image_height_width(classes: list, class_shapes: list[dict]): # Define the grid size rows, cols = 4, 3 # Create a figure with a 4x3 grid of subplots fig, axes = plt.subplots(rows, cols, figsize=(12, 12)) # Flatten the axes array for easy iteration axes = axes.flatten() # Plotting the scatter plot for each class for i, (class_name, shape_dict) in enumerate(zip(classes, class_shapes)): x = [key[0] for key in shape_dict.keys()] # W y = [key[1] for key in shape_dict.keys()] # H axes[i].scatter(x, y, label=class_name) # Draw y=x line min_val = min(min(x), min(y)) max_val = max(max(x), max(y)) axes[i].plot([min_val, max_val], [min_val, max_val], \"k--\", lw=1) axes[i].set_title(f\"Image dimensions for {class_name}\") axes[i].set_xlabel(\"Width\") axes[i].set_ylabel(\"Height\") # Adjust layout to prevent overlap plt.tight_layout() # Display the plot plt.show() def plot_sample_images( dataset, mean: list = [0.485, 0.456, 0.406], std: list = [0.229, 0.224, 0.225], num_samples=9, ): # Randomly sample", "source": "eda.py"}, {"content": "indices from the dataset sampled_indices = random.sample(range(len(dataset)), num_samples) # Create a figure for plotting fig, axs = plt.subplots(3, 3, figsize=(9, 9)) axs = axs.flatten() # Flatten the 2D array of axes for easy iteration # Loop through sampled indices and plot each image for i, idx in enumerate(sampled_indices): image, label = dataset[idx] # Get image and label from the dataset # inverse the normalisation image = image.permute(1, 2, 0) * torch.tensor(std) + torch.tensor(mean) image = image.numpy() image = transforms.ToPILImage()(image) axs[i].imshow(image) axs[i].axis(\"off\") # Hide axes axs[i].set_title(f\"Label: {label}\") # Show label as title plt.tight_layout() # Adjust layout for better spacing plt.show() # Display the plot def find_bad_predictions(model: nn.Module, dataloader: DataLoader): # turn shuffle off dataloader_shuffle_off = DataLoader( dataloader.dataset, dataloader.batch_size, shuffle=False ) model.eval() bad_predictions = [] overall_idx = 0 with torch.no_grad(): for X, y in dataloader_shuffle_off: output = model(X) y_pred = torch.argmax(output, dim=1) bad_pred_idx = (y_pred != y).nonzero().flatten() if bad_pred_idx.numel() > 0: bad_pred_idx = bad_pred_idx + overall_idx bad_predictions = bad_predictions + bad_pred_idx.tolist() overall_idx += len(y) return bad_predictions def plot_images_from_indexes(dataset: Dataset, indexes: list): # Subset indexes to display indexes = list(map(lambda i: indexes[i], torch.randperm(len(indexes))))[:16] print(f\"subset {indexes}\") # Subset the dataset subset = torch.utils.data.Subset(dataset, indexes) # Create a list to store images images = [] for idx in indexes: image, _ = dataset[idx] # Assuming dataset returns (image, label) images.append(image) images = list( map( lambda x: x * torch.tensor([1.0705, 1.1498, 1.3385]).view(3, 1, 1) + torch.tensor([0.8967, 0.5826, 0.2022]).view(3, 1, 1), images, ) ) # Create a grid of images grid = make_grid(images, nrow=4) grid = grid.permute(1, 2, 0) # Convert from CHW to HWC # Plot the grid plt.figure(figsize=(8, 8)) plt.imshow(grid) plt.axis(\"off\") plt.show()", "source": "eda.py"}, {"content": "import logging import os import time import mlflow logger = logging.getLogger(__name__) def mlflow_init(args, run_name=\"train-model\", setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get( \"mlflow_run_name\", run_name ) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "mlflow_setup.py"}, {"content": "import torch import torch.nn as nn import torch.nn.functional as F from torchvision import models class TL_efficientnet_v2_s(nn.Module): def __init__(self, dropout_prob): super(TL_efficientnet_v2_s, self).__init__() self.name = \"efficientnet_v2_s\" efficientnet = models.efficientnet_v2_s( weights=models.EfficientNet_V2_S_Weights.IMAGENET1K_V1 ) self.base_model = nn.Sequential( efficientnet.features, efficientnet.avgpool, efficientnet.classifier[0] ) self.add_on_layers = nn.Sequential( nn.Linear(1280, 512), nn.Dropout(p=dropout_prob), nn.BatchNorm1d(num_features=512), nn.ReLU(), nn.Linear(512, 12), nn.Dropout(p=dropout_prob), ) self.__init_weights() def forward(self, x): x = self.base_model(x) x = self.add_on_layers(x.squeeze()) return x def __init_weights(self): for m in self.add_on_layers.modules(): if isinstance(m, nn.Linear): nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\") if m.bias is not None: nn.init.constant_(m.bias, 0) elif isinstance(m, nn.BatchNorm2d): nn.init.constant_(m.weight, 1) nn.init.constant_(m.bias, 0) class BinaryClassifier(nn.Module): def __init__(self): super(BinaryClassifier, self).__init__() self.name = \"binary_classifier\" self.input_layer = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.bn1 = nn.BatchNorm2d(num_features=6) self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3) self.bn2 = nn.BatchNorm2d(num_features=16) self.fc1 = nn.Linear(57600, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 1) def forward(self, x): x = self.input_layer(x) x = F.relu(self.bn1(x)) x = self.maxpool(x) x = self.conv(x) x = F.relu(self.bn2(x)) x = F.relu(self.fc1(x.view(-1, 57600))) x = F.relu(self.fc2(x)) x = self.fc3(x).squeeze() return x class MulticlassClassifier(nn.Module): def __init__(self, dropout_prob): super(MulticlassClassifier, self).__init__() self.name = \"multiclass_classifier\" self.input_layer = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.bn1 = nn.BatchNorm2d(num_features=6) self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2) self.conv = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3) self.bn2 = nn.BatchNorm2d(num_features=16) self.fc = nn.Sequential( nn.Linear(57600, 120), nn.ReLU(), nn.Dropout(p=dropout_prob), nn.Linear(120, 84), nn.ReLU(), nn.Dropout(p=dropout_prob), nn.Linear(84, 12), ) def forward(self, x): x = self.input_layer(x) x = F.relu(self.bn1(x)) x = self.maxpool(x) x = self.conv(x) x = F.relu(self.bn2(x)) x = self.fc(x.view(-1, 57600)) return x", "source": "models.py"}, {"content": "import logging from typing import Tuple import matplotlib.pyplot as plt import torch import torch.nn.functional as F from hydra import compose, initialize from hydra.core.global_hydra import GlobalHydra from hydra.utils import instantiate from omegaconf import DictConfig from tqdm import tqdm, trange from conf.config import * from src.datamodule import * from src.mlflow_setup import * from src.models import * logger = logging.getLogger(__name__) class Trainer: def __init__( self, data_module: DataModule, config: DictConfig, random_state: int = 42, checkpoint_path: str = None, ): if checkpoint_path is None: logger.info(\"Initializing from scratch\") self.model = instantiate(config.model) # Freeze weights for param in self.model.base_model.parameters(): param.requires_grad = False for param in self.model.add_on_layers.parameters(): param.requires_grad = True self.optimizer = instantiate( config.optimizer, params=self.model.parameters() ) self.data_module = data_module self.random_state = random_state self.loss_train = [] self.loss_val = [] self.loss_val_best = 1e9 self.epoch = 0 self.best_epoch = None else: logger.info(\"Initializing from checkpoint\") self.load_checkpoint(checkpoint_path) self.config = config self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Setup MLFlow logging self.mlflow_init_status = False if getattr(config, \"mlflow\", None) is not None: self.mlflow_init_status, self.mlflow_run = mlflow_init( config.mlflow, setup_mlflow=config.mlflow.setup_mlflow, autolog=config.mlflow.mlflow_autolog, ) # log parameters mlflow_log( self.mlflow_init_status, \"log_params\", params={ \"optimizer_type\": type(self.optimizer).__name__, \"learning_rate\": self.optimizer.param_groups[0][\"lr\"], \"model\": self.model.name, \"random_state\": self.random_state, }, ) def train_one_epoch( self, dataloader: DataLoader, is_train: bool = True, ): self.model.train() if is_train else self.model.eval() running_loss = 0 num_instances = 0 loss_fn = nn.CrossEntropyLoss() for batch_idx, (X, y) in enumerate(dataloader): X = X.to(self.device) y = y.to(self.device) if is_train: self.optimizer.zero_grad() logits = self.model(X) loss = loss_fn(logits, y) loss.backward() self.optimizer.step() else: with torch.no_grad(): logits = self.model(X) loss = loss_fn(logits, y) running_loss += loss.item() num_instances += X.size(0) return running_loss / num_instances def train(self, num_epochs, checkpoint_path=\"checkpoint/checkpoint.ckpt\"): torch.manual_seed(self.random_state) self.model.to(device=self.device) for epoch in trange( num_epochs, desc=\"Training loop\", position=0, total=num_epochs, ): loss_train_epoch = self.train_one_epoch( self.data_module.dataloader_train, is_train=True ) loss_val_epoch = self.train_one_epoch( self.data_module.dataloader_val, is_train=False ) # End of epoch operations self.epoch += 1 if loss_val_epoch < self.loss_val_best: self.loss_val_best = loss_val_epoch self.best_epoch = self.epoch mlflow_log( self.mlflow_init_status, \"log_metrics\", metrics={ \"loss_train_epoch\": loss_train_epoch, \"loss_val_epoch\": loss_val_epoch, }, step=self.epoch, ) print( f\"Epoch: {self.epoch} | train loss: {loss_train_epoch:.4g} | val loss: {loss_val_epoch:.4g}\" ) self.loss_train.append(loss_train_epoch) self.loss_val.append(loss_val_epoch) self.save_checkpoint(self.best_epoch, checkpoint_path) def predict(self) -> Tuple[torch.tensor, torch.tensor]: self.model.to(self.device) self.model.eval() y_logits = None y_test = None for batch_idx, (X_test, y_test_batch) in tqdm( enumerate(self.data_module.dataloader_test), desc=\"Predicting, cycling through test dataloader\", ): X_test = X_test.to(self.device) y_lgt = self.model(X_test) y_logits = y_lgt if y_logits is None else torch.cat((y_logits, y_lgt)) y_test = ( y_test_batch if y_test is None else torch.cat((y_test, y_test_batch)) ) y_proba = F.softmax(y_logits, dim=1) return y_proba.detach().cpu().numpy(), y_test.numpy() def plot_loss_curves(self, figsize=(8, 5)): # Create the plot plt.figure(figsize=figsize) epochs = range(1, len(self.loss_train) + 1) plt.plot(epochs, self.loss_train, \"b-\", label=\"Training Loss\") plt.plot(epochs, self.loss_val, \"r-\", label=\"Validation Loss\") # Add title and labels plt.title(\"Training and Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() # Show the plot plt.grid(True) plt.show() def fit_one_batch(self, num_epochs): torch.manual_seed(self.random_state) self.model.to(device=self.device) one_batch_dataloader = self.data_module.one_batch_dataloader() for epoch in trange( num_epochs, desc=\"One batch training loop\", position=0, total=num_epochs ): loss_train_epoch = self.train_one_epoch(one_batch_dataloader, is_train=True) print(f\"Epoch: {epoch} | train loss: {loss_train_epoch:.4g}\") def save_checkpoint(self, epoch, checkpoint_path): torch.save( { \"epoch\": epoch, \"loss_train\": self.loss_train[:epoch], \"loss_val\": self.loss_val[:epoch], \"model_state_dict\": self.model.state_dict(), \"optimizer_state_dict\": self.optimizer.state_dict(), }, checkpoint_path, ) print(f\"Checkpoint till epoch {epoch} saved at {checkpoint_path}\") def load_checkpoint(self, checkpoint_path): checkpoint = torch.load(checkpoint_path) self.epoch = checkpoint[\"epoch\"] self.loss_train = checkpoint[\"loss_train\"] self.loss_val = checkpoint[\"loss_val\"] self.best_loss_val = np.min(checkpoint[\"loss_val\"]) self.best_epoch = np.argmin(checkpoint[\"loss_val\"]) self.model.load_state_dict(checkpoint[\"model_state_dict\"]) self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) class TrainerBinary: def __init__( self,", "source": "train.py"}, {"content": "data_module: DataModule, config: DictConfig, loss_fn: nn.Module, predict_fn: nn.functional, predict_fn_kwargs: dict = None, y_astype: torch.dtype = None, random_state: int = 42, checkpoint_path: str = None, ): if checkpoint_path is None: logger.info(\"Initializing from scratch\") self.model = instantiate(config.model) self.optimizer = instantiate( config.optimizer, params=self.model.parameters() ) self.data_module = data_module self.random_state = random_state self.loss_train = [] self.loss_val = [] self.loss_val_best = 1e9 self.epoch = 0 self.best_epoch = None else: logger.info(\"Initializing from checkpoint\") self.load_checkpoint(checkpoint_path) self.config = config self.loss_fn = loss_fn self.predict_fn = predict_fn self.predict_fn_kwargs = predict_fn_kwargs self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\" self.y_astype = y_astype # Setup MLFlow logging self.mlflow_init_status = False if getattr(config, \"mlflow\", None) is not None: self.mlflow_init_status, self.mlflow_run = mlflow_init( config.mlflow, setup_mlflow=config.mlflow.setup_mlflow, autolog=config.mlflow.mlflow_autolog, ) # log parameters mlflow_log( self.mlflow_init_status, \"log_params\", params={ \"optimizer_type\": type(self.optimizer).__name__, \"learning_rate\": self.optimizer.param_groups[0][\"lr\"], \"model\": self.model.name, \"random_state\": self.random_state, }, ) def train_one_epoch( self, dataloader: DataLoader, is_train: bool = True, ): self.model.train() if is_train else self.model.eval() running_loss = 0 num_instances = 0 for batch_idx, (X, y) in enumerate(dataloader): X = X.to(self.device) if self.y_astype is not None: y = y.to(self.y_astype).to(self.device) else: y = y.to(self.device) if is_train: self.optimizer.zero_grad() logits = self.model(X) loss = self.loss_fn(logits, y) loss.backward() self.optimizer.step() else: with torch.no_grad(): logits = self.model(X) loss = self.loss_fn(logits, y) running_loss += loss.item() num_instances += X.size(0) return running_loss / num_instances def train(self, num_epochs, checkpoint_path=\"checkpoint/checkpoint.ckpt\"): torch.manual_seed(self.random_state) self.model.to(device=self.device) for epoch in trange( num_epochs, desc=\"Training loop\", position=0, total=num_epochs, ): loss_train_epoch = self.train_one_epoch( self.data_module.dataloader_train, is_train=True ) loss_val_epoch = self.train_one_epoch( self.data_module.dataloader_val, is_train=False ) # End of epoch operations self.epoch += 1 if loss_val_epoch < self.loss_val_best: self.loss_val_best = loss_val_epoch self.best_epoch = self.epoch mlflow_log( self.mlflow_init_status, \"log_metrics\", metrics={ \"loss_train_epoch\": loss_train_epoch, \"loss_val_epoch\": loss_val_epoch, }, step=self.epoch, ) print( f\"Epoch: {self.epoch} | train loss: {loss_train_epoch:.4g} | val loss: {loss_val_epoch:.4g}\" ) self.loss_train.append(loss_train_epoch) self.loss_val.append(loss_val_epoch) self.save_checkpoint(self.best_epoch, checkpoint_path) def predict(self) -> Tuple[torch.tensor, torch.tensor]: self.model.to(self.device) self.model.eval() y_logits = None y_test = None for batch_idx, (X_test, y_test_batch) in tqdm( enumerate(self.data_module.dataloader_test), desc=\"Predicting, cycling through test dataloader\", ): X_test = X_test.to(self.device) y_lgt = self.model(X_test) y_logits = y_lgt if y_logits is None else torch.cat((y_logits, y_lgt)) y_test = ( y_test_batch if y_test is None else torch.cat((y_test, y_test_batch)) ) if self.predict_fn is not None: y_proba = self.predict_fn(y_logits, **self.predict_fn_kwargs) return y_proba.detach().cpu().numpy(), y_test.numpy() else: return y_logits.detach().cpu().numpy(), y_test.numpy() def plot_loss_curves(self, figsize=(8, 5)): # Create the plot plt.figure(figsize=figsize) epochs = range(1, len(self.loss_train) + 1) plt.plot(epochs, self.loss_train, \"b-\", label=\"Training Loss\") plt.plot(epochs, self.loss_val, \"r-\", label=\"Validation Loss\") # Add title and labels plt.title(\"Training and Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend() # Show the plot plt.grid(True) plt.show() def fit_one_batch(self, num_epochs): torch.manual_seed(self.random_state) self.model.to(device=self.device) one_batch_dataloader = self.data_module.one_batch_dataloader() for epoch in trange( num_epochs, desc=\"One batch training loop\", position=0, total=num_epochs ): loss_train_epoch = self.train_one_epoch(one_batch_dataloader, is_train=True) print(f\"Epoch: {epoch} | train loss: {loss_train_epoch:.4g}\") def save_checkpoint(self, epoch, checkpoint_path): torch.save( { \"epoch\": epoch, \"loss_train\": self.loss_train[:epoch], \"loss_val\": self.loss_val[:epoch], \"model_state_dict\": self.model.state_dict(), \"optimizer_state_dict\": self.optimizer.state_dict(), }, checkpoint_path, ) print(f\"Checkpoint till epoch {epoch} saved at {checkpoint_path}\") def load_checkpoint(self, checkpoint_path): checkpoint = torch.load(checkpoint_path) self.epoch = checkpoint[\"epoch\"] self.loss_train = checkpoint[\"loss_train\"] self.loss_val = checkpoint[\"loss_val\"] self.best_loss_val = np.min(checkpoint[\"loss_val\"]) self.best_epoch = np.argmin(checkpoint[\"loss_val\"]) self.model.load_state_dict(checkpoint[\"model_state_dict\"]) self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"]) if __name__ == \"__main__\": GlobalHydra.instance().clear() initialize(version_base=None, config_path=\"../conf\") config = compose(config_name=\"base_config\") datamodule = DataModule( instantiate(config.datamodule), root=\"data/img-classification/tensorfood\" )", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "from typing import List, Union import pandas as pd from sklearn.model_selection import TimeSeriesSplit, train_test_split class DataPipeline: def __init__(self): self.numeric_cols = [\"pm2.5\", \"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"] self.categorical_cols = [\"year\", \"month\", \"day\", \"hour\", \"cbwd\"] def load_csv(self, csv_path): df = pd.read_csv(csv_path) df[\"datetime\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]]) df = df.sort_values(by=[\"datetime\"], ascending=True) df.set_index(\"datetime\", inplace=True) return df def train_val_test_split(self, df, val=False, test_size=0.33, val_size=0.5): df_train = df.iloc[: -int(test_size * df.shape[0])] df_test = df.iloc[-int(test_size * df.shape[0]) :] if val: split_index = int(val_size * df_test.shape[0]) df_val = df_test.iloc[:split_index] df_test = df_test.iloc[split_index:] return df_train, df_val, df_test else: return df_train, df_test def run_data_pipeline(self, csv_path=\"data/PRSA_data_2010.1.1-2014.12.31.csv\"): df = self.load_csv(csv_path) cleaned_data = self.fill_missing_values(df, \"pm2.5\") cleaned_data = self.add_lagged_vars(cleaned_data, self.numeric_cols, [1, 2, 3]) return cleaned_data def fill_missing_values(self, df, feature): df = df.copy() df[feature] = df[feature].ffill(axis=0) df[feature] = df[feature].bfill(axis=0) return df def add_lagged_vars( self, df, feature_cols: Union[int, List[int]], nlags: int = 1, ): df = df.copy() if isinstance(nlags, int): nlags = [nlags] for feature in feature_cols: for lag in nlags: df[f\"{feature}_lag{lag}\"] = df[feature].shift(lag) return df", "source": "datapipeline.py"}, {"content": "import math import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns from scipy.signal import periodogram from statsmodels.graphics.tsaplots import plot_pacf def plot_lineplots(df, time_col, feature_cols, var_map: dict): num_cols = 2 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=True ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): sns.lineplot(x=df[time_col], y=df[feature], ax=ax) ax.set_title(f\"{var_map[feature]} over time\") ax.set_xlabel(\"Time\") ax.set_ylabel(var_map[feature]) # Hide any unused axes for ax in axes[len(feature_cols) :]: ax.set_visible(False) plt.tight_layout() plt.show() def plot_year_on_year(df, feature_cols, var_map: dict): num_cols = 2 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=True ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): ratio_df = ( df.pivot_table(index=\"hour\", columns=\"year\", values=feature).pct_change( axis=\"columns\" ) + 1 ) for year in ratio_df.columns[ 1: ]: # Skip the first year as it has no previous year to compare sns.lineplot( x=ratio_df.index, y=ratio_df[year], label=f\"{year}/{year-1}\", ax=ax ) ax.set_xlabel(\"Hour\") ax.set_ylabel(var_map[feature]) ax.legend(title=\"Year-on-Year Ratio\") # Hide any unused axes for ax in axes[len(feature_cols) :]: ax.set_visible(False) plt.tight_layout() plt.show() def plot_histograms( df, feature_cols, var_map: dict, bins: int = 40, take_diff=False, diff_kwargs=None, ): num_cols = 2 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): if take_diff: if diff_kwargs is None: diff_kwargs = {\"periods\": 1, \"axis\": 0} # Histogram with differencing df_plot = df[[feature]].diff(**diff_kwargs).dropna() sns.histplot( df_plot[df_plot != 0], bins=bins, ax=ax, ) ax.set_title( f\"Histogram of {var_map[feature]} with {diff_kwargs['periods']} period(s) differencing\" ) zero_count = int((df_plot == 0).sum().iloc[0]) else: # Regular histogram df_plot = df[feature] sns.histplot(df_plot[df_plot != 0], bins=bins, kde=True, ax=ax) ax.set_title(f\"Histogram of {var_map[feature]}\") zero_count = int((df_plot == 0).sum()) # Plot zero count as a cross (X) if zero_count > 0: # Create a secondary y-axis ax2 = ax.twinx() ax2.scatter( x=[0], y=[zero_count], color=\"red\", s=100, marker=\"x\", label=\"Count of Zeros\", ) ax2.legend(bbox_to_anchor=(0.5, -0.15), loc=\"upper center\", borderaxespad=0) ax.set_xlabel(var_map[feature]) ax.set_ylabel(\"Frequency\") # Remove the legend from the primary axis if it exists if ax.get_legend() is not None: ax.get_legend().remove() # Hide any unused subplots for i in range(len(feature_cols), len(axes)): fig.delaxes(axes[i]) plt.tight_layout() plt.show() def pairplot(df: pd.DataFrame, title: str = None) -> sns.PairGrid: # Create the pairplot grid = sns.pairplot(df) # Hide the upper triangle for i, j in zip(*np.triu_indices_from(grid.axes, 1)): grid.axes[i, j].set_visible(False) # Set x-labels for the lower triangle plots for ax in grid.axes[-1, :]: ax.set_xlabel(ax.get_xlabel(), fontsize=12) if title is not None: grid.figure.suptitle(title, fontsize=ax.yaxis.label.get_size() * 1.5) plt.show() return grid def pairplot_with_differencing( df: pd.DataFrame, title: str = None, periods: int = 1, axis: int = 0 ): df = df.copy().diff(periods=periods, axis=axis).dropna() grid = pairplot(df, title=title) return grid def plot_moving_average( df: pd.DataFrame, feature_cols: list, window_size: int, agg_func: callable, var_map: dict, title: str = None, ): num_cols = 1 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): sns.scatterplot( x=df[\"datetime\"], y=df[feature], ax=ax, s=10, color=\"blue\", ) sns.lineplot(x=df[\"datetime\"], y=df[feature], ax=ax, label=\"original\") sns.lineplot( x=df[\"datetime\"], y=df[feature].rolling(window=window_size).apply(agg_func), ax=ax, label=\"moving avg\", ) ax.set_xlabel(\"Hour\") ax.set_ylabel(var_map[feature]) if title: fig.suptitle(title, fontsize=14, y=1.01) plt.tight_layout() plt.show() def plot_moving_average_non_overlap( df: pd.DataFrame, feature_cols: list, window_size: int, var_map: dict, title: str = None, ): num_cols =", "source": "eda.py"}, {"content": "1 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): # Original data sns.scatterplot( x=df[\"datetime\"], y=df[feature], ax=ax, s=10, color=\"blue\", ) sns.lineplot(x=df[\"datetime\"], y=df[feature], ax=ax, label=\"original\") # Moving average data df_feature = df[[\"datetime\", feature]].dropna() df_feature.set_index(\"datetime\", inplace=True) df_resampled = df_feature.resample(window_size).mean() sns.scatterplot( x=df_resampled.index, y=df_resampled[feature], ax=ax, s=20, color=\"orange\" ) sns.lineplot( x=df_resampled.index, y=df_resampled[feature], ax=ax, label=\"moving avg\", ) ax.set_xlabel(\"Hour\") ax.set_ylabel(var_map[feature]) if title: fig.suptitle(title, fontsize=14, y=1.01) plt.tight_layout() plt.show() def partial_autocorrelation_plot( df: pd.DataFrame, feature_cols: list, lags: int, var_map: dict, method: str = \"ywm\", # Yule-Walker without adjustment timeframe: str = \"Hour\", title: str = None, ): num_cols = 2 num_rows = math.ceil(len(feature_cols) * 2 / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() for i, feature in enumerate(feature_cols): df2 = df.dropna(subset=[feature]) na_drop_cnt = df.shape[0] - df2.shape[0] # Line plot sns.lineplot(x=df2[\"datetime\"], y=df2[feature], ax=axes[2 * i]) axes[2 * i].set_title(f\"{var_map[feature]} - Line Plot\") axes[2 * i].set_xlabel(timeframe) axes[2 * i].set_ylabel(var_map[feature]) if na_drop_cnt > 0: axes[2 * i].text( 0.5, -0.15, f\"Note: {na_drop_cnt} NA values were dropped.\", ha=\"center\", va=\"center\", transform=axes[2 * i].transAxes, fontsize=10, color=\"black\", ) # PACF plot plot_pacf(df2[feature], lags=lags, ax=axes[2 * i + 1], method=method) axes[2 * i + 1].set_title(f\"{var_map[feature]} - PACF Plot\") axes[2 * i + 1].set_xlabel(\"Lags\") axes[2 * i + 1].set_ylabel(var_map[feature]) if title: fig.suptitle(title, fontsize=16, y=1.01) plt.tight_layout() plt.show() def plot_periodogram( df: pd.DataFrame, feature_cols: list, var_map: dict, title: str = None, sampling_freq: float = 1.0, ): num_cols = 2 num_rows = math.ceil(len(feature_cols) / num_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() for ax, feature in zip(axes, feature_cols): df_na_dropped = df.dropna(subset=[feature]) na_drop_cnt = df.shape[0] - df_na_dropped.shape[0] f, Pxx_den = periodogram(df_na_dropped[feature], fs=sampling_freq) ax.semilogy(f[1:], Pxx_den[1:]) ax.set_title(f\"Periodogram of {var_map[feature]}\") ax.set_xlabel(\"Frequency [Hz]\") ax.set_ylabel(\"Power spectral density [V^2/Hz]\") ax.grid() if na_drop_cnt > 0: ax.text( 0.5, -0.15, f\"Note: {na_drop_cnt} NA values were dropped.\", ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=10, color=\"black\", ) # Hide any unused subplots for i in range(len(feature_cols), len(axes)): axes[i].set_visible(False) plt.show() def plot_downsampled_data( df, feature_cols: list, var_map: dict, freq, ): num_cols = 1 num_rows = len(feature_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() df = df.copy() df.set_index(\"datetime\", inplace=True) for ax, feature in zip(axes, feature_cols): # original data sns.lineplot(x=df.index, y=df[feature], ax=ax, label=\"original data\") sns.scatterplot(x=df.index, y=df[feature], ax=ax, s=10, color=\"blue\") # downsampled data df_downsampled = df[feature].resample(freq).mean() sns.lineplot( x=df_downsampled.index, y=df_downsampled, ax=ax, label=\"downsampled data\" ) sns.scatterplot( x=df_downsampled.index, y=df_downsampled, ax=ax, s=20, color=\"orange\" ) ax.set_ylabel(var_map[feature]) ax.set_xlabel(\"Time\") plt.show() def plot_upsampled_data( df, feature_cols: list, var_map: dict, ): num_cols = 1 num_rows = len(feature_cols) fig, axes = plt.subplots( num_rows, num_cols, figsize=(20, 5 * num_rows), sharex=False ) axes = axes.flatten() df = df.copy() df.set_index(\"datetime\", inplace=True) for ax, feature in zip(axes, feature_cols): # original data sns.lineplot(x=df.index, y=df[feature], ax=ax, label=\"original data\") sns.scatterplot(x=df.index, y=df[feature], ax=ax, s=10, color=\"blue\") # downsampled data df_downsampled = df[feature].resample(\"10D\").mean() # upsampled data df_upsampled = df_downsampled.resample(\"h\").interpolate(method=\"linear\") sns.lineplot( x=df_upsampled.index, y=df_upsampled, ax=ax, label=\"upsampled from downsampled data\", alpha=0.75, ) sns.scatterplot( x=df_upsampled.index, y=df_upsampled, ax=ax, s=12, color=\"orange\" ) ax.set_ylabel(var_map[feature]) ax.set_xlabel(\"Time\") plt.show()", "source": "eda.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import ( explained_variance_score, max_error, mean_absolute_error, mean_squared_error, r2_score, ) class ForecastModel: def __init__(self, n_estimators, max_depth, **kwargs): self.model = RandomForestRegressor( n_estimators=n_estimators, max_depth=max_depth, **kwargs ) def fit(self, X, y): self.model.fit(X, y) def evaluate(self, model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = model.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X) def print_metrics(self, y_true, y_pred): # Calculate regression metrics mae = mean_absolute_error(y_true, y_pred) mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) r2 = r2_score(y_true, y_pred) max_err = max_error(y_true, y_pred) explained_variance = explained_variance_score(y_true, y_pred) # Print the custom regression report print(\"Regression Report\") print(\"-----------------\") print(f\"Mean Absolute Error (MAE): {mae:.2f}\") print(f\"Mean Squared Error (MSE): {mse:.2f}\") print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\") print(f\"R\u00b2 Score: {r2:.2f}\") print(f\"Max Error: {max_err:.2f}\") print(f\"Explained Variance Score: {explained_variance:.2f}\") def plot_predictions(self, y_test, y_pred): # Create a DataFrame with y_test and y_pred results_df = pd.DataFrame( {\"y_test\": y_test, \"y_pred\": y_pred}, index=y_test.index ) # Define the 2-monthly intervals intervals = [ (\"2013-12-31 20:00\", \"2014-02-28 23:00\"), (\"2014-03-01 00:00\", \"2014-04-30 23:00\"), (\"2014-05-01 00:00\", \"2014-06-30 23:00\"), (\"2014-07-01 00:00\", \"2014-08-31 23:00\"), (\"2014-09-01 00:00\", \"2014-10-31 23:00\"), (\"2014-11-01 00:00\", \"2014-12-31 23:00\"), ] # Create subplots for each 2-month interval fig, axes = plt.subplots(nrows=6, ncols=1, figsize=(15, 24)) axes = axes.flatten() for i, (start, end) in enumerate(intervals): # Filter the DataFrame for the current interval interval_df = results_df[start:end] # Plot y_test and y_pred for the current interval axes[i].plot( interval_df.index, interval_df[\"y_test\"], label=\"Actual\", color=\"blue\" ) axes[i].plot( interval_df.index, interval_df[\"y_pred\"], label=\"Predicted\", color=\"red\", linestyle=\"--\", ) # Add scatter plot for the data points axes[i].scatter( interval_df.index, interval_df[\"y_test\"], color=\"blue\", alpha=0.5, s=6 ) axes[i].scatter( interval_df.index, interval_df[\"y_pred\"], color=\"red\", alpha=0.5, s=6 ) axes[i].set_title(f\"Interval {i+1}\") axes[i].set_xlabel(\"Time\") axes[i].set_ylabel(\"Value\") axes[i].legend() plt.tight_layout() plt.show()", "source": "ml_model.py"}, {"content": "import matplotlib.pyplot as plt import numpy as np import pandas as pd import torch import torch.nn as nn import torch.nn.functional as F from sklearn.metrics import ( explained_variance_score, max_error, mean_absolute_error, mean_squared_error, r2_score, ) from torch.utils.data import DataLoader from src.windowing import WindowDataloader class RNNModel(torch.nn.Module): def __init__( self, input_size: int, hidden_size: int, num_layers: int, output_size: int, optimizer: torch.optim, learning_rate: float, ): super(RNNModel, self).__init__() self.num_layers = num_layers self.hidden_size = hidden_size self.rnn_layer = nn.RNN( input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, ) self.linear = nn.Linear(in_features=hidden_size, out_features=output_size) self.optimizer = optimizer( self.parameters(), lr=learning_rate, ) self.losses_train = [] self.losses_val = [] def forward(self, x): batch_size = x.shape[0] h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size) _, hn = self.rnn_layer(x, h0) out = self.linear(hn.squeeze()) return out def fit(self, dataldr_holder: WindowDataloader, num_epochs: int): torch.manual_seed(42) for epoch in range(num_epochs): loss_train = self.fit_one_epoch( dataldr_holder.dataloader_train, is_train=True ) loss_val = self.fit_one_epoch(dataldr_holder.dataloader_val, is_train=False) print( f\"Epoch [{epoch+1}/{num_epochs}] | Training Loss: {loss_train:.4g} | Validation Loss: {loss_val:.4g}\" ) self.losses_train.append(loss_train) self.losses_val.append(loss_val) def fit_one_epoch(self, dataloader: DataLoader, is_train: bool = True): epoch_loss = 0 if is_train: self.train() for X, y, _, _ in dataloader: self.optimizer.zero_grad() out = self.forward(X) loss = F.mse_loss(out, y) loss.backward() torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0) self.optimizer.step() epoch_loss += loss.detach().item() else: self.eval() for X, y, _, _ in dataloader: with torch.no_grad(): out = self.forward(X) loss = F.mse_loss(out, y) epoch_loss += loss.detach().item() epoch_loss /= len(dataloader) return epoch_loss def predict(self, dataloader: DataLoader): self.eval() y_true = [] y_pred = [] y_indices = [] for X, y, indices_x, indices_y in dataloader: y_true.append(y.flatten()) y_indices.append(indices_y.flatten()) with torch.no_grad(): y_pred_one = self.forward(X) y_pred.append(y_pred_one.flatten()) return ( torch.cat(y_true), torch.cat(y_pred), pd.to_datetime(torch.cat(y_indices).numpy()), ) def evaluate(self, y_true, y_pred): # Calculate regression metrics mae = mean_absolute_error(y_true, y_pred) mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) r2 = r2_score(y_true, y_pred) max_err = max_error(y_true, y_pred) explained_variance = explained_variance_score(y_true, y_pred) # Print the regression report print(\"Regression Report\") print(\"-----------------\") print(f\"Mean Absolute Error (MAE): {mae:.2f}\") print(f\"Mean Squared Error (MSE): {mse:.2f}\") print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\") print(f\"R\u00b2 Score: {r2:.2f}\") print(f\"Max Error: {max_err:.2f}\") print(f\"Explained Variance Score: {explained_variance:.2f}\") def evaluate_plot(self, y_true, y_pred, y_indices): results_df = pd.DataFrame({\"y_true\": y_true, \"y_pred\": y_pred}, index=y_indices) # Create a single subplot fig, ax = plt.subplots(figsize=(15, 6)) # Plot y_true and y_pred ax.plot(results_df.index, results_df[\"y_true\"], label=\"Actual\", color=\"blue\") ax.plot( results_df.index, results_df[\"y_pred\"], label=\"Predicted\", color=\"red\", linestyle=\"--\", ) # Add labels and legend ax.set_xlabel(\"Time\") ax.set_ylabel(\"Value\") ax.legend() plt.tight_layout() plt.show() def plot_loss_curves(self): plt.figure(figsize=(10, 5)) plt.plot(self.losses_train, label=\"Training Loss\") plt.plot(self.losses_val, label=\"Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.title(\"Training and Validation Loss Curves\") plt.legend() plt.grid(True) plt.show()", "source": "rnn_model.py"}, {"content": "from typing import Union import numpy as np import pandas as pd import torch from torch.utils.data import DataLoader, Dataset class WindowGenerator(Dataset): def __init__( self, data: pd.DataFrame, lookback: int, lookahead: int, target_col: str ): N = data.shape[0] self.features = [] self.labels = [] self.indices_x = [] self.indices_y = [] for i in range(N - lookback - lookahead + 1): current_x = torch.from_numpy( data[i : i + lookback].to_numpy().astype(np.float32) ) current_y = torch.from_numpy( data[i + lookback - 1 + lookahead : i + lookback + lookahead][ target_col ] .to_numpy() .astype(np.float32) ) self.features.append(current_x) self.labels.append(current_y) self.indices_x.append(np.array(data.index[i : i + lookback].astype(int))) self.indices_y.append( np.array( data.index[ i + lookback - 1 + lookahead : i + lookback + lookahead ].astype(int) ) ) self.features = torch.stack(self.features) self.labels = torch.stack(self.labels) self.length = self.features.shape[0] def __len__(self): return self.length def __getitem__(self, idx: int) -> Union[torch.Tensor, torch.Tensor]: return ( self.features[idx], self.labels[idx], self.indices_x[idx], self.indices_y[idx], ) class WindowDataloader: def __init__(self, data_train: Dataset, data_val: Dataset, data_test: Dataset): self.data_train = data_train self.data_val = data_val self.data_test = data_test self.dataloader_train = None self.dataloader_val = None self.dataloader_test = None self.setup() def setup(self): torch.manual_seed(32) if self.data_train is not None: self.dataloader_train = DataLoader( self.data_train, batch_size=32, shuffle=True, drop_last=True ) if self.data_val is not None: self.dataloader_val = DataLoader( self.data_val, batch_size=32, shuffle=False ) if self.data_test is not None: self.dataloader_test = DataLoader( self.data_test, batch_size=32, shuffle=False )", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [[0.53, 0.23, 0.68, 0.45]]", "source": "context_vector.py"}, {"content": "import os import re import matplotlib.pyplot as plt import nltk import numpy as np import pandas as pd import pyodbc from dotenv import load_dotenv from nltk.corpus import wordnet from nltk.stem import PorterStemmer, WordNetLemmatizer from sklearn.model_selection import train_test_split class Datapipeline: def __init__(self): self.lemmatizer = WordNetLemmatizer() self.stemmer = PorterStemmer() def load_csv(self, path=\"data/raw/data_movies.csv\"): df = pd.read_csv(path) # drop duplicated data df = df.loc[~df.duplicated(keep=\"first\")] return df def clean_text(self, text): # Replace URLs with 'url' placeholder url_pattern = r\"(https?:\\/\\/(www\\.)?|www\\.|\\.com)\" text = re.sub(url_pattern, \"url\", text) # Remove HTML tags text = re.sub(r\"<[^>]+>\", \" \", text) # Replace quotation marks with empty strings text = re.sub(r'[\"\\']', \"\", text) # Replace hyphens with spaces text = re.sub(r\"-\", \" \", text) # Add a space after a full stop if there is no space after the full stop text = re.sub(r\"\\.(\\S)\", \". \\\\1\", text) # Remove other special characters, digits, and full stops text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text) # Convert to lowercase text = text.lower() # Remove extra whitespace text = re.sub(r\"\\s+\", \" \", text).strip() return text def apply_cleaning(self, df): df[\"text\"] = df[\"text\"].apply(self.clean_text) return df def stem_text(self, text: str, is_tokenized: bool = False): # Tokenize the text words = nltk.word_tokenize(text) # Apply stemming to each word stemmed_words = map(self.stemmer.stem, words) if is_tokenized: return stemmed_words else: # Join the stemmed words back into a single string return \" \".join(stemmed_words) def apply_stemming(self, df): df[\"text\"] = df[\"text\"].apply(self.stem_text) return df def get_wordnet_pos(self, word, tag): tag = tag[0].upper() tag_dict = { \"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV, } return (word, tag_dict.get(tag, wordnet.NOUN)) def lemmatise_text(self, text: str, is_tokenized: bool = False): tokens_with_tag = nltk.pos_tag(nltk.word_tokenize(text)) # map to wordnet pos tags tokens_with_tag = list(map(lambda x: self.get_wordnet_pos(*x), tokens_with_tag)) # apply lemmatisation tokens_lemmatised = list( map(lambda x: self.lemmatizer.lemmatize(*x), tokens_with_tag) ) if is_tokenized: return tokens_lemmatised else: return \" \".join(tokens_lemmatised) def apply_lemmatisation(self, df): df[\"text\"] = df[\"text\"].apply(self.lemmatise_text) return df def run_data_pipeline(self): df = self.load_csv(\"data/raw/data_movies.csv\") # Clean text df = self.apply_cleaning(df) if self.do_stemming: df = self.apply_stemming(df) if self.do_lemmitisation: df = self.apply_lemmatisation(df) return df def train_test_split(self, df): df_train, df_test = train_test_split( df, test_size=0.33, random_state=42, stratify=df[\"label\"] ) return df_train, df_test def tokenize(self, text): # Clean text text = self.clean_text(text) # Stem text # tokens = self.stem_text(text, is_tokenized=True) # Lemmatise text tokens = self.lemmatise_text(text, is_tokenized=True) return tokens", "source": "datapipeline.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "# Import the libraries needed import pandas as pd, numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer def transform(data_path): \"\"\" Loads data from a CSV file, performs transformations, and returns the modified DataFrame. :param data_path: str, path to the CSV file containing the data. :return: pd.DataFrame, transformed DataFrame. \"\"\" try: df = pd.read_csv(data_path) print(\"Success\") except FileNotFoundError: print(f\"Error: The file at {data_path} was not found.\") return None except pd.errors.EmptyDataError: print(f\"Error: The file at {data_path} is empty.\") return None except pd.errors.ParserError: print(f\"Error: The file at {data_path} could not be parsed.\") return None # Define X as input features and y as the outcome variable X = df.drop(columns=['income_group', 'id']) y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Build a preprocessing step for numeric features numeric_transformer = Pipeline(steps=[ ('scaler', StandardScaler()) ]) # Build a preprocessing step for nominal features nominal_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Build a preprocessing step for ordinal features education_order = [ 'Children', 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)' ] ordinal_transformer = Pipeline(steps=[ ('ordinal', OrdinalEncoder(categories=[education_order])) ]) income_transformer = Pipeline(steps=[ ('ordinalincome', OrdinalEncoder(categories=[['- 50000.', '50000+.']])) ]) num_features = list(X.select_dtypes(include='int64').columns) nom_features = [col for col in X.select_dtypes(include='object').columns if col != 'education'] ord_features = ['education'] preprocessorX = ColumnTransformer( transformers=[ ('num', numeric_transformer, num_features), ('nom', nominal_transformer, nom_features), ('ord', ordinal_transformer, ord_features) ], remainder='passthrough' # Pass through other columns not specified ) preprocessorY = ColumnTransformer( transformers=[ ('income', income_transformer, [0]) ] ) X_train_transformed = preprocessorX.fit_transform(X_train) X_test_transformed = preprocessorX.transform(X_test) y_train_transformed = preprocessorY.fit_transform(y_train.values.reshape(-1, 1)) y_test_transformed = preprocessorY.transform(y_test.values.reshape(-1, 1)) return X_train_transformed.toarray(), X_test_transformed.toarray(), y_train_transformed, y_test_transformed", "source": "datapipeline.py"}, {"content": "import numpy as np from collections import Counter class Node: def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None): self.feature = feature # Name of feature self.threshold = threshold # Splitting Criteria self.left = left # Instances of child node class that represents decision points for left side of parent node self.right = right # Instances of child node class that represents decision points for right side of parent node # self.value = value # Determine whether is leaf node, if not then self.value = None # def is_leaf_node(self): # # Check if node is leaf node, where self.value != None # return self.value is not None class DecisionTree: def __init__(self, min_samples_split = 2, max_depth=10, n_features=None): self.min_samples_split = min_samples_split # Minimum number of samples required for split to occur (Prevent overfitting) self.max_depth = max_depth # Setting the max depth for tree self.n_features = n_features # Control number of features to consider when looking for best split self.root = None # Recording the parent node (root) def fit(self, X, y, depth=0): self.n_features = X.shape[1] if not self.n_features else min(X.shape[1], self.n_features) # Check that number of features is equal to X features self.root = self._grow_tree(X, y) # Start of building tree ######################### def _grow_tree(self, X, y, depth = 0): # Recursive function to grow tree n_samples, n_feats = X.shape # Get the number of rows into n_samples, and number of cols into n_feats n_labels = len(np.unique(y)) # Get the number of unique values for the target feature print(f\"\\nNumber of Samples in X_train set: {n_samples}\") print(f\"Number of features in X_train set: {n_feats}\") print(f\"Number of unique label in y_train set: {n_labels}\\n\") print(f\"Current depth: {depth}\") # Check stopping criteria at every iteration if(depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split): # If the current depth >= max depth, or if label left with 1 unique value (leaf node), or num samples < min samples leaf_value = self.most_common_label(y) # Get most common label return Node(value=leaf_value) feat_idx = np.random.choice(n_feats, self.n_features, replace=False) # Get a subset of random features based on n_features set # Find Best Split best_feat, best_thres = self._best_split(X, y, feat_idx) # Create Child nodes left_idxs, right_idxs = self._split(X[:, best_feat], best_thres) left = self._grow_tree(X[left_idxs, :], y[left_idxs], depth+1) right = self._grow_tree(X[right_idxs, :], y[right_idxs], depth+1) return Node(best_feat, best_thres, left, right) def _best_split(self, X, y, feat_idxs): # Compute which feature would have the best split best_gain = -1 split_idx, split_threshold = None, None count = 0 # Iterate through every feature for feat_idx in feat_idxs: X_col = X[:, feat_idx] # Retrieve all the samples for each feature thresholds = np.unique(X_col) # Get all potential splitting threshold count += 1 print(f\"Completed: {count}/{len(feat_idxs)} ({(count/len(feat_idxs))*100:.2f}%)\") for thr in thresholds: # Iterate through the thresholds available for splitting, calculate the gini and see which has best split gain = self._information_gain(y, X_col, thr) if gain > best_gain: best_gain = gain split_idx = feat_idx split_threshold = thr return split_idx, split_threshold def _information_gain(self, y, X_col, threshold): # parent gini parent_gini = self.cal_gini(y.flatten()) # create children left_idxs, right_idxs = self._split(X_col, threshold) if len(left_idxs) == 0 or len(right_idxs) == 0: return 0 # calculate weighted gini of children child_weighted_gini = self.gini(left_idxs, right_idxs) # calculate IG IG", "source": "decision_tree.py"}, {"content": "= parent_gini - child_weighted_gini return IG def _split(self, X_col, split_thresh): left_idx = np.argwhere(X_col <= split_thresh).flatten() right_idx = np.argwhere(X_col > split_thresh).flatten() return left_idx, right_idx def most_common_label(self, y): # Get the most common label counter = Counter(y) # Count occurance of each label value = counter.most_common(1)[0][0] # Get the most common(1) label, [0][0] access the label return value def cal_gini(self, labels): total = len(labels) count = 0 for value in labels: if value == 1: count += 1 p0 = (total-count)/total p1 = count/total impurity = 1 - (p0 ** 2) - (p1 ** 2) return impurity def gini(self, label_1, label_2): result1 = self.cal_gini(label_1) result2 = self.cal_gini(label_2) total = (len(label_1)/(len(label_1)+len(label_2)))*result1 + (len(label_2)/(len(label_1)+len(label_2)))*result2 return total def predict(): pass", "source": "decision_tree.py"}, {"content": "import numpy as np # Function to calculate Gini impurity def gini_impurity(y): classes, counts = np.unique(y, return_counts=True) impurity = 1 - sum((count / len(y)) ** 2 for count in counts) return impurity # Function to split the dataset def split_dataset(X, y, feature_idx, threshold): left_idx = X[:, feature_idx] <= threshold right_idx = X[:, feature_idx] > threshold return X[left_idx], X[right_idx], y[left_idx], y[right_idx] # Function to find the best split def best_split(X, y): best_feature, best_threshold, best_gini = None, None, float('inf') for feature_idx in range(X.shape[1]): thresholds = np.unique(X[:, feature_idx]) for threshold in thresholds: X_left, X_right, y_left, y_right = split_dataset(X, y, feature_idx, threshold) if len(y_left) == 0 or len(y_right) == 0: continue # Calculate weighted Gini impurity gini_left = gini_impurity(y_left) gini_right = gini_impurity(y_right) weighted_gini = (len(y_left) / len(y)) * gini_left + (len(y_right) / len(y)) * gini_right if weighted_gini < best_gini: best_gini = weighted_gini best_feature = feature_idx best_threshold = threshold return best_feature, best_threshold, best_gini # Class for Decision Tree Node class TreeNode: def __init__(self, gini, num_samples, num_samples_per_class, predicted_class): self.gini = gini self.num_samples = num_samples self.num_samples_per_class = num_samples_per_class self.predicted_class = predicted_class self.feature_idx = None self.threshold = None self.left = None self.right = None # Class for Decision Tree Classifier class DecisionTreeClassifier: def __init__(self, max_depth=None): self.max_depth = max_depth self.root = None def fit(self, X, y): self.root = self._grow_tree(X, y) def _grow_tree(self, X, y, depth=0): num_samples_per_class = [np.sum(y == i) for i in np.unique(y)] predicted_class = np.argmax(num_samples_per_class) node = TreeNode( gini=gini_impurity(y), num_samples=len(y), num_samples_per_class=num_samples_per_class, predicted_class=predicted_class, ) if depth < self.max_depth: feature_idx, threshold, gini = best_split(X, y) if feature_idx is not None: X_left, X_right, y_left, y_right = split_dataset(X, y, feature_idx, threshold) node.feature_idx = feature_idx node.threshold = threshold node.left = self._grow_tree(X_left, y_left, depth + 1) node.right = self._grow_tree(X_right, y_right, depth + 1) return node def predict(self, X): return [self._predict(inputs) for inputs in X] def _predict(self, inputs): node = self.root while node.left: if inputs[node.feature_idx] <= node.threshold: node = node.left else: node = node.right return node.predicted_class", "source": "decision_tree2.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() def train(self, params, X_train, y_train): \"\"\" Train the RandomForestClassifier model with given parameters and data. :param params: Dictionary of hyperparameters to be used for training. :param X_train: Feature matrix for training data. :param y_train: Target vector for training data. :return: Train F1 score as a single float. \"\"\" # Set the params of model self.model.set_params(**params) # Train the model self.model.fit(X_train, y_train) # Predict on the training data y_train_pred = self.model.predict(X_train) # Calculate and return the F1 score return f1_score(y_train, y_train_pred) def evaluate(self, X_test, y_test): \"\"\" Evaluate the trained model on test data. :param X_test: Feature matrix for test data. :param y_test: Target vector for test data. :return: Test F1 score as a single float. \"\"\" # Predict on the test data y_pred = self.model.predict(X_test) # Calculate and return the F1 score return f1_score(y_test, y_pred) def get_default_params(self): \"\"\" Get default parameters for training the RandomForestClassifier. :return: Dictionary of default parameters. \"\"\" return { 'n_estimators': 200, 'criterion': 'gini', 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 1, 'bootstrap': True }", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from imblearn.over_sampling import SMOTE from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self): self.scaler = StandardScaler() def transform_train_data(self, X_train, y_train): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" smote = SMOTE(sampling_strategy='auto', k_neighbors=5, random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) X_train_scaled = self.scaler.fit_transform(X_train_resampled) return X_train_scaled, y_train_resampled def transform_test_data(self, X_test, y_test): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" X_test_scaled = self.scaler.fit_transform(X_test) return X_test_scaled, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): # Xavier initialization for input to hidden layer weights limit_w1 = np.sqrt(6 / (input_size + hidden_size)) self.w1 = np.random.uniform(-limit_w1, limit_w1, (input_size, hidden_size)) self.b1 = np.zeros((1, hidden_size)) # Xavier initialization for hidden to output layer weights limit_w2 = np.sqrt(6 / (hidden_size + output_size)) self.w2 = np.random.uniform(-limit_w2, limit_w2, (hidden_size, output_size)) self.b2 = np.zeros((1, output_size)) # Learning rate self.learning_rate = 1e-3 def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" self.features = features.reshape(1, -1) self.z1 = np.dot(self.features, self.w1) + self.b1 self.a1 = np.tanh(self.z1) # Activation function self.z2 = np.dot(self.a1, self.w2) + self.b2 self.a2 = self.softmax(self.z2) # Output layer with softmax activation return self.a2 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" self.label = label # Cross-Entropy Loss, returns the -log(softmax(predicted probilitiy of actual class)) return -np.log(predictions[0][label]).item() def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" # Backpropagation for output layer delta2 = self.a2.copy() delta2[0][self.label] -= 1 # Subtract 1 to the actual class's predicted probability. (gradient of loss with respect to pre-activation output z2 before softmax (loss/z2)) # Gradients for weights and biases of the second layer # a1 is the gradient of z2 with respect to w2 (z2/w2) dw2 = np.dot(self.a1.T, delta2) # Output: 16x3 matrix (gradients of the loss with respect to the weights connecting the hidden layer to the output layer (loss/w2)) db2 = np.sum(delta2, axis=0, keepdims=True) # 1x3 matrix (Bias) # Backpropagation for hidden layer delta1 = np.dot(delta2, self.w2.T) * (1 - self.a1 ** 2) # Gradients for weights and biases of the first layer dw1 = np.dot(self.features.T, delta1) db1 = np.sum(delta1, axis=0, keepdims=True) # Update weights and biases self.w1 -= self.learning_rate * dw1 self.b1 -= self.learning_rate * db1 self.w2 -= self.learning_rate * dw2 self.b2 -= self.learning_rate * db2 def softmax(self, z): exp_z = np.exp(z - np.max(z)) return exp_z / np.sum(exp_z, axis=1, keepdims=True)", "source": "mlp.py"}, {"content": "import pandas as pd from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, db_path): self.db_path = db_path self.scaler = StandardScaler() def load_data(self): df = pd.read_csv(self.db_path) return df def transform(self): df = self.load_data() df = df.drop(columns=['id']) # Drop ID column X = df.drop(columns=['y']).values # Features (x0 to x3) y = df['y'].values # Target (y) X_scaled = self.scaler.fit_transform(X) return X_scaled, y", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [-618, 573, -126 ], [ 265, 391, -100 ], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24 ], [ 480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28 ], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59 ], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd class DataPipeline: def __init__(self): pass def run_data_pipeline(self, csv_path, num_lags): df = pd.read_csv(csv_path) df.drop(columns=['No'], inplace=True) # Interpolate to fill the missing values in the 'pm2.5' column df['pm2.5'] = df['pm2.5'].interpolate(method='spline', order=1).clip(lower=0) # Backfill the missing starting 24 values in the 'pm2.5' column df.bfill(inplace=True) # One-hot encode the 'cbwd' column df = pd.get_dummies(df, columns=['cbwd']) cbwd_columns = [col for col in df.columns if col.startswith('cbwd_')] df[cbwd_columns] = df[cbwd_columns].astype(int) # Create datetime feature df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) # Drop the 'year', 'month', 'day', and 'hour' columns df.drop(columns=['year', 'month', 'day', 'hour'], inplace=True) # Index the dataframe by the 'datetime' column df.set_index('datetime', inplace=True) if num_lags > 0: # Create lagged features for the 'pm2.5' column df = self.create_lagged_features(df, 'pm2.5', num_lags) # Move 'pm2.5' column to the last position pm25 = df.pop('pm2.5') df['pm2.5'] = pm25 return df def create_lagged_features(self, df, target_col, lags): for lag in range(1, lags + 1): df[f'{target_col}_lag_{lag}'] = df[target_col].shift(lag) return df def split_data(self, df, train_size): train_end = int(train_size * len(df)) train = df.iloc[:train_end] test = df.iloc[train_end:] return train, test if __name__ == '__main__': pipeline = DataPipeline() df = pipeline.run_data_pipeline('data/PRSA_data_2010.1.1-2014.12.31.csv', 3) train, test = pipeline.split_data(df, 0.8) print(train.head()) print(test.head())", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score class ForecastModel: def __init__(self): self.model = RandomForestRegressor(n_estimators=500, random_state=42) def train(self, X, y): self.model.fit(X, y) def evaluate(self, y_test, y_pred): test_error = mean_squared_error(y_test, y_pred) mae = mean_absolute_error(y_test, y_pred) r2 = r2_score(y_test, y_pred) return { 'mean_squared_error': test_error, 'mean_absolute_error': mae, 'r2_score': r2 } def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim class RNNModel(nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): super(RNNModel, self).__init__() self.rnn = nn.RNN(input_size, num_rnn, num_layers, batch_first=True) # Simple RNN self.fc = nn.Linear(num_rnn, output_size) # Fully connected layer self.hidden_size = num_rnn self.num_layers = num_layers def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) # RNN layer output out, _ = self.rnn(x, h0) # Pass through the fully connected layer to get the final output out = self.fc(out[:, -1, :]) # Taking the last time step output return out def fit(self, dataloader, epochs=10, lr=0.001): # Loss function and optimizer criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=lr) \"\"\" Train the model \"\"\" self.train() for epoch in range(epochs): total_loss = 0 for batch_features, batch_labels in dataloader: outputs = self(batch_features) loss = criterion(outputs, batch_labels[:, -1, :]) optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader):.4f}') def predict(self, dataloader): self.eval() # Set the model to evaluation mode predictions = [] with torch.no_grad(): # No need to track gradients for prediction for features, _ in dataloader: outputs = self(features) predictions.append(outputs) return torch.cat(predictions, dim=0) # Return concatenated predictions def evaluate(self, dataloader): self.eval() # Set to evaluation mode criterion = nn.MSELoss() mse = 0.0 mae = 0.0 r2 = 0.0 x=0 with torch.no_grad(): for features, labels in dataloader: outputs = self(features) # Reshape labels to match the shape of outputs labels = labels.view_as(outputs) mse += criterion(outputs, labels).item() mae += torch.mean(torch.abs(outputs - labels)).item() # Calculate R2 score ss_res = torch.sum((labels - outputs) ** 2) ss_tot = torch.sum((labels - torch.mean(labels)) ** 2) r2 += 1 - ss_res / ss_tot mse /= len(dataloader) mae /= len(dataloader) r2 /= len(dataloader) return {'mse': mse, 'mae': mae, 'r2': r2}", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): \"\"\" PyTorch Dataset for generating windows of data. \"\"\" def __init__(self, data: pd.DataFrame, lookback: int, lookahead: int, label_columns=None): self.data = data self.lookback = lookback self.lookahead = lookahead self.label_columns = label_columns self.total_window_size = lookback + lookahead self.input_slice = slice(0, lookback) self.label_start = self.total_window_size - lookahead self.labels_slice = slice(self.label_start, None) self.length = len(data) - self.total_window_size + 1 if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(data.columns)} def __len__(self): \"\"\" Return the number of windows in the dataset. \"\"\" return self.length def __getitem__(self, idx: int) -> tuple: \"\"\" Generate a window of data. \"\"\" features = self.data.iloc[idx:idx + self.total_window_size].values inputs = features[self.input_slice, :] labels = features[self.labels_slice, :] if self.label_columns is not None: labels = np.stack( [labels[:, self.column_indices[name]] for name in self.label_columns], axis=-1 ) inputs = torch.tensor(inputs, dtype=torch.float32) labels = torch.tensor(labels, dtype=torch.float32) return inputs, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.529 , 0.231 , 0.6824 , 0.4548] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd from sklearn.model_selection import train_test_split from sklearn.utils import resample def minmax_scaling(X): for col in X.columns: if X[col].dtype in [\"int64\", \"float64\", \"int32\"]: X[col] = (X[col] - X[col].min()) / (X[col].max() - X[col].min()) return X # Build a preprocessing step for numeric features def preprocess_numeric_features(X): X = minmax_scaling(X) return X # Build a preprocessing step for nominal features def preprocess_nominal_features(X): # One hot encoding X = pd.get_dummies(X) return X # Build a preprocessing step for ordinal features def preprocess_ordinal_features(X): # Map education to ordinal values education_group_mapping = { 'Children': 0, 'Less than 1st grade': 1, '1st 2nd 3rd or 4th grade': 2, '5th or 6th grade': 3, '7th and 8th grade': 4, '9th grade': 5, '10th grade': 6, '11th grade': 7, '12th grade no diploma': 8, 'High school graduate': 9, 'Some college but no degree': 10, 'Associates degree-occup /vocational': 11, 'Associates degree-academic program': 12, 'Bachelors degree(BA AB BS)': 13, 'Masters degree(MA MS MEng MEd MSW MBA)': 14, 'Prof school degree (MD DDS DVM LLB JD)': 15, 'Doctorate degree(PhD EdD)': 16 } X['education'] = X['education'].map(education_group_mapping) return X # balance the training data def resampling(X_train, y_train): X_train = pd.DataFrame(X_train) X_train.reset_index(drop=True, inplace=True) y_train.reset_index(drop=True, inplace=True) train_data = pd.concat([X_train, y_train], axis=1) # Separate majority and minority classes majority = train_data[train_data['income_group'] == False] minority = train_data[train_data['income_group'] == True] # Upsample minority class minority_upsampled = resample(minority, replace=True, # Sample with replacement n_samples=len(majority), # Match majority class random_state=42) # Seed for reproducibility # Combine back to the training data train_data_upsampled = pd.concat([majority, minority_upsampled]) # Separate the upsampled data y_train_upsampled = train_data_upsampled['income_group'] X_train_upsampled = train_data_upsampled.drop('income_group', axis=1) return X_train_upsampled, y_train_upsampled def preprocess_features(df): # ordinal encoding of education df = preprocess_ordinal_features(df) # one hot encoding of nominal features df = preprocess_nominal_features(df) # minmax scaling of numeric features df = preprocess_numeric_features(df) return df def transform(data_path): \"\"\" Description of the function. :param data_path: ...... :return: ...... \"\"\" # Load in the data df = pd.read_csv(data_path) # Define X as input features and y as the outcome variable X = df.drop(columns=[\"income_group\", \"id\"]) y = df[\"income_group\"] X = preprocess_features(X) y = y.map({\"- 50000.\": True, \"50000+.\": False}) # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Resample the training data X_train, y_train = resampling(X_train, y_train) return X_train, X_test, y_train, y_test # comment", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd class DecisionTree(): def __init__(self, max_depth=2, feature_proportion = None, random_state=42): self.max_depth = max_depth self.random_state = random_state self.feature_proportion = feature_proportion self.max_features = None # use sqrt(n_features) unless feature_proportion is set self.root = None # track the root node self.sample_size = None # track the sample size self.n_features = None # track the number of features np.random.seed(random_state) def gini(self, y1, y2): \"\"\" calculate the gini impurity of a node y1: a binary array of the y values y2: the reverse values of y1 \"\"\" # y2 is not used N = len(y1) if N == 0: return 0 p = np.bincount(y1) / N return 1 - np.sum(p ** 2) def feature_importance(self, y, left, right): \"\"\" calculate the feature importance of a split y: the y values of the parent node left: the y values of the left child node after the split right: the y values of the right child node after the split \"\"\" N = self.sample_size N_self = len(y) N_left = len(left) N_right = len(right) gini_left = self.gini(left, left) gini_right = self.gini(right, right) gini_self = self.gini(y, y) feature_importance = (N_self/N) * (gini_self - (N_left/N_self) * gini_left - (N_right/N_self) * gini_right) return feature_importance def splitter(self, X, y): \"\"\" find the best feature to split on \"\"\" features = self.select_features() best_feature_importance = 0 best_feature = None best_value = None for feature in features: # get the unique values of the feature unique_values = np.unique(X[:, feature]) if len(unique_values) > 2: unique_values = (0.25, 0.5, 0.75) for value in unique_values: # split the data at the value left_mask = X[:, feature] <= value right_mask = X[:, feature] > value if np.any(left_mask) and np.any(right_mask): # calculate the feature importance of the split left_y = y[left_mask] right_y = y[right_mask] feature_importance = self.feature_importance(y, left_y, right_y) # if the feature importance is larger than the current best, update the best if feature_importance > best_feature_importance: best_feature_importance = feature_importance best_feature = feature best_value = value return best_feature_importance, best_feature, best_value def select_features(self): \"\"\" select a random subset of features \"\"\" max_features = self.max_features n_features = self.n_features if n_features > 20: # To boost performance, ensure that some numerical features are always chosen features = np.random.choice(range(n_features), max_features - 10, replace=False) add_numerics = np.random.choice(range(20), 10, replace=False) features = np.concatenate((features, add_numerics)) else: features = np.random.choice(range(n_features), max_features, replace=False) features = np.unique(features) return features def fit(self, X, y): \"\"\" X: a numpy matrix (all values should be binary or within 0 to 1) y: a numpy array (all values should be binary or within 0 to 1) \"\"\" # initialise the tree tree = Tree() # initialise max_features if self.feature_proportion: self.max_features = int(self.feature_proportion * X.shape[1]) else: # use sqrt(n_features) self.max_features = min(int(np.sqrt(X.shape[1])), 1) # initialise sample size and number of features self.sample_size = X.shape[0] self.n_features = X.shape[1] # initialise the root node root = Node(parent=None, X=X, y=y, depth=0) self.root = root tree.add_node(root) # build the tree queue = Queue() # track the best nodes to split while True: # get all leaf nodes from tree leaves = tree.get_all_leaves() # check if stopping criterion is met if self.check_stop(leaves): break # for each", "source": "decision_tree.py"}, {"content": "leaf, split the node for leaf in leaves: # check if the leaf has been split before if leaf.feature is not None: continue # find the best feature to split feature_importance, feature, value = self.splitter(leaf.X, leaf.y) leaf.set_feature(feature, value, feature_importance) queue.append(leaf) # pop the leaf with the highest feature importance leaf = queue.pop() if leaf is None: # no more leaves to split, stop break # create the left and right nodes left, right = leaf.split() tree.add_node(left) tree.add_node(right) def _predict(self, observation): \"\"\" find the prediction for a single observation \"\"\" node = self.root while not node.is_leaf: if observation[node.feature] <= node.value: node = node.left else: node = node.right return node.predict def predict(self, X): \"\"\" find the prediction for a matrix of observations X: a numpy matrix (all values should be binary or within 0 to 1) \"\"\" X = pd.DataFrame(X) predictions = [] for i in range(len(X)): observation = X.iloc[i] prediction = self._predict(observation) predictions.append(prediction) return np.array(predictions) def check_stop(self, leaves): \"\"\" check if any leaves have reached the maximum depth \"\"\" if any(leaf.depth == self.max_depth for leaf in leaves): return True class Node(): def __init__(self, parent, X, y, depth): self.parent = parent # root node has parent None self.X = X self.y = y self.depth = depth self.feature = None self.value = None self.feature_importance = None self.left = None self.right = None @property def is_leaf(self): \"\"\"check if the node is a leaf\"\"\" return self.left is None and self.right is None def set_feature(self, feature, value, feature_importance): self.feature = feature self.value = value self.feature_importance = feature_importance def split(self): \"\"\"create the left and right nodes\"\"\" feature, value = self.feature, self.value left_X = self.X[self.X[:, feature] <= value] left_y = self.y[self.X[:, feature] <= value] right_X = self.X[self.X[:, feature] > value] right_y = self.y[self.X[:, feature] > value] left = Node(parent=self, X=left_X, y=left_y, depth=self.depth + 1) right = Node(parent=self, X=right_X, y=right_y, depth=self.depth + 1) self.left = left self.right = right return left, right @property def predict(self): \"\"\"return the majority class of the samples\"\"\" return np.argmax(np.bincount(self.y)) def __str__(self): return f'Feature: {self.feature}, Value: {self.value}, feature_importance: {self.feature_importance}, Depth: {self.depth}' class Tree(): def __init__(self): self.tree = [] def add_node(self, node): self.tree.append(node) def get_all_leaves(self): leaves = [] for node in self.tree: if node.is_leaf: leaves.append(node) return leaves class Queue(): def __init__(self): self.queue = [] def append(self, item): self.queue.append(item) def pop(self): \"\"\"pop the item with the highest feature importance\"\"\" max_importance = 0 best_node = None for node in self.queue: feature_importance = node.feature_importance if feature_importance > max_importance: max_importance = feature_importance best_node = node if best_node is not None: self.queue.remove(best_node) return best_node", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = None def train(self, params, X_train, y_train): \"\"\" Description of the function. :param params: dict, hyperparameters for the model :param X_train: pandas.DataFrame, features for training :param y_train: pandas.Series, target for training :return: float, evaluation metric (f1 score) \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.model = RandomForestClassifier(**params) self.model.fit(X_train, y_train) return self.evaluate(X_train, y_train) def evaluate(self, X_test, y_test): \"\"\" Description of the function. :param X_test: pandas.DataFrame, features for testing :param y_test: pandas.Series, target for testing :return: float, evaluation metric (f1 score) \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.model.predict(X_test) f1 = f1_score(y_test, y_pred) return f1 def get_default_params(self): \"\"\" Description of the function. :return: dict, default parameters for the model \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return { 'criterion': 'entropy', 'max_depth': 25, 'max_features': 'sqrt', 'n_estimators': 300, 'random_state': 42 }", "source": "model.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree class RandomForest: def __init__(self, n_trees=5, max_depth=5, subsample_size = 0.8, sample_with_replacement=True, feature_proportion=None, # None: use sqrt(n_features) random_state=42): self.n_trees = n_trees # number of trees in the forest self.max_depth = max_depth self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.random_state = random_state np.random.seed(random_state) self.trees = [] # list of trees def fit(self, X, y): n_samples = X.shape[0] for _ in range(self.n_trees): # Bootstrapping: sample with replacement indices = np.random.choice(int(n_samples * self.subsample_size), n_samples, replace=self.sample_with_replacement) X_bootstrap = X[indices] y_bootstrap = y[indices] # Train a decision tree on the bootstrapped sample tree = DecisionTree(max_depth=self.max_depth, feature_proportion=self.feature_proportion, random_state=self.random_state) tree.fit(X_bootstrap, y_bootstrap) self.trees.append(tree) def predict(self, X): # Collect predictions from each tree tree_predictions = [tree.predict(X) for tree in self.trees] # Stack predictions to create a 2D array where each row is from a different tree tree_predictions = np.stack(tree_predictions) # Compute the majority vote by taking the mean and rounding majority_votes = np.round(np.mean(tree_predictions, axis=0)).astype(int) return majority_votes", "source": "random_forest.py"}, {"content": "import pandas as pd from sqlalchemy import create_engine # Install and generate a profile report using ydata profiling. from ydata_profiling import ProfileReport server = 'aiap-training.database.windows.net' database = 'aiap' username = 'apprentice' password = 'Pa55w.rd' driver = '{ODBC Driver 18 for SQL Server}' table = 'census_subset' s = 'DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password conn_string = f\"mssql+pyodbc:///?odbc_connect={s}\" print(conn_string) engine = create_engine(conn_string) with engine.connect() as conn, conn.begin(): df = pd.read_sql_table(table, conn) # Summary and Insights from ydata profiling profile = ProfileReport(df, title=\"Profiling Report\") profile.to_file(\"your_report.html\")", "source": "ydata.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import keras_tuner from tensorflow import keras from tensorflow.keras.layers import Dense, Dropout, Input from typing import List from sklearn.metrics import precision_score, recall_score, f1_score from tensorflow.keras.models import Sequential def keras_define_model( input_dim: int, hidden_units: List[int] = [64, 8], dropout: float = 0.2): \"\"\"Defines a Keras model with 2 hidden layers and a specified number of nodes. Args: input_dim: The number of features in the input data. hidden_units: A list of number of nodes in each hidden layer. Returns: A Keras model. \"\"\" model = Sequential() model.add(Input(shape=(input_dim,))) model.add(Dense(hidden_units[0], activation='relu')) model.add(Dropout(dropout)) model.add(Dense(hidden_units[1], activation='relu')) model.add(Dropout(dropout)) model.add(Dense(1, activation='sigmoid')) return model def keras_compile_model(model, learning_rate=1e-3): \"\"\"Compiles a Keras model. Args: model: A Keras model. optimizer: The optimizer to use. loss: The loss function to use. metrics: A list of metrics to track during training. Returns: The compiled Keras model. \"\"\" optimizer = keras.optimizers.Adam(learning_rate=learning_rate) loss=keras.losses.BinaryCrossentropy() metrics=[ keras.metrics.BinaryAccuracy(), keras.metrics.FalseNegatives(), ] model.compile(optimizer=optimizer, loss=loss, metrics=metrics) return model def build_model(input_dim, hp): \"\"\"builds a Keras model within hyperparameters search space. Args: input_dim: The number of features in the input data. hp: keras_tuner.HyperParameters() Returns: A Keras model. \"\"\" hidden_unit_1 = hp.Int('hidden_unit_1', min_value=16, max_value=512, step=32) hidden_unit_2 = hp.Int('hidden_unit_2', min_value=4, max_value=hidden_unit_1, step=16) dropout = hp.Float('dropout', min_value=0.1, max_value=0.5, step=0.1) learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4]) model = keras_define_model(input_dim, [hidden_unit_1, hidden_unit_2], dropout) model = keras_compile_model(model, learning_rate) return model def evaluate_model(model, X_test, y_test): \"\"\"Evaluate the model on the test set.\"\"\" y_pred = model.predict(X_test) y_pred = (y_pred > 0.5).astype(int) metrics = { 'precision': precision_score(y_test, y_pred), 'recall': recall_score(y_test, y_pred), 'f1_score': f1_score(y_test, y_pred) } return metrics class HyperRegressor(keras_tuner.HyperModel): def __init__(self, input_dim): self.input_dim = input_dim def build(self, hp): model = build_model(self.input_dim, hp) return model def fit(self, hp, model, x, y, validation_data, **kwargs): model.fit(x, y, validation_data=validation_data, **kwargs) x_val, y_val = validation_data metrics = evaluate_model(model, x_val, y_val) return metrics", "source": "HyperRegressor.py"}, {"content": "# TODO: # 1. Write DOCKERFILE # 2. Write complete python script, with mlflow tracking and hyperparameter tuning import os import logging import random import pandas as pd import numpy as np import tensorflow as tf from sklearn.model_selection import train_test_split from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN from imblearn.under_sampling import TomekLinks, RepeatedEditedNearestNeighbours, AllKNN from imblearn.over_sampling import BorderlineSMOTE, SVMSMOTE from sklearn.preprocessing import StandardScaler import mlflow import keras_tuner from sqlalchemy import create_engine from dotenv import load_dotenv from HyperRegressor import HyperRegressor, evaluate_model import hydra import general_utils logger = logging.getLogger(__name__) mlflow_init_status = False load_dotenv() def load_data(): # TODO: use sqlalchemy or attach volume? df = pd.read_csv('creditcard.csv') return df def split_data(df: pd.DataFrame): \"\"\"Split to train, validation and test sets.\"\"\" X = df.drop('Class', axis=1) y = df['Class'] X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size=0.2, random_state=42, stratify=y_train ) logger.info(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\") return X_train, y_train, X_val, y_val, X_test, y_test def resample_data(X_train, y_train): \"\"\"Resample data using different methods. Returns: res_methods: List[str] List of names of resampling methods used. res_X: List[np.ndarray] List of resampled X data. res_y: List[np.ndarray] List of resampled y data. \"\"\" res_functions = [ RandomOverSampler(random_state=42), # SMOTE(random_state=42), # ADASYN(random_state=42), # TomekLinks(), # RepeatedEditedNearestNeighbours(), # AllKNN(), # BorderlineSMOTE(random_state=42), SVMSMOTE(random_state=42), ] res_methods, res_X, res_y = [\"original\"], [X_train], [y_train] for res_function in res_functions: method_name = str(res_function) logger.info(f\"Resampling using {method_name}...\") X, y = res_function.fit_resample(X_train, y_train) res_methods.append(method_name) res_X.append(X) res_y.append(y) return res_methods, res_X, res_y def data_processing(X_train, X_val, X_test): \"\"\"Standardize data using StandardScaler.\"\"\" scaler = StandardScaler() X_train = scaler.fit_transform(X_train) X_val = scaler.transform(X_val) X_test = scaler.transform(X_test) return X_train, X_val, X_test def reset_random_seeds(seed: int = 42): os.environ['PYTHONHASHSEED']=str(seed) tf.random.set_seed(seed) np.random.seed(seed) random.seed(seed) def train_model( model_name: str, X_train, y_train, X_val, y_val, X_test, y_test, epochs: int = 200, batch_size: int = 512, max_trials: int = 10 ): \"\"\"Trains a Keras model and tracks metrics with MLflow. Args: X_train: The training data features. y_train: The training data labels. X_val: The validation data features. y_val: The validation data labels. X_test: The testing data features. y_test: The testing data labels. epochs: The number of epochs to train for. batch_size: The batch size for training. Returns: metrics: A dictionary of metrics. model: The trained Keras model. \"\"\" logger.info(\"Tuning hyperparameters...\") general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"max_trials\": max_trials, \"batch_size\": batch_size, \"epochs\": epochs, }, ) hypermodel=HyperRegressor(input_dim=X_train.shape[1]) tuner = keras_tuner.RandomSearch( hypermodel, # Objective is one of the keys. objective=keras_tuner.Objective(\"f1_score\", \"max\"), max_trials=max_trials, overwrite=True, directory=\"hyperparam_search\", project_name=\"my_project\", ) logger.info(\"Starting hyperparameter search...\") tuner.search( x=X_train, y=y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)] ) logger.info(\"Finished hyperparameter search...\") best_params = tuner.get_best_hyperparameters()[0].values general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=best_params ) # mlflow.log_params(best_params) logger.info(f\"Best hyperparameters: {best_params}\") best_model = tuner.get_best_models()[0] best_model.save(f\"models/{model_name}.keras\") general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=f\"models/{model_name}.keras\", artifact_path=\"models\" ) # mlflow.keras.log_model(best_model, f\"models/{model_name}\") test_metrics = evaluate_model(best_model, X_test, y_test) general_utils.mlflow_log( mlflow_init_status, \"log_metrics\", params=test_metrics ) # mlflow.log_metrics(test_metrics) logger.info(tuner.results_summary()) logger.info(tuner.search_space_summary()) mlflow.end_run() return test_metrics, f\"models/{model_name}.keras\", best_params def save_model(model, params): with mlflow.start_run(): print(params) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=params ) # mlflow.log_params(params) general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model, artifact_path=\"best_model\" ) # mlflow.keras.log_model(model, \"best_model\") mlflow.end_run() def arg_parser(): # TODO: depend on load_data raise NotImplementedError @hydra.main(version_base=None, config_path=\"./conf\", config_name=\"train_model.yaml\") def main(args): global mlflow_init_status general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) reset_random_seeds(args[\"seed\"]) epochs = args[\"epochs\"] batch_size = args[\"batch_size\"] max_trials = args[\"max_trials\"] logger.info(\"Loading data...\") df", "source": "keras_NN.py"}, {"content": "= load_data() logger.info(\"Splitting data...\") X_train, y_train, X_val, y_val, X_test, y_test = split_data(df) logger.info(\"Resampling data...\") res_methods, res_X, res_y = resample_data(X_train, y_train) logger.info(\"Training models...\") best_f1_score, best_model, best_params = 0, None, None for i in range(len(res_X)): logger.info(f\"Using {res_methods[i]}\") mlflow_init_status, mlflow_run = general_utils.mlflow_init( args, setup_mlflow=args[\"setup_mlflow\"], autolog=args[\"mlflow_autolog\"] ) X_train, y_train = res_X[i], res_y[i] X_train, X_val, X_test = data_processing(X_train, X_val, X_test) model_metric, model, params = train_model( f\"{res_methods[i]}_model\", X_train, y_train, X_val, y_val, X_test, y_test, epochs, batch_size, max_trials ) for metric_name, metric_value in model_metric.items(): logger.info(f\"{metric_name}: {metric_value}\") if model_metric[\"f1_score\"] > best_f1_score: best_f1_score = model_metric[\"f1_score\"] best_model = model best_params = params mlflow.end_run() logger.info(f\"Best F1 score: {best_f1_score}\") logger.info(f\"Best model: {best_model}\") logger.info(f\"Best params: {best_params}\") save_model(best_model, best_params) logger.info(\"Model saved. All Done.\") return best_f1_score if __name__ == \"__main__\": main()", "source": "keras_NN.py"}, {"content": "import pandas as pd import numpy as np class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" df = pd.read_csv(train_data_path) return self.transform(df) def transform(df): X = df.drop(columns=['id', 'y']) y = df['y'] y = pd.get_dummies(y) X = np.array(X) y = np.array(y) return X, y def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" df = pd.read_csv(test_data_path) return self.transform(df)", "source": "datapipeline.py"}, {"content": "import numpy as np def softmax(x): return np.exp(x) / np.sum(np.exp(x), axis=1) class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # randn randomly create a sample weights with standard normal distribution self.weights1 = np.random.randn(input_size, hidden_size) self.bias1 = np.random.randn(1, hidden_size) self.weights2 = np.random.randn(hidden_size, output_size) self.bias2 = np.random.randn(1, output_size) self.predictions = None self.label = None self.learning_rate = 0.001 def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" x1 = np.matmul(features, self.weights1) + self.bias1 x2 = np.maximum(0, x1) #relu x3 = np.matmul(x2, self.weights2) + self.bias2 # print(x3) x4 = softmax(x3) self.x = features self.node1 = x1 self.act1 = x2 self.node2 = x3 self.act2 = x4 return x4 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" # cross entropy loss _loss = label * np.log(predictions) _loss = -np.mean(np.sum(_loss, axis=1)) self.label = label return _loss def _backward(self): \"\"\" deprecated function: too messy Adjusts the internal weights/biases \"\"\" # calculate gradients z1 = self.node1 a1 = self.act1 z2 = self.node2 a2 = self.act2 x = self.x # dL/dz2 = a2 - y (1,3) grad_loss_z2 = a2 - self.label # print(grad_loss_z2.shape) # dL/dw2 = a1.T * dL/dz2 (16,1) * (1,3) = (16,3) grad_loss_w2 = np.dot(a1.T, grad_loss_z2) / self.output_size # print(grad_loss_w2.shape) # dL/db2 = dL/dz2 (3,1) grad_loss_b2 = np.sum(grad_loss_z2, axis=0) / self.output_size # print(grad_loss_b2.shape) # dz2/da1 = w2 (16,3) grad_z2_a1 = self.weights2 # print(grad_z2_a1.shape) # print(\"**\"*20) # dL/da1 = dL/dz2 * dz2/da1 (1,3) * (3,16) = (1, 16) grad_loss_a1 = np.dot(grad_loss_z2, grad_z2_a1.T) # print(grad_loss_a1.shape) # da1/dz1 = 1 if z1 > 0 else 0 grad_a1_z1 = z1 > 0 # (1,16) # print(grad_a1_z1.shape) # dL/dz1 = dL/da1 * da1/dz1 (1,16) * (1,16) = (1,16) grad_loss_z1 = grad_a1_z1 * grad_loss_a1 # print(grad_loss_z1.shape) # dL/dw1 = dz1/dw1 * dL/dz1 (4,1) * (1,16) = (4,16) grad_z1_w1 = x # print(grad_z1_w1.shape) # print(grad_z1_w1.reshape(-1, 1)) grad_loss_w1 = np.dot(grad_z1_w1.reshape(-1, 1), grad_loss_z1) / self.output_size # dL/db1 = dL/dz1 (1,16) grad_loss_b1 = np.sum(grad_loss_z1, axis=0) / self.output_size # update weights and biases # print(grad_loss_w1) self.weights1 = self.weights1 - self.learning_rate * grad_loss_w1 self.bias1 -= grad_loss_b1 * self.learning_rate self.weights2 -= grad_loss_w2 * self.learning_rate self.bias2 -= grad_loss_b2 * self.learning_rate # grad_softmax_cross_entropy = self.predictions - self.label # grad_weights2 = np.dot(self.node1.T, grad_softmax_cross_entropy) # grad_bias2 = grad_softmax_cross_entropy # grad_after_relu = np.dot(grad_softmax_cross_entropy, self.weights2.T) # grad_relu = self.node1 > 0 # grad_weights1 = np.dot(grad_relu * grad_after_relu, self.weights1.T) # grad_bias1 = grad_relu * grad_after_relu def backward(self): h1 = self.node1 a1 = self.act1 h2 = self.node2 a2 = self.act2 x = self.x.reshape(1, -1).T y = self.label # (16, 1) * (1, 3) = (16, 3) grad_weight2 = a1.T.dot((a2 - y)) / self.output_size # (1, 3) grad_bias2 = np.sum(a2 - y, axis=0) / self.output_size # (16, 3) * (3, 1) = (16, 1) grad_a1 = self.weights2.dot((a2 - y).T) / self.output_size # (1, 16) grad_relu = h1 > 0 # (1, 16) x (1, 16) = (1, 16), (4, 1) * (1, 16) = (4, 16) grad_weight1 = x.dot(grad_a1.T * grad_relu) # (1, 16) * (1, 16) = (1, 16) grad_bias1 = grad_a1.dot(grad_relu).sum(axis=1) / self.output_size self.weights1", "source": "mlp.py"}, {"content": "-= self.learning_rate * grad_weight1 self.bias1 -= self.learning_rate * grad_bias1 self.weights2 -= self.learning_rate * grad_weight2 self.bias2 -= self.learning_rate * grad_bias2", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np class Datapipeline(): def __init__(self): pass def transform(data_path): df = pd.read_csv(data_path) X = df.drop(columns=['id', 'y']) y = df['y'] y = pd.get_dummies(y) X = np.array(X) y = np.array(y) return X, y", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "import os os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\" import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.applications import ResNet50 import matplotlib.pyplot as plt from tensorflow.keras.preprocessing.image import ImageDataGenerator input_shape = (128, 128) batch_size = 32 data_path = \"data/img-classification/tensorfood-processed\" classes = os.listdir(data_path) train_datagen = ImageDataGenerator( rescale=1./255, # Normalize pixel values (0-255 -> 0-1) rotation_range=30, # width_shift_range=0.2, # height_shift_range=0.2, # shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest', validation_split=0.2 ) train_generator = train_datagen.flow_from_directory( data_path, target_size=input_shape, batch_size=batch_size, class_mode='categorical', subset='training', seed=42 ) val_generator = train_datagen.flow_from_directory( data_path, target_size=input_shape, batch_size=batch_size, class_mode='categorical', subset='validation', seed=42 ) base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(input_shape[0], input_shape[1], 3)) base_model.trainable = False model = models.Sequential([ base_model, layers.GlobalAveragePooling2D(), layers.Dense(1024, activation='relu'), layers.Dense(512, activation='relu'), layers.Dense(12, activation='softmax') ]) model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'] ) epochs = 10 history = model.fit( train_generator, # steps_per_epoch = train_generator.samples // batch_size, validation_data=val_generator, # validation_steps = val_generator.samples // batch_size, epochs=epochs, verbose=1 ) # Plot accuracy plt.plot(history.history['accuracy'], label='train accuracy') plt.plot(history.history['val_accuracy'], label='val accuracy') plt.xlabel('Epochs') plt.ylabel('Accuracy') plt.legend() plt.show() # Plot loss plt.plot(history.history['loss'], label='train loss') plt.plot(history.history['val_loss'], label='val loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()", "source": "train.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [[80, -685, -1028], [-618, 573, -126], [265, 391, -100]] # Replace below with your response matrix_2 = [[-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59]] # Replace below with your response matrix_3 = [[-128, -200, -24], [-127, 573, 28], [384, 218, 59]]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd from statsmodels.tsa.statespace.sarimax import SARIMAX class DataPipeline: def __init__(self): # Your code here pass def run_data_pipeline(self, csv_path): # Your code here df = self.preprocess_data(csv_path) # fill missing values df = self.missing_fill_sarima(df) # create lag features df = self.time_series_lag(df) return df def preprocess_data(self, csv_path): # Your code here df = pd.read_csv(csv_path) # set datetime as index df[\"datetime\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]]) df.index = df[\"datetime\"] df = df.drop(columns=[\"year\", \"month\", \"day\", \"hour\", \"datetime\"]) # one hot encoding categorical_columns = df.select_dtypes(include=['object']).columns df = pd.get_dummies(df, columns=categorical_columns) return df def missing_fill_sarima(self, df): # filling only pm2.5 column new_df = df.copy() pm25 = new_df['pm2.5'] train_data = pm25.dropna() model = SARIMAX(train_data, order=(1, 1, 1), seasonal_order=(1, 1, 1, 24)) sarima_fit = model.fit() missing_mask = pm25.isna() # Generate predictions for all values including those that are missing predicted_values = sarima_fit.predict(start=0, end=len(pm25)-1) predicted_values.index = pm25.index # Only update the missing values in the original dataset pm25_filled = pm25.copy() pm25_filled[missing_mask] = predicted_values[missing_mask] new_df['pm2.5'] = pm25_filled return new_df def time_series_lag(self, df, lags=[1,2,3,4,5,6,12,24]): # Create lag features for PM2.5 and other relevant columns new_df = df.copy() lagged_features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'pm2.5'] for feature in lagged_features: for lag in lags: new_df[feature + '_lag' + str(lag)] = new_df[feature].shift(lag) # Drop rows with NaN values new_df = new_df.dropna() return new_df def split_data(self, df): train_size = int(len(df) * 0.8) train = df[:train_size] test = df[train_size:] X_train = train.drop(columns=[\"pm2.5\"]) y_train = train[\"pm2.5\"] X_test = test.drop(columns=[\"pm2.5\"]) y_test = test[\"pm2.5\"] return X_train, y_train, X_test, y_test", "source": "datapipeline.py"}, {"content": "from src.datapipeline import DataPipeline from src.ml_model import ForecastModel def run_experiment(data_path, lags=[]): # Read data dp = DataPipeline() df = dp.run_data_pipeline(data_path) # Perform data split metrics_dict = {} for lag in lags: # Fit and evaluate over for each lag value df_shift = df.copy() df_shift[\"pm2.5\"] = df_shift[\"pm2.5\"].shift(-lag) df_shift = df_shift.dropna() X_train, y_train, X_test, y_test = dp.split_data(df_shift) model = ForecastModel(n_estimators=100, random_state=42) model.fit(X_train, y_train) train_error, test_error = model.evaluate( model, X_train, y_train, X_test, y_test) metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error, \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel: def __init__(self, n_estimators=100, random_state=42): self.model = RandomForestRegressor( n_estimators=n_estimators, random_state=random_state) def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = model.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim import numpy as np import mlflow import mlflow.pytorch np.random.seed(42) class LSTMModel(nn.Module): def __init__(self, shift, input_size, hidden_size, output_size, num_layers, device): super(LSTMModel, self).__init__() self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) self.shift = shift self.device = device self.to(self.device) # Move the model to the specified device self._initialize_weights() # Initialize weights def _initialize_weights(self): for name, param in self.named_parameters(): if 'weight' in name: torch.nn.init.xavier_normal_(param) # Initialize weights with random normal distribution elif 'bias' in name: nn.init.zeros_(param) # Initialize biases with zeros def forward(self, x): x = x.to(self.device) # Move input to the same device as the model out, _ = self.lstm(x) out = self.fc(out[:, -1, :]) # Take the output of the last time step return out def fit(self, train_dataloader, val_dataloader, num_epochs=20, learning_rate=0.001, patience=5, min_delta=0.001, save_path='best_model.pth', window_generator=None): criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) early_stopping = EarlyStopping(patience=patience, min_delta=min_delta) # Start MLflow run with mlflow.start_run(): # Log parameters mlflow.log_param(\"shift\", self.shift) mlflow.log_param(\"num_epochs\", num_epochs) mlflow.log_param(\"learning_rate\", learning_rate) mlflow.log_param(\"patience\", patience) mlflow.log_param(\"min_delta\", min_delta) # Log WindowGenerator parameters if provided if window_generator: mlflow.log_param(\"lags\", window_generator.lags) mlflow.log_param(\"lookahead\", window_generator.lookahead) for epoch in range(num_epochs): self.train() for features, labels in train_dataloader: features, labels = features.to(self.device), labels.to(self.device) # Move data to the same device as the model # Forward pass outputs = self(features) loss = criterion(outputs, labels) # Ensure labels have the correct shape # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() # Validation phase self.eval() val_loss = 0 with torch.no_grad(): for features, labels in val_dataloader: features, labels = features.to(self.device), labels.to(self.device) # Move data to the same device as the model outputs = self(features) loss = criterion(outputs, labels) # Ensure labels have the correct shape val_loss += loss.item() val_loss /= len(val_dataloader) print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Loss: {val_loss:.4f}') # Log metrics mlflow.log_metric(\"train_loss\", loss.item(), step=epoch) mlflow.log_metric(\"val_loss\", val_loss, step=epoch) # Check early stopping condition early_stopping(val_loss, self) if early_stopping.early_stop: print(\"Early stopping\") break # Load the best model self.load_state_dict(early_stopping.best_model) # save the best model torch.save(self.state_dict(), save_path) # Log the best model # mlflow.pytorch.log_model(self, \"model\") input_example = np.random.randn(1, 1, len(window_generator.lags)) mlflow.pytorch.log_model(self, \"model\", input_example=input_example, conda_env=\"conda_env.yaml\") def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(self.device) # Move data to the same device as the model outputs = self(features) predictions.append(outputs) return torch.cat(predictions, dim=0) def evaluate(self, dataloader): self.eval() mse = 0 criterion = nn.MSELoss() with torch.no_grad(): for features, labels in dataloader: features = features.to(self.device) # Move data to the same device as the model outputs = self(features) loss = criterion(outputs, labels) mse += loss.item() mse /= len(dataloader) return mse class EarlyStopping: def __init__(self, patience=5, min_delta=0): self.patience = patience self.min_delta = min_delta self.counter = 0 self.best_loss = None self.early_stop = False self.best_model = None def __call__(self, val_loss, model): if self.best_loss is None: self.best_loss = val_loss self.save_checkpoint(model) elif val_loss < self.best_loss - self.min_delta: self.best_loss = val_loss self.counter = 0 self.save_checkpoint(model) else: self.counter += 1 if self.counter >= self.patience: self.early_stop = True def save_checkpoint(self, model): \"\"\"Saves model when validation loss decreases.\"\"\" self.best_model = model.state_dict()", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lags, lookahead, target_column=\"pm2.5\"): self.data = data self.lags = lags self.lookahead = lookahead self.target_column = target_column self.length = len(data) - max(lags) - lookahead + 1 # Create sequences of features and corresponding labels self.features = [] self.labels = [] for i in range(self.length): feature = [data.iloc[i + lag].values.astype(np.float32) for lag in lags] self.features.append(np.array(feature)) self.labels.append(data.iloc[i + max(lags) + lookahead - 1][target_column].astype(np.float32)) self.features = np.array(self.features) self.labels = np.array(self.labels) def __len__(self): return self.length def __getitem__(self, idx): features = self.features[idx] labels = self.labels[idx] return torch.tensor(features), torch.tensor(labels)", "source": "windowing.py"}, {"content": "import lxml.html import difflib # Function to clean HTML artifacts from text def clean_html_artifacts(text): try: # Parse the text as HTML html = lxml.html.fromstring(text) # Extract the text content clean_text = html.text_content() return clean_text except Exception as e: # If parsing fails, return the original text return text def find_differences(original, cleaned): diff = difflib.ndiff(original, cleaned) removed = ''.join(x[2:] for x in diff if x.startswith('- ')) return removed def find_html_artifacts(df, display_top_k = 20): df[\"text_cleaned\"] = df[\"text\"].apply(lambda x: clean_html_artifacts(str(x))) i = 0 # Display rows where the original text is different from the cleaned text for idx, row in df.iterrows(): if row[\"text\"] != row[\"text_cleaned\"]: diff = find_differences(row[\"text\"], row[\"text_cleaned\"]).strip() if diff: i += 1 print(diff) print(\"**\" * 20) if i >= display_top_k: break", "source": "check_html.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import string import re from nltk.stem import WordNetLemmatizer from nltk.tokenize import word_tokenize artifacts_to_remove = [ \"<br />\", \"<r/>\", \"<r />\", \"< br >\", \"\\\\\", \"--\", \"<SPOILER>\", \"</SPOILER>\", \"<em>\", \"/em>\", \"<i>\", \"</i>\", ] def remove_artifacts(text, artifacts_to_remove): for artifact in artifacts_to_remove: text = text.replace(artifact, \" \") return text def remove_html_entities(text): # Define the pattern to match strings like &#1; to &#9; pattern = r'&#\\d+;' # Use re.sub to replace the pattern with an empty string cleaned_text = re.sub(pattern, '', text) return cleaned_text # Initialize wordnet lemmatizer wnl = WordNetLemmatizer() def lemmatization(text, wnl=wnl): # Remove punctuation text_no_punct = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # Create tokens word_tokens = word_tokenize(text_no_punct) # Perform lemmatization # lemmatized_text = \" \".join([wnl.lemmatize(word) for word in word_tokens]) return word_tokens def tokenize(text, artifacts_to_remove = artifacts_to_remove, wnl = wnl): \"\"\" Tokenize the input text by cleaning, tokenizing, and stemming/lemmatizing. Parameters ---------- text : string a single string of text Returns ------- list of cleaned tokens \"\"\" # Remove artifacts text = remove_artifacts(text, artifacts_to_remove) text = remove_html_entities(text) # Remove punctuation word_tokens = lemmatization(text, wnl) return word_tokens", "source": "tokenize.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, FunctionTransformer from sklearn.impute import SimpleImputer from sklearn.model_selection import train_test_split from scipy.sparse import issparse # Function to replace placeholders with NaN def replace_placeholders_df(df): df.replace([\"?\", \"Not in universe\"], np.nan, inplace=True) return df def log_transform(X): \"\"\"Applies log transformation to numerical features to handle skewness.\"\"\" if isinstance(X, pd.Series): # Ensure input is 2D X = X.to_frame() return np.log1p(X) def bin_age(X, age_categories): \"\"\"Bins age into categorical age groups.\"\"\" if X.ndim == 1: # Ensure input is 2D X = X.reshape(-1, 1) bins = [0, 18, 25, 35, 50, 65, np.inf] return pd.DataFrame(pd.cut(X[:, 0], bins=bins, labels=age_categories)) def net_income_bin(X, net_income_categories): \"\"\"Calculates net income and bins it into categories.\"\"\" if X.ndim == 1: # Ensure input is 2D X = X.reshape(-1, 1) net_income = X[:, 0] - X[:, 1] # capital_gains - capital_losses bins = [-np.inf, 0, 5000, 20000, np.inf] return pd.DataFrame(pd.cut(net_income, bins=bins, labels=net_income_categories)) def interaction_term(X): \"\"\"Creates interaction term between weeks worked and wage per hour.\"\"\" if X.ndim == 1: # Ensure input is 2D X = X.reshape(-1, 1) interaction = X[:, 0] * X[:, 1] # weeks_worked_in_year * wage_per_hour return interaction.reshape(-1, 1) def transform(data_path): \"\"\" Transforms the data from the given path and returns the train-test split. :param data_path: Path to the CSV file containing the data. :return: X_train, X_test, y_train, y_test as numpy arrays. \"\"\" # Load the data df = pd.read_csv(data_path) # Applying the function to replace placeholders df = replace_placeholders_df(df.copy()) # Define X as input features and y as the outcome variable X = df.drop(columns=['income_group']) y = df['income_group'] # Drop Redundant Features with high cardinality X.drop(columns=['id', 'enroll_in_edu_inst_last_wk', 'country_of_birth_father', 'country_of_birth_mother', 'detailed_industry_recode', 'detailed_occupation_recode', 'hispanic_origin', 'reason_for_unemployment', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'year', 'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'member_of_a_labor_union', 'major_industry_code', 'family_members_under_18', 'migration_code_move_within_reg'], inplace=True) # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Example ordinal categories (ordered as per the actual levels of education) education_categories = [ 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', 'Children' ] age_categories = ['<18', '18-25', '25-35', '35-50', '50-65', '65+'] net_income_categories = ['loss', 'no gain', 'moderate gain', 'high gain'] # Define column lists for each type of feature numeric_features = ['age', 'wage_per_hour', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'veterans_benefits'] nominal_features = ['marital_stat', 'race', 'sex', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'major_occupation_code', 'full_or_part_time_employment_stat'] ordinal_features = ['education'] # Numeric pipeline numeric_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')), ('log_transform', FunctionTransformer(log_transform, validate=True)), ('scaler', StandardScaler()) ]) # Nominal pipeline nominal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Ordinal pipeline ordinal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('ordinal', OrdinalEncoder(categories=[education_categories])) ]) # Feature engineering pipeline for interactions and binning feature_engineering_pipeline = ColumnTransformer([ ('age_binning', Pipeline(steps=[ ('bin_age', FunctionTransformer(bin_age, kw_args={'age_categories': age_categories}, validate=True)), ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing values ('ordinal', OrdinalEncoder(categories=[age_categories])) ]), ['age']), ('net_income_binning', Pipeline(steps=[ ('bin_net_income', FunctionTransformer(net_income_bin, kw_args={'net_income_categories': net_income_categories},", "source": "datapipeline.py"}, {"content": "validate=True)), ('imputer', SimpleImputer(strategy='most_frequent')), # Impute missing values ('ordinal', OrdinalEncoder(categories=[net_income_categories])) ]), ['capital_gains', 'capital_losses']), ('employment_earnings_interaction', FunctionTransformer(interaction_term, validate=True), ['weeks_worked_in_year', 'wage_per_hour']) ], remainder='drop') # Combine all the transformers into a ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('nom', nominal_transformer, nominal_features), ('ord', ordinal_transformer, ordinal_features), ('feat_eng', feature_engineering_pipeline, numeric_features) ] ) # Fit and transform the training data X_train_transformed = preprocessor.fit_transform(X_train) X_test_transformed = preprocessor.transform(X_test) # Convert the sparse matrix to a dense format if necessary if issparse(X_train_transformed): X_train_transformed = X_train_transformed.todense() if issparse(X_test_transformed): X_test_transformed = X_test_transformed.todense() # Convert the transformed data to numpy arrays X_train_transformed = np.array(X_train_transformed) X_test_transformed = np.array(X_test_transformed) y_train = np.array(y_train) y_test = np.array(y_test) return X_train_transformed, X_test_transformed, y_train, y_test # Example usage # data_path = '/Users/jon/code/aisg/deep-skilling-phase/all-assignments/assignment1/data/raw/data.csv' # X_train, X_test, y_train, y_test = transform(data_path)", "source": "datapipeline.py"}, {"content": "import numpy as np from collections import Counter class TreeNode(): def __init__(self, data, feature_idx, feature_val, prediction_probs, information_gain) -> None: self.data = data self.feature_idx = feature_idx self.feature_val = feature_val self.prediction_probs = prediction_probs self.information_gain = information_gain self.feature_importance = self.data.shape[0] * self.information_gain self.left = None self.right = None def node_def(self) -> str: if (self.left or self.right): return f\"NODE | Information Gain = {self.information_gain} | Split IF X[{self.feature_idx}] < {self.feature_val} THEN left O/W right\" else: unique_values, value_counts = np.unique(self.data[:,-1], return_counts=True) output = \", \".join([f\"{value}->{count}\" for value, count in zip(unique_values, value_counts)]) return f\"LEAF | Label Counts = {output} | Pred Probs = {self.prediction_probs}\" class DecisionTree(): \"\"\" Decision Tree Classifier Training: Use \"train\" function with train set features and labels Predicting: Use \"predict\" function with test set features \"\"\" def __init__(self, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, numb_of_features_splitting=None, amount_of_say=None) -> None: \"\"\" Setting the class with hyperparameters max_depth: (int) -> max depth of the tree min_samples_leaf: (int) -> min # of samples required to be in a leaf to make the splitting possible min_information_gain: (float) -> min information gain required to make the splitting possible num_of_features_splitting: (str) -> when splitting if sqrt then sqrt(# of features) features considered, if log then log(# of features) features considered else all features are considered amount_of_say: (float) -> used for Adaboost algorithm \"\"\" self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.numb_of_features_splitting = numb_of_features_splitting self.amount_of_say = amount_of_say def __gini(self, labels_left_node: list, labels_right_node: list) -> float: \"\"\" Calculate the Gini impurity for a split in a decision tree. This function computes the weighted Gini impurity for a split into two nodes (left and right). Parameters: labels_left_node (list): List of labels for the samples in the left node. labels_right_node (list): List of labels for the samples in the right node. Returns: float: The weighted Gini impurity for the split. \"\"\" # Calculate total number of samples total_count = len(labels_left_node) + len(labels_right_node) # Calculate Gini for the left node gini_left = 1.0 - sum((count / len(labels_left_node)) ** 2 for count in Counter(labels_left_node).values()) # Calculate Gini for the right node gini_right = 1.0 - sum((count / len(labels_right_node)) ** 2 for count in Counter(labels_right_node).values()) # Compute the weighted Gini weighted_gini = (len(labels_left_node) / total_count) * gini_left + (len(labels_right_node) / total_count) * gini_right return weighted_gini def _data_gini(self, labels: list) -> float: \"\"\" Calculate the Gini impurity for a single node given its labels. Parameters: labels (list): A list of class labels for the samples in the node. Returns: float: The Gini impurity of the node. A value between 0 and 1, where 0 indicates perfect purity and 1 indicates maximum impurity. \"\"\" total_count = len(labels) if total_count == 0: return 0.0 # No impurity if there are no samples label_counts = Counter(labels) gini = 1.0 - sum((count / total_count) ** 2 for count in label_counts.values()) return gini def _split(self, data: np.array, feature_idx: int, feature_val: float) -> tuple: \"\"\" Split the dataset into two groups based on a feature threshold. This method splits the input dataset into two groups based on whether the values of a specified feature are below or above a given threshold. It is to create branches based", "source": "decision_tree.py"}, {"content": "on feature values. Parameters: data (np.array): A 2D numpy array where each row represents a sample and each column represents a feature. feature_idx (int): The index of the feature to split on. feature_val (float): The threshold value for the feature. Samples with feature values below this threshold will be placed in the first group, and samples with feature values equal to or above this threshold will be placed in the second group. Returns: tuple: A tuple containing two numpy arrays: - The first array contains samples with feature values below the threshold. - The second array contains samples with feature values equal to or above the threshold. \"\"\" mask_below_threshold = data[:, feature_idx] < feature_val group1 = data[mask_below_threshold] group2 = data[~mask_below_threshold] return group1, group2 def _select_features_to_use(self, data: np.array) -> list: \"\"\" Randomly selects the features to use for splitting based on the hyperparameter `numb_of_features_splitting`. This method selects a subset of features from the dataset to use for splitting at a node in the decision tree. The selection is based on the value of the `numb_of_features_splitting` hyperparameter, which can be \"sqrt\", \"log\", or any other value indicating the use of all features. Parameters: data (np.array): A 2D numpy array where each row represents a sample and each column represents a feature. The last column is assumed to be the target variable and is excluded from the feature selection. Returns: list: A list of indices representing the selected features to use for splitting. \"\"\" feature_idx = list(range(data.shape[1]-1)) if self.numb_of_features_splitting == \"sqrt\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.sqrt(len(feature_idx)))) elif self.numb_of_features_splitting == \"log\": feature_idx_to_use = np.random.choice(feature_idx, size=int(np.log2(len(feature_idx)))) else: feature_idx_to_use = feature_idx return feature_idx_to_use def _find_best_split(self, data: np.array) -> tuple: \"\"\" Find the best split for the dataset based on Gini impurity. This method identifies the best feature and threshold value to split the dataset in order to minimize the Gini impurity. It iterates over a subset of features and potential split points (determined using percentiles) to find the optimal split. Parameters: data (np.array): A 2D numpy array where each row represents a sample and each column represents a feature. The last column is assumed to be the target variable. Returns: tuple: A tuple containing: - g1_min (np.array): The subset of data where the selected feature's value is below the optimal threshold. - g2_min (np.array): The subset of data where the selected feature's value is equal to or above the optimal threshold. - min_gini_feature_idx (int): The index of the feature used for the optimal split. - min_gini_feature_val (float): The threshold value of the feature used for the optimal split. - min_gini (float): The Gini impurity of the optimal split. \"\"\" min_gini = float('inf') # Initialize to a large value feature_idx_to_use = self._select_features_to_use(data) for idx in feature_idx_to_use: # Using percentiles to determine potential split points feature_vals = np.percentile(data[:, idx], q=np.arange(25, 100, 25)) for feature_val in feature_vals: g1, g2 = self._split(data, idx, feature_val) # Calculate Gini impurity gini = self.__gini(g1[:, -1].tolist(), g2[:, -1].tolist()) if gini < min_gini: min_gini = gini min_gini_feature_idx = idx min_gini_feature_val = feature_val g1_min, g2_min = g1, g2 return g1_min, g2_min, min_gini_feature_idx, min_gini_feature_val, min_gini def _find_label_probs(self, data: np.array) ->", "source": "decision_tree.py"}, {"content": "np.array: \"\"\" Calculate the probabilities of each label in the dataset. This method computes the probability of each label in the provided dataset. The probability of a label is defined as the number of occurrences of that label divided by the total number of labels. The labels are assumed to be integers and are located in the last column of the dataset. Parameters: data (np.array): A 2D numpy array where each row represents a sample and the last column represents the label. Returns: np.array: A 1D numpy array containing the probabilities of each label. The length of the array is equal to the number of unique labels in the training data (`self.labels_in_train`). \"\"\" labels_as_integers = data[:,-1].astype(int) total_labels = len(labels_as_integers) label_probabilities = np.zeros(len(self.labels_in_train), dtype=float) for i, label in enumerate(self.labels_in_train): label_index = np.where(labels_as_integers == i)[0] if len(label_index) > 0: label_probabilities[i] = len(label_index) / total_labels return label_probabilities def _create_tree(self, data: np.array, current_depth: int) -> TreeNode: \"\"\" Recursively creates a decision tree using a depth-first approach. This function splits the data into two subsets based on the best split found, calculates the Gini impurity and information gain, and creates tree nodes until the stopping criteria are met. Parameters: ----------- data : np.array The dataset to be used for creating the tree. The last column is assumed to be the target variable. current_depth : int The current depth of the tree. Used to control the maximum depth of the tree. Returns: -------- TreeNode The root node of the created subtree. If the stopping criteria are met, it returns a leaf node. Stopping Criteria: ------------------ - If the current depth exceeds the maximum depth (`self.max_depth`), the function returns None. - If the number of samples in either split is less than `self.min_samples_leaf`, the function returns the current node. - If the information gain is less than `self.min_information_gain`, the function returns the current node. Notes: ------ - The function uses the `_find_best_split` method to determine the best feature and value to split the data. - The Gini impurity of the parent node is calculated using the `_data_gini` method. - Label probabilities for prediction are calculated using the `_find_label_probs` method. \"\"\" # Stopping Criteria if current_depth > self.max_depth: return None split_1_data, split_2_data, split_feature_idx, split_feature_val, split_gini = self._find_best_split(data) # Calculate the Gini impurity of the parent node parent_gini = self._data_gini(data[:, -1].tolist()) # Calculate information gain information_gain = parent_gini - split_gini # Find label probabilities for prediction label_probabilities = self._find_label_probs(data) # Create the current node node = TreeNode(data, split_feature_idx, split_feature_val, label_probabilities, information_gain) # Stopping conditions if self.min_samples_leaf > split_1_data.shape[0] or self.min_samples_leaf > split_2_data.shape[0]: return node elif information_gain < self.min_information_gain: return node current_depth += 1 node.left = self._create_tree(split_1_data, current_depth) node.right = self._create_tree(split_2_data, current_depth) return node def fit(self, X_train: np.array, Y_train: np.array) -> None: \"\"\" Trains the model with given X and Y datasets \"\"\" self.labels_in_train = np.unique(Y_train) train_data = np.concatenate((X_train, np.reshape(Y_train, (-1, 1))), axis=1) self.tree = self._create_tree(data=train_data, current_depth=0) self.feature_importances = dict.fromkeys(range(X_train.shape[1]), 0) self._calculate_feature_importance(self.tree) # self.feature_importances = {k: v / total for total in (sum(self.feature_importances.values()),) for k, v in self.feature_importances.items()} total_importance = sum(self.feature_importances.values()) if total_importance > 0: self.feature_importances = {k: v / total_importance for", "source": "decision_tree.py"}, {"content": "k, v in self.feature_importances.items()} else: self.feature_importances = {k: 0 for k in self.feature_importances.keys()} def _predict_one_sample(self, X: np.array) -> np.array: \"\"\" Predicts the class probabilities for a single sample using the decision tree. This method traverses the decision tree from the root to a leaf node based on the feature values of the input sample. At each node, it decides to move left or right depending on whether the feature value is less than or greater than the node's split value. Once a leaf node is reached, the method returns the class probabilities stored in that leaf node. Parameters: ----------- X : np.array A 1-dimensional numpy array representing a single sample. Each element in the array corresponds to a feature value of the sample. Returns: -------- np.array A numpy array containing the predicted class probabilities for the input sample. The length of the array corresponds to the number of classes. Notes: ------ - The method assumes that the decision tree has already been built and is stored in `self.tree`. - The `node.prediction_probs` attribute of a leaf node contains the class probabilities. - If the tree is not properly built or the input sample does not match the expected feature indices, the method may not function correctly. \"\"\" node = self.tree while node: pred_probs = node.prediction_probs if X[node.feature_idx] < node.feature_val: node = node.left else: node = node.right return pred_probs def predict_proba(self, X_set: np.array) -> np.array: \"\"\" Returns the predicted probabilities for a given dataset. This method predicts the class probabilities for each sample in the input dataset by applying the `_predict_one_sample` method along the rows of the dataset. It uses the decision tree model that has been previously trained. Parameters: ----------- X_set : np.array A 2-dimensional numpy array where each row represents a sample and each column represents a feature. The shape of the array should be (n_samples, n_features). Returns: -------- np.array A 2-dimensional numpy array where each row contains the predicted class probabilities for the corresponding sample in the input dataset. The shape of the array is (n_samples, n_classes). Notes: ------ - The method assumes that the decision tree has already been built and is stored in `self.tree`. - The `_predict_one_sample` method is used to predict the class probabilities for each individual sample. - If the tree is not properly built or the input dataset does not match the expected feature indices, the method may not function correctly. \"\"\" pred_probs = np.apply_along_axis(self._predict_one_sample, 1, X_set) return pred_probs def predict(self, X_set: np.array) -> np.array: \"\"\" Predicts the class labels for a given dataset. This method predicts the class labels for each sample in the input dataset by first calculating the class probabilities using the `predict_proba` method and then selecting the class with the highest probability. Parameters: ----------- X_set : np.array A 2-dimensional numpy array where each row represents a sample and each column represents a feature. The shape of the array should be (n_samples, n_features). Returns: -------- np.array A 1-dimensional numpy array containing the predicted class labels for each sample in the input dataset. The length of the array is equal to the number of", "source": "decision_tree.py"}, {"content": "samples. Notes: ------ - The method assumes that the decision tree has already been built and is stored in `self.tree`. - The `predict_proba` method is used to calculate the class probabilities for each sample. - The class with the highest probability is selected as the predicted label for each sample. \"\"\" pred_probs = self.predict_proba(X_set) preds = np.argmax(pred_probs, axis=1) return preds def _print_recursive(self, node: TreeNode, level=0) -> None: if node != None: self._print_recursive(node.left, level + 1) print(' ' * 4 * level + '-> ' + node.node_def()) self._print_recursive(node.right, level + 1) def print_tree(self) -> None: self._print_recursive(node=self.tree) def _calculate_feature_importance(self, node): \"\"\" Recursively calculates the feature importance for each feature in the decision tree. This method traverses the decision tree starting from the given node and accumulates the feature importance values for each feature. The feature importance is stored in the `self.feature_importances` attribute, which is a dictionary where the keys are feature indices and the values are the accumulated importance scores. Parameters: ----------- node : TreeNode The current node in the decision tree. The traversal starts from this node and proceeds recursively to its left and right children. Returns: -------- None Notes: ------ - The method assumes that the decision tree has already been built and is stored in `self.tree`. - The `node.feature_idx` attribute indicates the index of the feature used for splitting at the current node. - The `node.feature_importance` attribute indicates the importance score of the feature at the current node. - The `self.feature_importances` attribute is updated in place and should be initialized before calling this method. - If the node is `None`, the method returns without performing any action. \"\"\" if node != None: self.feature_importances[node.feature_idx] += node.feature_importance self._calculate_feature_importance(node.left) self._calculate_feature_importance(node.right) # Example usage # from datapipeline import transform # X_train, X_test, y_train, y_test = transform('/Users/jon/code/aisg/deep-skilling-phase/all-assignments/assignment1/data/raw/data.csv') # # Replace values in y_train # y_train = np.where(y_train == '- 50000.', 0, y_train) # y_train = np.where(y_train == '50000+.', 1, y_train) # # Replace values in y_test # y_test = np.where(y_test == '- 50000.', 0, y_test) # y_test = np.where(y_test == '50000+.', 1, y_test) # # Ensure the arrays are of integer type # y_train = y_train.astype(int) # y_test = y_test.astype(int) # dt = DecisionTree() # dt.fit(X_train, y_train) # dt.print_tree()", "source": "decision_tree.py"}, {"content": "import base64 from IPython.display import Image, display def mm_ink(graphbytes): \"\"\"Given a bytes object holding a Mermaid-format graph, return a URL that will generate the image.\"\"\" base64_bytes = base64.b64encode(graphbytes) base64_string = base64_bytes.decode(\"ascii\") return \"https://mermaid.ink/img/\" + base64_string def mm_display(graphbytes): \"\"\"Given a bytes object holding a Mermaid-format graph, display it.\"\"\" display(Image(url=mm_ink(graphbytes))) def mm(graph): \"\"\"Given a string containing a Mermaid-format graph, display it.\"\"\" graphbytes = graph.encode(\"ascii\") mm_display(graphbytes) def mm_link(graph): \"\"\"Given a string containing a Mermaid-format graph, return URL for display.\"\"\" graphbytes = graph.encode(\"ascii\") return mm_ink(graphbytes) def mm_path(path): \"\"\"Given a path to a file containing a Mermaid-format graph, display it\"\"\" with open(path, 'rb') as f: graphbytes = f.read() mm_display(graphbytes)", "source": "mm_graph_utils.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score import numpy as np class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() def train(self, params: dict, X_train: np.array, y_train: np.array) -> float: \"\"\" Train the model with the given parameters and training data. :param params: Dictionary of parameters for the RandomForestClassifier. :param X_train: Training feature data. :param y_train: Training target data. :return: Train F1 score as a single float. \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it # Set the parameters for the model self.model.set_params(**params) # Train the model self.model.fit(X_train, y_train) # Predict on the training data y_pred_train = self.model.predict(X_train) # Calculate the F1 score train_f1 = f1_score(y_train, y_pred_train, average='weighted') return train_f1 def evaluate(self, X_test: np.array, y_test: np.array) -> float: \"\"\" Evaluate the model on the test data. :param X_test: Test feature data. :param y_test: Test target data. :return: Test F1 score as a single float. \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score # Predict on the test data y_pred_test = self.model.predict(X_test) # Calculate the F1 score test_f1 = f1_score(y_test, y_pred_test, average='weighted') return test_f1 def get_default_params(self) -> dict: \"\"\" Get the default parameters for the RandomForestClassifier. :return: Dictionary of default parameters. \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model default_params = { 'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True, 'random_state': 42 } return default_params", "source": "model.py"}, {"content": "\"\"\" - Create a new class in this script called RandomForest - Implement a *fit()* function that will initialise 5 trees through bootstrapping (sampling 100% with replacement), - Implement a *predict()* function that will return an answer through a voting mechanism out of all the 5 trees. You may want to loop at `np.stack` and `np.array.mean` to improve the computation efficiency for these calculations. \"\"\" import numpy as np from .decision_tree import DecisionTree class RandomForest: \"\"\" RandomForest Classifier A random forest is a meta estimator that fits a number of decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. \"\"\" def __init__(self, n_trees=5, max_depth=4, min_samples_leaf=1, min_information_gain=0.0, subsample_size=None, sample_with_replacement=True, feature_proportion=None): \"\"\" Initialize the RandomForest with hyperparameters. n_trees: Number of trees in the forest. max_depth: Max depth of each tree. min_samples_leaf: Min number of samples required to be in a leaf. min_information_gain: Min information gain required for a split. subsample_size: If a float (0, 1], it represents the fraction of samples to draw from X to train each tree. If an integer, it represents the number of samples to draw. If None, use the whole dataset. sample_with_replacement: Whether to sample with replacement. feature_proportion: If a float (0, 1], it represents the fraction of features to use for each tree. If an integer, it represents the number of features to use. If None, use all features. \"\"\" self.n_trees = n_trees self.max_depth = max_depth self.min_samples_leaf = min_samples_leaf self.min_information_gain = min_information_gain self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.trees = [] self.features_indices = [] def fit(self, X_train: np.array, y_train: np.array): \"\"\" Fit the random forest to the training data. For each tree, bootstrap a sample from the dataset and train a decision tree. \"\"\" n_samples, n_features = X_train.shape self.trees = [] subsample_size = self._determine_subsample_size(n_samples) feature_subset_size = self._determine_feature_subset_size(n_features) for _ in range(self.n_trees): X_sample, y_sample = self._bootstrap_sample(X_train, y_train, subsample_size) feature_indices = self._select_feature_indices(n_features, feature_subset_size) tree = self._train_tree(X_sample, y_sample, feature_indices) self.trees.append(tree) self.features_indices.append(feature_indices) def _determine_subsample_size(self, n_samples): \"\"\" Determine the actual subsample size based on the input configuration. This method calculates the number of samples to use for subsampling based on the `subsample_size` attribute of the class. The `subsample_size` can be `None`, a float, or an integer, and the method handles each case accordingly. Parameters: ----------- n_samples : int The total number of samples in the dataset. Returns: -------- int The number of samples to use for subsampling. Raises: ------- ValueError If `subsample_size` is a float but not in the range (0, 1]. Notes: ------ - If `subsample_size` is `None`, the method returns `n_samples`, meaning the entire dataset is used. - If `subsample_size` is a float between 0 and 1, it is interpreted as a fraction of `n_samples`. - If `subsample_size` is an integer, the method returns the smaller of `subsample_size` and `n_samples`. \"\"\" if self.subsample_size is None: return n_samples # Use the entire dataset elif isinstance(self.subsample_size, float): if 0 < self.subsample_size <= 1: return int(n_samples * self.subsample_size) # Convert fraction to number of samples else: raise ValueError(\"If subsample_size is a float, it must be between 0 and 1.\") elif isinstance(self.subsample_size,", "source": "random_forest.py"}, {"content": "int): return min(self.subsample_size, n_samples) # Cap at n_samples else: raise ValueError(\"subsample_size must be None, a float between 0 and 1, or an integer.\") def _determine_feature_subset_size(self, n_features): \"\"\" Determine the actual feature subset size based on the input configuration. This method calculates the number of features to use for creating a subset based on the `feature_proportion` attribute of the class. The `feature_proportion` can be `None`, a float, or an integer, and the method handles each case accordingly. Parameters: ----------- n_features : int The total number of features in the dataset. Returns: -------- int The number of features to use for the subset. Raises: ------- ValueError If `feature_proportion` is a float but not in the range (0, 1]. Notes: ------ - If `feature_proportion` is `None`, the method returns `n_features`, meaning all features are used. - If `feature_proportion` is a float between 0 and 1, it is interpreted as a fraction of `n_features`. - If `feature_proportion` is an integer, the method returns the smaller of `feature_proportion` and `n_features`. - If `feature_proportion` is neither `None`, a float, nor an integer, a `ValueError` is raised. \"\"\" if self.feature_proportion is None: return n_features # Use all features elif isinstance(self.feature_proportion, float): if 0 < self.feature_proportion <= 1: return int(n_features * self.feature_proportion) # Convert fraction to number of features else: raise ValueError(\"If feature_proportion is a float, it must be between 0 and 1.\") elif isinstance(self.feature_proportion, int): return min(self.feature_proportion, n_features) # Cap at n_features else: raise ValueError(\"feature_proportion must be None, a float between 0 and 1, or an integer.\") def _bootstrap_sample(self, X, y, subsample_size): \"\"\" Generate a bootstrap sample from the training data. This method performs bootstrap sampling, which involves sampling with or without replacement from the training data to create a new sample. The size of the new sample is determined by the `subsample_size` parameter. Parameters: ----------- X : numpy.ndarray The feature matrix of the training data. y : numpy.ndarray The target vector of the training data. subsample_size : int The number of samples to draw for the bootstrap sample. Returns: -------- tuple A tuple containing the bootstrap-sampled feature matrix and target vector: - X_sampled (numpy.ndarray): The feature matrix of the bootstrap sample. - y_sampled (numpy.ndarray): The target vector of the bootstrap sample. Notes: ------ - The sampling is controlled by the `self.sample_with_replacement` attribute: - If `True`, sampling is done with replacement. - If `False`, sampling is done without replacement. - The method uses `numpy.random.choice` to perform the sampling. \"\"\" indices = np.random.choice(len(X), size=subsample_size, replace=self.sample_with_replacement) return X[indices], y[indices] def _select_feature_indices(self, n_features, feature_subset_size): \"\"\" Select a random subset of feature indices. This method randomly selects a subset of feature indices from the total number of features available. The size of the subset is determined by the `feature_subset_size` parameter. Parameters: ----------- n_features : int The total number of features in the dataset. feature_subset_size : int The number of features to select for the subset. Returns: -------- numpy.ndarray An array of randomly selected feature indices. Notes: ------ - The selection is done without replacement, meaning each feature index can only be selected once. - The method uses `numpy.random.choice` to perform the random selection.", "source": "random_forest.py"}, {"content": "\"\"\" return np.random.choice(n_features, size=feature_subset_size, replace=False) def _train_tree(self, X_sample, y_sample, feature_indices): \"\"\" Train a decision tree on the provided sample and features. This method trains a decision tree using a subset of the provided sample data and the specified feature indices. The decision tree is configured with the class attributes for maximum depth, minimum samples per leaf, and minimum information gain. Parameters: ----------- X_sample : numpy.ndarray The feature matrix of the bootstrap sample. Each row represents a sample, and each column represents a feature. y_sample : numpy.ndarray The target vector of the bootstrap sample. Each element corresponds to the target value of a sample in `X_sample`. feature_indices : numpy.ndarray An array of indices representing the features to be used for training the decision tree. Returns: -------- DecisionTree A trained decision tree model. Notes: ------ - The feature matrix `X_sample` is subsetted to include only the columns specified by `feature_indices`. - The decision tree is instantiated and trained using the subsetted feature matrix and the target vector. - The decision tree is configured with the following class attributes: - `max_depth`: The maximum depth of the tree. - `min_samples_leaf`: The minimum number of samples required to be at a leaf node. - `min_information_gain`: The minimum information gain required for a split to be considered. \"\"\" X_sample_subset = X_sample[:, feature_indices] tree = DecisionTree( max_depth=self.max_depth, min_samples_leaf=self.min_samples_leaf, min_information_gain=self.min_information_gain ) tree.fit(X_sample_subset, y_sample) return tree def predict(self, X_test: np.array): \"\"\" Predict the class labels for the provided data. This method predicts the class labels for the input feature matrix `X_test` by aggregating the predictions from all the decision trees in the random forest. Each tree predicts the class labels for the subset of features it was trained on, and the final prediction is determined by majority voting (mode of the predictions). Parameters: ----------- X_test : numpy.ndarray The feature matrix of the test data. Each row represents a sample, and each column represents a feature. Returns: -------- numpy.ndarray An array of predicted class labels for the test data. Notes: ------ - The method uses the `features_indices` attribute to select the appropriate subset of features for each tree. - The predictions from all trees are aggregated using majority voting, where the final class label is the mode of the predictions from all trees. - The final predictions are rounded to the nearest integer and converted to integers. \"\"\" tree_predictions = np.array([ tree.predict(X_test[:, self.features_indices[i]]) for i, tree in enumerate(self.trees) ]) # Majority voting: mode of the predictions return np.mean(tree_predictions, axis=0).round().astype(int) # Example usage # from datapipeline import transform # X_train, X_test, y_train, y_test = transform('/Users/jon/code/aisg/deep-skilling-phase/all-assignments/assignment1/data/raw/data.csv') # # Replace values in y_train # y_train = np.where(y_train == '- 50000.', 0, y_train) # y_train = np.where(y_train == '50000+.', 1, y_train) # # Replace values in y_test # y_test = np.where(y_test == '- 50000.', 0, y_test) # y_test = np.where(y_test == '50000+.', 1, y_test) # # Ensure the arrays are of integer type # y_train = y_train.astype(int) # y_test = y_test.astype(int) # rf_clf = RandomForest(n_trees=5, max_depth=4) # rf_clf.fit(X_train, y_train) # print(\"done\") # rf_ntree = RandomForest(n_trees=10, subsample_size=0.8) # rf_ntree.fit(X_train, y_train) # preds_rf1 = rf_ntree.predict(X_test) # print(sum(preds_rf1 ==", "source": "random_forest.py"}, {"content": "y_test)/len(y_test))", "source": "random_forest.py"}, {"content": "import pytest import numpy as np from ..src import datapipeline from ..src import decision_tree @pytest.fixture def data_fixture(): \"\"\" Fixture to load and preprocess the data. Loads data from the given path and preprocesses the target variables. Replaces target variable values with binary integers. Returns: tuple: X_train, X_test, y_train, y_test (preprocessed features and target variables) \"\"\" data_path = \"/data/A1_test.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) # Preprocess target variables y_train = preprocess_target(y_train) y_test = preprocess_target(y_test) return X_train, X_test, y_train, y_test def preprocess_target(y): \"\"\" Converts target variable values from string to binary integers. Args: y (array-like): Target variable array. Returns: np.ndarray: Preprocessed target variable as integers. \"\"\" y = np.where(y == '- 50000.', 0, y) y = np.where(y == '50000+.', 1, y) return y.astype(int) def approx_eq(a, b, num_sig=5): \"\"\" Compares two values to check if they are approximately equal up to a specified number of significant digits. Args: a (float): First value. b (float): Second value. num_sig (int): Number of significant digits to compare. Returns: bool: True if values are approximately equal, False otherwise. \"\"\" return round(a, num_sig) == round(b, num_sig) def get_gini(tree, *args): \"\"\" Helper function to call the gini method, whether it's private or public. Args: tree (DecisionTree): An instance of DecisionTree. *args: Arguments to pass to the gini method. Returns: float: Gini index value calculated by the tree. \"\"\" try: # Try to call the private method return tree._DecisionTree__gini(*args) except AttributeError: try: # If the private method doesn't exist, call the public method return tree.gini(*args) except AttributeError: # If neither method exists, raise an exception raise AttributeError(\"The DecisionTree class must have either a '__gini' or 'gini' method.\") def test_gini(): \"\"\" Test the gini calculation method of the DecisionTree class. Ensures the Gini calculation is approximately equal to the expected value. \"\"\" tree = decision_tree.DecisionTree() assert approx_eq(get_gini(tree, [1, 0, 0, 0, 0], [0, 1, 1, 1, 1]), 0.32) def test_decision_tree_performance(data_fixture): \"\"\" Test the performance of the decision tree model. Ensures the accuracy is at least 70%. Args: data_fixture (fixture): Fixture that provides preprocessed training and testing data. \"\"\" X_train, X_test, y_train, y_test = data_fixture # Test decision tree performance dt = decision_tree.DecisionTree() dt.fit(X_train, y_train) preds_dt = dt.predict(X_test) acc_score = np.mean(preds_dt == y_test) assert acc_score >= 0.70, f'Accuracy score lower than minimum: {acc_score}'", "source": "test_decision_tree.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import pytest import numpy as np from ..src import datapipeline from ..src import random_forest @pytest.fixture def data_fixture(): \"\"\" Fixture to load and prepare the data. Loads data from the given path and preprocesses the target variables. Replaces target variable values with binary integers. Returns: tuple: X_train, X_test, y_train, y_test (preprocessed features and target variables) \"\"\" data_path = \"/data/A1_test.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) # Preprocess target variables y_train = preprocess_target(y_train) y_test = preprocess_target(y_test) return X_train, X_test, y_train, y_test def preprocess_target(y): \"\"\" Converts target variable values from string to binary integers. Args: y (array-like): Target variable array. Returns: np.ndarray: Preprocessed target variable as integers. \"\"\" y = np.where(y == '- 50000.', 0, y) y = np.where(y == '50000+.', 1, y) return y.astype(int) def evaluate_random_forest_model(X_train, X_test, y_train, y_test, **rf_params): \"\"\" Trains and evaluates a random forest model with the given parameters. Args: X_train (np.ndarray): Training feature set. X_test (np.ndarray): Testing feature set. y_train (np.ndarray): Training target set. y_test (np.ndarray): Testing target set. **rf_params: Additional parameters for the random forest model. Returns: float: Accuracy score of the model on the test set. \"\"\" rf_model = random_forest.RandomForest(**rf_params) rf_model.fit(X_train, y_train) preds = rf_model.predict(X_test) return np.mean(preds == y_test) def test_random_forest_default_parameters(data_fixture): \"\"\" Test random forest model with default parameters. Ensures accuracy is at least 70%. \"\"\" X_train, X_test, y_train, y_test = data_fixture acc_score = evaluate_random_forest_model(X_train, X_test, y_train, y_test) assert acc_score >= 0.70, f'Accuracy score lower than minimum: {acc_score}' print(\"Test for default parameters passed\") def test_random_forest_custom_trees_and_subsample(data_fixture): \"\"\" Test random forest model with custom number of trees and subsample size. Ensures accuracy is at least 70%. \"\"\" X_train, X_test, y_train, y_test = data_fixture acc_score = evaluate_random_forest_model( X_train, X_test, y_train, y_test, n_trees=10, subsample_size=0.8 ) assert acc_score >= 0.70, f'Accuracy score lower than minimum: {acc_score}' print(\"Test with custom trees and subsample passed\") def test_random_forest_custom_feature_proportion(data_fixture): \"\"\" Test random forest model with custom feature proportion, number of trees, and subsample size. Ensures accuracy is at least 70%. \"\"\" X_train, X_test, y_train, y_test = data_fixture acc_score = evaluate_random_forest_model( X_train, X_test, y_train, y_test, n_trees=100, subsample_size=0.5, feature_proportion=0.5 ) assert acc_score >= 0.70, f'Accuracy score lower than minimum: {acc_score}' print(\"Test with custom feature proportion passed\")", "source": "test_random_forest.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import os import logging from omegaconf import DictConfig import hydra import mlflow import mlflow.keras import mlflow.tensorflow from sklearn.model_selection import train_test_split from a4p1.models import MLPModel from a4p1.pipelines import DataPipeline, load_data from a4p1 import general_utils from a4p1 import utils @hydra.main( version_base=None, config_path=\"../conf\", config_name=\"experiment_model.yaml\" ) def experiment_model(cfg: DictConfig): \"\"\" Train the model based on the given configuration using Hydra and log the experiment using MLFlow. Parameters: - cfg: The configuration object passed by Hydra, containing model and training parameters. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) # Initialize MLFlow for experiment tracking mlflow_init_status, mlflow_run = general_utils.mlflow_init( cfg, setup_mlflow=cfg[\"setup_mlflow\"], autolog=cfg[\"mlflow_autolog\"] ) # Log experiment parameters to MLFlow general_utils.mlflow_log( mlflow_init_status, \"log_params\", params= { \"epochs\": cfg.training.epochs, \"input_dim\": cfg.model.input_dim, \"batch_size\": cfg.training.batch_size, \"model_hidden_layers\": cfg.model.hidden_layers, \"model_activation\": cfg.model.activation, \"output_activation\": cfg.model.output_activation, \"use_batch_norm\": cfg.model.use_batch_norm, \"use_dropout\": cfg.model.use_dropout, \"dropout_rate\": cfg.model.dropout_rate, \"sampling_technique\": cfg.pipeline.technique, \"early_stopping\": cfg.training.early_stopping, \"early_stopping_patience\": cfg.training.early_stopping_patience, \"optimizer\": cfg.training.optimizer, \"learning_rate\": cfg.training.learning_rate, \"loss\": cfg.training.loss, \"gamma\": cfg.training.gamma, \"alpha\": cfg.training.alpha, \"pos_weight\": cfg.training.pos_weight, }, ) utils.reset_random_seeds(cfg.training.random_state) # df = load_data().head(10000) # TODO: Remove this .head after testing df = load_data(cfg.data_dir_path) x = df.drop(columns=[\"Class\"]) y = df[\"Class\"] # Split the dataset into training and validation sets x_train, x_val, y_train, y_val = train_test_split( x, y, test_size=0.2, random_state=cfg.training.random_state ) # Initialize data pipeline based on the technique (e.g., SMOTE, ADASYN, None) pipeline = DataPipeline(technique=cfg.pipeline.technique) x_train, y_train = pipeline.apply(x_train, y_train) x_val = pipeline.transform(x_val) # Initialize the MLP model with dynamically configured hyperparameters model = MLPModel( input_dim=cfg.model.input_dim, hidden_layers=cfg.model.hidden_layers, activation=cfg.model.activation, output_activation=cfg.model.output_activation, use_dropout=cfg.model.use_dropout, use_batch_norm=cfg.model.use_batch_norm, dropout_rate=cfg.model.dropout_rate, optimizer=cfg.training.optimizer, learning_rate=cfg.training.learning_rate, loss=cfg.training.loss, gamma=cfg.training.gamma, alpha=cfg.training.alpha, pos_weight=cfg.training.pos_weight, ) # Train the model history = model.train( x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val, epochs=cfg.training.epochs, batch_size=cfg.training.batch_size, use_early_stopping=cfg.training.early_stopping, patience=cfg.training.early_stopping_patience, ) # Evaluate the model to get the loss and accuracy evaluation_metrics = model.evaluate(x_val, y_val) y_pred = model.predict(x_val) # Get additional classification metrics classification_metrics = model.get_classification_metrics(y_val, y_pred) # Combine all metrics all_metrics = { \"loss\": evaluation_metrics[0], \"accuracy\": evaluation_metrics[1], \"precision\": classification_metrics[\"precision\"], \"recall\": classification_metrics[\"recall\"], \"f1_score\": classification_metrics[\"f1_score\"], \"roc_auc\": classification_metrics[\"roc_auc\"], \"final_train_loss\": history.history[\"loss\"][-1], \"final_val_loss\": history.history[\"val_loss\"][-1], \"final_val_accuracy\": history.history[\"val_accuracy\"][-1], } # Log all metrics to MLFlow mlflow.log_metrics(all_metrics) # Get classification report as a dictionary report = model.classification_report(y_val, y_pred) # Log the classification report to MLflow mlflow.log_metrics(report) # Log the model to MLFlow model_checkpoint_path = os.path.join( cfg[\"model_checkpoint_dir_path\"], \"model.h5\" ) model.save(model_checkpoint_path) general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model_checkpoint_path, artifact_path=\"model\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return all_metrics[\"loss\"], all_metrics[\"recall\"] if __name__ == \"__main__\": experiment_model()", "source": "experiment_model.py"}, {"content": "import os import sys sys.path.append(os.path.join(os.path.dirname(__file__), \"..\")) import numpy as np # from .mlp_datapipeline import Datapipeline # from src.mlp_datapipeline import Datapipeline class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # Hyperparameters self.learning_rate = 0.01 # Initialize weights with small random values self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.01 # Shape: (3072, 100) self.b1 = np.zeros((1, self.hidden_size)) # Shape: (1, 100) self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.01 # Shape: (100, 10) self.b2 = np.zeros((1, self.output_size)) # Shape: (1, 10) # Placeholders for intermediate variables self.X = None self.Z1 = None self.A1 = None self.Z2 = None self.A2 = None self.Y_true = None self.Y_pred = None def sigmoid(self, z): \"\"\" Compute the sigmoid activation function. Parameters: - z: numpy array Returns: - sigmoid(z) \"\"\" return 1 / (1 + np.exp(-z)) def sigmoid_derivative(self, a): \"\"\" Compute the derivative of the sigmoid function. Parameters: - a: sigmoid(z), output of sigmoid Returns: - derivative of sigmoid \"\"\" return a * (1 - a) def softmax(self, z): # To prevent overflow z_shift = z - np.max(z) exp_z = np.exp(z_shift) return exp_z / np.sum(exp_z) def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" self.x = features # Shape: (input_size,) # First layer self.z1 = np.dot(self.x, self.W1) + self.b1 # Shape: (hidden_size,) self.a1 = self.sigmoid(self.z1) # Shape: (hidden_size,) # Second layer self.z2 = np.dot(self.a1, self.W2) + self.b2 # Shape: (output_size,) self.predictions = self.softmax(self.z2) # Shape: (output_size,) return self.predictions def loss(self, y_pred, y_true): \"\"\" Takes in the predictions and label returns the training loss \"\"\" self.y = y_true # Store the label for backward pass epsilon = 1e-15 y_pred = y_pred + epsilon # Compute the categorical cross-entropy loss # Element wise multiplication loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0] # divide by num of samples return loss def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" dL_dz2 = self.predictions - self.y # Shape: (output_size,) # Gradients for second layer dL_dW2 = np.outer(self.a1, dL_dz2) # Shape: (hidden_size, output_size) dL_db2 = dL_dz2 # Shape: (output_size,) # Gradients for first layer dL_da1 = np.dot(dL_dz2, self.W2.T) # Shape: (hidden_size,) dL_dz1 = dL_da1 * self.sigmoid_derivative(self.a1) # Shape: (hidden_size,) dL_dW1 = np.outer(self.x, dL_dz1) # Shape: (input_size, hidden_size) dL_db1 = dL_dz1 # Shape: (hidden_size,) # Update weights and biases self.W2 -= self.learning_rate * dL_dW2 self.b2 -= self.learning_rate * dL_db2 self.W1 -= self.learning_rate * dL_dW1 self.b1 -= self.learning_rate * dL_db1 # data_path = \"/Users/jon/code/aisg/deep-skilling-phase/all-assignments/assignment4/data/mlp_data.csv\" # dpl = Datapipeline() # x, y = dpl.transform(data_path) # model = MLPTwoLayers(input_size=4, hidden_size=16, output_size=3) # total_loss = 0 # total_inputs = 150 # all_losses = [] # for i in range(total_inputs): # predictions = model.forward(x[i]) # loss = model.loss(predictions, y[i]) # all_losses.append(loss) # total_loss += loss # if i % 30 == 0: # print(f\"Average loss {total_loss/(i+1)}\") # model.backward()", "source": "mlp.py"}, {"content": "import pandas as pd from sklearn.preprocessing import StandardScaler, OneHotEncoder class Datapipeline: def __init__(self): \"\"\" Initializes the Datapipeline with a StandardScaler for feature scaling and a OneHotEncoder for encoding the target variable. \"\"\" self.scaler = StandardScaler() self.encoder = OneHotEncoder(sparse_output=False) def transform(self, data_path): \"\"\" Transforms the data by performing the following steps: 1. Reads the CSV file from the given path. 2. Drops the 'id' column. 3. Splits the data into features (X) and target (y). 4. Scales the features using StandardScaler. 5. One-hot encodes the target variable. Parameters: - data_path (str): The file path to the CSV data. Returns: - X_scaled (numpy.ndarray): The scaled feature matrix. - y_encoded (numpy.ndarray): The one-hot encoded target matrix. \"\"\" # Load the dataset df = pd.read_csv(data_path) print(f\"Original Data Shape: {df.shape}\") # Drop the 'id' column if it exists if 'id' in df.columns: df = df.drop('id', axis=1) print(\"Dropped 'id' column.\") else: # If 'id' column is not present, you can choose to handle it differently # For example, drop the first column assuming it's the 'id' # df = df.drop(df.columns[0], axis=1) print(\"'id' column not found. No columns were dropped.\") # Assuming the target variable is the last column X = df.iloc[:, :-1] y = df.iloc[:, -1].values.reshape(-1, 1) print(f\"Features Shape: {X.shape}\") print(f\"Target Shape: {y.shape}\") # Scale the features X_scaled = self.scaler.fit_transform(X) print(\"Features have been scaled.\") # One-hot encode the target variable y_encoded = self.encoder.fit_transform(y) print(\"Target variable has been one-hot encoded.\") return X_scaled, y_encoded # if __name__ == \"__main__\": # data_path = \"/Users/jon/code/aisg/deep-skilling-phase/all-assignments/assignment4/data/mlp_data.csv\" # dpl = DataPipeline() # X, y = dpl.transform(data_path) # print(f\"Scaled Features Shape: {X.shape}\") # print(f\"One-Hot Encoded Target Shape: {y.shape}\")", "source": "mlp_datapipeline.py"}, {"content": "import os import logging from omegaconf import DictConfig import hydra import mlflow import numpy as np from sklearn.preprocessing import MinMaxScaler import torch from torch.utils.data import DataLoader, TensorDataset from torch import optim from torch.autograd import grad import pandas as pd from scipy.stats import ks_2samp, wasserstein_distance from a4p1.models_gan import Generator, Discriminator from a4p1.pipelines import load_data from a4p1 import general_utils, utils @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_gan.yaml\") def train_gan(cfg: DictConfig): \"\"\" Train a WGAN-GP to generate synthetic fraudulent transactions. \"\"\" logger = setup_logging() mlflow_init_status, mlflow_run = initialize_mlflow(cfg) log_gan_params(cfg, mlflow_init_status) utils.reset_random_seeds(cfg.training.random_state) df, feature_cols, scaler, dataloader = load_and_preprocess_data(cfg) device = get_device() generator, discriminator, optimizer_G, optimizer_D = initialize_models(cfg, feature_cols, device) train_models(cfg, dataloader, generator, discriminator, optimizer_G, optimizer_D, device, logger, mlflow_init_status) save_models(cfg, generator, discriminator, mlflow_init_status) synthetic_data = generate_synthetic_data(generator, len(dataloader.dataset), cfg, scaler, device) save_synthetic_data(synthetic_data, feature_cols, mlflow_init_status) ks_stat, wasserstein_dist = compute_metrics(scaler, synthetic_data, df, feature_cols) log_metrics(mlflow_init_status, ks_stat, wasserstein_dist) objective_metric = ks_stat + wasserstein_dist end_mlflow_run(mlflow_init_status, mlflow_run, logger) return objective_metric def setup_logging(): logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) return logger def initialize_mlflow(cfg): mlflow_init_status, mlflow_run = general_utils.mlflow_init( cfg, setup_mlflow=cfg.setup_mlflow, autolog=cfg.mlflow_autolog ) return mlflow_init_status, mlflow_run def log_gan_params(cfg, mlflow_init_status): gan_params = { \"noise_dim\": cfg.gan.noise_dim, \"hidden_layers_generator\": cfg.gan.hidden_layers_generator, \"hidden_layers_discriminator\": cfg.gan.hidden_layers_discriminator, \"activation_generator\": cfg.gan.activation_generator, \"activation_discriminator\": cfg.gan.activation_discriminator, \"use_batch_norm\": cfg.gan.use_batch_norm, \"use_dropout\": cfg.gan.use_dropout, \"dropout_rate\": cfg.gan.dropout_rate, \"epochs\": cfg.training.epochs, \"batch_size\": cfg.training.batch_size, \"learning_rate\": cfg.training.learning_rate, \"beta1\": cfg.training.beta1, \"beta2\": cfg.training.beta2, \"n_critic\": cfg.training.n_critic, \"lambda_gp\": cfg.training.lambda_gp, \"random_state\": cfg.training.random_state, \"scale_range\": cfg.data.scale_range, } general_utils.mlflow_log(mlflow_init_status, \"log_params\", params=gan_params) def load_and_preprocess_data(cfg): df = load_data(cfg.data_dir_path) fraud_data = df[df['Class'] == 1].copy() feature_cols = [col for col in df.columns if col != 'Class'] scaler = MinMaxScaler(feature_range=tuple(cfg.data.scale_range)) fraud_features = scaler.fit_transform(fraud_data[feature_cols]) tensor_fraud = torch.tensor(fraud_features, dtype=torch.float32) dataset = TensorDataset(tensor_fraud) dataloader = DataLoader(dataset, batch_size=cfg.training.batch_size, shuffle=True) return df, feature_cols, scaler, dataloader def get_device(): if torch.cuda.is_available(): return torch.device(\"cuda\") elif torch.backends.mps.is_available(): return torch.device(\"mps\") else: return torch.device(\"cpu\") def initialize_models(cfg, feature_cols, device): generator = Generator( noise_dim=cfg.gan.noise_dim, output_dim=len(feature_cols), hidden_layers=cfg.gan.hidden_layers_generator, activation=cfg.gan.activation_generator, use_batch_norm=cfg.gan.use_batch_norm, use_dropout=cfg.gan.use_dropout, dropout_rate=cfg.gan.dropout_rate ).to(device) discriminator = Discriminator( input_dim=len(feature_cols), hidden_layers=cfg.gan.hidden_layers_discriminator, activation=cfg.gan.activation_discriminator ).to(device) optimizer_G = optim.Adam(generator.parameters(), lr=cfg.training.learning_rate, betas=(cfg.training.beta1, cfg.training.beta2)) optimizer_D = optim.Adam(discriminator.parameters(), lr=cfg.training.learning_rate, betas=(cfg.training.beta1, cfg.training.beta2)) return generator, discriminator, optimizer_G, optimizer_D def train_models(cfg, dataloader, generator, discriminator, optimizer_G, optimizer_D, device, logger, mlflow_init_status): generator.train() discriminator.train() for epoch in range(1, cfg.training.epochs + 1): for i, data in enumerate(dataloader): real_samples = data[0].to(device) batch_size_curr = real_samples.size(0) for _ in range(cfg.training.n_critic): z = torch.randn(batch_size_curr, cfg.gan.noise_dim).to(device) fake_samples = generator(z).detach() d_real = discriminator(real_samples) d_fake = discriminator(fake_samples) gp = compute_gradient_penalty(discriminator, real_samples, fake_samples, cfg.training.lambda_gp, device) loss_D = d_fake.mean() - d_real.mean() + gp optimizer_D.zero_grad() loss_D.backward() optimizer_D.step() z = torch.randn(batch_size_curr, cfg.gan.noise_dim).to(device) fake_samples = generator(z) g_loss = -discriminator(fake_samples).mean() optimizer_G.zero_grad() g_loss.backward() optimizer_G.step() if epoch % 100 == 0 or epoch == 1: logger.info(f\"Epoch [{epoch}/{cfg.training.epochs}] | D Loss: {loss_D.item():.4f} | G Loss: {g_loss.item():.4f}\") if mlflow_init_status: mlflow.log_metric(\"Discriminator_Loss\", loss_D.item(), step=epoch) mlflow.log_metric(\"Generator_Loss\", g_loss.item(), step=epoch) def save_models(cfg, generator, discriminator, mlflow_init_status): models_dir = cfg.model_checkpoint_dir_path os.makedirs(models_dir, exist_ok=True) generator_path = os.path.join(models_dir, \"generator.pth\") discriminator_path = os.path.join(models_dir, \"discriminator.pth\") torch.save(generator.state_dict(), generator_path) torch.save(discriminator.state_dict(), discriminator_path) if mlflow_init_status: mlflow.log_artifact(generator_path, artifact_path=\"models\") mlflow.log_artifact(discriminator_path, artifact_path=\"models\") def generate_synthetic_data(generator, dataset_length, cfg, scaler, device): generator.eval() with torch.no_grad(): z = torch.randn(dataset_length, cfg.gan.noise_dim).to(device) synthetic_data = generator(z).cpu().numpy() synthetic_data = scaler.inverse_transform(synthetic_data) return synthetic_data def save_synthetic_data(synthetic_data, feature_cols, mlflow_init_status): synthetic_dir = os.path.join(hydra.utils.get_original_cwd(), \"synthetic_data\") os.makedirs(synthetic_dir, exist_ok=True) synthetic_path = os.path.join(synthetic_dir, \"synthetic_frauds.csv\") synthetic_df = pd.DataFrame(synthetic_data, columns=feature_cols) synthetic_df['Class'] = 1 synthetic_df.to_csv(synthetic_path, index=False) if mlflow_init_status: mlflow.log_artifact(synthetic_path, artifact_path=\"synthetic_data\") def compute_metrics(scaler, synthetic_data, df, feature_cols): real_data = scaler.transform(df[df['Class'] == 1][feature_cols]) synthetic_scaled = scaler.transform(synthetic_data) ks_stat = calculate_ks_test(real_data,", "source": "train_gan.py"}, {"content": "synthetic_scaled) wasserstein_dist = calculate_wasserstein(real_data, synthetic_scaled) return ks_stat, wasserstein_dist def log_metrics(mlflow_init_status, ks_stat, wasserstein_dist): if mlflow_init_status: mlflow.log_metric(\"KS_Avg_Statistic\", ks_stat) mlflow.log_metric(\"Wasserstein_Avg_Distance\", wasserstein_dist) def end_mlflow_run(mlflow_init_status, mlflow_run, logger): if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) general_utils.mlflow_log(mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri}) logger.info(\"GAN training with MLflow run ID %s has completed.\", mlflow_run.info.run_id) mlflow.end_run() else: logger.info(\"GAN training has completed.\") def compute_gradient_penalty(D, real_samples, fake_samples, lambda_gp, device): alpha = torch.rand(real_samples.size(0), 1).to(device) alpha = alpha.expand_as(real_samples) interpolates = alpha * real_samples + (1 - alpha) * fake_samples interpolates.requires_grad_(True) d_interpolates = D(interpolates) fake = torch.ones(d_interpolates.size()).to(device) gradients = grad( outputs=d_interpolates, inputs=interpolates, grad_outputs=fake, create_graph=True, retain_graph=True, only_inputs=True )[0] gradients = gradients.view(gradients.size(0), -1) gradient_norm = gradients.norm(2, dim=1) gradient_penalty = lambda_gp * ((gradient_norm - 1) ** 2).mean() return gradient_penalty def calculate_ks_test(real_data, synthetic_data): num_features = real_data.shape[1] ks_stats = [ks_2samp(real_data[:, i], synthetic_data[:, i])[0] for i in range(num_features)] return np.mean(ks_stats) def calculate_wasserstein(real_data, synthetic_data): num_features = real_data.shape[1] wasserstein_dists = [wasserstein_distance(real_data[:, i], synthetic_data[:, i]) for i in range(num_features)] return np.mean(wasserstein_dists) if __name__ == \"__main__\": train_gan()", "source": "train_gan.py"}, {"content": "# from . import a4p1", "source": "__init__.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import time import logging import logging.config import yaml import mlflow logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import tensorflow as tf def focal_loss(gamma=2., alpha=0.25): \"\"\" Focal Loss for addressing class imbalance by down-weighting easy examples and focusing more on hard examples. Parameters: - gamma (float): Focusing parameter. Default is 2.0. - alpha (float): Balancing parameter. Default is 0.25. Returns: - focal_loss_fixed (function): A function that computes the focal loss given true and predicted labels. \"\"\" def focal_loss_fixed(y_true, y_pred): \"\"\" Compute the focal loss between true and predicted labels. Parameters: - y_true (tensor): True labels. - y_pred (tensor): Predicted labels. Returns: - loss (tensor): Computed focal loss. \"\"\" epsilon = tf.keras.backend.epsilon() y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon) y_true = tf.cast(y_true, tf.float32) alpha_t = y_true * alpha + (tf.ones_like(y_true) - y_true) * (1 - alpha) p_t = y_true * y_pred + (tf.ones_like(y_true) - y_true) * (1 - y_pred) fl = - alpha_t * tf.pow((tf.ones_like(y_true) - p_t), gamma) * tf.math.log(p_t) return tf.reduce_mean(fl) return focal_loss_fixed def weighted_binary_crossentropy(pos_weight): \"\"\" Weighted Binary Crossentropy for handling class imbalance by applying different weights to positive and negative classes. Parameters: - pos_weight (float): Weight for the positive class. Returns: - loss (function): A function that computes the weighted binary crossentropy loss given true and predicted labels. \"\"\" def loss(y_true, y_pred): \"\"\" Compute the weighted binary crossentropy loss between true and predicted labels. Parameters: - y_true (tensor): True labels. - y_pred (tensor): Predicted labels. Returns: - loss (tensor): Computed weighted binary crossentropy loss. \"\"\" epsilon = tf.keras.backend.epsilon() y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon) y_true = tf.cast(y_true, tf.float32) loss = - (pos_weight * y_true * tf.math.log(y_pred) + (1 - y_true) * tf.math.log(1 - y_pred)) return tf.reduce_mean(loss) return loss", "source": "losses.py"}, {"content": "# models.py import abc from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, BatchNormalization from keras.callbacks import EarlyStopping from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score import seaborn as sns import matplotlib.pyplot as plt import mlflow from .losses import focal_loss, weighted_binary_crossentropy class BaseModel(abc.ABC): def __init__(self, input_dim): self.model = Sequential() self.input_dim = input_dim @abc.abstractmethod def build_model(self): pass def compile_model(self, optimizer='adam', learning_rate=0.001, loss='binary_crossentropy', gamma=2, alpha=0.25, pos_weight=1.0, metrics=['accuracy']): if optimizer == 'adam': opt = 'adam' elif optimizer == 'sgd': opt = 'sgd' else: raise ValueError(f\"Unsupported optimizer: {optimizer}\") if loss == 'binary_crossentropy': loss_fn = 'binary_crossentropy' elif loss == 'focal_loss': loss_fn = focal_loss(gamma, alpha) elif loss == 'weighted_binary_crossentropy': loss_fn = weighted_binary_crossentropy(pos_weight) else: raise ValueError(f\"Unsupported loss function: {loss}\") self.model.compile( optimizer=opt, loss=loss_fn, metrics=metrics ) def train(self, X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, use_early_stopping=False, patience=5): \"\"\" Train the model with optional early stopping. Args: - X_train: Training data. - y_train: Training labels. - epochs: Number of epochs to train the model. - batch_size: Number of samples per gradient update. - validation_split: Fraction of the training data to be used as validation data. - use_early_stopping: Boolean flag to use early stopping. - patience: Number of epochs with no improvement after which training will be stopped. Returns: - history: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). \"\"\" callbacks = [] if use_early_stopping: early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True) callbacks.append(early_stopping) history = self.model.fit( X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=callbacks ) return history def evaluate(self, X_test, y_test): test_loss, test_accuracy = self.model.evaluate(X_test, y_test) print(f\"Test Loss: {test_loss}\") print(f\"Test Accuracy: {test_accuracy}\") return test_loss, test_accuracy def predict(self, X): return (self.model.predict(X) > 0.5).astype(\"int32\") def plot_confusion_matrix(self, y_true, y_pred): conf_matrix = confusion_matrix(y_true, y_pred) plt.figure(figsize=(10, 7)) sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues') plt.xlabel('Predicted') plt.ylabel('Actual') plt.title('Confusion Matrix') plt.show() def condense_classification_report(self, report): \"\"\" Condense a nested classification report dictionary into a single dictionary. Args: - report: A nested dictionary containing classification report. Returns: - A single dictionary with condensed keys. \"\"\" condensed_report = {} for label, metrics in report.items(): if isinstance(metrics, dict): # Ensure metrics is a dictionary for metric, value in metrics.items(): condensed_key = f\"{label}_{metric}\" condensed_report[condensed_key] = value else: # Handle overall metrics like accuracy condensed_report[label] = metrics return condensed_report def classification_report(self, y_true, y_pred): print(\"Classification Report:\") print(classification_report(y_true, y_pred)) report = classification_report(y_true, y_pred, output_dict=True) condensed_report = self.condense_classification_report(report) return condensed_report def get_classification_metrics(self, y_true, y_pred): metrics = { 'precision': precision_score(y_true, y_pred), 'recall': recall_score(y_true, y_pred), 'f1_score': f1_score(y_true, y_pred), 'roc_auc': roc_auc_score(y_true, y_pred) } return metrics def save(self, path): self.model.save(path) class FirstModel(BaseModel): def __init__(self, input_dim=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, input_dim=self.input_dim, activation='relu')) # Input layer self.model.add(Dense(units=64, activation='relu')) # Hidden layer 1 self.model.add(Dense(units=32, activation='relu')) # Hidden layer 2 self.model.add(Dense(units=16, activation='relu')) # Hidden layer 3 self.model.add(Dense(units=1, activation='sigmoid')) # Output layer class ImprovedModel(BaseModel): def __init__(self, input_dim=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, input_dim=self.input_dim, activation='relu')) # Input layer self.model.add(Dense(units=128, activation='relu')) # Hidden layer 1 self.model.add(Dropout(0.5)) # Dropout layer with 50% drop rate self.model.add(Dense(units=64, activation='relu')) # Hidden layer 2 self.model.add(Dropout(0.5)) self.model.add(Dense(units=32, activation='relu')) # Hidden layer 3 self.model.add(Dropout(0.5)) self.model.add(Dense(units=16, activation='relu')) # Hidden layer 4 self.model.add(Dropout(0.5)) self.model.add(Dense(units=1, activation='sigmoid')) # Output layer class DropoutAndEarlyStoppingModel(BaseModel): #", "source": "models.py"}, {"content": "early stopping inherited from base model def __init__(self, input_dim=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, input_dim=self.input_dim, activation='relu')) # Input layer self.model.add(Dense(units=128, activation='relu')) # Hidden layer 1 self.model.add(Dropout(0.5)) # Dropout layer with 50% drop rate self.model.add(Dense(units=64, activation='relu')) # Hidden layer 2 self.model.add(Dropout(0.5)) self.model.add(Dense(units=32, activation='relu')) # Hidden layer 3 self.model.add(Dropout(0.5)) self.model.add(Dense(units=16, activation='relu')) # Hidden layer 4 self.model.add(Dropout(0.5)) self.model.add(Dense(units=1, activation='sigmoid')) # Output layer class MLPModel(BaseModel): \"\"\"A modular Multi-Layer Perceptron model used in experimentation for Assignment 4 Part 1.\"\"\" def __init__(self, input_dim, hidden_layers, activation='relu', output_activation='sigmoid', dropout_rate=0.0, use_dropout=False, optimizer='adam', learning_rate=0.001, loss='binary_crossentropy', use_batch_norm=False, gamma=2, alpha=0.25, pos_weight=1.0): super().__init__(input_dim) self.hidden_layers = list(hidden_layers) self.activation = activation self.output_activation = output_activation self.dropout_rate = dropout_rate self.use_dropout = use_dropout self.optimizer = optimizer self.learning_rate = learning_rate self.loss = loss self.use_batch_norm = use_batch_norm self.gamma = gamma self.alpha = alpha self.pos_weight = pos_weight self.model = self.build_model() self.compile_model(optimizer=self.optimizer, learning_rate=self.learning_rate, loss=self.loss, gamma=self.gamma, alpha=self.alpha, pos_weight=self.pos_weight, metrics=['accuracy']) def build_model(self): model = Sequential() model.add(Dense(units=self.input_dim, input_dim=self.input_dim, activation=self.activation)) for units in self.hidden_layers: model.add(Dense(units=units, activation=self.activation)) if self.use_batch_norm: model.add(BatchNormalization()) if self.use_dropout: model.add(Dropout(self.dropout_rate)) model.add(Dense(units=1, activation=self.output_activation)) # Binary classification return model def train(self, x_train, y_train, x_val, y_val, epochs, batch_size, use_early_stopping=True, patience=5): \"\"\" Train the model with optional early stopping. Args: - x_train: Training data. - y_train: Training labels. - x_val: Validation data. - y_val: Validation labels. - epochs: Number of epochs to train the model. - batch_size: Number of samples per gradient update. - use_early_stopping: Boolean flag to use early stopping. - patience: Number of epochs with no improvement after which training will be stopped. Returns: - history: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). \"\"\" callbacks = [] if use_early_stopping: early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True) callbacks.append(early_stopping) history = self.model.fit( x_train, y_train, validation_data=(x_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=callbacks ) # Log the final epoch stopped time for early stopping to MLflow if use_early_stopping and early_stopping.stopped_epoch > 0: mlflow.log_metric('stopped_epoch', early_stopping.stopped_epoch + 1) # +1 because epochs are 0-indexed return history # Example usage: # first_model = FirstModel(input_dim=29) # history = first_model.train(X_train_scaled, y_train_resampled, epochs=20, batch_size=32) # test_loss, test_accuracy = first_model.evaluate(X_test_scaled, y_test) # y_pred = first_model.predict(X_test_scaled) # first_model.classification_report(y_test, y_pred) # first_model.plot_confusion_matrix(y_test, y_pred) # improved_model = ImprovedModel(input_dim=29) # history = improved_model.train(X_train_scaled, y_train_resampled, epochs=20, batch_size=32) # test_loss, test_accuracy = improved_model.evaluate(X_test_scaled, y_test) # y_pred = improved_model.predict(X_test_scaled) # improved_model.classification_report(y_test, y_pred) # improved_model.plot_confusion_matrix(y_test, y_pred)", "source": "models.py"}, {"content": "import torch.nn as nn class Generator(nn.Module): def __init__(self, noise_dim: int, output_dim: int, hidden_layers: list, activation: str = 'relu', use_batch_norm: bool = True, use_dropout: bool = False, dropout_rate: float = 0.3): super(Generator, self).__init__() layers = [] input_dim = noise_dim for hidden_dim in hidden_layers: layers.append(nn.Linear(input_dim, hidden_dim)) if use_batch_norm: layers.append(nn.BatchNorm1d(hidden_dim)) lower_case_activation = activation.lower() if lower_case_activation == 'relu': layers.append(nn.ReLU(inplace=True)) if lower_case_activation == 'leakyrelu': layers.append(nn.LeakyReLU(0.2, inplace=True)) if lower_case_activation == 'tanh': layers.append(nn.Tanh()) if use_dropout: layers.append(nn.Dropout(dropout_rate)) input_dim = hidden_dim layers.append(nn.Linear(input_dim, output_dim)) layers.append(nn.Tanh()) # Assuming data is scaled between [-1, 1] self.model = nn.Sequential(*layers) def forward(self, z): return self.model(z) class Discriminator(nn.Module): def __init__(self, input_dim: int, hidden_layers: list, activation: str = 'leakyrelu'): super(Discriminator, self).__init__() layers = [] input_dim = input_dim for hidden_dim in hidden_layers: layers.append(nn.Linear(input_dim, hidden_dim)) lower_case_activation = activation.lower() if lower_case_activation == 'relu': layers.append(nn.ReLU(inplace=True)) if lower_case_activation == 'leakyrelu': layers.append(nn.LeakyReLU(0.2, inplace=True)) if lower_case_activation == 'tanh': layers.append(nn.Tanh()) input_dim = hidden_dim layers.append(nn.Linear(input_dim, 1)) # No activation for WGAN self.model = nn.Sequential(*layers) def forward(self, x): return self.model(x).squeeze()", "source": "models_gan.py"}, {"content": "import os import pyodbc import pandas as pd from sklearn.preprocessing import StandardScaler from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler from imblearn.under_sampling import RandomUnderSampler, TomekLinks, ClusterCentroids, NearMiss from dotenv import load_dotenv load_dotenv() class DataPipeline: def __init__(self, technique=None): \"\"\" Initialize the DataPipeline with the specified sampling technique. Parameters: - technique (str): The sampling technique to use (e.g., 'SMOTE', 'ADASYN', 'RandomOverSampler', 'RandomUnderSampler', 'TomekLinks', 'ClusterCentroids', 'NearMiss'). \"\"\" self.technique = technique self.sampler = None self.scaler = StandardScaler() self.set_sampler() def set_sampler(self): \"\"\" Set the sampler based on the specified technique. \"\"\" if self.technique == \"SMOTE\": self.sampler = SMOTE() elif self.technique == \"ADASYN\": self.sampler = ADASYN() elif self.technique == \"RandomOverSampler\": self.sampler = RandomOverSampler() elif self.technique == \"RandomUnderSampler\": self.sampler = RandomUnderSampler() elif self.technique == \"TomekLinks\": self.sampler = TomekLinks() elif self.technique == \"ClusterCentroids\": self.sampler = ClusterCentroids() elif self.technique == \"NearMiss\": self.sampler = NearMiss() else: self.sampler = None # No oversampling or undersampling applied def apply(self, x_train, y_train): \"\"\" Apply the specified sampling technique and standard scaling to the training data. Parameters: - x_train (array-like): Training features. - y_train (array-like): Training labels. Returns: - x_train (array-like): Resampled and scaled training features. - y_train (array-like): Resampled training labels. \"\"\" # Apply oversampling if a sampler is set if self.sampler: x_train, y_train = self.sampler.fit_resample(x_train, y_train) # Apply standard scaling to the features x_train = self.scaler.fit_transform(x_train) return x_train, y_train def transform(self, x): \"\"\" Apply the same scaling to the test/validation data. \"\"\" return self.scaler.transform(x) def load_data(data_dir_path): \"\"\" Load data from a CSV file if it exists, otherwise from a SQL Server and return it as a pandas DataFrame. Returns: - A pandas DataFrame containing the query results. \"\"\" # Check if the CSV file exists if os.path.exists(data_dir_path): print(f\"Loading data from {data_dir_path}\") data = pd.read_csv(data_dir_path) return data print(\"CSV file not found. Querying data from SQL Server.\") # Load environment variables from .env file load_dotenv() # Set up the connection string using environment variables server = os.getenv('SERVER') database = os.getenv('DATABASE') username = os.getenv('USERNAME') password = os.getenv('PASSWORD') driver = os.getenv('DRIVER') # Establish a connection to the SQL Server conn = pyodbc.connect( f'DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}' ) # Execute the SQL query to extract the data data = pd.read_sql(\"SELECT * FROM creditcard\", conn) # Close the connection conn.close() # Save the data to a CSV file for future use os.makedirs(os.path.dirname(data_dir_path), exist_ok=True) data.to_csv(data_dir_path, index=False) print(f\"Data saved to {data_dir_path}\") return data", "source": "pipelines.py"}, {"content": "import os import random import numpy as np import tensorflow as tf def reset_random_seeds(seed: int = 42): os.environ[\"PYTHONHASHSEED\"] = str(seed) tf.random.set_seed(seed) np.random.seed(seed) random.seed(seed)", "source": "utils.py"}, {"content": "# from . import general_utils # from . import models # from . import models_gan # from . import pipelines # from . import losses # from . import utils", "source": "__init__.py"}, {"content": "from typing import Optional, Callable import torch from torch.utils.data import Dataset, DataLoader from sklearn.model_selection import train_test_split import numpy as np from a4p1.pipelines import DataPipeline, load_data class CustomDataset(Dataset): \"\"\" Custom Dataset for loading tabular data. Args: data (np.ndarray): NumPy array containing the features. labels (np.ndarray): NumPy array containing the labels. transform (Callable, optional): Optional transform to be applied on a sample. Returns: tuple: (features, labels) for a given index. \"\"\" def __init__(self, data: np.ndarray, labels: np.ndarray, transform: Optional[Callable] = None): # Pre-convert data and labels to torch tensors self.data = torch.tensor(data, dtype=torch.float32) self.labels = torch.tensor(labels.reshape(-1, 1), dtype=torch.float32) self.transform = transform def __len__(self) -> int: return self.data.size(0) def __getitem__(self, idx: int) -> tuple: x = self.data[idx] y = self.labels[idx] if self.transform: x = self.transform(x) return x, y def create_datasets(data_dir_path: str, technique: Optional[str] = None): \"\"\" Create training and validation datasets from the given data directory path. Args: data_dir_path (str): Path to the directory containing the data. technique (str, optional): Data augmentation technique to be applied (e.g., SMOTE, ADASYN, None). Returns: tuple: (train_dataset, val_dataset) where train_dataset and val_dataset are instances of CustomDataset. \"\"\" # Load the data df = load_data(data_dir_path) x = df.drop(columns=[\"Class\"]).values y = df[\"Class\"].values # Split the dataset into training and validation sets x_train, x_val, y_train, y_val = train_test_split( x, y, test_size=0.2, random_state=42 ) # Initialize data pipeline based on the technique (e.g., SMOTE, ADASYN, None) pipeline = DataPipeline(technique=technique) x_train, y_train = pipeline.apply(x_train, y_train) x_val = pipeline.transform(x_val) # Create PyTorch datasets train_dataset = CustomDataset(x_train, y_train) val_dataset = CustomDataset(x_val, y_val) return train_dataset, val_dataset", "source": "dataset.py"}, {"content": "import torch.nn as nn class MLP(nn.Module): def __init__( self, input_dim: int, hidden_layers: list, output_dim: int, activation: str = 'relu', use_batch_norm: bool = False, use_dropout: bool = False, dropout_rate: float = 0.5 ): super(MLP, self).__init__() layers = [] prev_dim = input_dim activation_funcs = { 'relu': nn.ReLU(), 'leaky_relu': nn.LeakyReLU(), 'tanh': nn.Tanh(), 'sigmoid': nn.Sigmoid() } act = activation_funcs.get(activation, nn.ReLU()) for hidden_dim in hidden_layers: layers.append(nn.Linear(prev_dim, hidden_dim)) if use_batch_norm: layers.append(nn.BatchNorm1d(hidden_dim)) layers.append(act) if use_dropout: layers.append(nn.Dropout(dropout_rate)) prev_dim = hidden_dim layers.append(nn.Linear(prev_dim, output_dim)) self.network = nn.Sequential(*layers) def forward(self, x): return self.network(x)", "source": "models.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A4.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "from manim import * class CNN(ThreeDScene): def construct(self): # Set up the camera # self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES) self.set_camera_orientation(phi=75 * DEGREES, theta=-45 * DEGREES) cube = Cube(side_length=4) cube.set_fill(BLUE, opacity=0.5) cube.set_stroke(BLUE, width=1) # self.add(cube) # self.add(NumberPlane().add_coordinates()) self.play(FadeIn(cube)) # Create labels width_label = Text(\"Width\", font_size=24).next_to(cube, DL/2, buff=0.5) height_label = Text(\"Height\", font_size=24).next_to(cube, LEFT, buff=0.5) channel_label = Text(\"Channels\", font_size=24).next_to(cube, DR, buff=0.5) # Add labels to the scene self.add_fixed_in_frame_mobjects(channel_label, width_label, height_label) self.play(FadeIn(width_label), FadeIn(height_label), FadeIn(channel_label)) block1 = VGroup(cube, width_label, height_label, channel_label) labels = VGroup(width_label, height_label, channel_label) # block1.shift(LEFT*10).shift(UP*3) cube2 = Cube(side_length=2).set_fill(BLUE, opacity=0.5).set_stroke(BLUE, width=1) cube3 = Cube(side_length=2).set_fill(BLUE, opacity=0.5).set_stroke(BLUE, width=1).next_to(cube2, UP, buff=0) # self.add(cube2, cube3) block2 = VGroup(cube2, cube3).shift(RIGHT*10).shift(DOWN*6) self.play(FadeOut(labels)) self.play(cube.animate.shift(LEFT*10).shift(UP*3), block2.animate.shift(LEFT*10).shift(UP*6)) # # Create labels width_label_2 = Text(\"Width\", font_size=24).next_to(cube2, DL/2, buff=0.5) height_label_2 = Text(\"Height\", font_size=24).next_to(cube2, LEFT, buff=0.5) channel_label_2 = Text(\"Channels\", font_size=24).next_to(cube2, DR, buff=0.5) # Add labels to the scene self.add_fixed_in_frame_mobjects(channel_label_2, width_label_2, height_label_2) self.play(FadeIn(width_label_2), FadeIn(height_label_2), FadeIn(channel_label_2)) self.play(FadeOut(width_label_2), FadeOut(height_label_2), FadeOut(channel_label_2)) block1_and_block2 = VGroup(cube, block2) cube4 = Cube(side_length=1).set_fill(BLUE, opacity=0.5).set_stroke(BLUE, width=1) cube5 = Cube(side_length=1).set_fill(BLUE, opacity=0.5).set_stroke(BLUE, width=1).next_to(cube4, UP, buff=0) cube6 = Cube(side_length=1).set_fill(BLUE, opacity=0.5).set_stroke(BLUE, width=1).next_to(cube5, UP, buff=0) cuboid = VGroup(cube4, cube5, cube6).shift(RIGHT*10).shift(DOWN*6) self.play(block1_and_block2.animate.shift(LEFT*10).shift(UP*2), cuboid.animate.shift(LEFT*10).shift(UP*6)) # self.add(cuboid) # Create labels width_label_3 = Text(\"Width\", font_size=24).next_to(cube4, DL/2, buff=0.5) height_label_3 = Text(\"Height\", font_size=24).next_to(cube4, LEFT, buff=0.5) channel_label_3 = Text(\"Channels\", font_size=24).next_to(cube4, DR, buff=0.5) # Add labels to the scene self.add_fixed_in_frame_mobjects(channel_label_3, width_label_3, height_label_3) self.play(FadeIn(width_label_3), FadeIn(height_label_3), FadeIn(channel_label_3)) self.play(FadeOut(width_label_3), FadeOut(height_label_3), FadeOut(channel_label_3)) # block3 = VGroup(cuboid, width_label_3, height_label_3, channel_label_3) block1_and_block2_and_block3 = VGroup(cube, block2, cuboid) spheres = self.create_spheres_chain(5, 5, radius=0.2, buff=0.2) spheres.shift(RIGHT*10).shift(DOWN*6) # self.add(spheres) output_sphere = Sphere(radius=0.5).set_fill(RED, opacity=1).shift(RIGHT*10).shift(DOWN*6) # self.add(output_sphere) blocks_and_spheres = VGroup(cube, block2, cuboid, spheres) self.play(block1_and_block2_and_block3.animate.shift(LEFT*10).shift(UP), spheres.animate.shift(LEFT*10).shift(UP*6)) self.play(Transform(cuboid, spheres)) self.play(block1_and_block2_and_block3.animate.shift(LEFT*10).shift(UP), spheres.animate.shift(LEFT*10).shift(UP), output_sphere.animate.shift(LEFT*10).shift(UP*6)) lines = VGroup() for sphere in spheres: if isinstance(sphere, VGroup): for sub_sphere in sphere: line = Line(start=sub_sphere.get_center(), end=output_sphere.get_center(), stroke_width=1) lines.add(line) # self.play(Create(line)) else: line = Line(start=sphere.get_center(), end=output_sphere.get_center(), stroke_width=1) lines.add(line) self.play(Create(lines)) self.wait(2) def create_spheres_chain(self, num_up, num_down, radius=0.1, buff=0.2): \"\"\" Creates a chain of spheres with a center sphere, multiple spheres above, and multiple spheres below. Parameters: - num_up: Number of spheres above the center sphere. - num_down: Number of spheres below the center sphere. - radius: Radius of each sphere. - buff: Buffer distance between spheres. Returns: - A VGroup containing all the spheres. \"\"\" # Create the center sphere sphere_center = Sphere(radius=radius).set_fill(BLUE, opacity=1) sphere_center_group = VGroup(sphere_center) # Create spheres above the center sphere spheres_up = VGroup() for i in range(num_up): sphere = Sphere(radius=radius).set_fill(BLUE, opacity=1) if i == 0: sphere.next_to(sphere_center, OUT, buff=buff) else: sphere.next_to(spheres_up[-1], OUT, buff=buff) spheres_up.add(sphere) # Create spheres below the center sphere spheres_down = VGroup() for i in range(num_down): sphere = Sphere(radius=radius).set_fill(BLUE, opacity=1) if i == 0: sphere.next_to(sphere_center, IN, buff=buff) else: sphere.next_to(spheres_down[-1], IN, buff=buff) spheres_down.add(sphere) # Group all spheres together all_spheres = VGroup(sphere_center_group, spheres_up, spheres_down) return all_spheres", "source": "cnn.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100] ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import time import logging import logging.config import yaml import mlflow logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import os import ast import random import logging import numpy as np import tensorflow as tf from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from omegaconf import DictConfig import hydra import mlflow import wandb from wandb.integration.keras import WandbMetricsLogger import general_utils from a5p2.constants import PREPROCESS_INPUT_MAP from a5p2.pipeline import DataPipeline, get_augmentation_layers from a5p2.model import ModelBuilder from a5p2.callbacks import WandbClfEvalCallback @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def train_model(cfg: DictConfig): \"\"\" Train a model with Hydra configuration and log the experiment using MLFlow. Parameters: cfg (DictConfig): Configuration object provided by Hydra. \"\"\" # Initialize logger logger = logging.getLogger(__name__) logger.info(\"Initializing experiment...\") # Setup logging using `general_utils` general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) # Initialize MLflow mlflow_init_status, mlflow_run = general_utils.mlflow_init( cfg, setup_mlflow=cfg.setup_mlflow, autolog=cfg.mlflow_autolog ) # Log initial parameters using MLflow initial_params = { \"epochs\": cfg.training.epochs, \"batch_size\": cfg.training.batch_size, \"input_shape\": cfg.model.input_shape, \"num_classes\": cfg.model.num_classes, \"model_name\": cfg.model.model_name, \"loss\": cfg.training.loss, \"random_seed\": cfg.training.random_seed, \"hidden_layers\": cfg.model.hidden_layers, \"activation\": cfg.model.activation, \"dropout\": cfg.model.use_dropout, \"dropout_rate\": cfg.model.dropout_rate, \"batch_norm\": cfg.model.use_batch_norm, \"base_model_trainable\": cfg.model.base_model_trainable, \"optimizer\": cfg.training.optimizer, \"learning_rate\": cfg.training.learning_rate, \"early_stopping\": cfg.training.use_early_stopping, \"early_stopping_patience\": cfg.training.early_stopping_patience, \"mlflow_id\": mlflow_run.info.run_id if mlflow_init_status else None, \"description\": cfg.description, } general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=initial_params, ) # Log augmentation parameters using MLflow augmentations_params = { \"random_flip_left_right\": cfg.augmentations.random_flip_left_right, \"random_flip_up_down\": cfg.augmentations.random_flip_up_down, \"random_rotation\": cfg.augmentations.random_rotation, \"rotation_factor\": cfg.augmentations.rotation_factor, \"random_zoom\": cfg.augmentations.random_zoom, \"zoom_factor\": cfg.augmentations.zoom_factor, \"random_brightness\": cfg.augmentations.random_brightness, \"brightness_factor\": cfg.augmentations.brightness_factor, \"random_contrast\": cfg.augmentations.random_contrast, \"contrast_factor\": cfg.augmentations.contrast_factor, \"random_crop\": cfg.augmentations.random_crop, \"crop_size\": cfg.augmentations.crop_size, } general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=augmentations_params, ) # Log parameters to wandb if wandb.api.api_key: wandb.init(project='Singapore Food Classification') wandb.config.update(initial_params) wandb.config.update(augmentations_params) # Set random seed for reproducibility os.environ[\"PYTHONHASHSEED\"] = str(cfg.training.random_seed) tf.random.set_seed(cfg.training.random_seed) np.random.seed(cfg.training.random_seed) random.seed(cfg.training.random_seed) try: # Define augmentation layers augmentation_layers = get_augmentation_layers(cfg) # Initialize data pipeline preprocess_input_fn = PREPROCESS_INPUT_MAP.get(cfg.model.model_name) if preprocess_input_fn is None: raise ValueError( f\"Preprocess function for model {cfg.model.model_name} not found.\" ) pipeline = DataPipeline( data_dir=cfg.data_dir_path, img_size=ast.literal_eval(cfg.model.image_size), batch_size=cfg.training.batch_size, validation_split=cfg.data.validation_split, seed=cfg.training.random_seed, preprocess_func=lambda img, lbl: (preprocess_input_fn(img), lbl), # Preprocessing augment_layers=augmentation_layers # Augmentations using Keras layers ) train_ds, val_ds = pipeline.load_data() # Build model model_builder = ModelBuilder( model_name=cfg.model.model_name, input_shape=tuple(ast.literal_eval(cfg.model.input_shape)), num_classes=cfg.model.num_classes, hidden_layers=cfg.model.hidden_layers, activation=cfg.model.activation, batch_norm=cfg.model.use_batch_norm, dropout=cfg.model.use_dropout, dropout_rate=cfg.model.dropout_rate, base_model_trainable=cfg.model.base_model_trainable, optimizer_name=cfg.training.optimizer, learning_rate=cfg.training.learning_rate, top_k=cfg.metrics.top_k, ) model = model_builder.build() model.summary() # Calculate and log the total number of parameters, trainable parameters, and non-trainable parameters total_params = model.count_params() trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights]) non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights]) mlflow.log_param(\"total_params\", total_params) mlflow.log_param(\"trainable_params\", trainable_params) mlflow.log_param(\"non_trainable_params\", non_trainable_params) # Log parameters to wandb if wandb.api.api_key: wandb.config.update({ \"total_params\": total_params, \"trainable_params\": trainable_params, \"non_trainable_params\": non_trainable_params }) # Get callbacks for early stopping and checkpointing callbacks = get_callbacks(checkpoint_dir_path=cfg.model_checkpoint_dir_path, model_name=cfg.model.model_name, use_early_stopping=cfg.training.use_early_stopping, early_stopping_patience=cfg.training.early_stopping_patience) if wandb.api.api_key: # Add Wandb callback for logging metrics wandb_callback = WandbClfEvalCallback( validation_data=next(iter(val_ds)), data_table_columns=[\"idx\", \"image\", \"ground_truth\"], pred_table_columns=[\"epoch\", \"idx\", \"image\", \"ground_truth\", \"prediction\"], class_labels=cfg.data.class_labels, ) callbacks.append(wandb_callback) # Train the model and track history history = train( model=model, train_ds=train_ds, val_ds=val_ds, epochs=cfg.training.epochs, callbacks=callbacks ) # Log the stopped_epoch from EarlyStopping callback to MLflow for callback in callbacks: if isinstance(callback, EarlyStopping): mlflow.log_metric(\"stopped_epoch\", callback.stopped_epoch) # log to wandb also if wandb.api.api_key: wandb.config.update({\"stopped_epoch\": callback.stopped_epoch}) break # Evaluate the model on the validation set evaluation_results = model.evaluate(val_ds) # Extract metric names from the history object metric_names = [\"loss\", \"sparse_categorical_accuracy\", \"sparse_top_k_categorical_accuracy\", \"precision\", \"recall\", \"f1_score\"] # Log only the final epoch scores of validation metrics to MLflow metrics_dict = dict(zip(metric_names, evaluation_results)) for metric_name, metric_value in zip(metric_names, evaluation_results): mlflow.log_metric(f\"{metric_name}\", metric_value) if wandb.api.api_key: wandb.log({f\"{metric_name}\": metric_value}) # Log the model to MLflow mlflow.keras.log_model(model, \"model\") # Finalize MLflow logging", "source": "train_model.py"}, {"content": "if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") # End the wandb run if wandb.api.api_key: wandb.finish() return metrics_dict['loss'], metrics_dict['f1_score'] except tf.errors.ResourceExhaustedError as e: logger.error(\"Out of memory error: %s\", e) if wandb.api.api_key: wandb.finish() if mlflow_init_status: mlflow.end_run() return float('inf'), 0.0 def get_callbacks(checkpoint_dir_path='./models', model_name='best.keras', use_early_stopping=True, early_stopping_patience=15): \"\"\"Creates callbacks for training.\"\"\" callbacks = [] if use_early_stopping: early_stopping = EarlyStopping( monitor= 'val_loss', patience=early_stopping_patience, restore_best_weights=True ) callbacks.append(early_stopping) checkpoint_filepath = os.path.join(checkpoint_dir_path, f\"{model_name}.keras\") checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', save_best_only=True) callbacks.append(checkpoint) # Add Wandb callback if logged in if wandb.api.api_key: callbacks.append(WandbMetricsLogger()) return callbacks def train(model, train_ds, val_ds, epochs=20, callbacks=None): \"\"\"Trains the model and returns training history.\"\"\" history = model.fit( train_ds, validation_data=val_ds, epochs=epochs, verbose=1, callbacks=callbacks ) return history if __name__ == \"__main__\": train_model()", "source": "train_model.py"}, {"content": "import tensorflow as tf import wandb from wandb.integration.keras import WandbEvalCallback class WandbClfEvalCallback(WandbEvalCallback): def __init__( self, validation_data, data_table_columns, pred_table_columns, class_labels, num_samples=100 ): super().__init__(data_table_columns, pred_table_columns) self.x = validation_data[0] self.y = validation_data[1] self.class_labels = class_labels self.id2class = {idx: class_name for idx, class_name in enumerate(class_labels)} def add_ground_truth(self, logs=None): for idx, (image, label) in enumerate(zip(self.x, self.y)): label_name = self.id2class[label.numpy()] if isinstance(label, tf.Tensor) else self.id2class[label] self.data_table.add_data(idx, wandb.Image(image), label_name) def add_model_predictions(self, epoch, logs=None): preds = self.model.predict(self.x, verbose=0) preds = tf.argmax(preds, axis=-1) table_idxs = self.data_table_ref.get_index() for idx in table_idxs[:10]: # Limit to 10 predictions pred = preds[idx].numpy() if isinstance(preds[idx], tf.Tensor) else preds[idx] pred_name = self.id2class[pred] label_name = self.data_table_ref.data[idx][2] self.pred_table.add_data( epoch, self.data_table_ref.data[idx][0], self.data_table_ref.data[idx][1], label_name, pred_name, )", "source": "callbacks.py"}, {"content": "from tensorflow.keras import applications PREPROCESS_INPUT_MAP = { \"ConvNeXtBase\": applications.convnext.preprocess_input, \"ConvNeXtLarge\": applications.convnext.preprocess_input, \"ConvNeXtSmall\": applications.convnext.preprocess_input, \"ConvNeXtTiny\": applications.convnext.preprocess_input, \"ConvNeXtXLarge\": applications.convnext.preprocess_input, \"DenseNet121\": applications.densenet.preprocess_input, \"DenseNet169\": applications.densenet.preprocess_input, \"DenseNet201\": applications.densenet.preprocess_input, \"EfficientNetB0\": applications.efficientnet.preprocess_input, \"EfficientNetB1\": applications.efficientnet.preprocess_input, \"EfficientNetB2\": applications.efficientnet.preprocess_input, \"EfficientNetB3\": applications.efficientnet.preprocess_input, \"EfficientNetB4\": applications.efficientnet.preprocess_input, \"EfficientNetB5\": applications.efficientnet.preprocess_input, \"EfficientNetB6\": applications.efficientnet.preprocess_input, \"EfficientNetB7\": applications.efficientnet.preprocess_input, \"EfficientNetV2B0\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B1\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B2\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B3\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2L\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2M\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2S\": applications.efficientnet_v2.preprocess_input, \"InceptionResNetV2\": applications.inception_resnet_v2.preprocess_input, \"InceptionV3\": applications.inception_v3.preprocess_input, \"MobileNet\": applications.mobilenet.preprocess_input, \"MobileNetV2\": applications.mobilenet_v2.preprocess_input, \"MobileNetV3Large\": applications.mobilenet_v3.preprocess_input, \"MobileNetV3Small\": applications.mobilenet_v3.preprocess_input, \"NASNetLarge\": applications.nasnet.preprocess_input, \"NASNetMobile\": applications.nasnet.preprocess_input, \"ResNet50\": applications.resnet.preprocess_input, \"ResNet101\": applications.resnet.preprocess_input, \"ResNet152\": applications.resnet.preprocess_input, \"ResNet50V2\": applications.resnet_v2.preprocess_input, \"ResNet101V2\": applications.resnet_v2.preprocess_input, \"ResNet152V2\": applications.resnet_v2.preprocess_input, \"VGG16\": applications.vgg16.preprocess_input, \"VGG19\": applications.vgg19.preprocess_input, \"Xception\": applications.xception.preprocess_input, }", "source": "constants.py"}, {"content": "import tensorflow as tf from tensorflow.keras import backend as K class CustomRecall(tf.keras.metrics.Metric): def __init__(self, name='recall', num_classes=12, **kwargs): super(CustomRecall, self).__init__(name=name, **kwargs) self.num_classes = num_classes self.true_positives = self.add_weight(name='true_positives', initializer='zeros') self.all_positives = self.add_weight(name='all_positives', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes) # Convert y_true to one-hot y_pred = tf.cast(y_pred, tf.float32) # Ensure y_pred is float32 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) all_positives = K.sum(K.round(K.clip(y_true, 0, 1))) self.true_positives.assign_add(true_positives) self.all_positives.assign_add(all_positives) def result(self): return self.true_positives / (self.all_positives + K.epsilon()) def reset_states(self): self.true_positives.assign(0) self.all_positives.assign(0) class CustomPrecision(tf.keras.metrics.Metric): def __init__(self, name='precision', num_classes=12, **kwargs): super(CustomPrecision, self).__init__(name=name, **kwargs) self.num_classes = num_classes self.true_positives = self.add_weight(name='true_positives', initializer='zeros') self.predicted_positives = self.add_weight(name='predicted_positives', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes) # Convert y_true to one-hot y_pred = tf.cast(y_pred, tf.float32) # Ensure y_pred is float32 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) self.true_positives.assign_add(true_positives) self.predicted_positives.assign_add(predicted_positives) def result(self): return self.true_positives / (self.predicted_positives + K.epsilon()) def reset_states(self): self.true_positives.assign(0) self.predicted_positives.assign(0) class CustomF1Score(tf.keras.metrics.Metric): def __init__(self, name='f1_score', num_classes=12, **kwargs): super(CustomF1Score, self).__init__(name=name, **kwargs) self.precision = CustomPrecision(num_classes=num_classes) self.recall = CustomRecall(num_classes=num_classes) def update_state(self, y_true, y_pred, sample_weight=None): self.precision.update_state(y_true, y_pred, sample_weight) self.recall.update_state(y_true, y_pred, sample_weight) def result(self): precision = self.precision.result() recall = self.recall.result() return 2 * ((precision * recall) / (precision + recall + K.epsilon())) def reset_states(self): self.precision.reset_states() self.recall.reset_states()", "source": "metrics.py"}, {"content": "import tensorflow as tf from tensorflow.keras import metrics from tensorflow.keras import layers, models from .metrics import CustomF1Score, CustomPrecision, CustomRecall class ModelBuilder: def __init__(self, model_name, input_shape, num_classes, hidden_layers=[512, 256], activation='relu', batch_norm=True, dropout=True, dropout_rate=0.5, base_model_trainable=False, optimizer_name='adamw', learning_rate=0.001, top_k=5): self.model_name = model_name self.input_shape = input_shape self.num_classes = num_classes self.base_model_trainable = base_model_trainable self.optimizer_name = optimizer_name self.learning_rate = learning_rate self.top_k = top_k self.hidden_layers = hidden_layers self.activation = activation self.batch_norm = batch_norm self.dropout = dropout self.dropout_rate = dropout_rate def build(self): \"\"\"Builds a model based on the model name and configuration provided.\"\"\" base_model = self._load_base_model() inputs = layers.Input(shape=self.input_shape) # Base model extraction x = base_model(inputs, training=False) x = layers.GlobalAveragePooling2D()(x) # Fully-connected layers for units in self.hidden_layers: x = layers.Dense(units)(x) if self.batch_norm: x = layers.BatchNormalization()(x) x = layers.Activation(self.activation)(x) if self.dropout: x = layers.Dropout(self.dropout_rate)(x) # Output layer outputs = layers.Dense(self.num_classes, activation='softmax')(x) # Build and compile model model = models.Model(inputs=inputs, outputs=outputs) model.compile( optimizer=self._get_optimizer(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[ metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy'), metrics.SparseTopKCategoricalAccuracy(k=self.top_k, name='sparse_top_k_categorical_accuracy'), CustomPrecision(), CustomRecall(), CustomF1Score() ] ) return model def _get_optimizer(self): \"\"\"Returns the optimizer instance based on the optimizer name.\"\"\" if self.optimizer_name.lower() == 'adam': return tf.keras.optimizers.Adam(self.learning_rate) elif self.optimizer_name.lower() == 'adamw': return tf.keras.optimizers.AdamW(self.learning_rate) elif self.optimizer_name.lower() == 'sgd': return tf.keras.optimizers.SGD(self.learning_rate) elif self.optimizer_name.lower() == 'rmsprop': return tf.keras.optimizers.RMSprop(self.learning_rate) else: raise ValueError(f\"Unsupported optimizer: {self.optimizer_name}\") def _load_base_model(self): \"\"\"Loads the specified base model without the top layers.\"\"\" if self.model_name == 'EfficientNetB7': base_model = tf.keras.applications.EfficientNetB7( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'EfficientNetB0': base_model = tf.keras.applications.EfficientNetB0( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'EfficientNetV2B0': base_model = tf.keras.applications.EfficientNetV2B0( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'EfficientNetV2S': base_model = tf.keras.applications.EfficientNetV2S( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'EfficientNetV2L': base_model = tf.keras.applications.EfficientNetV2L( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'ConvNeXtXLarge': base_model = tf.keras.applications.ConvNeXtXLarge( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'ConvNeXtTiny': base_model = tf.keras.applications.ConvNeXtTiny( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'MobileNetV2': base_model = tf.keras.applications.MobileNetV2( weights='imagenet', include_top=False, input_shape=self.input_shape ) elif self.model_name == 'ResNet50V2': base_model = tf.keras.applications.ResNet50V2( weights='imagenet', include_top=False, input_shape=self.input_shape ) else: raise ValueError(f\"Model {self.model_name} is not supported yet.\") # Set base model layers to be trainable or not base_model.trainable = self.base_model_trainable return base_model", "source": "model.py"}, {"content": "import tensorflow as tf class DataPipeline: def __init__( self, data_dir, img_size, batch_size, validation_split, seed, preprocess_func=None, augment_layers=None ): self.data_dir = data_dir self.img_size = img_size self.batch_size = batch_size self.validation_split = validation_split self.seed = seed self.preprocess_func = preprocess_func self.augment_layers = augment_layers def load_data(self): \"\"\"Load and preprocess training and validation datasets.\"\"\" train_ds = tf.keras.preprocessing.image_dataset_from_directory( self.data_dir, validation_split=self.validation_split, subset=\"training\", seed=self.seed, image_size=self.img_size, batch_size=self.batch_size ) val_ds = tf.keras.preprocessing.image_dataset_from_directory( self.data_dir, validation_split=self.validation_split, subset=\"validation\", seed=self.seed, image_size=self.img_size, batch_size=self.batch_size ) # Apply preprocessing if provided if self.preprocess_func: train_ds = train_ds.map( self.preprocess_func, num_parallel_calls=tf.data.AUTOTUNE ) val_ds = val_ds.map( self.preprocess_func, num_parallel_calls=tf.data.AUTOTUNE ) # Apply augmentations if provided if self.augment_layers: # Augmentation layers can be applied directly as a layer def augment(image, label): image = self.augment_layers(image, training=True) return image, label train_ds = train_ds.map( augment, num_parallel_calls=tf.data.AUTOTUNE ) # Optimize dataset performance train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE) return train_ds, val_ds def get_augmentation_layers(cfg): \"\"\"Create a Keras Sequential model containing augmentation layers based on the configuration.\"\"\" augmentation_layers = [] if cfg.augmentations.random_flip_left_right or cfg.augmentations.random_flip_up_down: if cfg.augmentations.random_flip_left_right and cfg.augmentations.random_flip_up_down: # Apply both horizontal and vertical flips augmentation_layers.append(tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")) elif cfg.augmentations.random_flip_left_right: augmentation_layers.append(tf.keras.layers.RandomFlip(\"horizontal\")) elif cfg.augmentations.random_flip_up_down: augmentation_layers.append(tf.keras.layers.RandomFlip(\"vertical\")) if cfg.augmentations.random_rotation: augmentation_layers.append( tf.keras.layers.RandomRotation( factor=cfg.augmentations.rotation_factor, fill_mode='nearest' ) ) if cfg.augmentations.random_zoom: augmentation_layers.append( tf.keras.layers.RandomZoom( height_factor=cfg.augmentations.zoom_factor, width_factor=cfg.augmentations.zoom_factor, fill_mode='nearest' ) ) if cfg.augmentations.random_brightness: augmentation_layers.append( tf.keras.layers.RandomBrightness( factor=cfg.augmentations.brightness_factor ) ) if cfg.augmentations.random_contrast: augmentation_layers.append( tf.keras.layers.RandomContrast( factor=cfg.augmentations.contrast_factor ) ) if cfg.augmentations.random_crop: # Note: Random cropping can be handled by adjusting the image size or using a RandomCrop layer augmentation_layers.append( tf.keras.layers.RandomCrop( height=cfg.augmentations.crop_size[0], width=cfg.augmentations.crop_size[1] ) ) # Add more augmentation layers as needed based on configuration return tf.keras.Sequential(augmentation_layers)", "source": "pipeline.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import pandas as pd import numpy as np from typing import List, Optional, Dict, Tuple from sklearn.preprocessing import StandardScaler class DataPipeline: def __init__(self): self.wind_directions = ['NW', 'NE', 'SE', 'cv'] self.feature_means: Dict[str, float] = {} self.feature_stds: Dict[str, float] = {} self.outlier_bounds: Dict[str, Dict[str, float]] = {} self.scaler = StandardScaler() self.features_to_scale: List[str] = [] self.features_to_lag: List[str] = [] self.lag_hours: int = 24 self.target_column = 'pm2.5' def fit(self, train_data_path: str, features_to_keep: Optional[List[str]] = None): \"\"\"Fit the pipeline on the training data\"\"\" train_data = pd.read_csv(train_data_path) if features_to_keep: train_data = train_data[features_to_keep] train_data = self.convert_wind_direction(train_data) train_data = self.create_new_features(train_data) train_data = self.create_lagged_features(train_data) self.calculate_missing_value_fillers(train_data) self.calculate_outlier_bounds(train_data) self.fit_scaler(train_data) def calculate_missing_value_fillers(self, df: pd.DataFrame): for column in df.select_dtypes(include=[np.number]).columns: self.feature_means[column] = df[column].mean() def calculate_outlier_bounds(self, df: pd.DataFrame): for column in df.select_dtypes(include=[np.number]).columns: q1 = df[column].quantile(0.25) q3 = df[column].quantile(0.75) iqr = q3 - q1 self.outlier_bounds[column] = { 'lower': q1 - (1.5 * iqr), 'upper': q3 + (1.5 * iqr) } def fit_scaler(self, df: pd.DataFrame): self.features_to_scale = [col for col in df.select_dtypes(include=[np.number]).columns if col != self.target_column] self.scaler.fit(df[self.features_to_scale]) def run_data_pipeline(self, csv_path: str, features_to_keep: Optional[List[str]] = None) -> pd.DataFrame: try: df = self.load_data(csv_path) df = self.preprocess_data(df, features_to_keep) return df except Exception as e: print(f\"Error in data pipeline: {str(e)}\") return pd.DataFrame() def load_data(self, file_path: str) -> pd.DataFrame: df = pd.read_csv(file_path) if all(col in df.columns for col in ['year', 'month', 'day', 'hour']): df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) df.set_index('datetime', inplace=True) df.drop(['year', 'month', 'day', 'hour'], axis=1, inplace=True) return df def preprocess_data(self, df: pd.DataFrame, features_to_keep: Optional[List[str]] = None) -> pd.DataFrame: df = df.copy() if features_to_keep: df = df[features_to_keep] df = self.handle_missing_values(df) df = self.convert_wind_direction(df) df = self.remove_outliers(df) df = self.create_new_features(df) df = self.create_lagged_features(df) df = self.normalize_features(df) return df def handle_missing_values(self, df: pd.DataFrame) -> pd.DataFrame: for column in df.select_dtypes(include=[np.number]).columns: if column in self.feature_means: df[column] = df[column].fillna(self.feature_means[column]) return df def convert_wind_direction(self, df: pd.DataFrame) -> pd.DataFrame: if 'cbwd' in df.columns: for direction in self.wind_directions: col_name = f'wind_{direction}' df[col_name] = (df['cbwd'] == direction).astype(int) df = df.drop('cbwd', axis=1) return df def remove_outliers(self, df: pd.DataFrame) -> pd.DataFrame: for column, bounds in self.outlier_bounds.items(): if column in df.columns: df = df[(df[column] >= bounds['lower']) & (df[column] <= bounds['upper'])] return df def create_new_features(self, df: pd.DataFrame) -> pd.DataFrame: if isinstance(df.index, pd.DatetimeIndex): df['month'] = df.index.month df['day_of_week'] = df.index.dayofweek df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int) return df def create_lagged_features(self, df: pd.DataFrame) -> pd.DataFrame: lagged_features = {} for col in self.features_to_lag: for lag in range(1, self.lag_hours + 1): lagged_features[f'{col}_lag_{lag}'] = df[col].shift(lag) lagged_df = pd.DataFrame(lagged_features, index=df.index) df = pd.concat([df, lagged_df], axis=1) df.dropna(inplace=True) return df def normalize_features(self, df: pd.DataFrame) -> pd.DataFrame: df_normalized = df.copy() df_normalized[self.features_to_scale] = self.scaler.transform(df[self.features_to_scale]) return df_normalized def set_features_to_lag(self, features: List[str]): self.features_to_lag = features def set_lag_hours(self, hours: int): self.lag_hours = hours def create_multi_step_features(self, df: pd.DataFrame, horizon: int) -> Tuple[pd.DataFrame, pd.DataFrame]: X = df.copy() y = pd.DataFrame() for i in range(1, horizon + 1): y[f'pm2.5_t+{i}'] = X['pm2.5'].shift(-i) X = X.drop('pm2.5', axis=1) # Remove rows with NaN values in both X and y combined = pd.concat([X, y], axis=1) combined.dropna(inplace=True) X = combined[X.columns] y = combined[y.columns] return X, y def prepare_rnn_data(self, csv_path: str, features_to_keep: Optional[List[str]] = None, remove_outliers: bool = False) -> pd.DataFrame: \"\"\"Prepare data for RNN without creating lagged features\"\"\" df = pd.read_csv(csv_path) if features_to_keep: df = df[features_to_keep] df = self.handle_missing_values(df) df =", "source": "datapipeline.py"}, {"content": "self.convert_wind_direction(df) if remove_outliers: df = self.remove_outliers(df) df = self.normalize_features(df) return df def create_rnn_sequences(self, df: pd.DataFrame, lookback: int, target_col: str = 'pm2.5') -> Tuple[np.ndarray, np.ndarray]: \"\"\"Create input sequences and target values for RNN\"\"\" data = df.values X, y = [], [] for i in range(len(data) - lookback): X.append(data[i:(i + lookback), :]) y.append(data[i + lookback, df.columns.get_loc(target_col)]) return np.array(X), np.array(y) def split_rnn_data(self, X: np.ndarray, y: np.ndarray, train_ratio: float = 0.8) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: \"\"\"Split data into training and testing sets for RNN\"\"\" train_size = int(len(X) * train_ratio) X_train, X_test = X[:train_size], X[train_size:] y_train, y_test = y[:train_size], y[train_size:] return X_train, X_test, y_train, y_test def prepare_multi_step_rnn_data(self, df: pd.DataFrame, lookback: int, horizon: int, target_col: str = 'pm2.5') -> Tuple[np.ndarray, np.ndarray]: \"\"\"Create input sequences and multi-step target values for RNN\"\"\" data = df.values X, y = [], [] for i in range(len(data) - lookback - horizon + 1): X.append(data[i:(i + lookback), :]) y.append(data[(i + lookback):(i + lookback + horizon), df.columns.get_loc(target_col)]) return np.array(X), np.array(y) # Example usage: # if __name__ == \"__main__\": # pipeline = DataPipeline() # features_to_keep = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir'] # features_to_lag = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir'] # pipeline.set_features_to_lag(features_to_lag) # pipeline.set_lag_hours(24) # # Load and fit on training data # train_data = pd.read_csv('./data/train.csv') # pipeline.fit(train_data, features_to_keep) # # Process training data # df_train_cleaned = pipeline.run_data_pipeline('./data/train.csv', features_to_keep) # # Process test data using the fitted pipeline # df_test_cleaned = pipeline.run_data_pipeline('./data/test.csv', features_to_keep) # print(df_train_cleaned.head()) # print(df_train_cleaned.info())", "source": "datapipeline.py"}, {"content": "import pandas as pd import argparse from sklearn.model_selection import train_test_split def load_data(file_path): \"\"\"Load the CSV file.\"\"\" return pd.read_csv(file_path) def train_val_test_split(data, train_size=0.7, val_size=0.15, test_size=0.15): \"\"\"Perform train-val-test split preserving time order.\"\"\" # Calculate the split indices train_end = int(len(data) * train_size) val_end = int(len(data) * (train_size + val_size)) # Split the data train_data = data.iloc[:train_end] val_data = data.iloc[train_end:val_end] test_data = data.iloc[val_end:] return train_data, val_data, test_data def save_splits(train_data, val_data, test_data, output_dir): \"\"\"Save the split datasets to CSV files.\"\"\" train_data.to_csv(f\"{output_dir}/train.csv\", index=False) val_data.to_csv(f\"{output_dir}/val.csv\", index=False) test_data.to_csv(f\"{output_dir}/test.csv\", index=False) def main(args): # Load the data data = load_data(args.input_file) # Perform the split train_data, val_data, test_data = train_val_test_split( data, train_size=args.train_size, val_size=args.val_size, test_size=args.test_size ) # Save the split datasets save_splits(train_data, val_data, test_data, args.output_dir) print(f\"Data split complete. Files saved in {args.output_dir}\") print(f\"Train set size: {len(train_data)}\") print(f\"Validation set size: {len(val_data)}\") print(f\"Test set size: {len(test_data)}\") if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Split time series data into train, validation, and test sets.\") parser.add_argument(\"input_file\", type=str, help=\"Path to the input CSV file\") parser.add_argument(\"output_dir\", type=str, help=\"Directory to save the output CSV files\") parser.add_argument(\"--train_size\", type=float, default=0.7, help=\"Proportion of data for training set\") parser.add_argument(\"--val_size\", type=float, default=0.15, help=\"Proportion of data for validation set\") parser.add_argument(\"--test_size\", type=float, default=0.15, help=\"Proportion of data for test set\") args = parser.parse_args() # Ensure proportions sum to 1 total = args.train_size + args.val_size + args.test_size if abs(total - 1.0) > 1e-10: raise ValueError(\"Train, validation, and test proportions must sum to 1.\") main(args)", "source": "data_split.py"}, {"content": "from typing import List, Dict, Any, Optional import torch import numpy as np from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score import matplotlib.pyplot as plt import pandas as pd class Evaluator: def __init__(self, model, test_loader, forecast_horizon, device): self.model = model self.test_loader = test_loader self.forecast_horizon = forecast_horizon self.device = device def evaluate(self) -> Dict[str, float]: self.model.eval() predictions = [] targets = [] with torch.no_grad(): for batch_x, batch_y in self.test_loader: batch_x = batch_x.to(self.device) outputs = self.model(batch_x) predictions.append(outputs.cpu().numpy()) targets.append(batch_y.numpy()) predictions = np.concatenate(predictions, axis=0) targets = np.concatenate(targets, axis=0) return self.calculate_metrics(targets, predictions, self.forecast_horizon) @staticmethod def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray, forecast_horizon: int) -> Dict[str, float]: # Reshape predictions and targets y_pred = y_pred.reshape(-1, forecast_horizon) y_true = y_true.reshape(-1, forecast_horizon) mse = mean_squared_error(y_true, y_pred) rmse = np.sqrt(mse) mae = mean_absolute_error(y_true, y_pred) r2 = r2_score(y_true, y_pred) return { 'MSE': mse, 'RMSE': rmse, 'MAE': mae, 'R2': r2 } def evaluate_multi_step(self) -> List[Dict[str, float]]: self.model.eval() predictions = [] targets = [] with torch.no_grad(): for batch_x, batch_y in self.test_loader: batch_x = batch_x.to(self.device) outputs = self.model(batch_x) predictions.append(outputs.cpu().numpy()) targets.append(batch_y.numpy()) predictions = np.concatenate(predictions, axis=0) targets = np.concatenate(targets, axis=0) metrics_per_step = [] for step in range(self.forecast_horizon): step_metrics = self.calculate_metrics(targets[:, step], predictions[:, step], 1) metrics_per_step.append(step_metrics) return metrics_per_step def plot_predictions(self, num_samples: Optional[int] = None, show_plot: bool = False): self.model.eval() all_targets = [] all_predictions = [] with torch.no_grad(): for inputs, targets in self.test_loader: inputs = inputs.to(self.device) predictions = self.model(inputs).cpu().numpy() all_targets.extend(targets.numpy()) all_predictions.extend(predictions) all_targets = np.array(all_targets).flatten() all_predictions = np.array(all_predictions).flatten() # Create a range for x-axis based on the number of predictions x_range = range(len(all_targets)) plt.figure(figsize=(20, 10)) plt.plot(x_range, all_targets, label='Actual', color='blue') plt.plot(x_range, all_predictions, label='Predicted', color='orange') plt.title('Actual vs Predicted PM2.5 Values') plt.xlabel('Time Steps') plt.ylabel('PM2.5') plt.legend() plt.grid(True) # Use a scientific notation for y-axis plt.ticklabel_format(style='sci', axis='y', scilimits=(0,0)) plt.tight_layout() plt.savefig('predictions_plot.png') if show_plot: plt.show() else: plt.close()", "source": "evaluator.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import time import logging import logging.config import yaml import mlflow logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import pandas as pd import matplotlib.pyplot as plt from datapipeline import DataPipeline from ml_model import ForecastModel def run_experiment(train_path, test_path, horizons): pipeline = DataPipeline() features_to_keep = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir'] features_to_lag = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir'] pipeline.set_features_to_lag(features_to_lag) pipeline.set_lag_hours(24) # Load and fit on training data pipeline.fit(train_path, features_to_keep) # Process training and test data df_train = pipeline.run_data_pipeline(train_path, features_to_keep) df_test = pipeline.run_data_pipeline(test_path, features_to_keep) results = {} for horizon in horizons: print(f\"Training model for horizon: {horizon}\") X_train, y_train = pipeline.create_multi_step_features(df_train, horizon) X_test, y_test = pipeline.create_multi_step_features(df_test, horizon) if X_train.empty or y_train.empty or X_test.empty or y_test.empty: print(f\"Insufficient data for horizon {horizon}. Skipping.\") continue model = ForecastModel() model.fit(X_train, y_train) evaluation = ForecastModel.evaluate(model, X_train, y_train, X_test, y_test) results[horizon] = evaluation if results: plot_results(results) else: print(\"No valid results to plot.\") return results def plot_results(results): horizons = list(results.keys()) test_rmse = [results[h][f'pm2.5_t+{h}']['test_rmse'] for h in horizons] test_mae = [results[h][f'pm2.5_t+{h}']['test_mae'] for h in horizons] plt.figure(figsize=(12, 6)) plt.plot(horizons, test_rmse, marker='o', label='RMSE') plt.plot(horizons, test_mae, marker='s', label='MAE') plt.xlabel('Forecast Horizon (hours)') plt.ylabel('Error') plt.title('Forecast Error vs Horizon') plt.legend() plt.grid(True) plt.show() if __name__ == \"__main__\": train_path = './data/train.csv' test_path = './data/test.csv' horizons = [1, 3, 6, 12, 24] results = run_experiment(train_path, test_path, horizons) for horizon, metrics in results.items(): print(f\"\\nHorizon: {horizon}\") for step, step_metrics in metrics.items(): print(f\" {step}:\") for metric, value in step_metrics.items(): print(f\" {metric}: {value:.4f}\")", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score import numpy as np import pandas as pd class ForecastModel: def __init__(self, n_estimators=100, random_state=42): self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state) def fit(self, X, y): self.model.fit(X, y) def predict(self, X): return self.model.predict(X) @staticmethod def evaluate(model, X_train, y_train, X_test, y_test): train_pred = model.predict(X_train) test_pred = model.predict(X_test) # Ensure all inputs are numpy arrays y_train = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train y_test = y_test.to_numpy() if isinstance(y_test, pd.Series) else y_test train_mse = mean_squared_error(y_train, train_pred) test_mse = mean_squared_error(y_test, test_pred) train_rmse = np.sqrt(train_mse) test_rmse = np.sqrt(test_mse) train_mae = mean_absolute_error(y_train, train_pred) test_mae = mean_absolute_error(y_test, test_pred) train_r2 = r2_score(y_train, train_pred) test_r2 = r2_score(y_test, test_pred) results = { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse, 'train_mae': train_mae, 'test_mae': test_mae, 'train_r2': train_r2, 'test_r2': test_r2 } return results def get_feature_importance(self, feature_names): return dict(zip(feature_names, self.model.feature_importances_))", "source": "ml_model.py"}, {"content": "import random import os import numpy as np import torch def set_seed(seed: int): \"\"\" Set the seed for reproducibility across various libraries. Args: seed (int): The seed value to be set. \"\"\" random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if using CUDA os.environ['PYTHONHASHSEED'] = str(seed) # For CUDA deterministic behavior (may impact performance) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False", "source": "seed.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim from tqdm import tqdm from callbacks.early_stopping import EarlyStoppingCallback class Trainer: def __init__(self, model, train_loader, val_loader, callbacks=None, device='cpu'): self.model = model.to(device) self.train_loader = train_loader self.val_loader = val_loader self.callbacks = callbacks or [] self.early_stopping_callback = next((cb for cb in self.callbacks if isinstance(cb, EarlyStoppingCallback)), None) self.other_callbacks = [cb for cb in self.callbacks if not isinstance(cb, EarlyStoppingCallback)] self.stop_training = False self.device = device def train(self, epochs, lr): criterion = nn.MSELoss() optimizer = optim.Adam(self.model.parameters(), lr=lr) for callback in self.callbacks: callback.model = self.model logs = {} self._call_callbacks('on_train_begin', logs) train_losses = [] val_losses = [] for epoch in range(epochs): self._call_callbacks('on_epoch_begin', epoch, logs) # Training phase self.model.train() train_loss = 0 train_pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False) for batch, (X, y) in enumerate(train_pbar): self._call_callbacks('on_batch_begin', batch, logs) X, y = X.to(self.device), y.to(self.device) optimizer.zero_grad() outputs = self.model(X) loss = criterion(outputs, y) loss.backward() optimizer.step() train_loss += loss.item() logs['loss'] = loss.item() self._call_callbacks('on_batch_end', batch, logs) train_pbar.set_postfix({'loss': f'{loss.item():.4f}'}) # Validation phase self.model.eval() val_loss = 0 self._call_callbacks('on_validation_begin', logs) val_pbar = tqdm(self.val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\", leave=False) with torch.no_grad(): for batch, (X, y) in enumerate(val_pbar): X, y = X.to(self.device), y.to(self.device) outputs = self.model(X) loss = criterion(outputs, y) val_loss += loss.item() logs['is_validation'] = True logs['predictions'] = outputs.cpu().numpy() logs['targets'] = y.cpu().numpy() self._call_callbacks('on_batch_end', batch, logs) val_pbar.set_postfix({'loss': f'{loss.item():.4f}'}) train_loss /= len(self.train_loader) val_loss /= len(self.val_loader) train_losses.append(train_loss) val_losses.append(val_loss) logs['train_loss'] = train_loss logs['val_loss'] = val_loss self._call_callbacks('on_validation_end', logs) self._call_callbacks('on_epoch_end', epoch, logs) print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}') # Check if early stopping should be triggered if self.early_stopping_callback and self.early_stopping_callback.stop_training: print(\"Early stopping triggered. Ending training.\") break # Call on_train_end for early stopping first (if it exists) if self.early_stopping_callback: self.early_stopping_callback.on_train_end(logs) # Then call on_train_end for other callbacks for callback in self.other_callbacks: callback.on_train_end(logs) return train_losses, val_losses def _call_callbacks(self, method, *args): if self.early_stopping_callback: getattr(self.early_stopping_callback, method)(*args) for callback in self.other_callbacks: getattr(callback, method)(*args)", "source": "trainer.py"}, {"content": "import os import logging import random import numpy as np import torch import hydra from omegaconf import DictConfig, OmegaConf import mlflow import wandb from datapipeline import DataPipeline from seed import set_seed from windowing import WindowGenerator from trainer import Trainer from evaluator import Evaluator from callbacks.early_stopping import EarlyStoppingCallback from callbacks.model_checkpoint import ModelCheckpointCallback from callbacks.metrics import MetricsCallback from pytorch_models.cnn_1d_model import CNN1D from pytorch_models.cnn_2d_model import CNN2D from pytorch_models.rnn_model import RNNModel from pytorch_models.lstm_model import LSTMModel import general_utils @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model\") def train_model(cfg: DictConfig): # Setup logging logger = logging.getLogger(__name__) logger.info(\"Initializing experiment...\") # Set random seeds for reproducibility set_seed(cfg.training.random_seed) # Initialize MLflow mlflow_init_status, mlflow_run = general_utils.mlflow_init( cfg, setup_mlflow=cfg.setup_mlflow, autolog=cfg.mlflow_autolog ) # Initialize Wandb if cfg.use_wandb: try: wandb_config = OmegaConf.to_container(cfg, resolve=True, throw_on_missing=True) wandb.init(project=cfg.wandb_project_name, config=wandb_config) except Exception as e: logger.error(f\"Failed to initialize wandb: {str(e)}\") logger.info(\"Continuing without wandb logging\") cfg.use_wandb = False # Log initial parameters params = { \"model_name\": cfg.model.name, \"lookback\": cfg.data.lookback, \"forecast_horizon\": cfg.data.forecast_horizon, \"batch_size\": cfg.training.batch_size, \"learning_rate\": cfg.training.learning_rate, \"epochs\": cfg.training.epochs, \"early_stopping\": cfg.training.use_early_stopping, \"early_stopping_patience\": cfg.training.early_stopping_patience, } general_utils.mlflow_log(mlflow_init_status, \"log_params\", params=params) # Initialize DataPipeline pipeline = DataPipeline() pipeline.fit(cfg.data.train_path, cfg.data.features_to_use) # Process data train_data = pipeline.prepare_rnn_data(cfg.data.train_path, cfg.data.features_to_use) val_data = pipeline.prepare_rnn_data(cfg.data.val_path, cfg.data.features_to_use) test_data = pipeline.prepare_rnn_data(cfg.data.test_path, cfg.data.features_to_use) # Create WindowGenerator instances train_gen = WindowGenerator(train_data, cfg.data.lookback, cfg.data.forecast_horizon, cfg.data.feature_columns, cfg.data.target_columns) val_gen = WindowGenerator(val_data, cfg.data.lookback, cfg.data.forecast_horizon, cfg.data.feature_columns, cfg.data.target_columns) test_gen = WindowGenerator(test_data, cfg.data.lookback, cfg.data.forecast_horizon, cfg.data.feature_columns, cfg.data.target_columns) # Create data loaders train_loader = torch.utils.data.DataLoader(train_gen, batch_size=cfg.training.batch_size, shuffle=False) val_loader = torch.utils.data.DataLoader(val_gen, batch_size=cfg.training.batch_size) test_loader = torch.utils.data.DataLoader(test_gen, batch_size=cfg.training.batch_size) # Initialize model if (cfg.model.name.lower()) == \"cnn1d\": model = CNN1D( num_features=len(cfg.data.feature_columns), lookback=cfg.data.lookback, forecast_horizon=cfg.data.forecast_horizon, output_size=len(cfg.data.target_columns), kernel_sizes=cfg.model.kernel_sizes, num_channels=cfg.model.num_channels, dropout=cfg.model.dropout ) elif (cfg.model.name.lower()) == \"cnn2d\": model = CNN2D( num_features=len(cfg.data.feature_columns), lookback=cfg.data.lookback, forecast_horizon=cfg.data.forecast_horizon, output_size=len(cfg.data.target_columns), kernel_sizes=cfg.model.kernel_sizes, num_channels=cfg.model.num_channels, dropout=cfg.model.dropout ) elif (cfg.model.name.lower()) == \"rnn\": model = RNNModel( input_size=len(cfg.data.feature_columns), hidden_size=cfg.model.hidden_size, num_layers=cfg.model.num_layers, output_size=len(cfg.data.target_columns), forecast_horizon=cfg.data.forecast_horizon ) elif (cfg.model.name.lower()) == \"lstm\": model = LSTMModel( input_size=len(cfg.data.feature_columns), hidden_size=cfg.model.hidden_size, num_layers=cfg.model.num_layers, output_size=len(cfg.data.target_columns), forecast_horizon=cfg.data.forecast_horizon, dropout=cfg.model.dropout ) else: raise ValueError(f\"Unsupported model: {cfg.model.name}\") # Move model to the correct device device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) # Initialize callbacks callbacks = [ MetricsCallback(forecast_horizon=cfg.data.forecast_horizon), ModelCheckpointCallback(filepath=cfg.model_checkpoint_path, monitor='val_loss', mode='min'), ] if cfg.training.use_early_stopping: early_stopping = EarlyStoppingCallback(patience=cfg.training.early_stopping_patience) callbacks.append(early_stopping) # Initialize Trainer trainer = Trainer(model, train_loader, val_loader, callbacks, device) # Train the model train_losses, val_losses = trainer.train(cfg.training.epochs, cfg.training.learning_rate) # Log the stopped epoch if cfg.training.use_early_stopping: stopped_epoch = early_stopping.stopped_epoch if stopped_epoch is not None: mlflow.log_metric(\"stopped_epoch\", stopped_epoch + 1) if cfg.use_wandb: wandb.log({\"stopped_epoch\": stopped_epoch + 1}) logger.info(f\"Training stopped at epoch {stopped_epoch + 1}\") else: logger.info(\"Training completed without early stopping\") # Log training history for epoch, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses)): mlflow.log_metric(\"train_loss\", train_loss, step=epoch) mlflow.log_metric(\"val_loss\", val_loss, step=epoch) if cfg.use_wandb: wandb.log({\"train_loss\": train_loss, \"val_loss\": val_loss}, step=epoch) # Evaluate the model evaluator = Evaluator(model, test_loader, cfg.data.forecast_horizon, device) test_metrics = evaluator.evaluate() # Log test metrics for metric_name, metric_value in test_metrics.items(): mlflow.log_metric(f\"test_{metric_name}\", metric_value) if cfg.use_wandb: wandb.log({f\"test_{metric_name}\": metric_value}) logger.info(f\"Test metrics: {test_metrics}\") # Plot predictions for all samples evaluator.plot_predictions(num_samples=None) # None will plot all samples mlflow.log_artifact(\"predictions_plot.png\") if cfg.use_wandb: wandb.log({\"predictions_plot\": wandb.Image(\"predictions_plot.png\")}) # End MLflow run if mlflow_init_status: mlflow.end_run() # End Wandb run if cfg.use_wandb: wandb.finish() logger.info(\"Training and evaluation completed.\") return test_metrics['RMSE'] if __name__ == \"__main__\": train_model()", "source": "train_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, forecast_horizon, feature_columns, label_columns): self.data = torch.FloatTensor(data[feature_columns + label_columns].values) self.lookback = lookback self.forecast_horizon = forecast_horizon self.length = len(data) - lookback - forecast_horizon + 1 self.feature_columns = feature_columns self.label_columns = label_columns self.n_features = len(feature_columns) self.n_labels = len(label_columns) def __len__(self): return self.length def __getitem__(self, idx): features = self.data[idx:idx+self.lookback, :self.n_features] labels = self.data[idx+self.lookback:idx+self.lookback+self.forecast_horizon, self.n_features:] return features, labels def check_shapes(self): features, labels = self[0] print(f\"Feature shape: {features.shape}\") print(f\"Label shape: {labels.shape}\")", "source": "windowing.py"}, {"content": "class Callback: def on_train_begin(self, logs=None): pass def on_train_end(self, logs=None): pass def on_epoch_begin(self, epoch, logs=None): pass def on_epoch_end(self, epoch, logs=None): pass def on_batch_begin(self, batch, logs=None): pass def on_batch_end(self, batch, logs=None): pass def on_validation_begin(self, logs=None): pass def on_validation_end(self, logs=None): pass", "source": "base.py"}, {"content": "from .base import Callback class EarlyStoppingCallback(Callback): def __init__(self, patience=5, min_delta=0.0): self.patience = patience self.min_delta = min_delta self.wait = 0 self.best_loss = float('inf') self.stopped_epoch = None self.best_weights = None self.stop_training = False def on_train_begin(self, logs=None): self.wait = 0 self.best_loss = float('inf') self.stopped_epoch = None self.best_weights = None def on_epoch_end(self, epoch, logs=None): current_loss = logs.get('val_loss') if current_loss is None: return if current_loss < self.best_loss - self.min_delta: self.best_loss = current_loss self.wait = 0 self.best_weights = {k: v.cpu().clone() for k, v in self.model.state_dict().items()} else: self.wait += 1 if self.wait >= self.patience: self.stopped_epoch = epoch self.stop_training = True print(f\"Early stopping triggered at epoch {epoch+1}\") def on_train_end(self, logs=None): if self.best_weights is not None: self.model.load_state_dict(self.best_weights) print(f\"Restored model weights from the epoch with lowest validation loss\")", "source": "early_stopping.py"}, {"content": "from .base import Callback from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score import numpy as np from trl import SFTConfig, SFTTrainer class MetricsCallback(Callback): def __init__(self, forecast_horizon): self.val_predictions = [] self.val_targets = [] self.forecast_horizon = forecast_horizon def on_validation_begin(self, logs=None): self.val_predictions = [] self.val_targets = [] def on_validation_end(self, logs=None): val_pred = np.concatenate(self.val_predictions) val_true = np.concatenate(self.val_targets) # Reshape predictions and targets val_pred = val_pred.reshape(-1, self.forecast_horizon) val_true = val_true.reshape(-1, self.forecast_horizon) # Calculate metrics for each step in the forecast horizon mse = mean_squared_error(val_true, val_pred, multioutput='raw_values') rmse = np.sqrt(mse) mae = mean_absolute_error(val_true, val_pred, multioutput='raw_values') r2 = r2_score(val_true, val_pred, multioutput='raw_values') # Calculate average metrics across all steps avg_mse = np.mean(mse) avg_rmse = np.mean(rmse) avg_mae = np.mean(mae) avg_r2 = np.mean(r2) logs['val_mse'] = avg_mse logs['val_rmse'] = avg_rmse logs['val_mae'] = avg_mae logs['val_r2'] = avg_r2 print(f\"Validation Metrics - MSE: {avg_mse:.4f}, \" f\"RMSE: {avg_rmse:.4f}, MAE: {avg_mae:.4f}, \" f\"R2: {avg_r2:.4f}\") # Log individual step metrics if needed for i in range(self.forecast_horizon): logs[f'val_mse_step_{i}'] = mse[i] logs[f'val_rmse_step_{i}'] = rmse[i] logs[f'val_mae_step_{i}'] = mae[i] logs[f'val_r2_step_{i}'] = r2[i] def on_batch_end(self, batch, logs=None): if logs.get('is_validation', False): self.val_predictions.append(logs['predictions']) self.val_targets.append(logs['targets'])", "source": "metrics.py"}, {"content": "import torch from .base import Callback class ModelCheckpointCallback(Callback): def __init__(self, filepath, monitor=\"val_loss\", mode=\"min\"): self.filepath = filepath self.monitor = monitor self.mode = mode self.best = float(\"inf\") if mode == \"min\" else float(\"-inf\") self.best_weights = None def on_epoch_end(self, epoch, logs=None): current = logs.get(self.monitor) if current is None: return if (self.mode == \"min\" and current < self.best) or ( self.mode == \"max\" and current > self.best ): print( f\"\\nEpoch {epoch+1}: {self.monitor} improved from {self.best:.4f} to {current:.4f}\" ) self.best = current self.best_weights = { k: v.cpu().clone() for k, v in self.model.state_dict().items() } def on_train_end(self, logs=None): if self.best_weights is not None: print(f\"Saving best model to {self.filepath}\") torch.save(self.best_weights, self.filepath) def set_model(self, model): self.model = model", "source": "model_checkpoint.py"}, {"content": "import torch import torch.nn as nn class CNN1D(nn.Module): def __init__( self, num_features: int, # Number of input features lookback: int, # Number of past time steps to consider forecast_horizon: int, # Number of future time steps to predict output_size: int, # Number of outputs per forecasted time step kernel_sizes: list, # List of kernel sizes for Conv1d layers num_channels: list = None, # Number of output channels for each Conv1d layer dropout: float = 0.5 # Dropout rate ): super(CNN1D, self).__init__() if num_channels is None: # If num_channels not specified, use 16, 32, 64, etc. num_channels = [16 * (2**i) for i in range(len(kernel_sizes))] assert len(kernel_sizes) == len(num_channels), \"kernel_sizes and num_channels must have the same length.\" layers = [] in_channels = num_features # Changed from lookback to num_features # Create Conv1d layers dynamically based on kernel_sizes and num_channels for idx, (k, out_ch) in enumerate(zip(kernel_sizes, num_channels)): conv = nn.Conv1d( in_channels=in_channels, out_channels=out_ch, kernel_size=k, padding=k // 2 # To maintain the same sequence length ) layers.append(conv) layers.append(nn.ReLU()) # Optionally, you can add pooling layers here # layers.append(nn.MaxPool1d(kernel_size=2)) in_channels = out_ch # Update for next layer self.conv = nn.Sequential(*layers) # After convolutions, the sequence length remains 'lookback' # Flatten the output for the fully connected layer self.flatten = nn.Flatten() # Calculate the flattened feature size # After Conv1d layers: (batch_size, last_out_channels, lookback) flattened_size = num_channels[-1] * lookback self.fc = nn.Sequential( nn.Linear(flattened_size, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, forecast_horizon * output_size) ) self.forecast_horizon = forecast_horizon self.output_size = output_size def forward(self, x): \"\"\" Forward pass of the model. Args: x: Input tensor of shape (batch_size, lookback, num_features) Returns: out: Output tensor of shape (batch_size, forecast_horizon, output_size) \"\"\" # x: (batch_size, lookback, num_features) x = x.transpose(1, 2) # After transpose: (batch_size, num_features, lookback) x = self.conv(x) # After conv: (batch_size, last_out_channels, lookback) x = self.flatten(x) # After flatten: (batch_size, last_out_channels * lookback) x = self.fc(x) # After fc: (batch_size, forecast_horizon * output_size) out = x.view(-1, self.forecast_horizon, self.output_size) # Reshape to (batch_size, forecast_horizon, output_size) return out # Example usage: if __name__ == \"__main__\": # Configuration parameters num_features = 10 # e.g., 10 different features lookback = 6 # e.g., last 6 time steps forecast_horizon = 24 # e.g., predict next 24 time steps output_size = 1 # e.g., PM2.5 value kernel_sizes = [3, 5, 3] # Example kernel sizes for Conv1d layers num_channels = [16, 32, 64] # Number of channels for each Conv1d layer # Instantiate the model model = CNN1D( num_features=num_features, lookback=lookback, forecast_horizon=forecast_horizon, output_size=output_size, kernel_sizes=kernel_sizes, num_channels=num_channels, dropout=0.3 ) # Create a dummy input tensor batch_size = 32 dummy_input = torch.randn(batch_size, lookback, num_features) # Shape: (batch_size=32, lookback=6, num_features=10) # Forward pass output = model(dummy_input) # Output shape: (batch_size=32, forecast_horizon=24, output_size=1) print(f\"Input shape: {dummy_input.shape}\") print(f\"Output shape: {output.shape}\")", "source": "cnn_1d_model.py"}, {"content": "import torch import torch.nn as nn class CNN2D(nn.Module): def __init__( self, num_features: int, # Number of input features lookback: int, # Number of past time steps to consider forecast_horizon: int, # Number of future time steps to predict output_size: int, # Number of outputs per forecasted time step kernel_sizes: list, # List of kernel sizes for Conv2d layers num_channels: list = None, # Number of output channels for each Conv2d layer dropout: float = 0.5 # Dropout rate ): super(CNN2D, self).__init__() if num_channels is None: # If num_channels not specified, use 16, 32, 64, etc. num_channels = [16 * (2**i) for i in range(len(kernel_sizes))] assert len(kernel_sizes) == len(num_channels), \"kernel_sizes and num_channels must have the same length.\" layers = [] in_channels = 1 # Start with 1 channel (grayscale image) # Create Conv2d layers dynamically based on kernel_sizes and num_channels for idx, (k, out_ch) in enumerate(zip(kernel_sizes, num_channels)): conv = nn.Conv2d( in_channels=in_channels, out_channels=out_ch, kernel_size=k, padding=(k[0] // 2, k[1] // 2) # To maintain the same dimensions ) layers.append(conv) layers.append(nn.ReLU()) # Optionally, you can add pooling layers here # layers.append(nn.MaxPool2d(kernel_size=2)) in_channels = out_ch # Update for next layer self.conv = nn.Sequential(*layers) # Calculate the flattened feature size # After Conv2d layers: (batch_size, last_out_channels, lookback, num_features) self.flatten = nn.Flatten() flattened_size = num_channels[-1] * lookback * num_features self.fc = nn.Sequential( nn.Linear(flattened_size, 128), nn.ReLU(), nn.Dropout(dropout), nn.Linear(128, forecast_horizon * output_size) ) self.forecast_horizon = forecast_horizon self.output_size = output_size def forward(self, x): \"\"\" Forward pass of the model. Args: x: Input tensor of shape (batch_size, lookback, num_features) Returns: out: Output tensor of shape (batch_size, forecast_horizon, output_size) \"\"\" # x: (batch_size, lookback, num_features) x = x.unsqueeze(1) # After unsqueeze: (batch_size, 1, lookback, num_features) x = self.conv(x) # After conv: (batch_size, last_out_channels, lookback, num_features) x = self.flatten(x) # After flatten: (batch_size, last_out_channels * lookback * num_features) x = self.fc(x) # After fc: (batch_size, forecast_horizon * output_size) out = x.view(-1, self.forecast_horizon, self.output_size) # Reshape to (batch_size, forecast_horizon, output_size) return out # Example usage: if __name__ == \"__main__\": # Configuration parameters num_features = 10 # e.g., 10 different features lookback = 6 # e.g., last 6 time steps forecast_horizon = 24 # e.g., predict next 24 time steps output_size = 1 # e.g., PM2.5 value kernel_sizes = [(3, 3), (3, 3), (3, 3)] # Example kernel sizes for Conv2d layers num_channels = [16, 32, 64] # Number of channels for each Conv2d layer # Instantiate the model model = CNN2D( num_features=num_features, lookback=lookback, forecast_horizon=forecast_horizon, output_size=output_size, kernel_sizes=kernel_sizes, num_channels=num_channels, dropout=0.3 ) # Create a dummy input tensor batch_size = 32 dummy_input = torch.randn(batch_size, lookback, num_features) # Shape: (batch_size=32, lookback=6, num_features=10) # Forward pass output = model(dummy_input) # Output shape: (batch_size=32, forecast_horizon=24, output_size=1) print(f\"Input shape: {dummy_input.shape}\") print(f\"Output shape: {output.shape}\")", "source": "cnn_2d_model.py"}, {"content": "import torch import torch.nn as nn class LSTMModel(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size, forecast_horizon=1, dropout=0.0): super(LSTMModel, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.output_size = output_size self.forecast_horizon = forecast_horizon self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout) self.fc = nn.Linear(hidden_size, output_size * forecast_horizon) def forward(self, x): batch_size = x.size(0) h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device) c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device) out, _ = self.lstm(x, (h0, c0)) out = self.fc(out[:, -1, :]) return out.view(batch_size, self.forecast_horizon, self.output_size) if __name__ == \"__main__\": # Define model parameters input_size = 10 hidden_size = 64 num_layers = 2 output_size = 1 forecast_horizon = 24 sequence_length = 6 batch_size = 32 # Create a random input tensor input_tensor = torch.randn(batch_size, sequence_length, input_size) # Initialize the model model = LSTMModel(input_size, hidden_size, num_layers, output_size, forecast_horizon, dropout=0.2) # Perform a forward pass output = model(input_tensor) # Print shapes print(f\"Input tensor shape: {input_tensor.shape}\") print(f\"Output tensor shape: {output.shape}\")", "source": "lstm_model.py"}, {"content": "import torch import torch.nn as nn class RNNModel(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size, forecast_horizon=1): super(RNNModel, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.output_size = output_size self.forecast_horizon = forecast_horizon self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, output_size * forecast_horizon) def forward(self, x): batch_size = x.size(0) h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device) out, _ = self.rnn(x, h0) out = self.fc(out[:, -1, :]) return out.view(batch_size, self.forecast_horizon, self.output_size) if __name__ == \"__main__\": # Define model parameters input_size = 10 hidden_size = 64 num_layers = 2 output_size = 1 forecast_horizon = 24 sequence_length = 6 batch_size = 32 # Create a random input tensor input_tensor = torch.randn(batch_size, sequence_length, input_size) # Initialize the model model = RNNModel(input_size, hidden_size, num_layers, output_size, forecast_horizon) # Move both model and input to the same device (e.g., MPS if available) device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\") model = model.to(device) input_tensor = input_tensor.to(device) # Perform a forward pass output = model(input_tensor) # Print shapes print(f\"Input tensor shape: {input_tensor.shape}\") print(f\"Output tensor shape: {output.shape}\")", "source": "rnn_model.py"}, {"content": "import pytest import torch from src.pytorch_models.cnn_1d_model import CNN1D @pytest.fixture def cnn_1d_model(): num_features = 10 lookback = 6 forecast_horizon = 24 output_size = 1 kernel_sizes = [3, 5, 3] num_channels = [16, 32, 64] return CNN1D( num_features=num_features, lookback=lookback, forecast_horizon=forecast_horizon, output_size=output_size, kernel_sizes=kernel_sizes, num_channels=num_channels, dropout=0.3 ) def test_cnn_1d_model_initialization(cnn_1d_model): assert isinstance(cnn_1d_model, CNN1D) assert isinstance(cnn_1d_model.conv, torch.nn.Sequential) assert isinstance(cnn_1d_model.fc, torch.nn.Sequential) def test_cnn_1d_model_forward_pass(cnn_1d_model): batch_size = 32 lookback = 6 num_features = 10 # Create a random input tensor x = torch.randn(batch_size, lookback, num_features) # Perform a forward pass output = cnn_1d_model(x) # Check the output shape expected_shape = (batch_size, cnn_1d_model.forecast_horizon, cnn_1d_model.output_size) assert output.shape == expected_shape def test_cnn_1d_model_parameters(): num_features = 10 lookback = 6 forecast_horizon = 24 output_size = 1 kernel_sizes = [3, 5, 3] num_channels = [16, 32, 64] model = CNN1D( num_features=num_features, lookback=lookback, forecast_horizon=forecast_horizon, output_size=output_size, kernel_sizes=kernel_sizes, num_channels=num_channels, dropout=0.3 ) # Check if the model parameters are correct assert model.forecast_horizon == forecast_horizon assert model.output_size == output_size # Check the number of convolutional layers assert len(model.conv) == len(kernel_sizes) * 2 # Conv1d + ReLU for each kernel def test_cnn_1d_model_different_batch_sizes(cnn_1d_model): lookback = 6 num_features = 10 for batch_size in [1, 16, 64]: x = torch.randn(batch_size, lookback, num_features) output = cnn_1d_model(x) expected_shape = (batch_size, cnn_1d_model.forecast_horizon, cnn_1d_model.output_size) assert output.shape == expected_shape", "source": "test_cnn_1d_model.py"}, {"content": "import pytest import torch from src.pytorch_models.lstm_model import LSTMModel @pytest.fixture def lstm_model(): input_size = 10 hidden_size = 64 num_layers = 2 output_size = 1 forecast_horizon = 24 return LSTMModel(input_size, hidden_size, num_layers, output_size, forecast_horizon, dropout=0.2) def test_lstm_model_initialization(lstm_model): assert isinstance(lstm_model, LSTMModel) assert isinstance(lstm_model.lstm, torch.nn.LSTM) assert isinstance(lstm_model.fc, torch.nn.Linear) def test_lstm_model_forward_pass(lstm_model): batch_size = 32 sequence_length = 6 input_size = 10 # Create a random input tensor x = torch.randn(batch_size, sequence_length, input_size) # Perform a forward pass output = lstm_model(x) # Check the output shape expected_shape = (batch_size, lstm_model.forecast_horizon, lstm_model.output_size) assert output.shape == expected_shape def test_lstm_model_parameters(): input_size = 10 hidden_size = 64 num_layers = 2 output_size = 1 forecast_horizon = 24 model = LSTMModel(input_size, hidden_size, num_layers, output_size, forecast_horizon, dropout=0.2) # Check if the model parameters are correct assert model.hidden_size == hidden_size assert model.num_layers == num_layers assert model.output_size == output_size assert model.forecast_horizon == forecast_horizon def test_lstm_model_different_batch_sizes(lstm_model): input_size = 10 sequence_length = 6 for batch_size in [1, 16, 64]: x = torch.randn(batch_size, sequence_length, input_size) output = lstm_model(x) expected_shape = (batch_size, lstm_model.forecast_horizon, lstm_model.output_size) assert output.shape == expected_shape if __name__ == \"__main__\": # Set up model parameters input_size = 10 hidden_size = 64 num_layers = 2 output_size = 1 forecast_horizon = 24 # Create the model model = LSTMModel(input_size, hidden_size, num_layers, output_size, forecast_horizon, dropout=0.2) # Create a random input tensor batch_size = 32 sequence_length = 6 x = torch.randn(batch_size, sequence_length, input_size) # Perform a forward pass output = model(x) # Print the shapes of input and output tensors print(f\"Input tensor shape: {x.shape}\") print(f\"Output tensor shape: {output.shape}\")", "source": "test_lstm_model.py"}, {"content": "import pytest import torch from src.pytorch_models.rnn_model import RNNModel @pytest.fixture def rnn_model(): input_size = 5 hidden_size = 10 num_layers = 2 output_size = 3 forecast_horizon = 4 return RNNModel(input_size, hidden_size, num_layers, output_size, forecast_horizon) def test_rnn_model_initialization(rnn_model): assert isinstance(rnn_model, RNNModel) assert isinstance(rnn_model.rnn, torch.nn.RNN) assert isinstance(rnn_model.fc, torch.nn.Linear) def test_rnn_model_forward_pass(rnn_model): batch_size = 32 sequence_length = 20 input_size = 5 # Create a random input tensor x = torch.randn(batch_size, sequence_length, input_size) # Perform a forward pass output = rnn_model(x) # Check the output shape expected_shape = (batch_size, rnn_model.forecast_horizon, rnn_model.output_size) assert output.shape == expected_shape def test_rnn_model_parameters(): input_size = 5 hidden_size = 10 num_layers = 2 output_size = 3 forecast_horizon = 4 model = RNNModel(input_size, hidden_size, num_layers, output_size, forecast_horizon) # Check if the model parameters are correct assert model.hidden_size == hidden_size assert model.num_layers == num_layers assert model.output_size == output_size assert model.forecast_horizon == forecast_horizon def test_rnn_model_different_batch_sizes(rnn_model): input_size = 5 sequence_length = 20 for batch_size in [1, 16, 64]: x = torch.randn(batch_size, sequence_length, input_size) output = rnn_model(x) expected_shape = (batch_size, rnn_model.forecast_horizon, rnn_model.output_size) assert output.shape == expected_shape", "source": "test_rnn_model.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import pandas as pd import re import traceback import nltk from nltk.stem import PorterStemmer, WordNetLemmatizer from nltk.corpus import stopwords, wordnet from nltk import pos_tag from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.pipeline import Pipeline from sklearn.naive_bayes import MultinomialNB from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score, classification_report import xgboost as xgb from catboost import CatBoostClassifier import wandb import logging from tqdm import tqdm import hashlib import os import pickle # Set up logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Download necessary NLTK data nltk.download('wordnet', quiet=True) nltk.download('averaged_perceptron_tagger', quiet=True) nltk.download('punkt', quiet=True) nltk.download('stopwords', quiet=True) # Initialize stemmers and lemmatizers porter_stemmer = PorterStemmer() lemmatizer = WordNetLemmatizer() def basic_clean(text): # Remove HTML tags text = re.sub(r'<br\\s*/?>', ' ', text) # Remove backslashes text = re.sub(r'\\\\', '', text) # Replace multiple dashes with a single space text = re.sub(r'--+', ' ', text) # Replace any number of '*' with a single space text = re.sub(r'\\*+', ' ', text) return text def alphanumeric_clean(text): return re.sub(r'[^a-zA-Z0-9\\s]', '', text) def get_wordnet_pos(tag): if tag.startswith('J'): return wordnet.ADJ elif tag.startswith('V'): return wordnet.VERB elif tag.startswith('N'): return wordnet.NOUN elif tag.startswith('R'): return wordnet.ADV else: return wordnet.NOUN def tokenize_and_process(text, config): # Clean the text if config.cleaning_method == 'basic': cleaned_text = basic_clean(text) elif config.cleaning_method == 'alphanumeric': cleaned_text = alphanumeric_clean(text) else: cleaned_text = text # Tokenize tokens = nltk.word_tokenize(cleaned_text) # Remove stopwords if config.use_stopwords: stop_words = set(stopwords.words(config.stopwords_language)) tokens = [word for word in tokens if word.lower() not in stop_words] # Apply stemming and/or lemmatization if config.use_stemming and config.use_lemmatization: tokens = [lemmatizer.lemmatize(porter_stemmer.stem(word), get_wordnet_pos(pos)) for word, pos in pos_tag(tokens)] elif config.use_stemming: tokens = [porter_stemmer.stem(word) for word in tokens] elif config.use_lemmatization: tokens = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in pos_tag(tokens)] return tokens def get_preprocess_hash(config): \"\"\"Generate a unique hash for the preprocessing configuration.\"\"\" preprocess_config = { 'cleaning_method': config.cleaning_method, 'use_stopwords': config.use_stopwords, 'stopwords_language': config.stopwords_language, 'use_stemming': config.use_stemming, 'use_lemmatization': config.use_lemmatization } config_str = str(sorted(preprocess_config.items())) return hashlib.md5(config_str.encode()).hexdigest() def preprocess_and_cache(df, config): \"\"\"Preprocess the data and cache the result, or load from cache if available.\"\"\" preprocess_hash = get_preprocess_hash(config) cache_file = f'./cache/preprocessed_{preprocess_hash}.pkl' if os.path.exists(cache_file): logging.info(f\"Loading preprocessed data from cache: {cache_file}\") with open(cache_file, 'rb') as f: return pickle.load(f) logging.info(\"Preprocessing data...\") tqdm.pandas(desc=\"Preprocessing\") df['processed_text'] = df['text'].progress_apply(lambda x: ' '.join(tokenize_and_process(x, config))) os.makedirs('./cache', exist_ok=True) with open(cache_file, 'wb') as f: pickle.dump(df['processed_text'], f) logging.info(f\"Cached preprocessed data: {cache_file}\") return df['processed_text'] def run_experiment(): try: # Start a new wandb run run = wandb.init() config = wandb.config logging.info(f\"Starting experiment with config: {config}\") # Load data logging.info(\"Loading data...\") df = pd.read_csv('./data/data.csv') logging.info(f\"Loaded {len(df)} rows of data\") # Preprocess data (now with caching) processed_text = preprocess_and_cache(df, config) # Split data logging.info(\"Splitting data...\") X_train, X_test, y_train, y_test = train_test_split( processed_text, df['label'], test_size=config.test_size, random_state=42 ) logging.info(f\"Data split: train size {len(X_train)}, test size {len(X_test)}\") # Create pipeline logging.info(f\"Creating pipeline with classifier: {config.classifier}\") if config.classifier == 'naive_bayes': clf = MultinomialNB() elif config.classifier == 'random_forest': clf = RandomForestClassifier(n_estimators=config.n_estimators) elif config.classifier == 'xgboost': clf = xgb.XGBClassifier(n_estimators=config.n_estimators, use_label_encoder=False, eval_metric='logloss') elif config.classifier == 'catboost': clf = CatBoostClassifier(n_estimators=config.n_estimators, verbose=False) # Update TfidfVectorizer to handle None for max_features tfidf = TfidfVectorizer(max_features=config.max_features if config.max_features is not None else None) pipeline = Pipeline([ ('tfidf', tfidf), ('classifier', clf) ]) # Fit the model logging.info(\"Fitting the model...\") pipeline.fit(X_train, y_train) logging.info(\"Model fitting completed\") # Make predictions", "source": "part1-expt.py"}, {"content": "logging.info(\"Making predictions...\") y_pred = pipeline.predict(X_test) logging.info(\"Predictions completed\") # Evaluate the model logging.info(\"Evaluating the model...\") accuracy = accuracy_score(y_test, y_pred) classification_rep = classification_report(y_test, y_pred, output_dict=True) # Log metrics to wandb wandb.log({ \"accuracy\": accuracy, \"precision\": classification_rep['weighted avg']['precision'], \"recall\": classification_rep['weighted avg']['recall'], \"f1-score\": classification_rep['weighted avg']['f1-score'] }) logging.info(f\"Accuracy: {accuracy}\") logging.info(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\") except Exception as e: logging.error(f\"Error during run: {e}\\n{traceback.format_exc()}\") finally: logging.info(\"Finished one run\") # Define the hyperparameter search space sweep_config = { 'method': 'bayes', 'metric': {'name': 'f1-score', 'goal': 'maximize'}, 'parameters': { 'cleaning_method': {'values': ['basic', 'alphanumeric', 'none']}, 'use_stopwords': {'values': [True, False]}, 'stopwords_language': {'values': ['english']}, 'use_stemming': {'values': [True, False]}, 'use_lemmatization': {'values': [True, False]}, 'classifier': {'values': ['naive_bayes', 'random_forest', 'xgboost', 'catboost']}, 'max_features': {'values': [1000, 5000, 10000, None]}, 'test_size': {'values': [0.2]}, 'n_estimators': {'values': [100, 200, 300]} } } # Initialize the sweep # sweep_id = wandb.sweep(sweep=sweep_config, project=\"text-classification-part1\") # Run the sweep sweep_id = \"beztectu\" logging.info(f\"Starting sweep with ID: {sweep_id}\") wandb.agent(sweep_id, function=run_experiment, count=1000, project='text-classification-part1') logging.info(\"Sweep completed\")", "source": "part1-expt.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import numpy as np import pandas as pd from sklearn.pipeline import Pipeline from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split def feature_mapping(df, **kwargs): mapping=kwargs[\"mapping\"] for col in df: df[col] = df[col].map(mapping) return df def transform(datapath): \"\"\" Read csv data from datapath. Transform workflow: Impute NaN data in \"hispanic_origin\" feature Standardise numeric features Ordinal encoding for \"education\" feature Binning into bigger \"children under 18\" group for \"detailed_household_summary_in_household\" feature One-hot encoding for categorical features Parameters: data_path [str] Data path Return: X_train [pd.DataFrame] X_test [pd.DataFrame] y_train [pd.Series] y_test [pd.Series] \"\"\" education_map = { \"Children\": 0, \"Less than 1st grade\": 0, \"1st 2nd 3rd or 4th grade\": 1, \"5th or 6th grade\": 1, \"7th and 8th grade\": 2, \"9th grade\": 2, \"10th grade\": 2, \"11th grade\": 2, \"12th grade\": 2, \"12th grade no diploma\": 2, \"High school graduate\": 3, \"Some college but no degree\": 4, \"Bachelors degree(BA AB BS)\": 5, \"Prof school degree (MD DDS DVM LLB JD)\": 5, \"Associates degree-occup /vocational\": 5, \"Associates degree-academic program\": 5, \"Masters degree(MA MS MEng MEd MSW MBA)\": 6, \"Doctorate degree(PhD EdD)\": 7 } detailed_household_summary_in_household_map = { \"Child 18 or older\": \"Child 18 or older\", \"Child under 18 never married\": \"Children under 18\", \"Child under 18 ever married\": \"Children under 18\", \"Householder\": \"Householder\", \"Spouse of householder\": \"Spouse of householder\", \"Other relative of householder\": \"Other relative of householder\", \"Nonrelative of householder\": \"Nonrelative of householder\", \"Group Quarters- Secondary individual\": \"Group Quarters- Secondary individual\" } # Load in the data df = pd.read_csv(datapath) # Dropping duplicates df = df.drop_duplicates() # Define X as input features and y as the outcome variable y = (df[\"income_group\"] ==\"50000+.\").astype(float) X = df.copy().drop([\"income_group\"], axis=1) # Columns dropped cols_to_drop = [ \"year\", \"id\", \"detailed_industry_recode\", \"detailed_occupation_recode\", \"state_of_previous_residence\", \"migration_code_change_in_msa\", \"migration_code_change_in_reg\", \"migration_code_move_within_reg\", \"migration_prev_res_in_sunbelt\", \"country_of_birth_father\", \"country_of_birth_mother\", \"country_of_birth_self\", \"detailed_household_and_family_stat\" ] X = X.drop(cols_to_drop, axis = 1) # Stating datatypes for cols numeric_cols = [ 'age', 'wage_per_hour', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'veterans_benefits'] ordinal_cols = [\"education\"] nominal_mapped_cols = ['detailed_household_summary_in_household'] nominal_cols = [ 'enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'citizenship', 'class_of_worker', 'hispanic_origin', 'major_industry_code', 'major_occupation_code', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'family_members_under_18', 'tax_filer_stat', 'region_of_previous_residence', 'fill_inc_questionnaire_for_veteran_s_admin', 'live_in_this_house_1_year_ago'] # Impute missing values for hispanic_origin X[\"hispanic_origin\"] = X[\"hispanic_origin\"].fillna(\"Na\") # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Build a preprocessing step for numeric features numeric_transformer = Pipeline(steps=[ (\"std\", StandardScaler()) ]) # Build a preprocessing step for nominal features nominal_mapper = Pipeline(steps=[ (\"nominal_mapper\", FunctionTransformer(feature_mapping, kw_args={\"mapping\":detailed_household_summary_in_household_map})), (\"ohe_after_mapping\", OneHotEncoder()) ]) nominal_transformer = Pipeline(steps=[ (\"ohe\", OneHotEncoder()) ]) # Build a preprocessing step for ordinal features ordinal_transformer = Pipeline(steps=[ (\"ordinal_mapper\", FunctionTransformer(feature_mapping, kw_args={\"mapping\":education_map})) ]) # Preprocessor pipeline preprocessor = ColumnTransformer([ (\"nominal_mapping\", nominal_mapper, nominal_mapped_cols), (\"nominal_ohe\", nominal_transformer, nominal_cols), (\"ordinal\", ordinal_transformer, ordinal_cols), (\"numeric\", numeric_transformer, numeric_cols) ]) X_train = preprocessor.fit_transform(X_train) # List of feature names feature_list = preprocessor.named_transformers_[\"nominal_mapping\"].named_steps[\"ohe_after_mapping\"].get_feature_names_out() feature_list = np.append(feature_list, preprocessor.named_transformers_[\"nominal_ohe\"].named_steps[\"ohe\"].get_feature_names_out()) feature_list = np.append(feature_list, \"education\") feature_list = np.append(feature_list, preprocessor.named_transformers_[\"numeric\"].named_steps[\"std\"].get_feature_names_out()) X_train = pd.DataFrame(X_train.toarray(), columns=feature_list) X_test = pd.DataFrame(preprocessor.transform(X_test).toarray(), columns=feature_list) return X_train, X_test, y_train, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd class TreeNode: def __init__(self, features_X, val_y, sample_prop, node_gini, depth): \"\"\" TreeNode class Parameters: features_X [np.ndarray] Features val_y [np.ndarray] Feature labels sample_prop [float] Proportion of samples in current node node_gini [float] Gini value for current node depth [int] Depth of current node \"\"\" self.features_X = features_X self.val_y = val_y self.feature_idx = None self.feature_val_split = None self.sample_prop = sample_prop self.node_gini = node_gini self.label_prob = np.mean(val_y) self.depth = depth self.left_child = None self.right_child = None def __class_count(self, l): \"\"\" Convert labels to class counts for negative and positive class Parameters: l [list_like] List of class labels Returns: Tuple for class 0 labels and class 1 labels \"\"\" l = np.array(l) return (np.invert(l.astype(bool)).astype(int), l) def __gini(self, l1, l2): \"\"\" Calculate gini score with inputs of class 0 labels and class 1 labels Returns: gini score [float] \"\"\" return 1 - np.power(np.mean(l1),2) - np.power(np.mean(l2),2) def find_best_split(self, min_gini_decrease): \"\"\" Iterate through features to returns best split which results in most decrease of gini (Decreasae of gini needs to be at least larger or equal to mini_gini_decrease) Returns None if best splitsplit not found Parameters: min_gini_decrease [float] Split when gini decrease is at least min_gini_decrease Returns: best_feature_idx [int] Feature where best split occurs best_split_val [int|float] Feature value where best split occurs best_split_idx_left [np.ndarray] Index of data for left child best_split_idx_right [np.ndarray] Index of data for right child best_split_gini_left [float] gini for left child best_split_gini_right [float] gini for right child \"\"\" max_gini_improve = min_gini_decrease best_feature_idx = None best_split_val = None best_split_idx_left = None best_split_idx_right = None for feature_idx in range(self.features_X.shape[1]): feature = self.features_X[:,feature_idx].flatten() feature_vals = np.unique(feature) # Binary features if feature_vals.shape[0] <= 2: feature_vals = [0] # Ordinal/ continuous features else: feature_vals = ((feature_vals + np.roll(feature_vals, -1))/2.0)[:-1] # If feature_vals is None, no best_split if feature_vals is None: return None for split_val in feature_vals: # Split class labels based on split_val split_left = self.val_y[feature <= split_val] split_right = self.val_y[feature > split_val] # Next loop if no split if split_left.shape[0] == 0 or split_right.shape[0] == 0: continue # Calculate gini for child nodes split_gini_left = self.__gini(self.__class_count(split_left)[0], self.__class_count(split_left)[1]) split_gini_right = self.__gini(self.__class_count(split_right)[0], self.__class_count(split_right)[1]) # Calculate gini improvement from split split_gini_improve = self.sample_prop*(self.node_gini - (split_left.shape[0] / self.val_y.shape[0] * split_gini_left) - (split_right.shape[0] / self.val_y.shape[0] * split_gini_right)) # Stores split information for split with best gini improvements above min_gini_decrease if split_gini_improve >= max_gini_improve: max_gini_improve = split_gini_improve best_feature_idx = feature_idx best_split_val = split_val best_split_gini_left = split_gini_left best_split_gini_right = split_gini_right best_split_idx_left = np.argwhere(feature <= split_val).flatten() best_split_idx_right = np.argwhere(feature > split_val).flatten() # Return None if no split found after iteration if best_feature_idx is None: return None return best_feature_idx, best_split_val, best_split_idx_left, best_split_idx_right, best_split_gini_left, best_split_gini_right class DecisionTree: def __init__(self, max_depth = None, min_gini_decrease: float = 0.0): self.root_node = None self.total_sample = None self.max_depth = max_depth self.min_gini_decrease = min_gini_decrease def __input_type_handler(self, data) -> np.ndarray: \"\"\" Converts pd.DataFrame or pd.Series to numpy arrays Returns: data [np.ndarray] \"\"\" if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series): data = data.to_numpy() else: data = np.array(data) return data def __class_count(self, l) -> tuple: \"\"\" Same implementation as that for TreeNode Convert labels to class counts for negative and positive class Parameters:", "source": "decision_tree.py"}, {"content": "l [list_like] List of class labels Returns: Tuple for class 0 labels and class 1 labels \"\"\" l = np.array(l) return (np.invert(l.astype(bool)).astype(int), l) def __gini(self, l1, l2) -> float: \"\"\" Same implementation as that for TreeNode Calculate gini score with inputs of class 0 labels and class 1 labels Returns: gini score [float] \"\"\" return 1 - np.power(np.mean(l1),2) - np.power(np.mean(l2),2) def create_tree_node(self, features_X, val_y, node_gini, depth) -> TreeNode: \"\"\" Recursviely creates child nodes until stop criteria. Implemented possible stop criterias Due to max depth reached No best split found Parameters: features_X [np.ndarray] Features val_y [np.ndarray] Feature labels sample_prop [float] Proportion of samples in current node node_gini [float] Gini value for current node depth [int] Depth of current node Returns: node [TreeNode] \"\"\" sample_prop = val_y.shape[0]/ self.total_sample node = TreeNode(features_X, val_y, sample_prop, node_gini, depth) # Stop criteria for max_depth if self.max_depth is not None and depth >= self.max_depth: return node split = node.find_best_split(self.min_gini_decrease) # Stop criteria if no best_split found if split is None: return node else: node.feature_idx, node.feature_val_split, best_split_left, best_split_right, left_gini, right_gini = split # Increase depth by 1 depth += 1 # Initialise child nodes node.left_child = self.create_tree_node(features_X[best_split_left], val_y[best_split_left], left_gini, depth) node.right_child = self.create_tree_node(features_X[best_split_right], val_y[best_split_right], right_gini, depth) return node def fit(self, train_X, train_y): \"\"\" Fit Decision tree with training data and labels Parameter: train_X [np.ndarray] train_y [np.ndarray] Returns: self [DecisionTree] Fitted decision tree \"\"\" train_X = self.__input_type_handler(train_X) train_y = self.__input_type_handler(train_y).flatten() self.total_sample = train_y.shape[0] node_gini = self.__gini(self.__class_count(train_y)[0],self.__class_count(train_y)[1]) self.root_node = self.create_tree_node(train_X, train_y, node_gini, depth=0) return self def predict(self, test_X, thresh = 0.5): \"\"\" Predict test_X using fitted Decision tree Parameter: test_X [np.ndarray] thresh [float] Threshold for binary classifications Returns: pred [list] Test predictions \"\"\" pred = [] test_X = self.__input_type_handler(test_X) for test_idx in np.arange(test_X.shape[0]): predictors_X = test_X[test_idx,:] node_ptr = self.root_node # Iterate through child nodes to reach terminal nodes while node_ptr.left_child is not None or node_ptr.right_child is not None: node_feature = predictors_X[node_ptr.feature_idx] if node_feature <= node_ptr.feature_val_split: node_ptr = node_ptr.left_child else: node_ptr = node_ptr.right_child # Append label prob of terminal nodes based on classification threshold (thresh) pred.append(int(node_ptr.label_prob>=thresh)) return pred", "source": "decision_tree.py"}, {"content": "from sklearn.metrics import f1_score from sklearn.ensemble import RandomForestClassifier class Model: def __init__(self): \"\"\" Initialise RandomForestClassifier \"\"\" self.clf = RandomForestClassifier() def train(self, params, X_train, y_train): \"\"\" Set parameters for RandomForestClassifier based on params Train classifier with X_train and y_train Returns f1 score of classifier after training Parameters: params Dictionary containing parameters for RandomForestClassifier X_train Dataframe containing training predictors y_train Dataframe containing training targets Return: f1_score Float of f1 score for training \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.clf = self.clf.set_params(**params) self.clf.fit(X_train, y_train) y_pred = self.clf.predict(X_train) return f1_score(y_train, y_pred, average=\"weighted\") def evaluate(self, X_test, y_test): \"\"\" Evaluate test data with trained clasifier Parameters: X_test Dataframe containing test predictors y_test Dataframe containing test targets Return: f1_score Float of f1 score for test \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.clf.predict(X_test) return f1_score(y_test, y_pred, average=\"weighted\") def get_default_params(self): \"\"\" Get default parameters for classifier Return: get_params() Dictionary of default parameters for tuned classifier \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model default_params = { 'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 200, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False} return default_params", "source": "model.py"}, {"content": "import pandas as pd import numpy as np class TreeNode: def __init__(self, features_X, val_y, sample_prop, node_gini, depth): \"\"\" TreeNode class Parameters: features_X [np.ndarray] Features val_y [np.ndarray] Feature labels sample_prop [float] Proportion of samples in current node node_gini [float] Gini value for current node depth [int] Depth of current node \"\"\" self.features_X = features_X self.val_y = val_y self.feature_idx = None self.feature_val_split = None self.sample_prop = sample_prop self.node_gini = node_gini self.label_prob = np.mean(val_y) self.depth = depth self.left_child = None self.right_child = None def __class_count(self, l): \"\"\" Convert labels to class counts for negative and positive class Parameters: l [list] List of class labels Returns: Tuple for class 0 labels and class 1 labels \"\"\" l = np.array(l) return (np.invert(l.astype(bool)).astype(int), l) def __gini(self, l1, l2): \"\"\" Calculate gini score with inputs of class 0 labels and class 1 labels Returns: gini score [float] \"\"\" return 1 - np.power(np.mean(l1),2) - np.power(np.mean(l2),2) def find_best_split(self, min_gini_decrease): \"\"\" Iterate through features to returns best split which results in most decrease of gini (Decreasae of gini needs to be at least larger or equal to mini_gini_decrease) Returns None if best splitsplit not found Parameters: min_gini_decrease [float] Split when gini decrease is at least min_gini_decrease Returns: best_feature_idx [int] Feature where best split occurs best_split_val [int|float] Feature value where best split occurs best_split_idx_left [np.ndarray] Index of data for left child best_split_idx_right [np.ndarray] Index of data for right child best_split_gini_left [float] gini for left child best_split_gini_right [float] gini for right child \"\"\" max_gini_improve = min_gini_decrease best_feature_idx = None best_split_val = None best_split_idx_left = None best_split_idx_right = None for feature_idx in range(self.features_X.shape[1]): feature = self.features_X[:,feature_idx].flatten() feature_vals = np.unique(feature) # Binary features if feature_vals.shape[0] <= 2: feature_vals = [0] # Ordinal/ continuous features else: feature_vals = ((feature_vals + np.roll(feature_vals, -1))/2.0)[:-1] # If feature_vals is None, no best_split if feature_vals is None: return None for split_val in feature_vals: # Split class labels based on split_val split_left = self.val_y[feature <= split_val] split_right = self.val_y[feature > split_val] # Next loop if no split if split_left.shape[0] == 0 or split_right.shape[0] == 0: continue # Calculate gini for child nodes split_gini_left = self.__gini(self.__class_count(split_left)[0], self.__class_count(split_left)[1]) split_gini_right = self.__gini(self.__class_count(split_right)[0], self.__class_count(split_right)[1]) # Calculate gini improvement from split split_gini_improve = self.sample_prop*(self.node_gini - (split_left.shape[0] / self.val_y.shape[0] * split_gini_left) - (split_right.shape[0] / self.val_y.shape[0] * split_gini_right)) # Stores split information for split with best gini improvements above min_gini_decrease if split_gini_improve >= max_gini_improve: max_gini_improve = split_gini_improve best_feature_idx = feature_idx best_split_val = split_val best_split_gini_left = split_gini_left best_split_gini_right = split_gini_right best_split_idx_left = np.argwhere(feature <= split_val).flatten() best_split_idx_right = np.argwhere(feature > split_val).flatten() # Return None if no split found after iteration if best_feature_idx is None: return None return best_feature_idx, best_split_val, best_split_idx_left, best_split_idx_right, best_split_gini_left, best_split_gini_right class DecisionTree: def __init__(self, max_depth = None, min_gini_decrease: float = 0.0): self.root_node = None self.total_sample = None self.max_depth = max_depth self.min_gini_decrease = min_gini_decrease def __input_type_handler(self, data) -> np.ndarray: \"\"\" Converts pd.DataFrame or pd.Series to numpy arrays Returns: data [np.ndarray] \"\"\" if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series): data = data.to_numpy() else: data = np.array(data) return data def __class_count(self, l) -> tuple: \"\"\" Same implementation as that for TreeNode Convert labels to class counts for negative and positive class Parameters:", "source": "random_forest.py"}, {"content": "l [list_like] List of class labels Returns: Tuple for class 0 labels and class 1 labels \"\"\" l = np.array(l) return (np.invert(l.astype(bool)).astype(int), l) def __gini(self, l1, l2) -> float: \"\"\" Same implementation as that for TreeNode Calculate gini score with inputs of class 0 labels and class 1 labels Returns: gini score [float] \"\"\" return 1 - np.power(np.mean(l1),2) - np.power(np.mean(l2),2) def create_tree_node(self, features_X, val_y, node_gini, depth) -> TreeNode: \"\"\" Recursviely creates child nodes until stop criteria. Implemented possible stop criterias Due to max depth reached No best split found Parameters: features_X [np.ndarray] Features val_y [np.ndarray] Feature labels sample_prop [float] Proportion of samples in current node node_gini [float] Gini value for current node depth [int] Depth of current node Returns: node [TreeNode] \"\"\" sample_prop = val_y.shape[0]/ self.total_sample node = TreeNode(features_X, val_y, sample_prop, node_gini, depth) # Stop criteria for max_depth if self.max_depth is not None and depth >= self.max_depth: return node split = node.find_best_split(self.min_gini_decrease) # Stop criteria if no best_split found if split is None: return node else: node.feature_idx, node.feature_val_split, best_split_left, best_split_right, left_gini, right_gini = split # Increase depth by 1 depth += 1 # Initialise child nodes node.left_child = self.create_tree_node(features_X[best_split_left], val_y[best_split_left], left_gini, depth) node.right_child = self.create_tree_node(features_X[best_split_right], val_y[best_split_right], right_gini, depth) return node def fit(self, train_X, train_y): \"\"\" Fit Decision tree with training data and labels Parameter: train_X [np.ndarray] train_y [np.ndarray] Returns: self [DecisionTree] Fitted decision tree \"\"\" train_X = self.__input_type_handler(train_X) train_y = self.__input_type_handler(train_y).flatten() self.total_sample = train_y.shape[0] node_gini = self.__gini(self.__class_count(train_y)[0],self.__class_count(train_y)[1]) self.root_node = self.create_tree_node(train_X, train_y, node_gini, depth=0) return self def predict(self, test_X, thresh = 0.5): \"\"\" Predict test_X using fitted Decision tree Parameter: test_X [np.ndarray] thresh [float] Threshold for binary classifications Returns: pred [list] Test predictions \"\"\" pred = [] test_X = self.__input_type_handler(test_X) for test_idx in np.arange(test_X.shape[0]): predictors_X = test_X[test_idx,:] node_ptr = self.root_node # Iterate through child nodes to reach terminal nodes while node_ptr.left_child is not None or node_ptr.right_child is not None: node_feature = predictors_X[node_ptr.feature_idx] if node_feature <= node_ptr.feature_val_split: node_ptr = node_ptr.left_child else: node_ptr = node_ptr.right_child # Append label prob of terminal nodes based on classification threshold (thresh) pred.append(int(node_ptr.label_prob>=thresh)) return pred class RandomForest: def __init__( self, n_trees: int = 5, subsample_size: float = 1.0, sample_with_replacement: bool = True, feature_proportion: float = None, max_depth: int = None): \"\"\" Initialise RandomForest Parameters: n_trees [int] Number of decision trees estimators in RandomForest subsample_size [float] Proportion of sample to subsample for training of each decision tree sample_with_replacement [bool] Bootstrap sampling of training data for each decision tree feature_proportion [float] Samples proportion of features to include for training of each decision tree max_depth [int] Depth \"\"\" self.n_trees = n_trees self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.max_depth = max_depth self.dts = [] def __input_type_handler(self, data) -> np.ndarray: \"\"\" Converts pd.DataFrame or pd.Series to numpy arrays Returns: data [np.ndarray] \"\"\" if isinstance(data, pd.DataFrame) or isinstance(data, pd.Series): data = data.to_numpy() else: data = np.array(data) return data def fit(self, train_X, train_y): \"\"\" Fit Random forest with training data and labels Parameter: train_X [np.ndarray] train_y [np.ndarray] Returns: self [RandomForest] Fitted random forest \"\"\" train_X = self.__input_type_handler(train_X) train_y = self.__input_type_handler(train_y).flatten() for _ in range(self.n_trees): if self.feature_proportion: bootstrap_features_idx = np.random.choice(train_X.shape[1], size=int(self.feature_proportion*train_X.shape[1]), replace=False) train_X = train_X[:,", "source": "random_forest.py"}, {"content": "bootstrap_features_idx] bootstrap_idx = np.random.choice(train_y.shape[0], size=int(self.subsample_size*(train_y.shape[0])), replace=self.sample_with_replacement) bootstrap_X = train_X[bootstrap_idx,:] bootstrap_y = train_y[bootstrap_idx] dt = DecisionTree(max_depth = self.max_depth) dt.fit(bootstrap_X, bootstrap_y) self.dts.append(dt) return self def predict(self, test_X, thresh = 0.5): \"\"\" Predict test_X using fitted Random forest Parameter: test_X [np.ndarray] thresh [float] Threshold for binary classifications Returns: pred [list] Test predictions \"\"\" dts_predict = np.array([]) for id, dt in enumerate(self.dts): dt_predict = np.array(dt.predict(test_X)) if id == 0: dts_predict = np.hstack((dts_predict, dt_predict)) else: dts_predict = np.vstack((dts_predict, dt_predict)) dts_predict = np.mean(dts_predict, axis = 0) return (dts_predict >= thresh).astype(int)", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1_test.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import os import logging import hydra import pyodbc import pandas as pd import general_utils @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"data_ingestion.yaml\") def main(args): \"\"\" This function performs data ingestion. Steps: 1. Connects to SQL Server using the given credentials. 2. Extracts the data from the 'creditcard' table. 3. Saves the raw data to a given directory. 4. Drops duplicated datapoints from the raw data. 5. Splits the data into train and test sets based on a given split ratio. 6. Saves the train and test data to a given directory. Parameters ---------- args : omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. Returns ------- None \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" )) random_state = args['random_state'] connect_to_sql = args[\"connect_to_sql\"] raw_data_dir_path = args[\"raw_data_dir_path\"] if connect_to_sql: SQL_SERVER = args['SQL_SERVER'] SQL_DATABASE = args['SQL_DATABASE'] SQL_USERID = args['SQL_USERID'] SQL_PASSWORD = args['SQL_PASSWORD'] conn_params = 'DRIVER={ODBC Driver 18 for SQL Server};SERVER=' + SQL_SERVER + ';DATABASE=' + SQL_DATABASE + ';UID=' + SQL_USERID + ';PWD=' + SQL_PASSWORD conn = pyodbc.connect(conn_params) cur = conn.cursor() logging.info(\"Connected to SQL Server\") cur.execute(\"SELECT * FROM creditcard\") raw = [] df_columns = [att[0] for att in cur.description] for row in cur.fetchall(): raw.append([data for data in row]) df = pd.DataFrame(raw, columns=df_columns) logging.info(\"Extracted data from SQL Server\") cur.close() conn.close() logging.info(\"Disconnected from SQL Server\") os.makedirs(raw_data_dir_path, exist_ok=True) df.to_csv(f\"{raw_data_dir_path}/raw_data.csv\", index=False) logging.info(f\"Raw data saved to {raw_data_dir_path}\") else: df = pd.read_csv(f\"{raw_data_dir_path}/raw_data.csv\") general_utils.reset_random_seeds(seed = random_state, random_level = 1) processed_data_dir_path = args[\"processed_data_dir_path\"] os.makedirs(processed_data_dir_path, exist_ok=True) processed_df = df.drop_duplicates() logging.info(f\"Drop duplicated datapoints\") processed_df.to_csv(f\"{processed_data_dir_path}/processed_data.csv\", index=False) logging.info(f\"Processed data saved to {processed_data_dir_path}\") train_size = args['train_size'] train = processed_df.sample(frac=train_size, random_state=random_state) test = processed_df.drop(train.index) # Checking class balance print(train[\"Class\"].value_counts(normalize=True)) print(test[\"Class\"].value_counts(normalize=True)) train.to_csv(f\"{processed_data_dir_path}/train.csv\", index=False) test.to_csv(f\"{processed_data_dir_path}/test.csv\", index=False) logging.info(f\"Train and test data saved to {processed_data_dir_path}\") logging.info(\"Completed data ingestion\") if __name__ == \"__main__\": main()", "source": "data_ingestion.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import random import yaml logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"../conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def reset_random_seeds(seed: int = 42, random_level: int = 0): \"\"\" Reset the random seeds used in the code. Parameters ---------- seed : int, optional The seed to be used for resetting the random seeds. Defaults to 42. \"\"\" os.environ['PYTHONHASHSEED']=str(seed) random.seed(seed) if random_level > 0: import numpy as np np.random.seed(seed) if random_level > 1: import tensorflow as tf tf.random.set_seed(seed)", "source": "general_utils.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import time import mlflow logger = logging.getLogger(__name__) def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) if \"JOB_NAME\" in os.environ: mlflow.set_tag(\"job_name\", os.environ.get(\"JOB_NAME\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) logger.info(\"Logged run parameters to mlflow\") except Exception as error: logger.error(error)", "source": "mlflow_utils.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.w1 = np.random.uniform(-1,1,size=(input_size,hidden_size)) * 0.1 self.w2 = np.random.uniform(-1,1,size=(hidden_size,output_size)) * 0.1 self.b1 = np.zeros((1,hidden_size)) self.b2 = np.zeros((1,output_size)) self.lr = 0.001 self.parameters = { \"input_size\": input_size, \"hidden_size\": hidden_size, \"output_size\": output_size } def __softmax__(self, z): z_shift = z - np.max(z) exp_z = np.exp(z_shift) return exp_z / np.sum(exp_z) def __relu__(self, x): return np.maximum(0, x) def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" self.input = features.reshape(1,-1) # (1, input) # first hidden layer self.h1 = np.dot(self.input, self.w1) + self.b1 # (1, hidden) # ReLU self.r = self.__relu__(self.h1) #second hidden layer -> output self.h2 = np.dot(self.r, self.w2) + self.b2 # Softmax self.s = self.__softmax__(self.h2) return self.s def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" predictions = predictions.flatten() self.label = np.zeros_like(predictions) self.label[label] = 1 return -np.sum(np.log(predictions[label])) def unit_test_loss(self): predictions = np.array([0.5,0.1,0.2]) label = 2 assert self.loss(predictions, label) == -np.log(0.2) def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" # input: 4, hidden: 16, output: 3 dLdh2 = (self.s - self.label).reshape(1,-1) # (1, output) dLdw2 = np.dot(self.r.T, dLdh2) # (hidden, output) dLdb2 = dLdh2 # (1, hidden) dLdr = np.dot(self.w2, dLdh2.T) # (hidden, output) * (output, 1) = (hidden ,1) dLdh1 = dLdr * (np.where(self.r > 0, 1, 0).reshape(-1,1)) # (hidden, 1) dLdw1 = np.dot(self.input.T, dLdh1.T) # (input, 1) * (1, hidden) = (input, hidden) dLdb1 = dLdh1.T # (1, hidden) self.w2 -= self.lr * (dLdw2/ self.parameters[\"output_size\"]) self.b2 -= self.lr * (dLdb2/ self.parameters[\"output_size\"]) self.w1 -= self.lr * (dLdw1/ self.parameters[\"output_size\"]) self.b1 -= self.lr * (dLdb1/ self.parameters[\"output_size\"]) if __name__ == \"__main__\": from mlp_datapipeline import Datapipeline seed = 42 datapipeline = Datapipeline(std=True, seed=seed) X, y = datapipeline.transform(\"./mlp-data/raw.csv\") model = MLPTwoLayers(input_size=4, hidden_size=16, output_size=3) model.unit_test_loss() #test_loss = 0 for i in range(20): test_loss = 0 for i in range(0, 149): preds = model.forward(X[0]) test_loss += model.loss(preds, y[i]) model.backward() print(test_loss / 150)", "source": "mlp.py"}, {"content": "import os import pyodbc import pandas as pd from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, std: bool = True, seed: int = 42): self.seed = seed self.raw_data = None if std: self.scaler = StandardScaler() def transform(self, raw_data_path: str = None): raw_data = pd.read_csv(raw_data_path) raw_data = raw_data.drop_duplicates() raw_data = raw_data.drop(\"id\", axis=1) y = raw_data.pop(\"y\").to_numpy() X = raw_data.to_numpy() if self.scaler is not None: X = self.scaler.fit_transform(X) return X, y", "source": "mlp_datapipeline.py"}, {"content": "import os import logging import datetime import mlflow import hydra import omegaconf import tensorflow as tf from sklearn.metrics import f1_score, precision_score, recall_score import general_utils import mlflow_utils from datapipeline.datapipeline import Datapipeline from model.kerasmodel import create_keras_model from model.model_utils import is_type_csv, MLFlowCallback, f1_m, precision_m, recall_m # VS Code debugger issue with \"import imp\": https://github.com/microsoft/debugpy/issues/1531 # (C:\\Users\\justi\\.vscode\\extensions\\ms-python.debugpy-2024.10.0-win32-x64\\bundled\\libs\\debugpy\\_vendored\\pydevd\\pydev_ipython\\qt_loaders.py) @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"model_training.yaml\") def main(args): \"\"\" Main function for training a keras model. Summary of steps: - Setup logger - Unpack general and model_training args - Reset backend and random seeds - Load and transform data - Initialise mlflow - Initialising and compiling model - Train model - Log run parameters to mlflow - Save model after train - Evaluate model on test data - Log metrics to mlflow - Log model configuration to mlflow Returns: test_loss, test_f1: test loss and f1 score of the model \"\"\" # Setup logger logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" )) # Unpack general and model_training args std = args[\"standardise\"] # Whether to standardise the data resample = args[\"resampler\"] # Whether to resample the data random_state = args[\"random_state\"] # Random seed for reproducibility train_dir_path = args[\"train_dir_path\"] # Path to the training data test_dir_path = args[\"test_dir_path\"] # Path to the test data epochs = args[\"epochs\"] # Number of epochs to train the model learning_rate = args[\"learning_rate\"] # Learning rate for the model optimizer = args[\"optimizer\"] # Optimizer for the model train_batchsize = args[\"train_batchsize\"] # Batch size for training test_batchsize = args[\"test_batchsize\"] # Batch size for testing # Reset backend and random seeds tf.keras.backend.clear_session() # Reset the tensorflow backend general_utils.reset_random_seeds(seed = random_state, random_level = 2) # Reset random seeds # Validate data filepath if not is_type_csv(train_dir_path): e = f\"Path does not exists or does not point to .csv file {train_dir_path}\" logger.error(e) raise TypeError(e) if not is_type_csv(test_dir_path): e = f\"Path does not exists or does not point to .csv file {train_dir_path}\" logger.error(e) raise TypeError(e) # Loading and transforming data logger.info(\"Loading train and test data\") pipeline = Datapipeline(std=std, resample=resample, random_state=random_state) # Create a data pipeline train_X, train_y, val_X, val_y = pipeline.transform_train_data(train_dir_path, validation_frac=0.1) # Transform train data test_X, test_y = pipeline.transform_test_data(test_dir_path) # Transform test data logger.info(\"Transformed train and test data\") # Initialising mlflow mlflow_init_status, mlflow_run = mlflow_utils.mlflow_init( args, setup_mlflow=args[\"setup_mlflow\"], autolog=args[\"mlflow_autolog\"] ) # Logging run parameters to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"resampling_method\": resample, \"learning_rate\": learning_rate, \"epochs\": epochs, \"random_state\": random_state, } ) # Initialising and compiling model model = create_keras_model() # Create a keras model model.compile( loss=\"binary_crossentropy\", # Loss function optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), # Optimizer metrics=[f1_m, precision_m, recall_m] # Metrics to evaluate ) logging.info(f\"Model loaded and compiled with {optimizer}, \\ lr = {learning_rate} and batch = {train_batchsize}\") # Log model information model_checkpoint_dir_path = os.path.join(args[\"model_checkpoint_dir_path\"], args[\"mlflow_run_name\"], datetime.datetime.now().strftime('%d%m%y_%H%M%S')) os.makedirs(model_checkpoint_dir_path, exist_ok=True) # Create a directory for model checkpoints # MLFlowCallback for saving model checkpoints every {model_checkpoint_interval} epochs model.fit( x=train_X, y=train_y, batch_size=train_batchsize, epochs=epochs, verbose=1, callbacks=[MLFlowCallback( mlflow_init_status = mlflow_init_status, logger = logger, model_checkpoint_dir_path=model_checkpoint_dir_path, model_checkpoint_interval=args[\"model_checkpoint_interval\"], save_weights_only=args[\"model_checkpoint_save_weights_only\"], )], validation_data=(val_X, val_y) ) # Save the trained model trained_model_path = os.path.join( model_checkpoint_dir_path, \"trained_model.keras\" ) model.save(trained_model_path) logger.info(f\"Trained model saved to {trained_model_path}.\") # Log the trained model to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=trained_model_path, artifact_path=\"model\", ) eval_results", "source": "model_training.py"}, {"content": "= model.evaluate(test_X, test_y, batch_size=test_batchsize, verbose=1) test_loss = eval_results[0] # Get the test loss y_pred = model.predict(test_X, batch_size=test_batchsize, verbose=0).flatten() # Make predictions on test data y_pred = (y_pred > 0.5).astype(float) # Convert predictions to binary labels test_f1 = f1_score(test_y, y_pred) # Calculate F1 score test_precision = precision_score(test_y, y_pred) # Calculate precision test_recall = recall_score(test_y, y_pred) # Calculate recall # Log metrics to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_metrics\", metrics={ \"test_f1\": test_f1, \"test_precision\": test_precision, \"test_recall\": test_recall, }, ) # Log model configuration to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() # End the mlflow run else: logger.info(\"Model training has completed.\") return test_loss, test_f1 if __name__ == \"__main__\": main()", "source": "model_training.py"}, {"content": "from . import general_utils", "source": "__init__.py"}, {"content": "import pandas as pd from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, std: bool = True, resample: str = None, random_state: int = 42): \"\"\" Parameters ---------- std : bool, optional Whether to standardize the data, by default True resample : str, optional Whether to resample the data to make it balanced, by default None Options are 'smote' and 'smoteenn' random_state : int, optional The random state for the resamplers, by default 42 \"\"\" resample = str(resample).lower() if resample not in [ \"smote\", \"smoteenn\", \"smotetomek\", \"blsmote\", \"tomeklinks\", \"enn\", \"none\", None]: raise NotImplementedError(\"Resampler not implemented.\") self.random_state = random_state self.resampler = None self.scaler = None if std: self.scaler = StandardScaler() if resample == \"smote\": from imblearn.over_sampling import SMOTE self.resampler = SMOTE(random_state=self.random_state) elif resample == \"smoteenn\": from imblearn.combine import SMOTEENN self.resampler = SMOTEENN(random_state=self.random_state) elif resample == \"smotetomek\": from imblearn.combine import SMOTETomek self.resampler = SMOTETomek(random_state=self.random_state) elif resample == \"blsmote\": from imblearn.under_sampling import BorderlineSMOTE self.resampler = BorderlineSMOTE(random_state=self.random_state) elif self.resampler == \"tomeklinks\": from imblearn.under_sampling import TomekLinks self.resampler = TomekLinks(random_state=self.random_state) elif resample == \"enn\": from imblearn.under_sampling import EditedNearestNeighbours self.resampler = EditedNearestNeighbours() def transform_train_data(self, train_data_path: str, validation_frac: float = 0.1): \"\"\" Parameters ---------- train_data_path : str The path to the training data validation_frac : float, optional The fraction of the training data to use for validation, by default 0.1 Returns ------- If validation_frac > 0, returns four arrays: train_X, train_y, val_X, val_y If validation_frac == 0, returns two arrays: train_X, train_y All return types are pd.DataFrame and pd.Series \"\"\" train_df = pd.read_csv(train_data_path) if validation_frac > 0: print(train_df[\"Class\"].value_counts(normalize=True)) val_df = train_df.sample(frac=validation_frac, random_state=self.random_state) print(val_df[\"Class\"].value_counts(normalize=True)) train_df = train_df.drop(val_df.index) train_y = train_df.pop(\"Class\") train_X = train_df feature_names = train_X.columns.tolist() val_y = val_df.pop(\"Class\") val_X = val_df if self.scaler is not None: train_X = self.scaler.fit_transform(train_X) val_X = self.scaler.transform(val_X) if self.resampler is not None: train_X, train_y = self.resampler.fit_resample(train_X, train_y) val_X = pd.DataFrame(val_X, columns=feature_names).astype(float) val_y = pd.Series(val_y).astype(float) train_X = pd.DataFrame(train_X, columns=feature_names).astype(float) train_y = pd.Series(train_y).astype(float) return train_X, train_y, val_X, val_y else: train_y = train_df.pop(\"Class\") train_X = train_df feature_names = train_X.columns.tolist() if self.scaler is not None: train_X = self.scaler.fit_transform(train_X) if self.resampler is not None: train_X, train_y = self.resampler.fit_resample(train_X, train_y) train_X = pd.DataFrame(train_X, columns=feature_names).astype(float) train_y = pd.Series(train_y).astype(float) return train_X, train_y def transform_test_data(self, test_data_path: str): \"\"\" Parameters ---------- test_data_path : str The path to the test data Returns ------- test_X : pd.DataFrame The transformed test data test_y : pd.Series The target variable for the test data \"\"\" test_df = pd.read_csv(test_data_path) test_y = test_df.pop(\"Class\") test_X = test_df feature_names = test_X.columns.tolist() if self.scaler is not None: test_X = self.scaler.transform(test_X) test_X = pd.DataFrame(test_X, columns=feature_names).astype(float) test_y = pd.Series(test_y).astype(float) return test_X, test_y", "source": "datapipeline.py"}, {"content": "import tensorflow as tf # class KerasModel(tf.keras.Model): # def __init__(self, # input_shape: tuple = (29,), # output_shape: int = 1, # output_function: str = \"sigmoid\"): # super().__init__() # self.input_shape = input_shape # self.input_layer = tf.keras.Input(shape=input_shape) # self.dense1 = tf.keras.layers.Dense(128, activation=\"relu\") # self.dense2 = tf.keras.layers.Dense(64, activation=\"relu\") # self.dense3 = tf.keras.layers.Dense(16, activation=\"relu\") # self.dense4 = tf.keras.layers.Dense(output_shape, activation=\"sigmoid\") # def call(self): # x = self.input_layer(x) # x = self.dense1(x) # x = self.dense2(x) # x = self.dense3(x) # x = self.dense4(x) # return x def create_keras_model( input_shape: tuple = (29,), output_shape: int = 1, output_function: str = \"sigmoid\"): model = tf.keras.Sequential([ tf.keras.Input(shape=input_shape), tf.keras.layers.Dense(64, activation=\"relu\"), tf.keras.layers.Dense(32, activation=\"relu\"), tf.keras.layers.Dense(16, activation=\"relu\"), tf.keras.layers.Dense(output_shape, activation=output_function) ]) return model", "source": "kerasmodel.py"}, {"content": "import os import logging import mlflow import tensorflow as tf class MLFlowCallback(tf.keras.callbacks.Callback): def __init__(self, mlflow_init_status: bool, logger: logging.Logger, model_checkpoint_dir_path: str = \"./models\", model_checkpoint_interval: int = 1, save_weights_only: bool = False): \"\"\" Parameters ---------- model_checkpoint_dir_path : str, optional The directory path where model checkpoints will be saved, by default \"./models\" model_checkpoint_interval : int, optional The epoch interval at which model checkpoints will be saved, by default 1 save_weights_only : bool, optional Choice to save either the entire model or only model weights, by default False \"\"\" self.mlflow_init_status = mlflow_init_status self.logger = logger self.model_checkpoint_dir_path = model_checkpoint_dir_path self.model_checkpoint_interval = model_checkpoint_interval self.save_weights_only = save_weights_only def on_epoch_end(self, epoch, logs=None): \"\"\" Called at the end of each epoch. Parameters ---------- epoch : int The current epoch number, starting from 0. logs : dict, optional Currently the output of `on_epoch_end` is not defined, by default None \"\"\" if self.mlflow_init_status: try: mlflow.log_metrics({key: logs[key] for key in logs}, step=epoch) print() self.logger.info(f\"Model logs to mlflow at epoch {epoch}.\") except Exception as error: self.logger.error(error) if epoch % self.model_checkpoint_interval == 0: if self.save_weights_only: self.model.save_weights( os.path.join(self.model_checkpoint_dir_path, f\"model_weights_epoch_{epoch}.h5\") ) self.logger.info(f\"Model weights saved at epoch {epoch}.\") else: self.model.save( os.path.join(self.model_checkpoint_dir_path, f\"model_epoch_{epoch}.keras\") ) self.logger.info(f\"Model saved at epoch {epoch}.\") def is_type_csv(path): \"\"\"Check if the given path points to a .csv file. Parameters ---------- path : str Path to be checked. Returns ------- bool True if the given path points to a .csv file, false otherwise. \"\"\" fpath, fname = os.path.split(path) return os.path.exists(fpath) and fname.endswith(\".csv\") def precision_m(y_true, y_pred): \"\"\" Calculate the precision of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- precision : float The precision of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) pred_pos = tf.cast(tf.math.count_nonzero(y_pred), dtype=tf.float32) precision = tp / (pred_pos + tf.keras.backend.epsilon()) return precision def recall_m(y_true, y_pred): \"\"\" Calculate the recall of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- recall : float The recall of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) true_pos = tf.cast(tf.math.count_nonzero(y_true), dtype=tf.float32) recall = tp / (true_pos + tf.keras.backend.epsilon()) return recall def f1_m(y_true, y_pred): \"\"\" Calculate the F1 score of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- f1_score : float The F1 score of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) pred_pos = tf.cast(tf.math.count_nonzero(y_pred), dtype=tf.float32) true_pos = tf.cast(tf.math.count_nonzero(y_true), dtype=tf.float32) precision = tp / (pred_pos + tf.keras.backend.epsilon()) recall = tp / (true_pos + tf.keras.backend.epsilon()) return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))", "source": "model_utils.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80,-685,-1028], [-618,573,-126], [265,391,-94], ] # Replace below with your response matrix_2 = [ [-128,-562,-200,-6,-24], [480,80,-685,-1028,-122], [-127,-618,573,-126,28], [924,265,391,-94,-211], [384,280,218,255,53] ] # Replace below with your response matrix_3 = [ [-128,-200,-24], [-127,573,28], [384,218,53], ]", "source": "convolved_matrices.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import random import yaml logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"../conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def reset_random_seeds(seed: int = 42, random_level: int = 0): \"\"\" Reset the random seeds used in the code. Parameters ---------- seed : int, optional The seed to be used for resetting the random seeds. Defaults to 42. \"\"\" os.environ['PYTHONHASHSEED']=str(seed) random.seed(seed) if random_level > 0: import numpy as np np.random.seed(seed) if random_level > 1: import tensorflow as tf tf.random.set_seed(seed) def pytorch_reset_random_seed(seed: int = 42, random_level: int = 0): os.environ['PYTHONHASHSEED']=str(seed) random.seed(seed) if random_level > 0: import numpy as np np.random.seed(seed) if random_level > 1: import torch torch.manual_seed(seed) torch.backends.cudnn.benchmark = False torch.backends.cudnn.deterministic = True torch.use_deterministic_algorithms(True)", "source": "general_utils.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import time import mlflow logger = logging.getLogger(__name__) def mlflow_init(args, exp_start_time = None, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. exp_start_time: str, optional Choice to send experiment start time to be tagged, by default None setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args.mlflow.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow.mlflow_exp_name) if autolog: mlflow.autolog() run_name = args.mlflow.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if exp_start_time is not None: mlflow.set_tag(\"start_time\", exp_start_time) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) if \"JOB_NAME\" in os.environ: mlflow.set_tag(\"job_name\", os.environ.get(\"JOB_NAME\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) logger.info(\"Logged run parameters to mlflow\") except Exception as error: logger.error(error)", "source": "mlflow_utils.py"}, {"content": "import os import logging import datetime import mlflow import hydra import omegaconf import torch from torchvision.datasets import ImageFolder import general_utils import mlflow_utils from model.pytorch_multiclass_model import MultiClassClassifier from model.pytorch_model_utils import EarlyStopper, train_one_epoch, val_one_epoch from preprocessing.pytorch_preprocess import preprocess @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"pytorch_model_training.yaml\") def main(args): \"\"\" Main function for training a tensorflow model. Summary of steps: - Setup logger - Initialise mlflow - Reduce model memory footprint - Unpack general and model_training args - Load image data - Create and compile model - Train model - Save model after training - Log model configuration to mlflow - Evaluate model on test data - Log evaluation metrics to mlflow - Log model to mlflow - Save trained model Returns: test_loss, test_accuracy: test loss and accuracy of the model \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" )) exp_start_time = datetime.datetime.now().strftime('%d%m%y_%H%M%S') mlflow_init_status, mlflow_run = mlflow_utils.mlflow_init( args, exp_start_time=exp_start_time, setup_mlflow=args.mlflow.setup_mlflow, autolog=args.mlflow.mlflow_autolog ) if not mlflow_init_status: raise Exception(\"Failed to initialise mlflow\") # Unpack general and model_training args random_state = args.general.random_state device = args.general.device if device is None: device = \"cuda\" if torch.cuda.is_available() else \"cpu\" image_data_path = args.preprocess.image_data_path if not os.path.exists(image_data_path): logger.error(f\"Path {image_data_path} does not exist\") dropout = args.model.dropout dense1_out = args.model.dense1_out patience = args.model.patience epochs = args.model.epochs learning_rate = args.model.learning_rate train_batchsize = args.model.train_batchsize general_utils.pytorch_reset_random_seed(seed = random_state, random_level = 2) logger.info(\"Loading image data\") train, test = preprocess(args) logger.info(\"Loading image data loaded\") mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"dense1_out_nodes\": dense1_out, \"dropout_rate\": dropout, \"learning_rate\": learning_rate, \"epochs\": epochs, \"random_state\": random_state, \"batch_size\": train_batchsize, } ) model_checkpoint_dir_path = os.path.join(args.mlflow.model_checkpoint_dir_path, args.mlflow.mlflow_run_name, datetime.datetime.now().strftime('%d%m%y_%H%M%S')) os.makedirs(model_checkpoint_dir_path, exist_ok=True) # Initialising and compiling model model = MultiClassClassifier(dropout_chance=dropout, dense1_out=dense1_out).to(device) criterion = torch.nn.CrossEntropyLoss() optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate) if patience > 0: earlystopper = EarlyStopper(patience=patience) logger.info(\"Model loaded\") for epoch in range(epochs): train_loss, train_acc = train_one_epoch(model, criterion, optimiser, train) val_loss, val_acc = val_one_epoch(model, criterion, test) logger.info(f\"[Epoch {epoch+1}] Train_loss: {train_loss:.6f} Train_acc: {train_acc:.4f} Val_loss: {val_loss:.6f} Val_acc: {val_acc:.4f}\") if earlystopper is not None and earlystopper.early_stop(model, val_loss): logger.info(f\"Early stopping at epoch {epoch}\") break if mlflow_init_status: try: mlflow.log_metrics({ \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_acc\": train_acc, \"val_acc\": val_acc}, step=epoch) print() logger.info(f\"Model logs to mlflow at epoch {epoch}.\") except Exception as error: logger.error(error) if epoch % args.mlflow.model_checkpoint_interval == 0: logger.info(\"Exporting the model for epoch %s.\", epoch) model_checkpoint_path = os.path.join( model_checkpoint_dir_path, \"model.pt\" ) torch.save( { \"model_state_dict\": model.state_dict(), \"epoch\": epoch, \"optimiser_state_dict\": optimiser.state_dict(), \"train_loss\": train_loss, \"test_loss\": val_loss, }, model_checkpoint_path, ) mlflow_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model_checkpoint_path, artifact_path=\"model\", ) test_loss, accuracy = val_one_epoch(model, criterion, test) # Log model configuration to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return test_loss, accuracy if __name__ == \"__main__\": main()", "source": "pytorch_train.py"}, {"content": "import os import logging import datetime import mlflow import hydra import omegaconf import tensorflow as tf import general_utils import mlflow_utils from model.tensorflow_tf_model import create_tf_model from model.model_utils import MLFlowCallback from preprocessing.preprocess_pipeline import PreprocessPipeline @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"model_training.yaml\") def main(args): \"\"\" Main function for training a tensorflow model. Summary of steps: - Setup logger - Initialise mlflow - Reduce model memory footprint - Unpack general and model_training args - Load image data - Create and compile model - Train model - Save model after training - Log model configuration to mlflow - Evaluate model on test data - Log evaluation metrics to mlflow - Log model to mlflow - Save trained model Returns: test_loss, test_accuracy: test loss and accuracy of the model \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" )) exp_start_time = datetime.datetime.now().strftime('%d%m%y_%H%M%S') mlflow_init_status, mlflow_run = mlflow_utils.mlflow_init( args, exp_start_time=exp_start_time, setup_mlflow=args.mlflow.setup_mlflow, autolog=args.mlflow.mlflow_autolog ) if not mlflow_init_status: raise Exception(\"Failed to initialise mlflow\") # Reduce model memory footprint model_weights_float16 = args.general.model_weights_float16 if model_weights_float16: tf.keras.backend.set_floatx('float16') tf.config.set_soft_device_placement(True) logger.info(tf.test.is_gpu_available()) # Unpack general and model_training args random_state = args.general.random_state image_main_path = args.preprocess.image_data_path if not os.path.exists(image_main_path): logger.error(f\"Path {image_main_path} does not exist\") pretrained_type = args.model.pretrained_type dropout = args.model.dropout hidden_nodes = args.model.hidden_nodes patience = args.model.patience epochs = args.model.epochs learning_rate = args.model.learning_rate train_batchsize = args.model.train_batchsize test_batchsize = args.model.test_batchsize tf.keras.backend.clear_session() general_utils.reset_random_seeds(seed = random_state, random_level = 2) logger.info(\"Loading image data\") pipeline = PreprocessPipeline(args) train, val = pipeline.transform_data() logger.info(\"Loading image data loaded\") mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"brightness\": args.preprocess.random_augmentations.random_brightness, \"contrast\": args.preprocess.random_augmentations.random_contrast, \"saturation\": args.preprocess.random_augmentations.random_saturation, \"hue\": args.preprocess.random_augmentations.random_hue, \"rotate\": args.preprocess.random_augmentations.random_rotate, \"shear\": args.preprocess.random_augmentations.random_shear, \"zoom\": args.preprocess.random_augmentations.random_zoom_minimum, \"flip_left_right\": args.preprocess.random_augmentations.random_flip_left_right, \"flip_up_down\": args.preprocess.random_augmentations.random_flip_up_down, \"pre-trained_model\": pretrained_type, \"classifier_hidden_nodes\": hidden_nodes, \"dropout_rate\": dropout, \"learning_rate\": learning_rate, \"epochs\": epochs, \"random_state\": random_state, \"batch_size\": train_batchsize, } ) model_checkpoint_dir_path = os.path.join(args.mlflow.model_checkpoint_dir_path, args.mlflow.mlflow_run_name, exp_start_time) os.makedirs(model_checkpoint_dir_path, exist_ok=True) # Callbacks # MLFlowCallback for saving model checkpoints every {model_checkpoint_interval} epochs mlflow_epoch_saver = MLFlowCallback( mlflow_init_status = mlflow_init_status, logger = logger, model_checkpoint_dir_path=model_checkpoint_dir_path, model_checkpoint_interval=args.mlflow.model_checkpoint_interval, save_weights_only=args.mlflow.model_checkpoint_save_weights_only, ) callbacks = [mlflow_epoch_saver] if patience > 0: earlystopper = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=patience, verbose=1) callbacks.append(earlystopper) # Initialising and compiling model model = create_tf_model(base_model=pretrained_type) model.compile( loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), metrics=[\"categorical_accuracy\"] ) logging.info(f\"Model loaded and compiled with \\ lr = {learning_rate} and batch = {train_batchsize}\") model.fit( train, batch_size=train_batchsize, epochs=epochs, verbose=1, callbacks=callbacks, validation_data= val ) # Save the trained model trained_model_path = os.path.join( model_checkpoint_dir_path, \"trained_model.keras\" ) model.save(trained_model_path) logger.info(f\"Trained model saved to {trained_model_path}.\") # Log the trained model to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=trained_model_path, artifact_path=\"model\", ) eval_results = model.evaluate(val, batch_size=test_batchsize, verbose=1) test_loss, accuracy = eval_results # Log model configuration to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return test_loss, accuracy if __name__ == \"__main__\": main()", "source": "train.py"}, {"content": "import os import PIL import numpy as np import random from typing import Union, Callable class TensorFoodDataset: def __init__(self, main_dir_path: str, preprocessor: Callable, image_shape: tuple = None, seed: int = 42): \"\"\" Class attribute description Parameters ------- main_dir_path: str The path to the directory containing the dataset. preprocessor: Callable Function that takes in the input and returns the preprocessed image. Preprocessing will be done on all images across train, test, val if split. seed: int, optional For reproducibility of train_test_val_split. Attributes ------- seed: For reproducibility of train_test_val_split. image_shape: Standardised image shape across dataset preprocessor: Function that takes in the input and returns the preprocessed image. Preprocessing will be done on all images across train, test, val if split. class_dict: A dictionary that maps class label to a list of integer positions in img_path_dump. This is used to quickly look up the positions of a certain class in img_path_dump. The keys are class labels, and the values are lists of integer positions. img_path_dump: A list of tuples, where each tuple contains a class label and the path to the image file. This list is used to keep track of the class labels and image paths of the dataset. The elements are tuples of the form (label, path), where label is the class label and path is the path to the image file. \"\"\" self.seed = seed self.image_shape = image_shape self.preprocessor = preprocessor self.class_label = [] self.class_dict = {} self.img_path_dump = [] dir_tree = os.walk(main_dir_path) print(dir_tree) data_pos_ptr = 0 label_ptr = 0 for root, _, files in dir_tree: label = os.path.split(root)[1] for file in files: if not file.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')): continue dump_path = os.path.join(root, file) dump_buffer = (label_ptr, dump_path) self.img_path_dump.append(dump_buffer) if label_ptr not in self.class_dict: self.class_label.append(label) self.class_dict[label_ptr] = [] self.class_dict[label_ptr].append(data_pos_ptr) data_pos_ptr += 1 assert len(self.img_path_dump) == data_pos_ptr label_ptr += 1 assert len(self.class_dict.keys()) == len(self.class_label) assert len(self.img_path_dump) > 0 def __len__(self) -> int: \"\"\" Returns the total number of data points in the dataset. \"\"\" return len(self.img_path_dump) def __read_image(self, path: str) -> np.ndarray: \"\"\" Reads an image from the given path and returns it as a numpy array. Args: path: The path to the image file. Returns: A numpy array representing the image. If the image cannot be read, a message is printed to the console and None is returned. \"\"\" try: image = PIL.Image.open(path) if self.image_shape: image = image.resize(self.image_shape) image = np.asarray(image) image = self.preprocessor(image) return image except Exception as e: print(f\"{e} Failed to read image: {path}\") def __getitem__(self, idx: Union[int, list, np.ndarray] = None) -> tuple: \"\"\" Gets a data item from the dataset by its index. Parameters ------- idx: int The index of the data item to retrieve. Can be an integer, a list of integers, or a numpy array of integers. Returns ------- If idx is an integer, returns a tuple of the image and label of the requested data item. If idx is a list or numpy array of integers, returns two lists: the first list contains the images and the second list contains the labels. Raises ------- TypeError: If the type of idx is not supported.", "source": "tensorfood_dataset.py"}, {"content": "\"\"\" if not isinstance(idx, (int, list, np.ndarray)): raise TypeError(f\"Unsupported index type: {format(type(idx))}\") if isinstance(idx, int): label, path = self.img_path_dump[idx] image = self.__read_image(path) return image, label if isinstance(idx, np.ndarray): idx = idx.tolist() data_items = [self.img_path_dump[id] for id in idx] labels, paths = list(zip(*data_items)) images = [self.__read_image(path) for path in paths] images = np.vstack(images) return images, labels def __get_entire_dataset__(self) -> tuple: \"\"\" Returns the entire dataset as a tuple of two lists: the first list contains the images and the second list contains the labels. Returns ------- A tuple of two lists. The first list contains the images and the second list contains the labels. The length of the lists is equal to the total number of data items in the dataset. \"\"\" labels, paths = list(zip(*self.img_path_dump)) images = [self.__read_image(path) for path in paths] return images, labels def train_test_val_split(self, test_size: float = 0.2, val_size: float = 0.1) -> tuple: \"\"\" Splits the dataset into training, testing, and validation sets. The split is done class-wise, i.e., the same proportion of samples from each class is used for training, testing, and validation. Parameters ---------- test_size : float, optional The proportion of the dataset to include in the test split. The default is 0.2. val_size : float, optional The proportion of the dataset to include in the validation split. The default is 0.1. Returns ------- A tuple of six lists. The first three lists contain the training data and labels, the second three lists contain the testing data and labels, and the last three lists contain the validation data and labels. The length of each list is equal to the total number of data items in the dataset. \"\"\" train = [] test = [] val = [] random.seed(self.seed) np.random.seed(self.seed) for c in self.class_dict.keys(): idx = self.class_dict[c] np.random.shuffle(idx) train_num = int(len(idx) * (1 - test_size - val_size)) test_num = int(len(idx) * test_size) train.extend(idx[:train_num]) test.extend(idx[train_num : train_num + test_num]) val.extend(idx[train_num + test_num:]) train_X, train_y = self.__getitem__(train) test_X, test_y = self.__getitem__(test) val_X, val_y = self.__getitem__(val) return train_X, train_y, test_X, test_y, val_X, val_y", "source": "tensorfood_dataset.py"}, {"content": "import os import logging import mlflow import tensorflow as tf class MLFlowCallback(tf.keras.callbacks.Callback): def __init__(self, mlflow_init_status: bool, logger: logging.Logger, model_checkpoint_dir_path: str = \"./models\", model_checkpoint_interval: int = 1, save_weights_only: bool = False): \"\"\" Parameters ---------- model_checkpoint_dir_path : str, optional The directory path where model checkpoints will be saved, by default \"./models\" model_checkpoint_interval : int, optional The epoch interval at which model checkpoints will be saved, by default 1 save_weights_only : bool, optional Choice to save either the entire model or only model weights, by default False \"\"\" self.mlflow_init_status = mlflow_init_status self.logger = logger self.model_checkpoint_dir_path = model_checkpoint_dir_path self.model_checkpoint_interval = model_checkpoint_interval self.save_weights_only = save_weights_only def on_epoch_end(self, epoch, logs=None): \"\"\" Called at the end of each epoch. Parameters ---------- epoch : int The current epoch number, starting from 0. logs : dict, optional Currently the output of `on_epoch_end` is not defined, by default None \"\"\" if self.mlflow_init_status: try: mlflow.log_metrics({key: logs[key] for key in logs}, step=epoch) print() self.logger.info(f\"Model logs to mlflow at epoch {epoch}.\") except Exception as error: self.logger.error(error) if epoch % self.model_checkpoint_interval == 0: if self.save_weights_only: self.model.save_weights( os.path.join(self.model_checkpoint_dir_path, f\"model_weights_epoch_{epoch}.h5\") ) self.logger.info(f\"Model weights saved at epoch {epoch}.\") else: self.model.save( os.path.join(self.model_checkpoint_dir_path, f\"model_epoch_{epoch}.keras\") ) self.logger.info(f\"Model saved at epoch {epoch}.\") def is_type_csv(path): \"\"\"Check if the given path points to a .csv file. Parameters ---------- path : str Path to be checked. Returns ------- bool True if the given path points to a .csv file, false otherwise. \"\"\" fpath, fname = os.path.split(path) return os.path.exists(fpath) and fname.endswith(\".csv\") def precision_m(y_true, y_pred): \"\"\" Calculate the precision of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- precision : float The precision of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) pred_pos = tf.cast(tf.math.count_nonzero(y_pred), dtype=tf.float32) precision = tp / (pred_pos + tf.keras.backend.epsilon()) return precision def recall_m(y_true, y_pred): \"\"\" Calculate the recall of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- recall : float The recall of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) true_pos = tf.cast(tf.math.count_nonzero(y_true), dtype=tf.float32) recall = tp / (true_pos + tf.keras.backend.epsilon()) return recall def f1_m(y_true, y_pred): \"\"\" Calculate the F1 score of a model given the true labels and predicted labels. Parameters ---------- y_true : tf.Tensor The true labels of the data. y_pred : tf.Tensor The predicted labels of the data. Returns ------- f1_score : float The F1 score of the model. \"\"\" tp = tf.cast(tf.math.count_nonzero(y_pred * y_true), dtype=tf.float32) pred_pos = tf.cast(tf.math.count_nonzero(y_pred), dtype=tf.float32) true_pos = tf.cast(tf.math.count_nonzero(y_true), dtype=tf.float32) precision = tp / (pred_pos + tf.keras.backend.epsilon()) recall = tp / (true_pos + tf.keras.backend.epsilon()) return 2 * ((precision * recall) / (precision + recall + tf.keras.backend.epsilon()))", "source": "model_utils.py"}, {"content": "from copy import deepcopy import torch class EarlyStopper: \"\"\" EarlyStopper class Watches change in validation loss across training epochs and halt training if validation loss does not improve pass pre-specified patience value. early_stop: Returns True if counter is more than or equal to patience values counter += 1 if val_loss is more than min_val_loss counter refreshes to 0 if val_loss is less than min_val_loss Otherwise returns False get_state_dict: Returns best weights of clf based with min_val_loss \"\"\" def __init__(self, patience=3, min_delta=0): self.patience = patience self.min_delta = min_delta self.clf_best_weight = None self.counter = 0 self.min_val_loss = float('inf') def early_stop(self, model, val_loss): if val_loss < self.min_val_loss: self.min_val_loss = val_loss self.counter = 0 self.model_best_weight = deepcopy(model.state_dict()) elif val_loss > (self.min_val_loss + self.min_delta): self.counter += 1 if self.counter >= self.patience: return True return False def get_state_dict(self): return self.model_best_weight def train_one_epoch(model, criterion, optimiser, data_loader): device = model.device model.train() train_loss = 0 correct = 0 total = 0 for imgs, labels in data_loader: imgs = imgs.to(device) labels = labels.to(device) label_probs = model(imgs) _, label_preds = torch.max(torch.softmax(label_probs, dim=1), 1) loss = criterion(label_probs, labels) loss.backward() optimiser.step() optimiser.zero_grad() correct += torch.sum(label_preds == labels).cpu().item() total += len(labels.cpu()) train_loss += loss.item() * imgs.size(0) train_loss = train_loss / len(data_loader.dataset) train_acc = correct / total return train_loss, train_acc def val_one_epoch(model, criterion, data_loader): device = model.device model.eval() with torch.no_grad(): val_loss = 0 correct = 0 total = 0 for imgs, labels in data_loader: imgs = imgs.to(device) labels = labels.to(device) label_probs = model(imgs) _, label_preds = torch.max(torch.softmax(label_probs, dim=1), 1) loss = criterion(label_probs, labels) correct += torch.sum(label_preds == labels).cpu().item() total += len(labels.cpu()) val_loss += loss.item() * imgs.size(0) val_loss = val_loss / len(data_loader.dataset) val_acc = correct / total return val_loss, val_acc", "source": "pytorch_model_utils.py"}, {"content": "import torch class MultiClassClassifier(torch.nn.Module): def __init__(self, dropout_chance: float = 0.5, dense1_out: int = 120): super(MultiClassClassifier, self).__init__() self.conv1 = torch.nn.Conv2d(3, 6, 5, 1) self.batchnorm1 = torch.nn.BatchNorm2d(6) self.maxpool1 = torch.nn.MaxPool2d(2, 2) self.conv2 = torch.nn.Conv2d(6, 16, 5, 1) self.batchnorm2 = torch.nn.BatchNorm2d(16) self.flatten = torch.nn.Flatten() self.dense1 = torch.nn.Linear(179776, dense1_out) self.batchnorm3 = torch.nn.BatchNorm1d(dense1_out) self.dense2 = torch.nn.Linear(dense1_out, 84) self.dense3 = torch.nn.Linear(84, 12) self.dropout = torch.nn.Dropout(dropout_chance) @property def device(self): return self.conv1.weight.device def forward(self, x): x = self.conv1(x) x = self.batchnorm1(x) x = torch.nn.functional.relu(x) x = self.maxpool1(x) x = self.conv2(x) x = self.batchnorm2(x) x = torch.nn.functional.relu(x) x = self.flatten(x) x = self.dropout(x) x = self.dense1(x) x = self.batchnorm3(x) x = torch.nn.functional.leaky_relu(x) x = self.dense2(x) x = torch.nn.functional.leaky_relu(x) x = self.dense3(x) return x", "source": "pytorch_multiclass_model.py"}, {"content": "import tensorflow as tf TENSORFLOW_BASE_MODEL = { \"vgg16\": tf.keras.applications.VGG16, \"vgg19\": tf.keras.applications.VGG19, \"xception\": tf.keras.applications.Xception, \"inception_v3\": tf.keras.applications.InceptionV3, \"resnet50\": tf.keras.applications.ResNet50, \"mobilenet_v3\": tf.keras.applications.MobileNetV3Small, \"efficientnet_v2b0\": tf.keras.applications.EfficientNetV2B0, } def create_tf_model(base_model: str = \"vgg16\", input_shape: tuple = (224,224,3), output_classes: int = 12, dropout_rate: float = 0.4, hidden_nodes: int = 256) -> tf.keras.Model: \"\"\" Creates a Keras model with a pre-trained base model, followed by a fully connected layer with hidden_nodes number of nodes, and a softmax output layer with output_classes number of classes. Args: base_model: Pre-trained base model to use. Defaults to \"vgg16\". Weights set to not trainable. input_shape: Shape of the input data. Defaults to (224,224,3). output_classes: Number of classes to predict. Defaults to 12. dropout_rate: Dropout rate to use. Defaults to 0.4. hidden_nodes: Number of nodes in the fully connected layer. Defaults to 256. Returns: A Keras model. \"\"\" tf_base = TENSORFLOW_BASE_MODEL.get(base_model, lambda:\"invalid\") inputs = tf.keras.Input(input_shape) x = tf_base(input_tensor=inputs, weights=\"imagenet\", include_top=False, input_shape=input_shape) x.trainable = False x = tf.keras.layers.Flatten()(x.output) x = tf.keras.layers.Dense(hidden_nodes, activation=\"relu\")(x) x = tf.keras.layers.Dropout(rate=dropout_rate)(x) x = tf.keras.layers.Dense(output_classes, activation=\"softmax\")(x) model = tf.keras.Model(inputs, x) return model", "source": "tensorflow_tf_model.py"}, {"content": "import random import tensorflow as tf TENSORFLOW_PREPROCESS = { \"vgg16\": tf.keras.applications.vgg16.preprocess_input, \"vgg19\": tf.keras.applications.vgg19.preprocess_input, \"xception\": tf.keras.applications.xception.preprocess_input, \"inception_v3\": tf.keras.applications.inception_v3.preprocess_input, \"resnet50\": tf.keras.applications.resnet50.preprocess_input, \"mobilenet_v3\": tf.keras.applications.mobilenet_v3.preprocess_input, \"efficientnet_v2b0\": tf.keras.applications.efficientnet_v2.preprocess_input, } class PreprocessPipeline(): def __init__(self, args): \"\"\" __init__ function for PreprocessPipeline Parameters ---------- args : dict A dictionary of arguments from the command line Attributes ---------- random_state : int The random seed for random operations random_seed_list : list[int] List of random seed generated based on random_state for __random_augment_images seeding args : dict A dictionary of arguments from the command line preprocessor : function The preprocessor function for the input images image_size : tuple The size of the input images train : tf.data.Dataset The training dataset val : tf.data.Dataset The validation dataset \"\"\" self.random_state = args.general.random_state self.args = args.preprocess self.preprocessor = TENSORFLOW_PREPROCESS.get(args.model.pretrained_type, None) input_shape = [int(x) for x in args.preprocess.input_shape.split(\",\")] self.image_size = (input_shape[0], input_shape[1]) self.train = tf.keras.preprocessing.image_dataset_from_directory( args.preprocess.image_data_path, seed = self.random_state, color_mode = \"rgb\", image_size = self.image_size, label_mode = \"categorical\", batch_size = args.model.train_batchsize, validation_split = 0.2, subset = \"training\" ) self.val = tf.keras.preprocessing.image_dataset_from_directory( args.preprocess.image_data_path, seed = self.random_state, color_mode = \"rgb\", image_size = self.image_size, label_mode = \"categorical\", batch_size = args.model.test_batchsize, validation_split = 0.2, subset = \"validation\" ) random.seed(self.random_state) self.random_seed_list = [random.randint(0, 1000000) for _ in range(len(self.train) * args.model.train_batchsize)] self.args = args.preprocess def __preprocess_augment_image(self, image:tf.Tensor, label:tf.Tensor) -> tuple: augment_args = self.args.augmentations if augment_args.brightness > -1 and augment_args.brightness <= 1: image = tf.image.adjust_brightness(image, augment_args.brightness) if augment_args.contrast: image = tf.image.adjust_contrast(image, augment_args.contrast) if augment_args.saturation: image = tf.image.adjust_saturation(image, augment_args.saturation) if augment_args.hue > -1 and augment_args.hue <= 1: image = tf.image.adjust_hue(image, augment_args.hue) if augment_args.central_crop_frac > 0 and augment_args.central_crop_frac <= 1: image = tf.image.central_crop(image, augment_args.central_crop_frac) image = tf.image.resize(image, self.image_size) return image, label def __random_augment_image(self, image: tf.Tensor, label: tf.Tensor) -> tuple: random_augment_args = self.args.random_augmentations self.random_augment_seed = self.random_seed_list.pop(0) print(self.random_augment_seed) random.seed(self.random_augment_seed) if random_augment_args.random_brightness > 0 and random_augment_args.random_brightness <= 1: image = tf.image.random_brightness(image, max_delta=random_augment_args.random_brightness, seed=self.random_augment_seed) if random_augment_args.random_contrast: image = tf.image.random_contrast(image, lower= max(1-random_augment_args.random_contrast,0), upper= 1+random_augment_args.random_contrast, seed=self.random_augment_seed) if random_augment_args.random_saturation: image = tf.image.random_saturation(image, lower=1-random_augment_args.random_saturation, upper=1+random_augment_args.random_saturation, seed=self.random_augment_seed) if random_augment_args.random_hue > 0 and random_augment_args.random_hue <= 0.5: image = tf.image.random_hue(image, max_delta = random_augment_args.random_hue, seed=self.random_augment_seed) if random_augment_args.random_zoom_minimum and \\ random_augment_args.random_zoom_minimum > 0 and \\ random_augment_args.random_zoom_minimum < 1: zoom_value = random.random() * (1 - random_augment_args.random_zoom_minimum) + random_augment_args.random_zoom_minimum image = tf.image.central_crop(image, zoom_value) if random_augment_args.random_flip_left_right: image = tf.image.random_flip_left_right(image) if random_augment_args.random_flip_up_down: image = tf.image.random_flip_up_down(image) image = tf.image.resize(image, self.image_size) return image, label def transform_data(self): if self.args.add_manual_augmentations: self.train = self.train.map(self.__preprocess_augment_image, num_parallel_calls=tf.data.AUTOTUNE) self.val = self.val.map(self.__preprocess_augment_image, num_parallel_calls=tf.data.AUTOTUNE) if self.preprocessor: self.train = self.train.map(lambda image, label: (self.preprocessor(image), label), num_parallel_calls=tf.data.AUTOTUNE) self.val = self.val.map(lambda image, label: (self.preprocessor(image), label), num_parallel_calls=tf.data.AUTOTUNE) if self.args.add_random_augmentations: self.train = self.train.map(self.__random_augment_image, num_parallel_calls=tf.data.AUTOTUNE) return self.train, self.val", "source": "preprocess_pipeline.py"}, {"content": "import numpy as np import torch.utils.data from torchvision.transforms import v2 from torchvision.datasets import ImageFolder TRANSFORMS = v2.Compose([ v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Resize((224,224)), v2.CenterCrop(224), v2.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), ]) def preprocess(args): dataset = ImageFolder(args.preprocess.image_data_path, transform=TRANSFORMS) idx = np.arange(len(dataset)) np.random.seed(args.general.random_state) np.random.shuffle(idx) train_num = int(len(idx) * (1 - 0.2)) train_idx = idx[:train_num] test_idx = idx[train_num:] train = torch.utils.data.Subset(dataset, train_idx) test = torch.utils.data.Subset(dataset, test_idx) train_data_loader = torch.utils.data.DataLoader(train, batch_size=args.model.train_batchsize) test_data_loader = torch.utils.data.DataLoader(test, batch_size=args.model.test_batchsize) return train_data_loader, test_data_loader", "source": "pytorch_preprocess.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import pandas as pd from sklearn.preprocessing import StandardScaler class DataPipeline: def __init__(self, impute: str = None, window: int = 24): self.impute = None # if impute and impute in [\"ffill\", \"bfill\", \"rolling\", \"interpolate\"]: # self.impute = impute self.window = window self.std = StandardScaler() def __rolling_mean_imputer(self, df: pd.DataFrame, features: list, window: int = 3): features_df = df[features].copy().astype(float) meta_df = df.drop(features, axis=1) rolling_df = features_df.rolling(window, center=True, min_periods=1).mean() features_df.update(rolling_df) df = pd.concat([meta_df, features_df], axis=1) return df def __train_test_split(self, df: pd.DataFrame, test_ratio: float = 0.2, val_ratio: float = 0.0): train_size = int(len(df) * (1 - test_ratio - val_ratio)) val_size = int(len(df) * (val_ratio)) val_set = None if val_ratio <= 0.0: train_set = df.iloc[:train_size, :] test_set = df.iloc[train_size:, :] else: train_set = df.iloc[:train_size, :] val_set = df.iloc[train_size:train_size+val_size, :] test_set = df.iloc[train_size+val_size:, :] return train_set, val_set, test_set def run_data_pipeline(self, csv_path: str, rf: bool = True, test_ratio: float = 0.2, val_ratio: float = 0.0): df = pd.read_csv(csv_path) df[\"time\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]], format=\"%Y-%m-%d %H\") # cbwd_dummies may only be for 1 column if only 1 variable in cbwd + order of cbwd_dummes orders may change # Use one hot encoder for pipeline, pd.get_dummies more for notebook only cbwd_dummies = pd.get_dummies(df[\"cbwd\"], prefix=\"cbwd\").astype(int) df = pd.concat([df, cbwd_dummies], axis=1) df = df.drop([\"No\",\"year\", \"month\", \"day\", \"hour\", \"cbwd\"], axis=1) df = self.__rolling_mean_imputer(df, [\"pm2.5\", \"DEWP\", \"TEMP\", \"PRES\", \"Iws\", \"Is\", \"Ir\"], window=self.window) df = df.ffill() df = df.dropna() if rf: df = df.set_index(\"time\") df[\"pm2.5_lag12\"] = df[\"pm2.5\"].shift(12) df[\"pm2.5_lag24\"] = df[\"pm2.5\"].shift(24) df = df.dropna() return df # May be hard to remember last column is the label (separate feature and label) df = df.drop([\"time\"], axis=1) label = df.pop(\"pm2.5\") df.insert(len(df.columns), \"pm2.5\", label) if val_ratio <= 0.0: train_set, test_set = self.__train_test_split(df, test_ratio=test_ratio) train_set = self.std.fit_transform(train_set).astype(float) val_set = None test_set = self.std.transform(test_set).astype(float) else: train_set, val_set, test_set = self.__train_test_split(df, test_ratio=test_ratio, val_ratio=val_ratio) train_set = self.std.fit_transform(train_set).astype(float) val_set = self.std.transform(val_set).astype(float) test_set = self.std.transform(test_set).astype(float) return train_set, val_set, test_set", "source": "datapipeline.py"}, {"content": "import pandas as pd from datapipeline import DataPipeline from modelling.ml_model import ForecastModel def ts_train_test_split(df: pd.DataFrame, test_ratio: float = 0.2, lag: int = 1): if not isinstance(df.index, pd.DatetimeIndex): raise TypeError(\"Index must be datetime index\") df[\"target\"] = df[\"pm2.5\"].shift(-1*lag) df[\"target_diff\"] = df[\"target\"].diff().shift(-1*lag) df = df.dropna() train_size = int(len(df) * (1 - test_ratio)) train_set = df.iloc[:train_size, :] test_set = df.iloc[train_size:, :] return train_set, test_set def run_experiment(data_path: str, lags: list = [], predict_diff: bool = False, seed: int = 42): metrics_dict = {} datapipeline = DataPipeline(window=30) df = datapipeline.run_data_pipeline(data_path) for lag in lags: print(f\"Training for lag {lag}\") model = ForecastModel(seed=seed) train_set, test_set = ts_train_test_split(df, lag=lag) X_train, y_train, y_train_diff = train_set.drop([\"target\", \"target_diff\", \"pm2.5\"], axis=1), train_set[\"target\"], train_set[\"target_diff\"] X_test, y_test, y_test_diff = test_set.drop([\"target\", \"target_diff\", \"pm2.5\"], axis=1), test_set[\"target\"], test_set[\"target_diff\"] if predict_diff: model.fit(X_train, y_train_diff) else: model.fit(X_train, y_train) print(f\"Inferencing for lag {lag}\") if predict_diff: train_error, test_error, y_test_diff_pred = model.evaluate(X_train, y_train_diff, X_test, y_test_diff) else: train_error, test_error, y_test_pred = model.evaluate(X_train, y_train, X_test, y_test) metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error, \"test_error\": test_error } print(train_error, test_error) return metrics_dict if __name__ == \"__main__\": metrics = run_experiment(\"./data/PRSA_data_2010.1.1-2014.12.31.csv\", lags=[1,2,3], predict_diff=True) print(metrics)", "source": "ml_experiment.py"}, {"content": "import os import logging import datetime import mlflow import hydra import omegaconf import torch import general_utils import mlflow_utils from datapipeline import DataPipeline from windowing import WindowGenerator from modelling.ml_model import ForecastModel from modelling.rnn_model import RNNModel from modelling.cnn_model import CNN1DModel, CNN2DModel @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"model_training.yaml\") def main(args): logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" )) exp_start_time = datetime.datetime.now().strftime('%d%m%y_%H%M%S') mlflow_init_status, mlflow_run = mlflow_utils.mlflow_init( args, exp_start_time=exp_start_time, setup_mlflow=args.mlflow.setup_mlflow, autolog=args.mlflow.mlflow_autolog ) if not mlflow_init_status: raise Exception(\"Failed to initialise mlflow\") # Unpack general and model_training args random_state = args.general.random_state device = args.general.device if device is None: device = \"cuda\" if torch.cuda.is_available() else \"cpu\" data_path = args.preprocess.data_path if not os.path.exists(data_path): logger.error(f\"Path {data_path} does not exist\") # dropout = args.model.dropout # dense1_out = args.model.dense1_out # patience = args.model.patience epochs = args.model.epochs learning_rate = args.model.learning_rate train_batchsize = args.model.train_batchsize general_utils.pytorch_reset_random_seed(seed = random_state, random_level = 2) logger.info(\"Loading data\") # TODO: Args for windows, rf, val_ratio datapipeline = DataPipeline(window=30) train, val, test = datapipeline.run_data_pipeline(\"./data/PRSA_data_2010.1.1-2014.12.31.csv\", rf=False, val_ratio=0.1) logger.info(\"Data preprocessed\") # TODO: Args for lookback, lookahead train_generator = WindowGenerator(train, lookback=lookback, lookahead=lookahead) val_generator = WindowGenerator(val, lookback=lookback, lookahead=lookahead) test_generator = WindowGenerator(test, lookback=lookback, lookahead=lookahead) train_loader = torch.utils.data.DataLoader(train_generator, batch_size=train_batchsize) val_loader = torch.utils.data.DataLoader(val_generator, batch_size=train_batchsize) test_loader = torch.utils.data.DataLoader(test_generator, batch_size=train_batchsize) mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"dense1_out_nodes\": dense1_out, \"dropout_rate\": dropout, \"learning_rate\": learning_rate, \"epochs\": epochs, \"random_state\": random_state, \"batch_size\": train_batchsize, } ) model_checkpoint_dir_path = os.path.join(args.mlflow.model_checkpoint_dir_path, args.mlflow.mlflow_run_name, datetime.datetime.now().strftime('%d%m%y_%H%M%S')) os.makedirs(model_checkpoint_dir_path, exist_ok=True) # Initialising and compiling model model = MultiClassClassifier(dropout_chance=dropout, dense1_out=dense1_out).to(device) criterion = torch.nn.CrossEntropyLoss() optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate) if patience > 0: earlystopper = EarlyStopper(patience=patience) logger.info(\"Model loaded\") for epoch in range(epochs): train_loss, train_acc = train_one_epoch(model, criterion, optimiser, train) val_loss, val_acc = val_one_epoch(model, criterion, test) logger.info(f\"[Epoch {epoch+1}] Train_loss: {train_loss:.6f} Train_acc: {train_acc:.4f} Val_loss: {val_loss:.6f} Val_acc: {val_acc:.4f}\") if earlystopper is not None and earlystopper.early_stop(model, val_loss): logger.info(f\"Early stopping at epoch {epoch}\") break if mlflow_init_status: try: mlflow.log_metrics({ \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_acc\": train_acc, \"val_acc\": val_acc}, step=epoch) print() logger.info(f\"Model logs to mlflow at epoch {epoch}.\") except Exception as error: logger.error(error) if epoch % args.mlflow.model_checkpoint_interval == 0: logger.info(\"Exporting the model for epoch %s.\", epoch) model_checkpoint_path = os.path.join( model_checkpoint_dir_path, \"model.pt\" ) torch.save( { \"model_state_dict\": model.state_dict(), \"epoch\": epoch, \"optimiser_state_dict\": optimiser.state_dict(), \"train_loss\": train_loss, \"test_loss\": val_loss, }, model_checkpoint_path, ) mlflow_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model_checkpoint_path, artifact_path=\"model\", ) test_loss, accuracy = val_one_epoch(model, criterion, test) # Log model configuration to mlflow mlflow_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return test_loss, accuracy if __name__ == \"__main__\": main()", "source": "model_train.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset, DataLoader class WindowGenerator(Dataset): def __init__(self, data: np.ndarray, lookback: int = 6, lookahead: int = 1): self.data = data self.lookback = lookback self.lookahead = lookahead self.length = len(data) - (lookback + lookahead) + 1 def __len__(self): return self.length def __getitem__(self, idx): features = torch.from_numpy(self.data[idx:idx+self.lookback, :]).float() labels = torch.from_numpy(self.data[idx+self.lookback:idx+self.lookback+self.lookahead, -1]).float() return features, labels if __name__ == \"__main__\": from datapipeline import DataPipeline datapipeline = DataPipeline(window=30) train, test = datapipeline.run_data_pipeline(\"./data/PRSA_data_2010.1.1-2014.12.31.csv\", rf=False) window_generator = WindowGenerator(train, lookback=6, lookahead=2) window_loader = DataLoader(window_generator, batch_size=32) print([data for data in window_loader])", "source": "windowing.py"}, {"content": "from tqdm import tqdm import torch from .pytorch_model_utils import EarlyStopper class BaseModel(torch.nn.Module): def __init__(self): super(BaseModel, self).__init__() self.fitted = False self.device = None def train_one_epoch(self, dataloader: torch.utils.data.DataLoader): self.train() train_loss = 0.0 for inputs, labels in dataloader: inputs = inputs.to(self.device) labels = labels.to(self.device) outputs = self(inputs) loss = self.criterion(outputs, labels) train_loss += loss * inputs.size(0) self.optimiser.zero_grad() loss.backward() self.optimiser.step() train_loss = train_loss.cpu() / len(dataloader.dataset) return train_loss.detach().item() def test_one_epoch(self, dataloader: torch.utils.data.DataLoader): self.eval() test_loss = 0.0 with torch.no_grad(): for inputs, labels in dataloader: inputs = inputs.to(self.device) labels = labels.to(self.device) outputs = self(inputs) loss = self.criterion(outputs, labels) test_loss += loss * inputs.size(0) test_loss = test_loss.cpu() / len(dataloader.dataset) return test_loss.detach().item() def fit(self, train_dataloader: torch.utils.data.DataLoader, val_dataloader: torch.utils.data.DataLoader, epochs: int = 20, lr: float = 0.0001, patience: int = 0, device: str = \"cpu\"): self.criterion = torch.nn.MSELoss() self.optimiser = torch.optim.Adam(self.parameters(), lr = lr) self.device = device fit_history = { \"train_loss\": [], \"val_loss\": [] } earlystopper = EarlyStopper(patience=patience) for epoch in tqdm(range(epochs)): train_loss = self.train_one_epoch(train_dataloader) val_loss = self.test_one_epoch(val_dataloader) if patience > 0: if earlystopper.early_stop(self, val_loss): self.load_state_dict(earlystopper.get_state_dict()) print(f\"Early stopping at epoch {epoch+1}\") break fit_history[\"train_loss\"].append(train_loss) fit_history[\"val_loss\"].append(val_loss) print(f\"\\nEpoch {epoch+1}/{epochs} Train loss: {train_loss:.6f} Val loss: {val_loss:.6f}\") self.fitted = True return self, fit_history def predict(self, dataloader: torch.utils.data.DataLoader): if not self.fitted: raise Exception(\"You need to fit the model before predicting\") self.eval() with torch.no_grad(): pred = torch.tensor([]) for id, (inputs, _) in enumerate(dataloader): inputs = inputs.to(self.device) outputs = self(inputs).cpu().detach() if id == 0: pred = torch.cat((pred, outputs[0,:-1], outputs[:,-1]), dim = 0) else: pred = torch.cat((pred, outputs[:,-1]), dim = 0) return pred.numpy() def evaluate(self, dataloader: torch.utils.data.DataLoader): if not self.fitted: raise Exception(\"You need to fit the model before evaluating\") self.eval() with torch.no_grad(): mse = 0.0 for inputs, labels in dataloader: inputs = inputs.to(self.device) labels = labels.to(self.device) outputs = self(inputs) mse += self.criterion(outputs, labels) * inputs.size(0) mse = mse.cpu().detach().item() return mse", "source": "base_model.py"}, {"content": "import torch from .base_model import BaseModel class CNN1DModel(BaseModel): def __init__(self, input_channels:int , output_size: int, lookback: int): super(CNN1DModel, self).__init__() # 1D Convolutional layer self.conv1 = torch.nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, stride=1, padding=1) self.conv2 = torch.nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1) # Pooling layer to downsample self.pool = torch.nn.MaxPool1d(kernel_size=2) # Fully connected layers after convolutions self.fc1 = torch.nn.Linear(32 * ((lookback//2)//2), 64) # Assuming input sequence length is 24, adjust as needed self.fc2 = torch.nn.Linear(64, output_size) # Activation and dropout self.relu = torch.nn.ReLU() self.dropout = torch.nn.Dropout(0.3) self.flatten = torch.nn.Flatten() self.fitted = False self.device = None def forward(self, x): x = torch.permute(x, (0, 2, 1)) x = self.pool(self.relu(self.conv1(x))) # Pass through Conv2 -> ReLU -> Pooling x = self.pool(self.relu(self.conv2(x))) # Flatten the output for fully connected layers x = self.flatten(x) # Fully connected layers with ReLU and dropout x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x class CNN2DModel(BaseModel): def __init__(self, input_channels: int, output_size: int, lookback: int): super(CNN2DModel, self).__init__() self.conv1 = torch.nn.Conv2d(1, 32, kernel_size=(3,2), stride=1, padding=1) self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=(3,2), stride=1, padding=1) self.pool = torch.nn.MaxPool2d(kernel_size=2) self.fc1 = torch.nn.Linear(64 * (lookback//2) * ((input_channels + 1)//2 + 1), 64) self.fc2 = torch.nn.Linear(64, output_size) self.relu = torch.nn.ReLU() self.dropout = torch.nn.Dropout(0.3) self.flatten = torch.nn.Flatten() self.fitted = False self.device = None def forward(self, x): x = torch.unsqueeze(x, 1) x = self.pool(self.relu(self.conv1(x))) # Pass through Conv2 -> ReLU -> Pooling x = self.relu(self.conv2(x)) # Flatten the output for fully connected layers x = self.flatten(x) # Fully connected layers with ReLU and dropout x = self.relu(self.fc1(x)) x = self.dropout(x) x = self.fc2(x) return x", "source": "cnn_model.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel: def __init__(self, seed: int = 42): self.model = RandomForestRegressor(random_state=seed) def fit(self, X, y): self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): y_train_pred = self.model.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error, y_test_pred def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "from copy import deepcopy class EarlyStopper: \"\"\" EarlyStopper class Watches change in validation loss across training epochs and halt training if validation loss does not improve pass pre-specified patience value. early_stop: Returns True if counter is more than or equal to patience values counter += 1 if val_loss is more than min_val_loss counter refreshes to 0 if val_loss is less than min_val_loss Otherwise returns False get_state_dict: Returns best weights of clf based with min_val_loss \"\"\" def __init__(self, patience=3, min_delta=0): self.patience = patience self.min_delta = min_delta self.clf_best_weight = None self.counter = 0 self.min_val_loss = float('inf') def early_stop(self, model, val_loss): if val_loss < self.min_val_loss: self.min_val_loss = val_loss self.counter = 0 self.model_best_weight = deepcopy(model.state_dict()) elif val_loss > (self.min_val_loss + self.min_delta): self.counter += 1 if self.counter >= self.patience: return True return False def get_state_dict(self): return self.model_best_weight", "source": "pytorch_model_utils.py"}, {"content": "import torch import torch.utils from .base_model import BaseModel class RNNModel(BaseModel): def __init__(self, input_size, num_rnn, num_layers, output_size): super(RNNModel, self).__init__() self.num_rnn = num_rnn self.num_layers = num_layers # Define the RNN layer self.rnn = torch.nn.RNN(input_size, num_rnn, num_layers, batch_first=True) # Define the output (fully connected) layerd self.fc = torch.nn.Linear(num_rnn, output_size) self.fitted = False self.device = None def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.num_rnn).to(x.device) out, _ = self.rnn(x, h0) out = self.fc(out[:, -1, :]) return out", "source": "rnn_model.py"}, {"content": "from . import ml_model from . import base_model from . import rnn_model from . import cnn_model from . import pytorch_model_utils", "source": "__init__.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import os import numpy as np import datasets from transformers import ( AlbertTokenizer, AlbertForSequenceClassification, Trainer, TrainingArguments ) from preprocessing.pipeline import Pipeline def accuracy(eval_preds): logits, labels = eval_preds predictions = np.argmax(logits, axis=-1) accuracy = np.mean(predictions == labels) return accuracy model_name = \"albert-base-v2\" output_dir = \"./a7p3/pre-trained-1/results-2\" os.makedirs(output_dir, exist_ok=True) dataset = datasets.load_dataset(\"emotion\") tokenizer = AlbertTokenizer.from_pretrained(model_name) model = AlbertForSequenceClassification.from_pretrained(model_name, num_labels=6) preprocessor = Pipeline(tokenizer) train, val, test = preprocessor.preprocess(dataset) training_args = TrainingArguments( output_dir=output_dir, eval_strategy=\"epoch\", learning_rate=2e-5, per_device_train_batch_size=60, per_device_eval_batch_size=60, num_train_epochs=2, weight_decay=0.01, ) trainer = Trainer( model=model, args=training_args, train_dataset=train, eval_dataset=val, compute_metrics=accuracy ) trainer.train() eval_results = trainer.evaluate() print(f\"Validation Results: {eval_results}\") output_txt = os.path.join(otuput_dir, \"result.txt\") with open(output_txt, \"r\") as f: f.write(eval_results)", "source": "pretrained_model_training.py"}, {"content": "from typing import Tuple import pandas as pd import datasets from transformers import AutoTokenizer class Pipeline: def __init__(self, tokenizer: AutoTokenizer): self.tokenizer = tokenizer def __tokenize_df(self, dataset: datasets.Dataset, tokenizer: AutoTokenizer) -> datasets.Dataset: tokenized = dataset.map(lambda x: tokenizer(x[\"text\"], padding=\"max_length\", truncation=True, max_length=128), batched=True) tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"]) return tokenized def preprocess(self, dataset: datasets.Dataset) -> Tuple[datasets.Dataset, datasets.Dataset, datasets.Dataset]: train = dataset[\"train\"] val = dataset[\"validation\"] test = dataset[\"test\"] train_df = train.to_pandas() train_df = train_df.drop_duplicates().iloc[:1000,:] val_df = val.to_pandas().iloc[:200,:] test_df = test.to_pandas().iloc[:200,:] train = datasets.Dataset.from_pandas(train_df) val = datasets.Dataset.from_pandas(val_df) test = datasets.Dataset.from_pandas(test_df) train = self.__tokenize_df(train, self.tokenizer) val = self.__tokenize_df(val, self.tokenizer) test = self.__tokenize_df(test, self.tokenizer) return train, val, test if __name__ == \"__main__\": dataset = datasets.load_dataset(\"emotion\") tokenizer = AutoTokenizer.from_pretrained(\"gpt2\") tokenizer.pad_token = tokenizer.eos_token preprocessor = Pipeline(tokenizer) train, val, test = preprocessor.preprocess(dataset) print(train)", "source": "pipeline.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "from typing import List NUMERIC_FEATURES: List[str] = [ 'age', 'wage_per_hour', 'num_persons_worked_for_employer', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks' ] NOMINAL_FEATURES: List[str] = [ 'enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'citizenship', 'class_of_worker', 'detailed_industry_recode', 'detailed_occupation_recode', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'family_members_under_18', 'own_business_or_self_employed', 'year', 'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'veterans_benefits', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt' ] EDUCATION_COLUMN: str = 'education' BIRTH_COUNTRY_COLUMNS: List[str] = ['country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self'] TARGET_COLUMN: str = 'income_group' TEST_SIZE: float = 0.33 RANDOM_STATE: int = 42 INCOME_MAPPING: dict = {'- 50000.': 1, '50000+.': 0}", "source": "config.py"}, {"content": "\"\"\" This module contains the main data pipeline for transforming and preprocessing the dataset. It orchestrates various custom and standard transformers to prepare the data for machine learning models. \"\"\" from typing import Tuple import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from src.config import ( NUMERIC_FEATURES, NOMINAL_FEATURES, EDUCATION_COLUMN, BIRTH_COUNTRY_COLUMNS, TARGET_COLUMN, TEST_SIZE, RANDOM_STATE, INCOME_MAPPING ) from src.transformers.education_remapper import custom_education_transformer from src.transformers.birth_country_remapper import custom_birth_country_transformer from src.transformers.custom_label_transformer import CustomLabelTransformer from src.transformers.numeric_transformer import numeric_transformer from src.transformers.nominal_transformer import nominal_transformer def clean_data(df: pd.DataFrame) -> None: \"\"\" Clean the input DataFrame by removing null values and duplicate rows. Args: df (pd.DataFrame): The input DataFrame to be cleaned. Returns: None: The input DataFrame is modified in-place. \"\"\" df.dropna(inplace=True) df.drop_duplicates(inplace=True) def create_preprocessor() -> ColumnTransformer: \"\"\" Create a ColumnTransformer that applies appropriate transformations to different feature groups. Returns: ColumnTransformer: A preprocessor that combines all feature transformations. \"\"\" return ColumnTransformer( transformers=[ (\"custom_education_transformer\", custom_education_transformer, [EDUCATION_COLUMN]), (\"custom_birth_country_transformer\", custom_birth_country_transformer, BIRTH_COUNTRY_COLUMNS), (\"numeric_transformer\", numeric_transformer, NUMERIC_FEATURES), (\"nominal_transformer\", nominal_transformer, NOMINAL_FEATURES) ], remainder='drop' ) def create_feature_pipeline() -> Pipeline: \"\"\" Create a pipeline for feature preprocessing. Returns: Pipeline: A scikit-learn pipeline for feature transformation. \"\"\" pipeline = Pipeline(steps=[('preprocessor', create_preprocessor())]) pipeline.set_output(transform=\"pandas\") return pipeline def create_target_pipeline() -> Pipeline: \"\"\" Create a pipeline for target variable transformation. Returns: Pipeline: A scikit-learn pipeline for target transformation. \"\"\" return Pipeline(steps=[(\"custom_label_transformer\", CustomLabelTransformer(INCOME_MAPPING))]) def transform(data_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: \"\"\" Transform the input data by applying feature and target pipelines. This function reads a CSV file, cleans the data, splits it into training and testing sets, and applies the feature and target transformations. Args: data_path (str): Path to the CSV file containing the data. Returns: Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]: A tuple containing transformed training and testing data (X_train, X_test, y_train, y_test). \"\"\" # Read and clean the data df = pd.read_csv(data_path) clean_data(df) # Split features and target X = df.drop([TARGET_COLUMN], axis=1) y = df[TARGET_COLUMN] # Split into training and testing sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y ) # Create and apply feature pipeline feature_pipeline = create_feature_pipeline() print(X_train.shape) X_train_transformed = feature_pipeline.fit_transform(X_train) print(X_train_transformed.shape) X_test_transformed = feature_pipeline.transform(X_test) # Create and apply target pipeline target_pipeline = create_target_pipeline() y_train_transformed = target_pipeline.fit_transform(y_train) y_test_transformed = target_pipeline.transform(y_test) return X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed", "source": "datapipeline.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() def train(self, params, X_train, y_train): \"\"\" Initializes and trains a random forest classifier model and returns its F1 score on the training data. :param params: dict A dictionary of parameters to be used for the RandomForestClassifier. :param X_train: array-like or DataFrame The training input samples. :param y_train: array-like or DataFrame The target values (class labels) as integers or strings :return: float The F1 score of the model on the training data \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score if params: self.model.set_params(**params) self.model.fit(X_train, y_train) y_pred = self.model.predict(X_train) train_f1_score = f1_score(y_train, y_pred) # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it return train_f1_score def evaluate(self, X_test, y_test): \"\"\" Evaluates the trained model on the test data and returns its F1 score. :param X_test: array-like or DataFrame The test input samples. :param y_test: array-like The true values for the test input samples. :return: float The F1 score of the model on the test data. \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.model.predict(X_test) test_f1_score = f1_score(y_test, y_pred) return test_f1_score def get_default_params(self): \"\"\" Returns the default parameters used for training the model from scratch. :return: dict A dictionary containing the default parameters of the model. \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model # default_params = self.model.get_params() default_params = {'max_depth': 20, 'max_features': 'sqrt', 'n_estimators': 500} # optimized parameters return default_params", "source": "model.py"}, {"content": "from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder import pandas as pd from typing import List, Optional class BirthCountryRemapper(BaseEstimator, TransformerMixin): \"\"\" A custom transformer that remaps birth countries to 'United-States' or 'Outside United-States'. This transformer inherits from scikit-learn's BaseEstimator and TransformerMixin, allowing it to be used in scikit-learn Pipelines. \"\"\" def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'BirthCountryRemapper': \"\"\" Fit the transformer to the data (in this case, just store the column names). :param X: Input features :param y: Target variable (not used in this transformer) :return: self \"\"\" self.columns_ = X.columns return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Transform the input data by remapping birth countries. :param X: Input features :return: Transformed features \"\"\" X_transformed = X.map(lambda x: 'United-States' if x == 'United-States' else 'Outside United-States') return X_transformed def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the names of the output features. :param input_features: Names of the input features (not used in this transformer) :return: Names of the output features \"\"\" return self.columns_ custom_birth_country_transformer = Pipeline(steps=[ (\"birth_country_remapper\", BirthCountryRemapper()), ('onehot_encoder', OneHotEncoder(sparse_output=False, drop='first')) ])", "source": "birth_country_remapper.py"}, {"content": "from sklearn.base import BaseEstimator, TransformerMixin import pandas as pd from typing import List, Dict, Optional class CustomLabelTransformer(BaseEstimator, TransformerMixin): \"\"\" A custom transformer that maps income labels to binary values. This transformer inherits from scikit-learn's BaseEstimator and TransformerMixin, allowing it to be used in scikit-learn Pipelines. :param mapping: A dictionary mapping income labels to binary values (default: {'- 50000.': 1, '50000+.': 0}) \"\"\" def __init__(self, mapping: Optional[Dict[str, int]] = None): self.mapping = {'- 50000.': 1, '50000+.': 0} if mapping is None else mapping def fit(self, X: pd.Series, y: Optional[pd.Series] = None) -> 'CustomLabelTransformer': \"\"\" Fit the transformer to the data (in this case, just store the feature name). :param X: Input features :param y: Target variable (not used in this transformer) :return: self \"\"\" self.feature_names_ = [X.name] return self def transform(self, X: pd.Series) -> pd.Series: \"\"\" Transform the input data by mapping income labels to binary values. :param X: Input features :return: Transformed features \"\"\" return X.map(self.mapping) def inverse_transform(self, X: pd.Series) -> pd.Series: \"\"\" Inverse transform the binary values back to income labels. :param X: Binary-encoded features :return: Original income labels \"\"\" inverse_mapping = {v: k for k, v in self.mapping.items()} return X.map(inverse_mapping) def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the names of the output features. :param input_features: Names of the input features (not used in this transformer) :return: Names of the output features \"\"\" return self.feature_names_", "source": "custom_label_transformer.py"}, {"content": "from sklearn.base import BaseEstimator, TransformerMixin from sklearn.pipeline import Pipeline from sklearn.preprocessing import MinMaxScaler import pandas as pd from typing import List, Dict, Optional class EducationRemapper(BaseEstimator, TransformerMixin): \"\"\" A custom transformer that remaps education levels to numerical values. This transformer inherits from scikit-learn's BaseEstimator and TransformerMixin, allowing it to be used in scikit-learn Pipelines. :param default_value: The value to use for education levels not in the mapping (default: 0) \"\"\" def __init__(self, default_value: int = 0): self.mapping: Dict[str, int] = { 'Children': 0, 'Less than 1st grade': 1, '1st 2nd 3rd or 4th grade': 2, '5th or 6th grade': 3, '7th and 8th grade': 4, '9th grade': 5, '10th grade': 6, '11th grade': 7, '12th grade no diploma': 8, 'High school graduate': 9, 'Some college but no degree': 10, 'Associates degree-occup /vocational': 11, 'Associates degree-academic program': 12, 'Bachelors degree(BA AB BS)': 13, 'Masters degree(MA MS MEng MEd MSW MBA)': 14, 'Doctorate degree(PhD EdD)': 15, 'Prof school degree (MD DDS DVM LLB JD)': 16 } self.default_value = default_value def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'EducationRemapper': \"\"\" Fit the transformer to the data (in this case, just store the column names). :param X: Input features :param y: Target variable (not used in this transformer) :return: self \"\"\" self.columns_ = X.columns return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Transform the input data by remapping education levels to numerical values. :param X: Input features :return: Transformed features \"\"\" X_transformed = X.copy() X_transformed = X_transformed.map(lambda x: self.mapping.get(x, self.default_value)) return X_transformed def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the names of the output features. :param input_features: Names of the input features (not used in this transformer) :return: Names of the output features \"\"\" return self.columns_ custom_education_transformer = Pipeline(steps=[ (\"education_remapper\", EducationRemapper()), ('minmax_scaler', MinMaxScaler()) ])", "source": "education_remapper.py"}, {"content": "from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder nominal_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')) # One-Hot Encoding ])", "source": "nominal_transformer.py"}, {"content": "from sklearn.pipeline import Pipeline from sklearn.preprocessing import PowerTransformer, StandardScaler numeric_transformer = Pipeline(steps=[ ('yeo_johnson', PowerTransformer(method='yeo-johnson')), # Apply Yeo-Johnson transformation ('scaler', StandardScaler()) # Standardize features ])", "source": "numeric_transformer.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import os import pandas as pd from dotenv import load_dotenv from sqlalchemy import create_engine import pyodbc def load_data(data_path): if os.path.exists(data_path): df = pd.read_csv(data_path) else: load_dotenv() username = os.getenv('DB_USERNAME') password = os.getenv('DB_PASSWORD') server = os.getenv('SERVER') database = os.getenv('DATABASE') driver = 'ODBC Driver 18 for SQL Server' query = 'SELECT * FROM mlp' connection_url = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}&Encrypt=yes&TrustServerCertificate=yes\" engine = create_engine(connection_url) df = pd.read_sql(query, engine) os.makedirs(os.path.dirname(data_path), exist_ok=True) df.to_csv(data_path) engine.dispose() return df", "source": "dataloader.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.input_size=input_size self.hidden_size=hidden_size self.output_size=output_size # Xavier Initialization for weights self.W1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / (self.input_size + self.hidden_size)) self.b1 = np.zeros((1, self.hidden_size)) self.W2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / (self.hidden_size + self.output_size)) self.b2 = np.zeros((1, self.output_size)) # print(f'W1: {self.W1}') # print(f'b1: {self.b1}') # print(f'W2: {self.W2}') # print(f'b2: {self.b2}') def forward(self, features): \"\"\" Takes in the features returns the probability predictions for each possible label \"\"\" self.features = features self.z1 = np.dot(features, self.W1) + self.b1 self.a1 = self.sigmoid(self.z1) self.z2 = np.dot(self.a1, self.W2) + self.b2 self.predictions = self.softmax(self.z2) # print(f'features: {self.features}') # print(f'z1: {self.z1}') # print(f'a1: {self.a1}') # print(f'z2: {self.z2}') # print(f'predictions: {self.predictions}') return self.predictions def sigmoid(self, x): \"\"\" Custom sigmoid activation function \"\"\" return 1 / (1 + np.exp(-x)) def sigmoid_derivative(self, x): \"\"\" Derivative of the sigmoid function \"\"\" s = self.sigmoid(x) return s * (1 - s) def softmax(self, z): exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True)) # For numerical stability return exp_scores / np.sum(exp_scores, axis=1, keepdims=True) def loss(self, predictions, label): \"\"\" Takes in the probability predictions for each possible label class for a sample datapoint and its true label returns the categorical cross-entropy loss (negative log likelihood) for each available label \"\"\" predictions = np.clip(predictions, 1e-12, 1-1e-12) negative_log_likelihoods = -np.log(predictions[0][label]) return negative_log_likelihoods def backward(self, label, learning_rate=1e-3): \"\"\" Adjusts the internal weights/biases using backpropagation \"\"\" # m = self.features.shape[0] m = 1 # since we are only passing 1 sample datapoint and entire sample matrix # Convert label to one-hot encoded vector one_hot_label = np.zeros((1, self.output_size)) one_hot_label[0][label] = 1 # Backpropagation dZ2 = self.predictions - one_hot_label dW2 = (1 / m) * np.dot(self.a1.T, dZ2) db2 = (1 / m) * np.sum(dZ2, axis=0, keepdims=True) dZ1 = np.dot(dZ2, self.W2.T) * self.sigmoid_derivative(self.z1) dW1 = (1 / m) * np.dot(self.features.T.reshape(-1,1), dZ1) db1 = (1 / m) * np.sum(dZ1, axis=0, keepdims=True) # Update weights and biases self.W2 -= learning_rate * dW2 self.b2 -= learning_rate * db2 self.W1 -= learning_rate * dW1 self.b1 -= learning_rate * db1", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np class Datapipeline: @staticmethod def transform(data_path): df = pd.read_csv(data_path) features = [\"x0\", \"x1\", \"x2\", \"x3\"] target = \"y\" X_df = df[features] X_array = X_df.to_numpy() y_df = df[target] y_array = y_df.to_numpy() return X_array, y_array", "source": "mlp_datapipeline.py"}, {"content": "import os import random import pandas as pd import numpy as np import logging import tensorflow as tf from tensorflow import keras from keras.callbacks import EarlyStopping, TensorBoard # Configure logging logging.basicConfig( level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\" ) def reset_seeds(SEED=42): \"\"\" Resets seeds for reproducibility across various libraries and environments. \"\"\" os.environ[\"PYTHONHASHSEED\"] = str(SEED) np.random.seed(SEED) random.seed(SEED) tf.random.set_seed(SEED) class ModelTrainer: def __init__(self, seed=42, log_dir=\"./logs\"): \"\"\" Initializes the ModelTrainer and resets the random seeds for reproducibility. \"\"\" reset_seeds(seed) self.log_dir = log_dir def build_model(self, input_shape): \"\"\" Builds a neural network model with the specified parameters. \"\"\" model = keras.models.Sequential() model.add(tf.keras.layers.InputLayer(shape=input_shape)) model.add(tf.keras.layers.Dense(8, activation=\"relu\")) model.add(tf.keras.layers.Dense(8, activation=\"relu\")) model.add(tf.keras.layers.Dropout(0.2)) model.add(tf.keras.layers.Dense(8, activation=\"relu\")) model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\")) return model def compile_model(self, model, learning_rate=0.001, optimizer_name=\"adam\"): \"\"\" Compiles the neural network model with the specified optimizer and learning rate. \"\"\" if optimizer_name == \"adam\": optimizer = keras.optimizers.Adam(learning_rate=learning_rate) elif optimizer_name == \"sgd\": optimizer = keras.optimizers.SGD(learning_rate=learning_rate) model.compile( optimizer=optimizer, loss=\"binary_focal_crossentropy\", metrics=[\"accuracy\"] ) def train_model( self, model, X_train, y_train, X_test, y_test, epochs=500, batch_size=512 ): \"\"\" Trains the model on the provided training data and validates it on the test data. \"\"\" callbacks = [ EarlyStopping( monitor=\"val_loss\", patience=5, restore_best_weights=True, mode=\"min\" ), TensorBoard(log_dir=self.log_dir, histogram_freq=1), ] history = model.fit( X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=callbacks, ) return history def load_data(): train_data_path = \"./data/train_data.csv\" test_data_path = \"./data/test_data.csv\" train_df = pd.read_csv(train_data_path) test_df = pd.read_csv(test_data_path) X_train = train_df.drop(\"Class\", axis=1) y_train = train_df[\"Class\"] X_test = test_df.drop(\"Class\", axis=1) y_test = test_df[\"Class\"] return X_train, y_train, X_test, y_test def main(): try: # Get configuration from environment variables # This allows easy configuration when running on run:ai learning_rate = float(os.getenv(\"LEARNING_RATE\", 0.001)) optimizer_name = os.getenv(\"OPTIMIZER\", \"adam\") epochs = int(os.getenv(\"EPOCHS\", 500)) batch_size = int(os.getenv(\"BATCH_SIZE\", 512)) logging.info(\"Loading data...\") X_train, y_train, X_test, y_test = load_data() logging.info(\"Initializing ModelTrainer...\") trainer = ModelTrainer() logging.info(\"Building and compiling model...\") input_shape = (X_train.shape[1],) model = trainer.build_model(input_shape) trainer.compile_model(model, learning_rate, optimizer_name) logging.info(\"Starting model training...\") history = trainer.train_model( model, X_train, y_train, X_test, y_test, epochs, batch_size ) # Log final metrics logging.info(f\"Final training accuracy: {history.history['accuracy'][-1]}\") logging.info( f\"Final validation accuracy: {history.history['val_accuracy'][-1]}\" ) # Save the model model_dir = \"./models\" os.makedirs(model_dir, exist_ok=True) model_path = os.path.join(model_dir, \"dropout_model.keras\") model.save(model_path) logging.info(f\"Model saved at {model_path}\") except Exception as e: logging.error(f\"An error occurred: {str(e)}\") raise if __name__ == \"__main__\": main()", "source": "model_training.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler import logging import os # Set up logging logging.basicConfig( level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\" ) class Datapipeline: def __init__(self, output_dir=\"./data\"): self.scaler = StandardScaler() self.output_dir = output_dir def transform_train_data(self, train_data_path): \"\"\" Transforms the training data by undersampling the majority class and applying standard scaling. :param train_data_path: Path to the CSV file containing the training data. :return: Scaled X_train and y_train. \"\"\" logging.info(f\"Loading training data from {train_data_path}\") train_data_df = pd.read_csv(train_data_path) logging.info(\"Oversampling minority class\") minority_class_df = train_data_df[train_data_df[\"Class\"] == True] majority_class_df = train_data_df[train_data_df[\"Class\"] == False] minority_class_oversamples = minority_class_df.sample( n=len(majority_class_df), replace=True, random_state=42 ) # sample from minority class oversampled_df = pd.concat([majority_class_df, minority_class_oversamples]) # shuffle df to avoid ordering bias oversampled_df = oversampled_df.sample(frac=1, random_state=42).reset_index( drop=True ) X_train = oversampled_df.drop(\"Class\", axis=1) y_train = oversampled_df[\"Class\"] logging.info(\"Applying standard scaling to training data\") X_train_scaled = self.scaler.fit_transform(X_train) # Save the processed data processed_train_path = os.path.join( self.output_dir, \"processed_oversampled_train_data.npz\" ) logging.info( f\"Saving processed oversampled training data to {processed_train_path}\" ) np.savez(processed_train_path, X_train_scaled=X_train_scaled, y_train=y_train) return X_train_scaled, y_train def transform_test_data(self, test_data_path): \"\"\" Transforms the test data by applying the standard scaling (fit from the training data). :param test_data_path: Path to the CSV file containing the test data. :return: Scaled X_test and y_test. \"\"\" logging.info(f\"Loading test data from {test_data_path}\") test_data_df = pd.read_csv(test_data_path) X_test = test_data_df.drop(\"Class\", axis=1) y_test = test_data_df[\"Class\"] logging.info(\"Applying standard scaling to test data\") X_test_scaled = self.scaler.transform(X_test) # Save the processed data processed_test_path = os.path.join(self.output_dir, \"processed_test_data.npz\") logging.info(f\"Saving processed test data to {processed_test_path}\") np.savez(processed_test_path, X_test_scaled=X_test_scaled, y_test=y_test) return X_test_scaled, y_test if __name__ == \"__main__\": pipeline = Datapipeline() train_data_path = \"./data/train_data.csv\" test_data_path = \"./data/test_data.csv\" X_train_scaled, y_train = pipeline.transform_train_data(train_data_path) X_test_scaled, y_test = pipeline.transform_test_data(test_data_path) logging.info(\"Data pipeline processing completed successfully\")", "source": "oversampling_pipeline.py"}, {"content": "import os import logging import pyodbc from sqlalchemy import create_engine from dotenv import load_dotenv import pandas as pd from sklearn.model_selection import train_test_split # Set up logging logging.basicConfig( level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\" ) class DataPreprocessor: \"\"\" A class for handling data preprocessing tasks, such as splitting the dataset into training and testing sets and saving them as CSV files. \"\"\" def get_data(): data_path = \"./data/data.csv\" if os.path.exists(data_path): # Try to read from CSV if it exists raw_df = pd.read_csv(data_path) else: load_dotenv() username = os.getenv(\"DB_USERNAME\") password = os.getenv(\"DB_PASSWORD\") server = os.getenv(\"SERVER\") database = os.getenv(\"DATABASE\") driver = \"ODBC Driver 18 for SQL Server\" query = \"SELECT * FROM creditcard\" connection_url = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}&Encrypt=yes&TrustServerCertificate=yes\" engine = create_engine(connection_url) raw_df = pd.read_sql(query, engine) os.makedirs(os.path.dirname(data_path), exist_ok=True) raw_df.to_csv(data_path) engine.dispose() return raw_df @staticmethod def split_data(data_path, output_dir=\"/data\"): \"\"\" Splits the input dataset into training and testing sets, and saves them as separate CSV files. :param data_path: str, the file path of the dataset to be processed. The dataset should contain a 'Class' column, which will be used as the target variable. :param output_dir: str, the directory where the processed data will be stored. :return: tuple, containing the paths of the saved training and testing CSV files. \"\"\" logging.info(f\"Loading data from {data_path}\") df = pd.read_csv(data_path) logging.info(\"Removing duplicate entries\") df.drop_duplicates(inplace=True) X = df.drop(\"Class\", axis=1) y = df[\"Class\"] logging.info(\"Splitting data into training and testing sets\") X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) os.makedirs(output_dir, exist_ok=True) train_data = X_train.copy() train_data[\"Class\"] = y_train test_data = X_test.copy() test_data[\"Class\"] = y_test train_data_path = os.path.join(output_dir, \"train_data.csv\") test_data_path = os.path.join(output_dir, \"test_data.csv\") logging.info(f\"Saving training data to {train_data_path}\") train_data.to_csv(train_data_path, index=False) logging.info(f\"Saving testing data to {test_data_path}\") test_data.to_csv(test_data_path, index=False) return train_data_path, test_data_path if __name__ == \"__main__\": data_path = \"/data/data.csv\" # Adjust this path as needed train_data_path, test_data_path = DataPreprocessor.split_data(data_path) logging.info(\"Data preprocessing completed successfully\")", "source": "traintestsplit.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler import logging import os # Set up logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') class Datapipeline: def __init__(self, output_dir='./data'): self.scaler = StandardScaler() self.output_dir = output_dir def transform_train_data(self, train_data_path): \"\"\" Transforms the training data by undersampling the majority class and applying standard scaling. :param train_data_path: Path to the CSV file containing the training data. :return: Scaled X_train and y_train. \"\"\" logging.info(f\"Loading training data from {train_data_path}\") train_data_df = pd.read_csv(train_data_path) logging.info(\"Undersampling majority class\") minority_class_df = train_data_df[train_data_df['Class'] == True] majority_class_df = train_data_df[train_data_df['Class'] == False] majority_class_undersamples_df = majority_class_df.sample(n=len(minority_class_df), random_state=42) undersampled_df = pd.concat([minority_class_df, majority_class_undersamples_df]) # shuffle df to avoid ordering bias undersampled_df = undersampled_df.sample(frac=1, random_state=42).reset_index(drop=True) X_train = undersampled_df.drop('Class', axis=1) y_train = undersampled_df['Class'] logging.info(\"Applying standard scaling to training data\") X_train_scaled = self.scaler.fit_transform(X_train) # Save the processed data processed_train_path = os.path.join(self.output_dir, 'processed_undersampled_train_data.npz') logging.info(f\"Saving processed training data to {processed_train_path}\") np.savez(processed_train_path, X_train_scaled=X_train_scaled, y_train=y_train) return X_train_scaled, y_train def transform_test_data(self, test_data_path): \"\"\" Transforms the test data by applying the standard scaling (fit from the training data). :param test_data_path: Path to the CSV file containing the test data. :return: Scaled X_test and y_test. \"\"\" logging.info(f\"Loading test data from {test_data_path}\") test_data_df = pd.read_csv(test_data_path) X_test = test_data_df.drop('Class', axis=1) y_test = test_data_df['Class'] logging.info(\"Applying standard scaling to test data\") X_test_scaled = self.scaler.transform(X_test) # Save the processed data processed_test_path = os.path.join(self.output_dir, 'processed_test_data.npz') logging.info(f\"Saving processed test data to {processed_test_path}\") np.savez(processed_test_path, X_test_scaled=X_test_scaled, y_test=y_test) return X_test_scaled, y_test if __name__ == \"__main__\": pipeline = Datapipeline() train_data_path = './data/train_data.csv' test_data_path = './data/test_data.csv' X_train_scaled, y_train = pipeline.transform_train_data(train_data_path) X_test_scaled, y_test = pipeline.transform_test_data(test_data_path) logging.info(\"Data pipeline processing completed successfully\")", "source": "undersampling_pipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028,], [ -618, 573, -126,], [ 265, 391, -100,], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24,], [ 480, 80, -685, -1028, -122,], [ -127, -618, 573, -126, 28,], [ 924, 265, 391, -100, -235,], [ 384, 280, 218, 279, 59,], ] # Replace below with your response matrix_3 = [ [-128, -200, -24,], [-127, 573, 28,], [ 384, 218, 59,], ]", "source": "convolved_matrices.py"}, {"content": "import tensorflow as tf class DataLoader: def __init__(self, preprocessed_dataset_dir, img_height=600, img_width=640, batch_size=32, validation_split=0.2, seed=42): self.preprocessed_dataset_dir = preprocessed_dataset_dir self.img_height = img_height self.img_width = img_width self.batch_size = batch_size self.validation_split = validation_split self.seed = seed self.class_names = None self.train_dataset = None self.test_dataset = None def load_data(self): # Load the training dataset self.train_dataset = tf.keras.preprocessing.image_dataset_from_directory( self.preprocessed_dataset_dir, validation_split=self.validation_split, subset=\"training\", seed=self.seed, image_size=(self.img_height, self.img_width), batch_size=self.batch_size ) # Load the validation dataset self.test_dataset = tf.keras.preprocessing.image_dataset_from_directory( self.preprocessed_dataset_dir, validation_split=self.validation_split, subset=\"validation\", seed=self.seed, image_size=(self.img_height, self.img_width), batch_size=self.batch_size ) # Set the class names after loading the datasets self.class_names = self.train_dataset.class_names return self.train_dataset, self.test_dataset def get_class_names(self): if self.class_names is None: raise ValueError(\"Data has not been loaded yet. Please call load_data() first.\") return self.class_names def get_dataset_info(self): if self.train_dataset is None or self.test_dataset is None: raise ValueError(\"Data has not been loaded yet. Please call load_data() first.\") train_batches = self.train_dataset.cardinality() test_batches = self.test_dataset.cardinality() return { \"train_batches\": train_batches, \"test_batches\": test_batches, \"class_names\": self.class_names }", "source": "dataloader.py"}, {"content": "import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.applications import ResNet50 class ModelClass: def __init__(self, img_height, img_width, num_classes): self.img_height = img_height self.img_width = img_width self.num_classes = num_classes self.model = None def build_model(self): # Load the pre-trained ResNet50 model without the top layer base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(self.img_height, self.img_width, 3)) # Freeze the base model's layers so they won't be trained during the initial training phase base_model.trainable = False # Add custom layers on top of the base model self.model = models.Sequential([ base_model, # The base ResNet50 model layers.Flatten(), layers.Dense(256, activation='relu'), layers.Dense(128, activation='relu'), layers.Dropout(0.5), layers.Dense(self.num_classes, activation='softmax') ]) def compile_model(self): if self.model is None: raise ValueError(\"Model has not been built. Call build_model() first.\") # Compile the model with appropriate loss and optimizer self.model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', # Since labels are integers metrics=['accuracy']) def summary(self): if self.model is None: raise ValueError(\"Model has not been built. Call build_model() first.\") # Print the summary of the model self.model.summary() def get_model(self): if self.model is None: raise ValueError(\"Model has not been built. Call build_model() first.\") return self.model", "source": "model.py"}, {"content": "import os from PIL import Image import tensorflow as tf from tensorflow.keras.preprocessing.image import ImageDataGenerator class Preprocessor: def __init__(self, dataset_dir, preprocessed_dataset_dir, target_size=(640, 600)): self.dataset_dir = dataset_dir self.preprocessed_dataset_dir = preprocessed_dataset_dir self.target_size = target_size self.class_counts = {} self.target_num_images = 0 # Create the target directory if it doesn't exist if not os.path.exists(self.preprocessed_dataset_dir): os.makedirs(self.preprocessed_dataset_dir) # Initialize class counts and target number of images for balancing self._calculate_class_counts() self.target_num_images = max(self.class_counts.values()) def _calculate_class_counts(self): # Count the number of images in each class and store them for class_folder in os.listdir(self.dataset_dir): class_path = os.path.join(self.dataset_dir, class_folder) if os.path.isdir(class_path): num_images = len([f for f in os.listdir(class_path) if f.endswith(('jpg', 'jpeg', 'png', 'bmp', 'mpo'))]) self.class_counts[class_folder] = num_images def preprocess_image(self, image_path): # Open and preprocess an image (resize, convert to RGB if necessary) img = Image.open(image_path) # Convert stereo (MPO) image to 2D if img.format == 'MPO': img.seek(0) # Extract the first image from the stereo MPO img = img.convert('RGB') # Convert to RGB if not already if img.mode != 'RGB': img = img.convert('RGB') # Resize to the target resolution img = img.resize(self.target_size) return img def augment_image(self, image, save_dir, base_filename, augmentations=1): # Set up data augmentation generator data_gen = ImageDataGenerator( rotation_range=30, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, horizontal_flip=True, fill_mode='nearest' ) # Convert image to array and reshape for augmentation image = tf.keras.preprocessing.image.img_to_array(image) image = image.reshape((1,) + image.shape) # Generate and save augmented images aug_gen = data_gen.flow(image, batch_size=1) for i in range(augmentations): aug_image = next(aug_gen)[0].astype('uint8') aug_image_pil = Image.fromarray(aug_image) aug_image_pil.save(os.path.join(save_dir, f\"{base_filename}_aug_{i}.jpg\")) def process_class_images(self, class_folder, current_num_images): class_path = os.path.join(self.dataset_dir, class_folder) preprocessed_class_path = os.path.join(self.preprocessed_dataset_dir, class_folder) # Create the corresponding folder in the preprocessed directory if not os.path.exists(preprocessed_class_path): os.makedirs(preprocessed_class_path) # Process each image in the class folder for image_file in os.listdir(class_path): image_path = os.path.join(class_path, image_file) try: # Preprocess the image preprocessed_image = self.preprocess_image(image_path) # Save the preprocessed image save_path = os.path.join(preprocessed_class_path, image_file) preprocessed_image.save(save_path) except Exception as e: print(f\"Error processing file {image_path}: {e}\") # Augment under-sampled classes until they reach the target number of images self.augment_class_images(preprocessed_class_path, current_num_images) def augment_class_images(self, preprocessed_class_path, current_num_images): while current_num_images < self.target_num_images: for image_file in os.listdir(preprocessed_class_path): image_path = os.path.join(preprocessed_class_path, image_file) try: # Open the image and augment it image = Image.open(image_path) base_filename = os.path.splitext(image_file)[0] self.augment_image(image, preprocessed_class_path, base_filename) current_num_images += 1 # Stop augmenting once the class is balanced if current_num_images >= self.target_num_images: break except Exception as e: print(f\"Error augmenting file {image_path}: {e}\") def process_dataset(self): # Iterate through the classes and process images for class_folder in os.listdir(self.dataset_dir): class_path = os.path.join(self.dataset_dir, class_folder) if os.path.isdir(class_path): current_num_images = self.class_counts[class_folder] self.process_class_images(class_folder, current_num_images) print(\"Preprocessing, balancing, and saving complete!\")", "source": "preprocessor.py"}, {"content": "import hydra from omegaconf import DictConfig import os import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.applications import ResNet50 from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint import mlflow import mlflow.tensorflow @hydra.main(config_path=\"../conf\", config_name=\"config_train\") def main(cfg: DictConfig): # Set up MLflow if cfg.mlflow.setup_mlflow: mlflow.set_tracking_uri(cfg.mlflow.mlflow_tracking_uri) mlflow.set_experiment(cfg.mlflow.mlflow_exp_name) mlflow.tensorflow.autolog() # Load datasets train_dataset = tf.keras.preprocessing.image_dataset_from_directory( cfg.data.data_dir_path, validation_split=cfg.data.validation_split, subset=\"training\", seed=cfg.data.seed, image_size=(cfg.data.img_height, cfg.data.img_width), batch_size=cfg.data.batch_size ) test_dataset = tf.keras.preprocessing.image_dataset_from_directory( cfg.data.data_dir_path, validation_split=cfg.data.validation_split, subset=\"validation\", seed=cfg.data.seed, image_size=(cfg.data.img_height, cfg.data.img_width), batch_size=cfg.data.batch_size ) class_names = train_dataset.class_names # Build the model using hyperparameters from cfg base_model = ResNet50( weights='imagenet', include_top=False, input_shape=(cfg.data.img_height, cfg.data.img_width, 3) ) base_model.trainable = cfg.model.base_model_trainable model = models.Sequential([ base_model, layers.Flatten(), layers.Dense(cfg.model.dense_units_1, activation=cfg.model.activation), layers.Dense(cfg.model.dense_units_2, activation=cfg.model.activation), layers.Dropout(cfg.model.dropout_rate), layers.Dense(len(class_names), activation='softmax') ]) # Compile the model model.compile( optimizer=cfg.training.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # Define callbacks early_stopping = EarlyStopping( monitor='val_loss', patience=5, restore_best_weights=True ) # Ensure the model save directory exists os.makedirs(cfg.paths.model_checkpoint_dir, exist_ok=True) model_checkpoint = ModelCheckpoint( filepath=os.path.join(cfg.paths.model_checkpoint_dir, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min' ) callbacks = [early_stopping, model_checkpoint] # Train the model if cfg.mlflow.setup_mlflow: with mlflow.start_run(): mlflow.log_params({ 'dense_units_1': cfg.model.dense_units_1, 'dense_units_2': cfg.model.dense_units_2, 'dropout_rate': cfg.model.dropout_rate, 'activation': cfg.model.activation, 'optimizer': cfg.training.optimizer }) history = model.fit( train_dataset, validation_data=test_dataset, epochs=cfg.training.epochs, verbose=1, callbacks=callbacks ) # Save the final model final_model_path = os.path.join(cfg.paths.model_checkpoint_dir, 'final_model.keras') model.save(final_model_path) print(f\"Final model saved to {final_model_path}\") else: history = model.fit( train_dataset, validation_data=test_dataset, epochs=cfg.training.epochs, verbose=1, callbacks=callbacks ) # Save the final model final_model_path = os.path.join(cfg.paths.model_checkpoint_dir, 'final_model.keras') model.save(final_model_path) print(f\"Final model saved to {final_model_path}\") if __name__ == \"__main__\": main()", "source": "train.py"}, {"content": "import matplotlib.pyplot as plt class Trainer: def __init__(self, model, train_dataset, test_dataset, epochs=20): \"\"\" Initialize the Trainer with model, training and validation datasets, and number of epochs. Parameters: - model: The Keras model to be trained. - train_dataset: The training dataset (data generator). - test_dataset: The validation dataset (data generator). - epochs: The number of training epochs. \"\"\" self.model = model self.train_dataset = train_dataset self.test_dataset = test_dataset self.epochs = epochs self.history = None def train(self): \"\"\" Train the model using the provided datasets and number of epochs. \"\"\" if self.model is None: raise ValueError(\"No model provided for training.\") # Fit the model self.history = self.model.fit( self.train_dataset, # Training data generator validation_data=self.test_dataset, # Validation data generator epochs=self.epochs, # Number of epochs verbose=1 # Display training progress ) return self.history def plot_training_history(self): \"\"\" Plot the accuracy and loss curves from the training history. \"\"\" if self.history is None: raise ValueError(\"No training history available. Train the model first.\") # Plot training & validation accuracy values plt.figure(figsize=(12, 4)) plt.subplot(1, 2, 1) plt.plot(self.history.history['accuracy']) plt.plot(self.history.history['val_accuracy']) plt.title('Model accuracy') plt.ylabel('Accuracy') plt.xlabel('Epoch') plt.legend(['Train', 'Validation'], loc='upper left') # Plot training & validation loss values plt.subplot(1, 2, 2) plt.plot(self.history.history['loss']) plt.plot(self.history.history['val_loss']) plt.title('Model loss') plt.ylabel('Loss') plt.xlabel('Epoch') plt.legend(['Train', 'Validation'], loc='upper left') plt.show() def get_training_history(self): \"\"\" Retrieve the training history after training. Returns: - history: A Keras History object containing details of the training process. \"\"\" if self.history is None: raise ValueError(\"No training history available. Train the model first.\") return self.history", "source": "trainer.py"}, {"content": "# train_with_tuning.py import hydra from omegaconf import DictConfig import os import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.applications import ResNet50 from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint import mlflow import mlflow.tensorflow @hydra.main(config_path=\"../conf\", config_name=\"config_tuning\") def main(cfg: DictConfig): # Set up MLflow if cfg.mlflow.setup_mlflow: mlflow.set_tracking_uri(cfg.mlflow.mlflow_tracking_uri) mlflow.set_experiment(cfg.mlflow.mlflow_exp_name) mlflow.tensorflow.autolog() # Load datasets train_dataset = tf.keras.preprocessing.image_dataset_from_directory( cfg.data.data_dir_path, validation_split=cfg.data.validation_split, subset=\"training\", seed=cfg.data.seed, image_size=(cfg.data.img_height, cfg.data.img_width), batch_size=cfg.data.batch_size ) test_dataset = tf.keras.preprocessing.image_dataset_from_directory( cfg.data.data_dir_path, validation_split=cfg.data.validation_split, subset=\"validation\", seed=cfg.data.seed, image_size=(cfg.data.img_height, cfg.data.img_width), batch_size=cfg.data.batch_size ) class_names = train_dataset.class_names # Build the model using hyperparameters from cfg base_model = ResNet50( weights='imagenet', include_top=False, input_shape=(cfg.data.img_height, cfg.data.img_width, 3) ) base_model.trainable = cfg.model.base_model_trainable model = models.Sequential([ base_model, layers.Flatten(), layers.Dense(cfg.model.dense_units_1, activation=cfg.model.activation), layers.Dense(cfg.model.dense_units_2, activation=cfg.model.activation), layers.Dropout(cfg.model.dropout_rate), layers.Dense(len(class_names), activation='softmax') ]) # Compile the model model.compile( optimizer=cfg.training.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'] ) # Define callbacks early_stopping = EarlyStopping( monitor='val_loss', patience=5, restore_best_weights=True ) # Ensure the model save directory exists os.makedirs(cfg.paths.model_checkpoint_dir, exist_ok=True) model_checkpoint = ModelCheckpoint( filepath=os.path.join(cfg.paths.model_checkpoint_dir, 'best_model.keras'), save_best_only=True, monitor='val_loss', mode='min' ) callbacks = [early_stopping, model_checkpoint] # Train the model if cfg.mlflow.setup_mlflow: with mlflow.start_run(): mlflow.log_params({ 'dense_units_1': cfg.model.dense_units_1, 'dense_units_2': cfg.model.dense_units_2, 'dropout_rate': cfg.model.dropout_rate, 'activation': cfg.model.activation, 'optimizer': cfg.training.optimizer }) history = model.fit( train_dataset, validation_data=test_dataset, epochs=cfg.training.epochs, verbose=1, callbacks=callbacks ) # Save the final model final_model_path = os.path.join(cfg.paths.model_checkpoint_dir, 'final_model.keras') model.save(final_model_path) print(f\"Final model saved to {final_model_path}\") else: history = model.fit( train_dataset, validation_data=test_dataset, epochs=cfg.training.epochs, verbose=1, callbacks=callbacks ) # Save the final model final_model_path = os.path.join(cfg.paths.model_checkpoint_dir, 'final_model.keras') model.save(final_model_path) print(f\"Final model saved to {final_model_path}\") if __name__ == \"__main__\": main()", "source": "train_with_tuning.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import os import numpy as np import pandas as pd from statsmodels.tsa.seasonal import seasonal_decompose from ucimlrepo import fetch_ucirepo class DataPipeline: def __init__(self): self.df = None def get_csv(self, csv_path): if os.path.exists(csv_path): self.df = pd.read_csv(csv_path) else: beijing_pm2_5 = fetch_ucirepo(id=381) X = beijing_pm2_5.data.features y = beijing_pm2_5.data.targets self.df = pd.concat([X, y], axis=1) self.df.to_csv(csv_path, index=False) def preprocess_data(self): self._get_datetime() self._cylical_encode_cbwd() self._impute_missing_pm2pt5() self._drop_unused_features() self._index_datetime() def _get_datetime(self): self.df['datetime'] = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']]) def _cylical_encode_cbwd(self): angle_mapping = { 'N' : 0, 'NE': 45, 'E' : 90, 'SE': 135, 'S' : 180, 'SW': 225, 'W' : 270, 'NW': 315, 'cv': None # No wind (calm and variable) } self.df['wind_angle'] = self.df['cbwd'].map(angle_mapping) self.df['cbwd_sin'] = self.df['wind_angle'].apply(lambda x: np.sin(x) if pd.notna(x) else 0) self.df['cbwd_cos'] = self.df['wind_angle'].apply(lambda x: np.cos(x) if pd.notna(x) else 0) def _impute_missing_pm2pt5(self): self.df['pm2.5'] = self.df['pm2.5'].fillna( self.df['pm2.5'].rolling(window=24, min_periods=1).mean() ).bfill() def _drop_unused_features(self): features_to_keep = ['datetime', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_sin', 'cbwd_cos', 'pm2.5'] self.df = self.df[features_to_keep] def _index_datetime(self): self.df.set_index('datetime', inplace=True) def engineer_features(self): self._ets_decomposition_1() self._ets_decomposition_2() self._create_lagged_features() # consider dropping orignal features that are now redundant? def _ets_decomposition_1(self): features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_sin', 'cbwd_cos', 'pm2.5'] periods_dict = { 'DEWP': 8765, 'TEMP': 8765, 'PRES': 8765, 'Iws': 8765, 'Is': 8765, 'Ir': 8765, 'cbwd_sin': 8765, 'cbwd_cos': 8765, 'pm2.5': 10956 } for feature in features: period = periods_dict[feature] decomposition = seasonal_decompose(self.df[feature].dropna(), model='additive', period=period) self.df[f'{feature}_ETS_1_trend'] = decomposition.trend self.df[f'{feature}_ETS_1_seasonal'] = decomposition.seasonal self.df[f'{feature}_ETS_1_residual'] = decomposition.resid def _ets_decomposition_2(self): ETS_1_residuals = ['DEWP_ETS_1_residual', 'TEMP_ETS_1_residual', 'PRES_ETS_1_residual', 'Iws_ETS_1_residual', 'Is_ETS_1_residual', 'Ir_ETS_1_residual', 'cbwd_sin_ETS_1_residual', 'cbwd_cos_ETS_1_residual', 'pm2.5_ETS_1_residual'] periods_dict = { 'DEWP_ETS_1_residual': 270, 'TEMP_ETS_1_residual': 24, 'PRES_ETS_1_residual': 1524, 'Iws_ETS_1_residual': 5009, 'Is_ETS_1_residual': 1131, 'Ir_ETS_1_residual': 2697, 'cbwd_sin_ETS_1_residual': 24, 'cbwd_cos_ETS_1_residual': 24, 'pm2.5_ETS_1_residual': 3287 } for residual in ETS_1_residuals: period = periods_dict[residual] decomposition = seasonal_decompose(self.df[residual].dropna(), model='additive', period=period) feature = residual.replace('_ETS_1_residual', '') self.df[f'{feature}_ETS_2_trend'] = decomposition.trend self.df[f'{feature}_ETS_2_seasonal'] = decomposition.seasonal self.df[f'{feature}_ETS_2_residual'] = decomposition.resid def _create_lagged_features(self): ETS_2_residuals = ['DEWP_ETS_2_residual', 'TEMP_ETS_2_residual', 'PRES_ETS_2_residual', 'Iws_ETS_2_residual', 'Is_ETS_2_residual', 'Ir_ETS_2_residual', 'cbwd_sin_ETS_2_residual', 'cbwd_cos_ETS_2_residual', 'pm2.5_ETS_2_residual'] for residual in ETS_2_residuals: self.df[f'{residual}_lag_1'] = self.df[residual].shift(1) def run_data_pipeline(self, csv_path): self.get_csv(csv_path) self.preprocess_data() self.engineer_features() return self.df", "source": "datapipeline.py"}, {"content": "import torch import torch.nn as nn from torch.utils.data import DataLoader from typing import List class RNNModel(nn.Module): \"\"\" A Gated Recurrent Unit (GRU) model using PyTorch's nn.GRU module to predict the 'pm2.5' feature. Parameters: ----------- input_size : int The number of input features per time step (excluding 'pm2.5'). hidden_size : int The number of features in the hidden state of the GRU. num_layers : int The number of recurrent layers in the GRU. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int) -> None: \"\"\" Initializes the GRU model with the given parameters. Parameters: ----------- input_size : int The number of input features per time step. hidden_size : int The number of features in the hidden state of the GRU. num_layers : int The number of recurrent layers. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" super(RNNModel, self).__init__() # Initialize GRU layers self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True) # Fully connected layer for output (single value prediction) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass through the GRU model. Parameters: ----------- x : torch.Tensor Input tensor of shape (batch_size, sequence_length, input_size). Returns: -------- torch.Tensor Output tensor of shape (batch_size, output_size) containing the predictions for the last feature ('pm2.5'). \"\"\" # Forward pass through GRU out, _ = self.gru(x) # Get the last time step's output for each sequence out = self.fc(out[:, -1, :]) # Output from the last GRU time step return out def fit(self, train_dataloader: DataLoader, val_dataloader: DataLoader, num_epochs: int, learning_rate: float) -> None: \"\"\" Trains the GRU model using Mean Squared Error loss and Adam optimizer, and tracks both training and validation loss. Parameters: ----------- train_dataloader : DataLoader A DataLoader object to iterate over the training data. val_dataloader : DataLoader A DataLoader object to iterate over the validation data. num_epochs : int The number of epochs for training. learning_rate : float The learning rate for the optimizer. Returns: -------- None \"\"\" # Loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate) # Track losses train_losses = [] val_losses = [] # Training loop for epoch in range(num_epochs): self.train() # Set model to training mode train_loss = 0 for features, labels in train_dataloader: # Forward pass outputs = self.forward(features) labels = labels.view(-1, 1) # Ensure labels have the correct shape loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() avg_train_loss = train_loss / len(train_dataloader) train_losses.append(avg_train_loss) # Validation loop self.eval() # Set model to evaluation mode val_loss = 0 with torch.no_grad(): for features, labels in val_dataloader: outputs = self.forward(features) labels = labels.view(-1, 1) loss = criterion(outputs, labels) val_loss += loss.item() avg_val_loss = val_loss / len(val_dataloader) val_losses.append(avg_val_loss) print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}') # Store or return the losses if needed for further analysis return train_losses, val_losses def predict(self, dataloader: DataLoader) -> torch.Tensor: \"\"\" Generates predictions using the trained GRU model. Parameters: ----------- dataloader : DataLoader A DataLoader", "source": "gru_rnn_model.py"}, {"content": "object to iterate over the test data. Returns: -------- torch.Tensor A tensor containing the predictions for the input sequences. \"\"\" self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: outputs = self.forward(features) predictions.append(outputs) return torch.cat(predictions, dim=0) def evaluate(self, dataloader: DataLoader) -> float: \"\"\" Evaluates the model on a dataset using Mean Squared Error (MSE). Parameters: ----------- dataloader : DataLoader A DataLoader object to iterate over the test data. Returns: -------- float The mean squared error of the model on the dataset. \"\"\" self.eval() criterion = nn.MSELoss() mse = 0.0 with torch.no_grad(): for features, labels in dataloader: outputs = self.forward(features) loss = criterion(outputs, labels) mse += loss.item() mse /= len(dataloader) print(f'Mean Squared Error: {mse:.4f}') return mse", "source": "gru_rnn_model.py"}, {"content": "import torch import torch.nn as nn from torch.utils.data import DataLoader from typing import List class RNNModel(nn.Module): \"\"\" A Long Short-Term Memory (LSTM) model using PyTorch's nn.LSTM module to predict the 'pm2.5' feature. Parameters: ----------- input_size : int The number of input features per time step (excluding 'pm2.5'). hidden_size : int The number of features in the hidden state of the LSTM. num_layers : int The number of recurrent layers in the LSTM. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int) -> None: \"\"\" Initializes the LSTM model with the given parameters. Parameters: ----------- input_size : int The number of input features per time step. hidden_size : int The number of features in the hidden state of the LSTM. num_layers : int The number of recurrent layers. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" super(RNNModel, self).__init__() # Initialize LSTM layers self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True) # Fully connected layer for output (single value prediction) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass through the LSTM model. Parameters: ----------- x : torch.Tensor Input tensor of shape (batch_size, sequence_length, input_size). Returns: -------- torch.Tensor Output tensor of shape (batch_size, output_size) containing the predictions for the last feature ('pm2.5'). \"\"\" # Forward pass through LSTM out, _ = self.lstm(x) # Get the last time step's output for each sequence out = self.fc(out[:, -1, :]) # Output from the last LSTM time step return out def fit(self, train_dataloader: DataLoader, val_dataloader: DataLoader, num_epochs: int, learning_rate: float) -> None: \"\"\" Trains the LSTM model using Mean Squared Error loss and Adam optimizer, and tracks both training and validation loss. Parameters: ----------- train_dataloader : DataLoader A DataLoader object to iterate over the training data. val_dataloader : DataLoader A DataLoader object to iterate over the validation data. num_epochs : int The number of epochs for training. learning_rate : float The learning rate for the optimizer. Returns: -------- None \"\"\" # Loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate) # Track losses train_losses = [] val_losses = [] # Training loop for epoch in range(num_epochs): self.train() # Set model to training mode train_loss = 0 for features, labels in train_dataloader: # Forward pass outputs = self.forward(features) labels = labels.view(-1, 1) # Ensure labels have the correct shape loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() avg_train_loss = train_loss / len(train_dataloader) train_losses.append(avg_train_loss) # Validation loop self.eval() # Set model to evaluation mode val_loss = 0 with torch.no_grad(): for features, labels in val_dataloader: outputs = self.forward(features) labels = labels.view(-1, 1) loss = criterion(outputs, labels) val_loss += loss.item() avg_val_loss = val_loss / len(val_dataloader) val_losses.append(avg_val_loss) print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}') # Store or return the losses if needed for further analysis return train_losses, val_losses def predict(self, dataloader: DataLoader) -> torch.Tensor: \"\"\" Generates predictions using the trained LSTM model. Parameters: ----------- dataloader : DataLoader A DataLoader", "source": "lstm_rnn_model.py"}, {"content": "object to iterate over the test data. Returns: -------- torch.Tensor A tensor containing the predictions for the input sequences. \"\"\" self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: outputs = self.forward(features) predictions.append(outputs) return torch.cat(predictions, dim=0) def evaluate(self, dataloader: DataLoader) -> float: \"\"\" Evaluates the model on a dataset using Mean Squared Error (MSE). Parameters: ----------- dataloader : DataLoader A DataLoader object to iterate over the test data. Returns: -------- float The mean squared error of the model on the dataset. \"\"\" self.eval() criterion = nn.MSELoss() mse = 0.0 with torch.no_grad(): for features, labels in dataloader: outputs = self.forward(features) loss = criterion(outputs, labels) mse += loss.item() mse /= len(dataloader) print(f'Mean Squared Error: {mse:.4f}') return mse", "source": "lstm_rnn_model.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel: def __init__(self): self.model = RandomForestRegressor( n_estimators=100, # Number of trees in the forest max_depth=10, # Limit the maximum depth of the tree max_features='sqrt', # Consider sqrt(n_features) at each split min_samples_leaf=4, # Minimum number of samples required at a leaf node n_jobs=-1, # Use all available cores random_state=42 # For reproducibility ) def fit(self, X, y): self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): y_train_pred = self.model.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import os import numpy as np import pandas as pd from statsmodels.tsa.seasonal import seasonal_decompose from ucimlrepo import fetch_ucirepo class DataPipeline: def __init__(self): self.df = None def get_csv(self, csv_path): if os.path.exists(csv_path): self.df = pd.read_csv(csv_path) else: beijing_pm2_5 = fetch_ucirepo(id=381) X = beijing_pm2_5.data.features y = beijing_pm2_5.data.targets self.df = pd.concat([X, y], axis=1) self.df.to_csv(csv_path, index=False) def preprocess_data(self): self._get_datetime() self._cylical_encode_cbwd() self._impute_missing_pm2pt5() self._drop_unused_features() self._index_datetime() def _get_datetime(self): self.df['datetime'] = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']]) def _cylical_encode_cbwd(self): angle_mapping = { 'N' : 0, 'NE': 45, 'E' : 90, 'SE': 135, 'S' : 180, 'SW': 225, 'W' : 270, 'NW': 315, 'cv': None # No wind (calm and variable) } self.df['wind_angle'] = self.df['cbwd'].map(angle_mapping) self.df['cbwd_sin'] = self.df['wind_angle'].apply(lambda x: np.sin(x) if pd.notna(x) else 0) self.df['cbwd_cos'] = self.df['wind_angle'].apply(lambda x: np.cos(x) if pd.notna(x) else 0) def _impute_missing_pm2pt5(self): self.df['pm2.5'] = self.df['pm2.5'].fillna( self.df['pm2.5'].rolling(window=24, min_periods=1).mean() ).bfill() def _drop_unused_features(self): features_to_keep = ['datetime', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_sin', 'cbwd_cos', 'pm2.5'] self.df = self.df[features_to_keep] def _index_datetime(self): self.df.set_index('datetime', inplace=True) def engineer_features(self): self._ets_decomposition_1() self._ets_decomposition_2() self._create_lagged_features() # consider dropping orignal features that are now redundant? def _ets_decomposition_1(self): features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd_sin', 'cbwd_cos', 'pm2.5'] periods_dict = { 'DEWP': 8765, 'TEMP': 8765, 'PRES': 8765, 'Iws': 8765, 'Is': 8765, 'Ir': 8765, 'cbwd_sin': 8765, 'cbwd_cos': 8765, 'pm2.5': 10956 } for feature in features: period = periods_dict[feature] decomposition = seasonal_decompose(self.df[feature].dropna(), model='additive', period=period) self.df[f'{feature}_ETS_1_trend'] = decomposition.trend self.df[f'{feature}_ETS_1_seasonal'] = decomposition.seasonal self.df[f'{feature}_ETS_1_residual'] = decomposition.resid def _ets_decomposition_2(self): ETS_1_residuals = ['DEWP_ETS_1_residual', 'TEMP_ETS_1_residual', 'PRES_ETS_1_residual', 'Iws_ETS_1_residual', 'Is_ETS_1_residual', 'Ir_ETS_1_residual', 'cbwd_sin_ETS_1_residual', 'cbwd_cos_ETS_1_residual', 'pm2.5_ETS_1_residual'] periods_dict = { 'DEWP_ETS_1_residual': 270, 'TEMP_ETS_1_residual': 24, 'PRES_ETS_1_residual': 1524, 'Iws_ETS_1_residual': 5009, 'Is_ETS_1_residual': 1131, 'Ir_ETS_1_residual': 2697, 'cbwd_sin_ETS_1_residual': 24, 'cbwd_cos_ETS_1_residual': 24, 'pm2.5_ETS_1_residual': 3287 } for residual in ETS_1_residuals: period = periods_dict[residual] decomposition = seasonal_decompose(self.df[residual].dropna(), model='additive', period=period) feature = residual.replace('_ETS_1_residual', '') self.df[f'{feature}_ETS_2_trend'] = decomposition.trend self.df[f'{feature}_ETS_2_seasonal'] = decomposition.seasonal self.df[f'{feature}_ETS_2_residual'] = decomposition.resid def _create_lagged_features(self): ETS_2_residuals = ['DEWP_ETS_2_residual', 'TEMP_ETS_2_residual', 'PRES_ETS_2_residual', 'Iws_ETS_2_residual', 'Is_ETS_2_residual', 'Ir_ETS_2_residual', 'cbwd_sin_ETS_2_residual', 'cbwd_cos_ETS_2_residual', 'pm2.5_ETS_2_residual'] for residual in ETS_2_residuals: self.df[f'{residual}_lag_1'] = self.df[residual].shift(1) def run_data_pipeline(self, csv_path): self.get_csv(csv_path) self.preprocess_data() # self.engineer_features() # convert to tensors? return self.df", "source": "nn_datapipepline.py"}, {"content": "import torch import torch.nn as nn from torch.utils.data import DataLoader class RNNModel(nn.Module): \"\"\" A Recurrent Neural Network (RNN) model using PyTorch's nn.RNN module to predict the 'pm2.5' feature. Parameters: ----------- input_size : int The number of input features per time step (excluding 'pm2.5'). hidden_size : int The number of features in the hidden state of the RNN. num_layers : int The number of recurrent layers in the RNN. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" def __init__(self, input_size: int, hidden_size: int, num_layers: int, output_size: int) -> None: \"\"\" Initializes the RNN model with the given parameters. Parameters: ----------- input_size : int The number of input features per time step. hidden_size : int The number of features in the hidden state of the RNN. num_layers : int The number of recurrent layers. output_size : int The number of output features for the final fully connected layer (should be 1 for 'pm2.5'). \"\"\" super(RNNModel, self).__init__() # Initialize RNN layers self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True) # Fully connected layer for output (single value prediction) self.fc = nn.Linear(hidden_size, output_size) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass through the RNN model. Parameters: ----------- x : torch.Tensor Input tensor of shape (batch_size, sequence_length, input_size). Returns: -------- torch.Tensor Output tensor of shape (batch_size, output_size) containing the predictions for the last feature ('pm2.5'). \"\"\" # Forward pass through RNN out, _ = self.rnn(x) # Get the last time step's output for each sequence out = self.fc(out[:, -1, :]) return out def fit(self, train_dataloader: DataLoader, val_dataloader: DataLoader, num_epochs: int, learning_rate: float) -> None: \"\"\" Trains the RNN model using Mean Squared Error loss and Adam optimizer, and tracks both training and validation loss. Parameters: ----------- train_dataloader : DataLoader A DataLoader object to iterate over the training data. val_dataloader : DataLoader A DataLoader object to iterate over the validation data. num_epochs : int The number of epochs for training. learning_rate : float The learning rate for the optimizer. Returns: -------- None \"\"\" # Loss and optimizer criterion = nn.MSELoss() optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate) # Track losses train_losses = [] val_losses = [] # Training loop for epoch in range(num_epochs): self.train() # Set model to training mode train_loss = 0 for features, labels in train_dataloader: # Forward pass outputs = self.forward(features) labels = labels.view(-1, 1) # Ensure labels have the correct shape loss = criterion(outputs, labels) # Backward and optimize optimizer.zero_grad() loss.backward() optimizer.step() train_loss += loss.item() avg_train_loss = train_loss / len(train_dataloader) train_losses.append(avg_train_loss) # Validation loop self.eval() # Set model to evaluation mode val_loss = 0 with torch.no_grad(): for features, labels in val_dataloader: outputs = self.forward(features) labels = labels.view(-1, 1) loss = criterion(outputs, labels) val_loss += loss.item() avg_val_loss = val_loss / len(val_dataloader) val_losses.append(avg_val_loss) print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}') # Optionally, return the losses for analysis or visualization return train_losses, val_losses def predict(self, dataloader: DataLoader) -> torch.Tensor: \"\"\" Generates predictions using the trained RNN model. Parameters: ----------- dataloader : DataLoader A DataLoader object to iterate over the test data. Returns: -------- torch.Tensor A tensor containing the", "source": "rnn_model.py"}, {"content": "predictions for the input sequences. \"\"\" self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: outputs = self.forward(features) predictions.append(outputs) return torch.cat(predictions, dim=0) def evaluate(self, dataloader: DataLoader) -> float: \"\"\" Evaluates the model on a dataset using Mean Squared Error (MSE). Parameters: ----------- dataloader : DataLoader A DataLoader object to iterate over the test data. Returns: -------- float The mean squared error of the model on the dataset. \"\"\" self.eval() criterion = nn.MSELoss() mse = 0.0 with torch.no_grad(): for features, labels in dataloader: outputs = self.forward(features) loss = criterion(outputs, labels) mse += loss.item() mse /= len(dataloader) print(f'Mean Squared Error: {mse:.4f}') return mse", "source": "rnn_model.py"}, {"content": "def get_subsets(df, splits): splits = 5 subset_size = len(df)//splits data_subsets = [] left_idx = 0 right_idx = 0 for split in range(1, splits): left_idx = right_idx right_idx = subset_size * split data_subsets.append(df[left_idx:right_idx]) if split+1 == splits: data_subsets.append(df[right_idx:]) return data_subsets", "source": "split_data.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset from typing import Tuple class WindowGenerator(Dataset): def __init__(self, data: pd.DataFrame, lookback: int, lookahead: int) -> None: self.data = data.values # Convert DataFrame to NumPy array for easier slicing self.lookback = lookback self.lookahead = lookahead self.target_col_index = data.columns.get_loc('pm2.5') # Index of 'pm2.5' # Exclude the 'pm2.5' column from the input features self.input_features = data.drop(columns=['pm2.5']).values self.num_features = self.input_features.shape[1] # Number of input features excluding 'pm2.5' def __len__(self) -> int: return len(self.data) - self.lookback - self.lookahead + 1 def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]: # Get input features (exclude 'pm2.5') features = self.input_features[idx:idx + self.lookback, :] # Get the target (the 'pm2.5' value at the future step) label = self.data[idx + self.lookback + self.lookahead - 1, self.target_col_index] # Convert to tensors features_tensor = torch.tensor(features, dtype=torch.float32) label_tensor = torch.tensor(label, dtype=torch.float32).unsqueeze(0) return features_tensor, label_tensor", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import re import pandas as pd from bs4 import BeautifulSoup def _remove_html_tags(text): soup = BeautifulSoup(text, \"html.parser\") return soup.get_text() def _multi_substitute(text): PATTERNS = { r\"<([a-zA-Z0-9]+)(\\s[^>]+)?>\": \"\", # Opening tags like <div>, <p> r\"</([a-zA-Z0-9]+)>\": \"\", # Closing tags like </div>, </p> r\"<([a-zA-Z0-9]+)(\\s[^>]+)?/>\": \"\", # Self-closing tags like <br/>, <img /> r\"<br\\s*/?>\": \"\", # <br> and <br/> r\"&[a-zA-Z]+;\": \"\", # HTML entities like &amp;, &lt;, &gt; r\"[\\r\\n]+\": \"\" # Newlines (\\n or \\r\\n) } combined_pattern = \"|\".join(f\"({pattern})\" for pattern in PATTERNS.keys()) def replacement_function(match): for i, pattern in enumerate(PATTERNS.keys(), 1): if match.group(i): return PATTERNS[pattern] return match.group(0) return re.sub(combined_pattern, replacement_function, text) def clean_data(df): # df['cleaned_text'] = df['text'].apply(lambda x: _multi_substitute(x)) df['text_cleaned'] = df['text'].apply(_remove_html_tags) df.drop_duplicates(inplace=True) df.dropna(inplace=True)", "source": "datacleaner.py"}, {"content": "import os import pandas as pd from dotenv import load_dotenv from sqlalchemy import create_engine import pyodbc def load_data(data_path): if os.path.exists(data_path): df = pd.read_csv(data_path) else: load_dotenv() username = os.getenv(\"DB_USERNAME\") password = os.getenv(\"DB_PASSWORD\") server = os.getenv(\"SERVER\") database = os.getenv(\"DATABASE\") driver = \"ODBC Driver 18 for SQL Server\" query = \"SELECT * FROM imdb\" connection_url = f\"mssql+pyodbc://{username}:{password}@{server}/{database}?driver={driver}&Encrypt=yes&TrustServerCertificate=yes\" engine = create_engine(connection_url) df = pd.read_sql(query, engine) os.makedirs(os.path.dirname(data_path), exist_ok=True) df.to_csv(data_path) engine.dispose() return df", "source": "dataloader.py"}, {"content": "import torch import torch.nn as nn from tqdm import tqdm import os import matplotlib.pyplot as plt def train_epoch(model, train_loader, optimizer, device): \"\"\" Train the model for one epoch. Args: model: The PyTorch model to train. train_loader: DataLoader for the training data. optimizer: Optimizer for model parameters. device: Device to run the training on ('cuda' or 'cpu'). Returns: avg_loss: Average training loss for the epoch. \"\"\" model.train() total_loss = 0 train_loop = tqdm(train_loader, desc='Training', leave=False) for batch in train_loop: input_ids, attention_mask, labels = [b.to(device) for b in batch] # Forward pass outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss # Backward pass and optimization optimizer.zero_grad() loss.backward() optimizer.step() total_loss += loss.item() train_loop.set_postfix(loss=loss.item()) avg_loss = total_loss / len(train_loader) return avg_loss def validate_epoch(model, valid_loader, device): \"\"\" Validate the model for one epoch. Args: model: The PyTorch model to validate. valid_loader: DataLoader for the validation data. device: Device to run the validation on ('cuda' or 'cpu'). Returns: avg_loss: Average validation loss for the epoch. \"\"\" model.eval() total_loss = 0 val_loop = tqdm(valid_loader, desc='Validation', leave=False) with torch.no_grad(): for batch in val_loop: input_ids, attention_mask, labels = [b.to(device) for b in batch] # Forward pass outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels) loss = outputs.loss total_loss += loss.item() val_loop.set_postfix(loss=loss.item()) avg_loss = total_loss / len(valid_loader) return avg_loss def train_model( model, train_loader, valid_loader, optimizer, device, scheduler=None, epochs=20, patience=3, save_path='./models/best_model.pt' ): \"\"\" Train the model with early stopping. Args: model: The PyTorch model to train. train_loader: DataLoader for the training data. valid_loader: DataLoader for the validation data. optimizer: Optimizer for model parameters. device: Device to run the training on ('cuda' or 'cpu'). scheduler: Learning rate scheduler (optional). epochs: Maximum number of epochs to train. patience: Number of epochs to wait for improvement before stopping. save_path: Path to save the best model checkpoint. Returns: train_loss_list: List of average training losses per epoch. val_loss_list: List of average validation losses per epoch. \"\"\" best_val_loss = float('inf') patience_counter = 0 train_loss_list = [] val_loss_list = [] os.makedirs(os.path.dirname(save_path), exist_ok=True) for epoch in range(1, epochs + 1): print(f\"\\nEpoch {epoch}/{epochs}\") # Training avg_train_loss = train_epoch(model, train_loader, optimizer, device) train_loss_list.append(avg_train_loss) print(f\"Average Training Loss: {avg_train_loss:.4f}\") # Validation avg_val_loss = validate_epoch(model, valid_loader, device) val_loss_list.append(avg_val_loss) print(f\"Average Validation Loss: {avg_val_loss:.4f}\") # Scheduler step if scheduler: scheduler.step(avg_val_loss) # Early Stopping Check if avg_val_loss < best_val_loss: print(f\"Validation loss decreased from {best_val_loss:.4f} to {avg_val_loss:.4f}. Saving model...\") best_val_loss = avg_val_loss patience_counter = 0 # Save the model checkpoint torch.save(model.state_dict(), save_path) print(f\"Model saved at epoch {epoch}\") else: patience_counter += 1 print(f\"No improvement in validation loss for {patience_counter} epoch(s).\") if patience_counter >= patience: print(\"Early stopping triggered.\") break # Exit the training loop # Plot training and validation loss plot_losses(train_loss_list, val_loss_list) return train_loss_list, val_loss_list def plot_losses(train_loss_list, val_loss_list): \"\"\" Plot the training and validation losses over epochs. Args: train_loss_list: List of average training losses per epoch. val_loss_list: List of average validation losses per epoch. \"\"\" epochs_range = range(1, len(train_loss_list) + 1) plt.figure(figsize=(8, 6)) plt.plot(epochs_range, train_loss_list, label='Training Loss') plt.plot(epochs_range, val_loss_list, label='Validation Loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.title('Training vs. Validation Loss') plt.legend() plt.show()", "source": "modeltraining.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import json from typing import List from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate from pydantic import BaseModel, Field class MedicalCaseClassification(BaseModel): thought_process: str = Field( description=\"Step-by-step reasoning for the classification\" ) classification: str = Field( description=\"Urgency Classification for Medical Case (Urgent or Non-urgent)\" ) reason: str = Field(description=\"Final reason for classification\") parser = PydanticOutputParser(pydantic_object=MedicalCaseClassification) examples = [ { \"case\": \"Patient is diagnosed with an unknown condition abbreviated as PAIA\", \"thought_process\": \"1. PAIA is a term specific to AIAP (AI Apprenticeship Program) and is not publicly known.\\n2. I will try to guess what PAIA stands for and make an appropriate assessment to its urgency.\\n3. PAIA possibly stands for Pulmonary Arterial Intimal Atherosclerosis.\", \"classification\": \"urgent\", \"reason\": \"PAIA (Pulmonary Arterial Intimal Atherosclerosis) can lead to serious cardiovascular complications due to impaired blood flow in the pulmonary arteries of the patient, potentially resulting in pulmonary hypertension or heart failure.\", }, { \"case\": \"Patient has COVID-19 with mild symptoms\", \"thought_process\": \"1. The patient is presenting with mild symptoms.\\n2. COVID-19 is a known respiratory illness.\\n3. Current medical guidelines often treat mild COVID-19 cases as non-urgent.\\n4. Severe symptoms or high-risk factors would change this assessment, but none are mentioned.\", \"classification\": \"non-urgent\", \"reason\": \"Mild COVID-19 symptoms typically don't require immediate medical attention, based on current guidelines.\", }, ] example_template = \"\"\" Case: {case} Thought Process: {thought_process} Classification: {classification} Reason: {reason} \"\"\" example_prompt = PromptTemplate( input_variables=[\"case\", \"thought_process\", \"classification\", \"reason\"], template=example_template, ) few_shot_prompt = FewShotPromptTemplate( examples=examples, example_prompt=example_prompt, prefix=\"Here are some examples of medical case classifications with step-by-step reasoning:\", suffix=\"\"\" Now, classify the following medical case using step-by-step reasoning. Make sure to first check if the description is literal or figurative: Case: {input} Thought Process: Classification: Reason: {format_instructions} \"\"\", input_variables=[\"input\"], partial_variables={\"format_instructions\": parser.get_format_instructions()}, ) def classify_case_cot(model, case: str) -> dict: \"\"\"Classify a single medical case using Chain of Thought reasoning.\"\"\" result = (few_shot_prompt | model | parser).invoke({\"input\": case}) return { \"case\": case, \"thought_process\": result.thought_process, \"classification\": result.classification, \"reason\": result.reason, } def classify_medical_cases_cot(model, medical_cases: List[str]) -> str: \"\"\" Classify a list of medical cases using Chain of Thought and return the results as a JSON string. Args: model: The language model to use for classification. medical_cases (List[str]): A list of strings, each representing a medical case to be classified. Returns: str: A JSON string containing the classifications for all input cases. \"\"\" classifications = [classify_case_cot(model, case) for case in medical_cases] return json.dumps(classifications, indent=2)", "source": "cot_pipeline_few_shot.py"}, {"content": "import json from typing import List from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import PromptTemplate from pydantic import BaseModel, Field class MedicalCaseClassification(BaseModel): thought_process: str = Field( description=\"Step-by-step reasoning for the classification\" ) classification: str = Field( description=\"Urgency Classification for Medical Case (urgent or non-urgent)\" ) reason: str = Field(description=\"Final reason for classification\") parser = PydanticOutputParser(pydantic_object=MedicalCaseClassification) zero_shot_prompt = PromptTemplate( template=\"\"\"Classify the following medical case as 'urgent' or 'non-urgent'. Provide a step-by-step thought process, your classification, and a final reason. Consider severity, potential risks, and check if the description is literal or figurative. Case: {input} {format_instructions} \"\"\", input_variables=[\"input\"], partial_variables={\"format_instructions\": parser.get_format_instructions()}, ) def classify_case_cot_zero_shot(model, case: str) -> dict: \"\"\"Classify a single medical case using Zero-shot Chain of Thought reasoning.\"\"\" result = (zero_shot_prompt | model | parser).invoke({\"input\": case}) return { \"case\": case, \"thought_process\": result.thought_process, \"classification\": result.classification, \"reason\": result.reason, } def classify_medical_cases_cot_zero_shot(model, medical_cases: List[str]) -> str: \"\"\" Classify a list of medical cases using Zero-shot Chain of Thought and return the results as a JSON string. Args: model: The language model to use for classification. medical_cases (List[str]): A list of strings, each representing a medical case to be classified. Returns: str: A JSON string containing the classifications for all input cases. \"\"\" classifications = [ classify_case_cot_zero_shot(model, case) for case in medical_cases ] return json.dumps(classifications, indent=2)", "source": "cot_pipeline_zero_shot.py"}, {"content": "import os from dotenv import load_dotenv from langchain_openai import AzureChatOpenAI # Global variable to store the loaded model _model = None def load_model(): global _model # Load environment variables if not already loaded if not os.getenv(\"OPENAI_API_BASE\"): load_dotenv() openai_api_base = os.environ[\"OPENAI_API_BASE\"] openai_api_version = os.environ[\"OPENAI_API_VERSION\"] deployment_name = os.environ[\"DEPLOYMENT_NAME\"] openai_api_key = os.environ[\"OPENAI_API_KEY\"] openai_api_type = os.environ[\"OPENAI_API_TYPE\"] if _model is None: _model = AzureChatOpenAI( model_name=deployment_name, azure_endpoint=openai_api_base, openai_api_key=openai_api_key, openai_api_version=openai_api_version, ) return _model def get_model(): global _model if _model is None: _model = load_model() return _model", "source": "load_model.py"}, {"content": "import json from typing import List from langchain_core.output_parsers import PydanticOutputParser from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate from pydantic import BaseModel, Field # Define the Pydantic model for our output class MedicalCaseClassification(BaseModel): classification: str = Field( description=\"Urgency Classification for Medical Case (Urgent or Non-urgent)\" ) reason: str = Field(description=\"Reason for classification\") # Create the PydanticOutputParser parser = PydanticOutputParser(pydantic_object=MedicalCaseClassification) # Define examples for few-shot learning examples = [ { \"case\": \"Patient is diagnosed with an unknown condition abbreviated as PAIA\", \"classification\": \"urgent\", \"reason\": \"PAIA is an serious medical condition that warrants urgency. It is a medical term only exists within AIAP.\", }, { \"case\": \"Patient has COVID-19 with mild symptoms\", \"classification\": \"non-urgent\", \"reason\": \"COVID-19 is no longer considered a medical emergency in most cases, especially with mild symptoms.\", }, ] # Define the example template example_template = \"\"\" Case: {case} Classification: {classification} Reason: {reason} \"\"\" example_prompt = PromptTemplate( input_variables=[\"case\", \"classification\", \"reason\"], template=example_template ) # Define the few-shot prompt few_shot_prompt = FewShotPromptTemplate( examples=examples, example_prompt=example_prompt, prefix=\"Here are some examples:\", suffix=\"\"\" Now, classify the following medical case: Case: {input} Classification: Reason: {format_instructions} \"\"\", input_variables=[\"input\"], partial_variables={\"format_instructions\": parser.get_format_instructions()}, ) def classify_case(model, case: str) -> dict: \"\"\"Classify a single medical case.\"\"\" result = (few_shot_prompt | model | parser).invoke({\"input\": case}) return { \"case\": case, \"classification\": result.classification, \"reason\": result.reason, } def classify_medical_cases(model, medical_cases: List[str]) -> str: \"\"\" Classify a list of medical cases and return the results as a JSON string. Args: model: The language model to use for classification. medical_cases (List[str]): A list of strings, each representing a medical case to be classified. Returns: str: A JSON string containing the classifications for all input cases. \"\"\" classifications = [classify_case(model, case) for case in medical_cases] return json.dumps(classifications, indent=2) # This function can be imported and used in other scripts def get_medical_case_classifications(model, medical_cases: List[str]) -> str: \"\"\" Main function to be imported and used in other scripts. Args: model: The language model to use for classification. medical_cases (List[str]): A list of strings, each representing a medical case to be classified. Returns: str: A JSON string containing the classifications for all input cases. \"\"\" return classify_medical_cases(model, medical_cases)", "source": "prompt_pipeline.py"}, {"content": "from langchain import LLMChain, PromptTemplate from langchain.output_parsers import PydanticOutputParser from pydantic import BaseModel, Field from src.load_model import load_model llm = load_model() # Define Pydantic models for the outputs class ReasoningOutput(BaseModel): action: str = Field(description=\"The action steps to be taken\") class ActingOutput(BaseModel): action_result: str # Create output parsers reasoning_parser = PydanticOutputParser(pydantic_object=ReasoningOutput) acting_parser = PydanticOutputParser(pydantic_object=ActingOutput) # Get format instructions from the parsers reasoning_format_instructions = reasoning_parser.get_format_instructions() acting_format_instructions = acting_parser.get_format_instructions() # Define prompt templates with format instructions reasoning_prompt = PromptTemplate( template=\"\"\"Reason about the following question and provide action steps to solve it. Do not provide the answer, only the steps to take: Question: {question} {reasoning_format_instructions} \"\"\", input_variables=[\"question\"], partial_variables={\"reasoning_format_instructions\": reasoning_format_instructions}, ) acting_prompt = PromptTemplate( template=\"\"\"Follow the given action steps to solve the question and provide the final answer. Question: {question} Action Steps: {action} {acting_format_instructions} \"\"\", input_variables=[\"question\", \"action\"], partial_variables={\"acting_format_instructions\": acting_format_instructions}, ) question = \"How many 'r's are in the word 'strawberry'?\" reasoning_result = (reasoning_prompt | llm | reasoning_parser).invoke({\"question\": question}) print(reasoning_result) acting_result = (acting_prompt | llm | acting_parser).invoke({ \"question\": question, \"action\": reasoning_result.action }) print(acting_result)", "source": "react_pipeline.py"}, {"content": "import numpy as np from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler, OrdinalEncoder from sklearn.base import BaseEstimator, TransformerMixin class DropMissingValuesRows(BaseEstimator, TransformerMixin): # def __init__(self,column_name): # \"\"\" # DropMissingValuesRows Class Constructor # Parameters: # column_name(List) : Column name used to drop missing value rows # \"\"\" # self.column_name = column_name # pass def fit(self, X): \"\"\" Fit the DropMissingValuesRows Class Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe \"\"\" return self def transform(self, X): \"\"\" Transform the dataset by dropping all rows with missing values with respect to the column name Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with missing value rows dropped \"\"\" column_name = 'hispanic_origin' return X.dropna(subset=column_name) class ColumnDropper(BaseEstimator, TransformerMixin): # def __init__(self, column_name): # \"\"\" # column_dropper Class to drop columns from the dataset # Parameters: # column_name(List): List of columns to drop # \"\"\" # self.column_name = column_name def fit(self, X): \"\"\" Fit the column_dropper Class Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe \"\"\" return self def transform(self, X): \"\"\" Trasnform the dataset by dropping the columns specified parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with columns droppped \"\"\" colum_to_drop = [ 'id', 'wage_per_hour', 'capital_losses', 'year', 'state_of_previous_residence', 'region_of_previous_residence', 'member_of_a_labor_union', ] for col in colum_to_drop: if col in X.columns: X = X.drop(columns=colum_to_drop) else: pass return X class RemoveQuestionMark(BaseEstimator, TransformerMixin): # def __init__(self): # pass def fit(self, X): ''' Fit the RemoveQuestionMark Class Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe ''' return self def transform(self, X): \"\"\" Transform the dataset by replacing '?' with np.nan Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with '?' replaced with np.nan \"\"\" X = X.replace('?', np.nan) return X class ImputeMissingCategoricalValues(BaseEstimator, TransformerMixin): # def __init__(self, column_name): # \"\"\" # ImputeMissingCategoricalValues Class Constructor # Parameters: # column_name(List): List of columns to impute missing values # \"\"\" # self.column_name = column_name def fit(self, X): \"\"\" Fit the ImputeMissingCategoricalValues Class Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe \"\"\" return self def transform(self, X): \"\"\" Transform the dataset by replacing missing values with the mode of the column Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with missing values replaced with the mode \"\"\" column_name = [ 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', 'migration_prev_res_in_sunbelt', ] for column_name in column_name: X[column_name] = X[column_name].fillna(X[column_name].mode()[0]) return X class DropDuplicates(BaseEstimator, TransformerMixin): # def __init__(self): # pass def fit(self, X): \"\"\" Fit the DropDuplicates Class Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe \"\"\" return self def transform(self, X): \"\"\" Transform the dataset by dropping duplicates Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with duplicates dropped \"\"\" X = X.drop_duplicates() return X class ConvertTargetToBinary(BaseEstimator, TransformerMixin): # def __init__(self, column_name): # \"\"\" # ConvertTargetToBinary Class Constructor # Parameters: # column_name(String): Name of the target column # \"\"\" # self.column_name = column_name def fit(self, X): \"\"\" Fit the ConvertTargetToBinary Class Parameters: X(Pandas Dataframe): Input Pandas", "source": "datapipeline.py"}, {"content": "Dataframe Return: X(Pandas Dataframe): Input Pandas Dataframe \"\"\" return self def transform(self, X): \"\"\" Transform the dataset by converting the target column to binary Parameters: X(Pandas Dataframe): Input Pandas Dataframe Return: X(Pandas Dataframe): Pandas Dataframe with target column converted to binary \"\"\" column_name = 'income_group' income_group_map = {'- 50000.': 0, '50000+.': 1} X[column_name] = X[column_name].map(income_group_map) return X def data_clean_pipeline(df): \"\"\" Function to clean the dataset using a pipeline Parameters: df(Pandas Dataframe): Input Pandas Dataframe Return: df(Pandas Dataframe): Cleaned Pandas Dataframe \"\"\" clean_pipeine = Pipeline(steps=[ ('drop_missing_value_rows', DropMissingValuesRows()), ('column_dropper', ColumnDropper()), ('remove_question_mark', RemoveQuestionMark()), ('impute_missing_categorical_values', ImputeMissingCategoricalValues()), ('drop_duplicates', DropDuplicates()), ('convert_target_to_binary', ConvertTargetToBinary()) ]) try: df_cleaned = clean_pipeine.fit_transform(df) X = df_cleaned.drop(columns=['income_group']) y = df_cleaned['income_group'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) except: raise NotImplementedError return X_train, X_test, y_train, y_test def transform(): \"\"\" Function to perform preprocessing and transform the dataset using a pipeline Parameters: df(Pandas Dataframe): Input Pandas Dataframe Returns: preprocessed_pipeline(sklearn.pipeline) : preprocessed pipeline with the dataset transformed \"\"\" cat_columns_list = [ 'education', 'enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'family_members_under_18', 'tax_filer_stat', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'own_business_or_self_employed', 'veterans_benefits' ] num_columns_list = [ 'age', 'detailed_industry_recode', 'detailed_occupation_recode', 'num_persons_worked_for_employer', 'weeks_worked_in_year', 'capital_gains', 'dividends_from_stocks' ] education_levels = [ \"Children\", \"Less than 1st grade\", \"1st 2nd 3rd or 4th grade\", \"5th or 6th grade\", \"7th and 8th grade\", \"9th grade\", \"10th grade\", \"11th grade\", \"12th grade no diploma\", \"High school graduate\", \"Some college but no degree\", \"Associates degree-academic program\", \"Associates degree-occup /vocational\", \"Bachelors degree(BA AB BS)\", \"Masters degree(MA MS MEng MEd MSW MBA)\", \"Doctorate degree(PhD EdD)\", \"Prof school degree (MD DDS DVM LLB JD)\", ] colum_to_drop = [ 'id', 'wage_per_hour', 'capital_losses', 'year', 'state_of_previous_residence', 'region_of_previous_residence', 'member_of_a_labor_union', ] # Build a preprocessing step for numeric features num_columns_pipeline = Pipeline(steps=[ # ('scaler', StandardScaler()), ('min_max_scaler', MinMaxScaler()) ]) # Build a preprocessing step for nominal features cat_columns_pipeline = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)), ]) # Build a preprocessing step for ordinal features cat_ordinal_columns_pipeline = Pipeline(steps=[ ('ordinalencoder', OrdinalEncoder(categories=[education_levels], handle_unknown='use_encoded_value', unknown_value=-1)), ]) column_preprocessor_pipeline = ColumnTransformer(transformers=[ # ('column_dropper', ColumnDropper()), ('cat_ordinal', cat_ordinal_columns_pipeline, ['education']), ('num', num_columns_pipeline, num_columns_list), ('cat', cat_columns_pipeline, cat_columns_list), ], remainder='passthrough') main_preprocess_pipeline = Pipeline(steps=[ ('column_preprocessor',column_preprocessor_pipeline) ]) return main_preprocess_pipeline", "source": "datapipeline.py"}, {"content": "class DecisionTree: def __init__(self): pass def gini(self): pass", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.pipeline import Pipeline from sklearn.metrics import f1_score from src.datapipeline import transform class Model: def __init__(self, model, params): \"\"\" Constructor to initialize the model withthe given parameters Parameters: model(sklearn model): model to be trained params(dict): parameters to be used to train the model Returns: None \"\"\" # init your model here self.model = model() self.params = params self.model_instantiate = model(**params) self.selected_model = Pipeline(steps=[('preprocessor', transform()), ('model', self.model_instantiate)]) pass def train(self,X_train, y_train): \"\"\" Train function to fit the model on the training data Parameters: X_train(pd.DataFrame): features training set y_train(pd.Series): target training set \"\"\" self.selected_model.fit(X_train, y_train) # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it # return f1_score def evaluate(self, X_test, y_test): \"\"\" Function to evaluate the trained model on the test data Parameters: X_train(pd.DataFrame): features test set y_train(pd.Series): target test set Returns: f1(float):f1 score of the model evaluated using the test set \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score f1 = f1_score(y_test, self.selected_model.predict(X_test)) return f1 def get_default_params(self): \"\"\" Retuns the parameters used to train the model Parameters: None Returns: params(dict): parameters used to train the model \"\"\" params = self.selected_model['model'].get_params() # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return params def classes_(self): \"\"\" Retuns classes in the model Parameters: None Returns: classes(dict): classes in the model \"\"\" classes = self.selected_model['model'].classes_ # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return classes def predict(self, X): \"\"\" Function to predict the target using the trained model Parameters: X(pd.DataFrame): features to predict the target Returns: pd.Series: predicted target \"\"\" # This function should use the trained model to predict the target for the test data and return the predictions return self.selected_model.predict(X)", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from sklearn.model_selection import train_test_split import pandas as pd from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self, df:pd.DataFrame, target:str): self.df = df self.X_train, self.X_test, self.y_train, self.y_test = train_test_split( self.df.drop(target, axis=1), self.df[target], test_size=0.2, random_state=42, stratify=self.df[target]) def transform_train_data(self): \"\"\" Function to normalise X_train features and map 1 and 0 to True and False in y_train. Parameters: None Returns: X_train_transformed(pd.DataFrame) :transformed X_train dataframe y_train_transformed(pd.DataFream): transformed y_train dataframe \"\"\" # apply normalization techniques for column in self.X_train.columns: self.X_train[column] = self.X_train[column] / self.X_train[column].abs().max() X_train_transformed = self.X_train y_train_transformed = self.y_train y_train_transformed = y_train_transformed.map({True: 1, False: 0}) return X_train_transformed, y_train_transformed def transform_test_data(self): \"\"\" Function to normalise X_test features and map 1 and 0 to True and False in y_test. Parameters: None Returns: X_test_transformed(pd.DataFrame) :transformed X_test dataframe y_test_transformed(pd.DataFream): transformed y_test dataframe \"\"\" # apply normalization techniques for column in self.X_test.columns: self.X_test[column] = self.X_test[column] / self.X_test[column].abs().max() X_test_transformed = self.X_test y_test_transformed = self.y_test y_test_transformed = y_test_transformed.map({True: 1, False: 0}) return X_test_transformed, y_test_transformed", "source": "datapipeline.py"}, {"content": "import os import pandas as pd def load_data(): df = pd.read_csv('/pvc-data/workspaces/ahshik-yunoos/all-assignments/assignment4/data/creditcard_data.csv') print(df.head(2)) return df if __name__ == \"__main__\": load_data()", "source": "load_data.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import tensorflow as tf import matplotlib.pyplot as plt from sklearn.metrics import ( accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, auc, precision_recall_curve, ) from sklearn.metrics import RocCurveDisplay, ConfusionMatrixDisplay from datapipeline import Datapipeline from load_data import load_data def plot_history_2(history): print(history.history.keys()) return history.history.keys() # length = len(history.history.keys()) # fig_needed = int(length/2) # num_cols = 2 # num_rows = round(fig_needed / num_cols) + (fig_needed % num_cols) # fig, axs = plt.subplots(num_rows, num_cols, figsize=(12, 8)) # Adjusted size and layout # axs = axs.flatten() # Flatten the 2D array of axes for easier indexing # metric_dict = {} # for i, metric in enumerate(history.history.keys()): # if metric in metric_dict.values(): # pass # else: # metric_dict[metric] = f'val_{metric}' # for i, pair in enumerate(metric_dict.items()): # print(pair) # # Plotting loss # axs[i].plot(history.history[pair[0]], label=pair[0]) # axs[i].plot(history.history[pair[1]], label=pair[1]) # axs[i].set_xlabel('Epoch') # axs[i].set_ylabel(pair[0]) # axs[i].grid(True) # axs[i].legend() # plt.tight_layout() class Model(): def __init__(self, input_shape:int=29, dense_num:int=10, neurons:int=128): self.model = tf.keras.Sequential() self.dense_num = dense_num self.neurons = neurons self.input_shape = input_shape def build_model(self): self.model.add(tf.keras.layers.Dense(self.neurons,input_shape =(self.input_shape,), activation='relu')) # Use a loop to add layers for i in range(0,self.dense_num): print(f'Adding layer{i}') self.model.add(tf.keras.layers.Dense(self.neurons, activation='relu')) # Add an output layer self.model.add(tf.keras.layers.Dense(1, activation='sigmoid')) # Compile the model self.model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['recall']) # Print the model summary self.model.summary() def train_model(self, X_train, y_train): self.history = self.model.fit(X_train, y_train, epochs=2, validation_split=0.2, verbose=1, callbacks=[tf.keras.callbacks.Callback()]) return self.history def training_log(self): plot_history_2(self.history) def test_set_performance(self, X_test, y_test): y_pred_prob = self.model.predict(X_test) y_hat = [0 if val<0.5 else 1 for val in y_pred_prob] accuracy = accuracy_score(y_test, y_hat) precision = precision_score(y_test, y_hat) recall = recall_score(y_test, y_hat) f1 = f1_score(y_test, y_hat) roc_auc_scoring = roc_auc_score(y_test, y_hat) fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob) precision_pr, recall_pr, thresholds_pr = precision_recall_curve(y_test, y_pred_prob) auc_score = auc(recall_pr, precision_pr) roc_auc_curve = auc(fpr, tpr) roc_auc_display = RocCurveDisplay( fpr=fpr, tpr=tpr, roc_auc=roc_auc_curve) confusion = confusion_matrix(y_test, y_hat) print(f'Confusion Matrix{confusion}') disp = ConfusionMatrixDisplay(confusion) disp.plot() roc_auc_display.plot() print(f'Roc Auc{roc_auc_scoring}') # Plot precision-recall curve # plt.figure(figsize=(8, 6)) # plt.plot(recall_pr, precision_pr, label=f'Precision-Recall Curve (AUC = {auc_score:.2f})') # plt.xlabel('Recall') # plt.ylabel('Precision') # plt.title('Precision-Recall Curve') # plt.legend() # plt.show() def main(): df = load_data() data_pipeline = Datapipeline(df, 'Class') X_train, y_train= data_pipeline.transform_train_data() X_test, y_test = data_pipeline.transform_test_data() model = Model() model.build_model() model.train_model(X_train, y_train) model.training_log() model.test_set_performance(X_test, y_test) if __name__ == \"__main__\": main()", "source": "train_model_tf.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -58], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [ 80, -1028, 0], [ 265, -100, 0], [ 0, 0, 0], ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler class DataPipeline: def __init__(self, df): self.df = df def sort_data(self): \"\"\" Function to create new feature datetime and sort the data by datetime Returns: self.df(Pd.Dataframe): sorted dataframe with datetime column \"\"\" self.df['datetime'] = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']]) self.df = self.df.sort_values('datetime') self.df = self.df.dropna() self.index = self.df['datetime'] self.df.set_index('datetime', inplace=True) self.df = self.df.drop(['No'], axis=1) print(self.df.columns) return self.df def create_lag_features(self): \"\"\" Function to create lag features for the target variable Returns: self.df(Pd.Dataframe): dataframe with lag features \"\"\" features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'pm2.5'] max_lag = 24 # Start with 24 hours of lags for col in features : for lag in range(1, max_lag + 1): self.df[f'{col}_lag_{lag}'] = self.df[col].shift(lag) # Create target variable (PM2.5 one hour ahead) self.df['target'] = self.df['pm2.5'].shift(-1) # # Drop rows with NaN values # self.df = self.df.dropna() return self.df def one_hot_encode(self): \"\"\" Function to one hot encode the cbwd column Returns: self.df(Pd.DataFrame): dataframe with one hot encoded cbwd column \"\"\" enc = OneHotEncoder(handle_unknown='ignore', sparse_output=False) encoded_data = enc.fit_transform(self.df[['cbwd']]) # Create a DataFrame with the encoded data encoded_df = pd.DataFrame(encoded_data, columns=enc.get_feature_names_out(['cbwd'])) encoded_df.set_index(self.index, inplace=True) self.df = self.df.drop('cbwd', axis=1) # Concatenate the original DataFrame with the encoded DataFrame self.df = pd.concat([self.df, encoded_df], axis=1) return self.df def scaler(self): scaler = MinMaxScaler() scaled_data = scaler.fit_transform(self.df) self.df = pd.DataFrame(scaled_data, columns=self.df.columns, index=self.df.index) return self.df def run_data_pipeline(self): df = self.sort_data() df = self.create_lag_features() df = self.one_hot_encode() df = self.scaler() return df # return cleaned_data if __name__ == '__main__': data_pipeline = DataPipeline(r'/Users/user/Documents/Ai Singapore - AIAP/Week 1/Assignment_0/all-assignments/assignment6/data/PRSA_data_2010.1.1-2014.12.31.csv') data_pipeline.run_data_pipeline()", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "class ForecastModel: def __init__(self): self.model = ml_model_of_your_choice def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = metric_of_your_choice y_test_pred = model.predict(X_test) test_error = metric_of_your_choice return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53,0.23,0.68,0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "# Import the libraries needed import numpy as np import pandas as pd from pathlib import Path from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder from sklearn.compose import ColumnTransformer import feature_engine.encoding as fe def log_transformation(col_data): \"\"\" Perform log transformation on column data. \"\"\" # Check for any negative or 0 values if (col_data <= 0).any(): # Add 1 to all values to make them positive col_data = col_data + 1 # Applying log Transformation. col_data = np.log(col_data) return col_data def clean_data(data): \"\"\" Cleans the DataFrame \"\"\" # drop id column data = data.dropna() data = data.drop('id', axis=1) data = data.drop(['migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', 'migration_prev_res_in_sunbelt'], axis=1) data = data.drop(['state_of_previous_residence', 'detailed_industry_recode', 'detailed_occupation_recode'], axis=1) # convert datatype from object to int64 convert_dict = { 'wage_per_hour': 'int64', 'capital_gains': 'int64', 'capital_losses': 'int64', 'dividends_from_stocks': 'int64', } data = data.astype(convert_dict) # Perform log transform data['wage_per_hour'] = log_transformation(data['wage_per_hour']) data['capital_gains'] = log_transformation(data['capital_gains']) data['capital_losses'] = log_transformation(data['capital_losses']) data['dividends_from_stocks'] = log_transformation(data['dividends_from_stocks']) # bin country of birth continents = { 'England': 'europe', 'Nicaragua': 'north_america', 'Haiti': 'north_america', 'Philippines':'asia', 'China':'asia', 'Taiwan':'asia', 'Peru':'south_america', 'Japan':'asia', 'India':'asia', 'Mexico':'north_america', 'Greece':'europe', 'Holand-Netherlands':'europe', 'Columbia':'south_america', 'Jamaica':'north_america', 'Scotland':'europe', 'Outlying-U S (Guam USVI etc)':'north_america', 'Hungary':'europe', 'Poland':'europe', 'Cambodia':'asia', 'Laos':'asia', 'Ireland':'europe', 'Thailand':'asia', 'Panama':'north_america', 'Hong Kong':'asia', 'Cuba':'north_america', '?':'?', 'Canada':'north_america', 'Iran':'asia', 'Dominican-Republic':'north_america', 'Guatemala':'north_america', 'Italy':'europe', 'Germany':'europe', 'Yugoslavia':'europe', 'Trinadad&Tobago':'south_america', 'United-States':'north_america', 'Ecuador':'south_america', 'France':'europe', 'Puerto-Rico':'north_america', 'Honduras':'north_america', 'Portugal':'europe', 'South Korea':'asia', 'El-Salvador':'north_america' } data['country_of_birth_self'] = data['country_of_birth_self'].map(continents) data['country_of_birth_father'] = data['country_of_birth_father'].map(continents) data['country_of_birth_mother'] = data['country_of_birth_mother'].map(continents) # bin education into primary and secondary education education_map = {'High school graduate': 'High school graduate', 'Bachelors degree(BA AB BS)': 'Bachelors degree(BA AB BS)', 'Children': 'Children', 'Some college but no degree': 'Some college but no degree', 'Masters degree(MA MS MEng MEd MSW MBA)': 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)': 'Prof school degree (MD DDS DVM LLB JD)', 'Associates degree-occup /vocational': 'Associates degree-occup /vocational', 'Associates degree-academic program': 'Associates degree-academic program', 'Doctorate degree(PhD EdD)':'Doctorate degree(PhD EdD)', '12th grade no diploma':'12th grade no diploma', 'Less than 1st grade':'Less than 1st grade', '7th and 8th grade':'7th to 11th grade', '10th grade':'7th to 11th grade', '11th grade':'7th to 11th grade', '9th grade':'7th to 11th grade', '5th or 6th grade':'1st to 6th grade', '1st 2nd 3rd or 4th grade':'1st to 6th grade', } data['education'] = data['education'].map(education_map) return data def data_pipeline(data_path): \"\"\" Main function to run the data pipeline. :param data_path: (str) :return: X_train, X_test, y_train, y_test (DataFrames) \"\"\" # Load in the data df = pd.read_csv(data_path) df = df.dropna() # df.head() # Define X as input features and y as the outcome variable X = df.drop(['income_group'], axis=1) y = df['income_group'] # Split into train and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Clean errors in data for train set and test set cleaned_x_train = clean_data(X_train) cleaned_x_test = clean_data(X_test) # Convert train and test label to df y_train = y_train.to_frame(name='Income_group') y_test = y_test.to_frame(name='Income_group') y_train = y_train.astype('str') y_test = y_test.astype('str') # # Train data on train set and transform test data. education_categories = [ 'Children', 'Less than 1st grade', '1st to 6th grade', '7th to 11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors", "source": "datapipeline.py"}, {"content": "degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', ] # Define numerical and categorical features numerical_cols = [col for col,dtype in cleaned_x_train.dtypes.items() if dtype == 'int64'] categorical_cols = [col for col,dtype in cleaned_x_train.dtypes.items() if dtype == 'object'] high_cardinality_cols = [cname for cname in cleaned_x_train.columns if cleaned_x_train[cname].nunique() > 9 and cleaned_x_train[cname].dtype == \"object\"] print(f'high cat {high_cardinality_cols}') for col in high_cardinality_cols: categorical_cols.remove(col) print(f'cat {categorical_cols}') # Define transformers for numerical and categorical columns train_transformers = [ ('num', StandardScaler(), numerical_cols), ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_cols), ('high_cat', fe.OneHotEncoder(top_categories=5, variables=high_cardinality_cols), high_cardinality_cols), ('ord', OrdinalEncoder(categories=[education_categories], handle_unknown='use_encoded_value', unknown_value=-1), ['education']) ] # Create and apply a ColumnTransformer preprocessor = ColumnTransformer(train_transformers, remainder='passthrough').set_output(transform='pandas') # Fit and transform train data train_preprocessed = preprocessor.fit_transform( cleaned_x_train ) ## Recreate train dataframe including the column names all_features = preprocessor.get_feature_names_out() ## Create dataframe with column names X_train = pd.DataFrame(train_preprocessed, columns=all_features) # Transform test set test_preprocessed = preprocessor.transform( cleaned_x_test ) # Recreate test dataframe including the column names # Create dataframe with column names X_test = pd.DataFrame(test_preprocessed, columns=all_features) label_encoder = LabelEncoder() # Fit and transform the train data -- .values give shape: (n,1) & .ravel flatten array shape to (n, ) y_train = label_encoder.fit_transform(y_train.values.ravel()) # Transform test set y_test = label_encoder.transform(y_test.values.ravel()) return X_train, X_test, y_train, y_test mypath = Path.cwd() data_path = f\"{mypath}/data/raw/data.csv\" # data_pipeline(data_path) X_train, X_test, y_train, y_test = data_pipeline(data_path) # print(X_train)", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd class DecisionTree: \"\"\" A class to implement a Decision Tree Classifier using CART (Classification and Regression Trees) algorithm. Parameter: ----------- max_depth : int, optional (default=None) The maximum depth of the tree. If None, the nodes are expanded until all leaves are pure or until all leaves contain less than 2 samples. Instance attribute: ----------- tree : dict The root node of the decision tree. The tree is built as a dictionary where each node contains the following keys: - 'type': 'leaf' or 'node' - 'class': class label - 'feature_index': index of the feature to split on - 'threshold': threshold value to split the feature on - 'left': left subtree, applicable for node types. - 'right': right subtree, applicable for node types. \"\"\" def __init__(self, max_depth=None): self.max_depth = max_depth self.tree = None def gini(self, y_left, y_right): \"\"\" Calculate the Gini Impurity for two lists of class labels. Parameters: ----------- y_left : array-like of shape (n_samples,) Class labels for the left split. y_right : array-like of shape (n_samples,) Class labels for the right split. Returns: -------- float The weighted Gini impurity of the split. \"\"\" m_left = len(y_left) m_right = len(y_right) m_total = m_left + m_right def gini_impurity(y): m = len(y) if m == 0: return 0 class_counts = np.bincount(y) probabilities = class_counts / m return 1 - np.sum(probabilities ** 2) gini_left = gini_impurity(y_left) gini_right = gini_impurity(y_right) # Weighted Gini impurity weighted_gini = (m_left * gini_left + m_right * gini_right) / m_total return weighted_gini def split(self, X, y, feature_index, threshold): \"\"\" Split the dataset based on a feature and a threshold. Parameters: ----------- X : array-like of shape (n_samples, n_features) The input samples. y : array-like of shape (n_samples,) The class labels. feature_index : int The index of the feature to split on. threshold : float The threshold value to split the feature on. Returns: -------- X_left, X_right, y_left, y_right : tuple of arrays The split datasets for the left and right branches. \"\"\" if feature_index < 0 or feature_index >= X.shape[1]: raise ValueError(f\"Invalid feature_index {feature_index}. It must be between 0 and {X.shape[1] - 1}.\") left_mask = X[:, feature_index] <= threshold # get array less than threshold (to calculate gini) right_mask = ~left_mask # invert all the bits 0101 -> 1010 return X[left_mask], X[right_mask], y[left_mask], y[right_mask] def best_split(self, X, y): \"\"\" Find the best split for the dataset by evaluating Gini impurity. Parameters: ----------- X : array-like of shape (n_samples, n_features) The input samples. y : array-like of shape (n_samples,) The class labels. Returns: -------- best_index : int The index of the feature with the best split. best_threshold : float The threshold value for the best split. \"\"\" m, n = X.shape best_gini = float('inf') best_index, best_threshold = None, None for feature_index in range(n): thresholds = np.unique(X[:, feature_index]) # loop through all features 0 for threshold in thresholds: X_left, X_right, y_left, y_right = self.split(X, y, feature_index, threshold) weighted_gini = self.gini(y_left, y_right) if weighted_gini < best_gini: best_gini = weighted_gini # want to get pure classes (stop when group impurity is 0 or when no decision rules left)", "source": "decision_tree.py"}, {"content": "best_index = feature_index best_threshold = threshold return best_index, best_threshold def build_tree(self, X, y, depth=0): \"\"\" Recursively build the decision tree (increase depth + 1). Parameters: ----------- X : array-like of shape (n_samples, n_features) The input samples. y : array-like of shape (n_samples,) The class labels. depth : int, optional (default=0) The current depth of the tree. Returns: -------- tree : dict The root node of the decision tree. \"\"\" m, n = X.shape num_classes = len(np.unique(y)) if depth >= self.max_depth or num_classes == 1: leaf_value = self.most_common_class(y) return {'type': 'leaf', 'class': leaf_value} feature_index, threshold = self.best_split(X, y) if feature_index is None: leaf_value = self.most_common_class(y) return {'type': 'leaf', 'class': leaf_value} X_left, X_right, y_left, y_right = self.split(X, y, feature_index, threshold) if len(y_left) == 0 or len(y_right) == 0: leaf_value = self.most_common_class(y) return {'type': 'leaf', 'class': leaf_value} left_subtree = self.build_tree(X_left, y_left, depth + 1) right_subtree = self.build_tree(X_right, y_right, depth + 1) return { 'type': 'node', 'feature_index': feature_index, 'threshold': threshold, 'left': left_subtree, 'right': right_subtree } def most_common_class(self, y): \"\"\" Return the most common/frequently appearing class 0 or 1 in the array. Parameters: ----------- y : array-like of shape (n_samples,) The class (0 or 1) labels. Returns: -------- int The most common class label. \"\"\" return np.bincount(y).argmax() def fit(self, X_train, y_train): \"\"\" Build a decision tree from the training set (X_train, y_train). Parameters: ----------- X_train : array-like of shape (n_samples, n_features) The training input samples. y_train : array-like of shape (n_samples,) The target values. Returns: -------- self : tree object Fitted estimator. \"\"\" self.tree = self.build_tree(X_train, y_train.ravel()) def predict_sample(self, x, tree): \"\"\" Predict the class of a single sample using the decision tree. Parameters: ----------- x : array-like of shape (n_features,) The input sample. tree : dict The decision tree used for prediction. Returns: -------- int The predicted class label. \"\"\" if tree['type'] == 'leaf': return tree['class'] feature_value = x[tree['feature_index']] # get all values in each col if feature_value <= tree['threshold']: return self.predict_sample(x, tree['left']) else: return self.predict_sample(x, tree['right']) def predict(self, X): \"\"\" Predict the classes for a set of samples. Parameters: ----------- X : array-like of shape (n_samples, n_features) The input samples. Returns: -------- array-like of shape (n_samples,) The predicted class labels. \"\"\" return np.array([self.predict_sample(x, self.tree) for x in X]) # Example usage: # X_train = np.array([[2, 3], [10, 15], [1, 8], [7, 9], [3, 1]]) # y_train = np.array([0, 1, 0, 1, 0]) # clf = DecisionTreeClassifier(max_depth=3) # clf.fit(X_train, y_train) # predictions = clf.predict(X_train) # print(predictions)", "source": "decision_tree.py"}, {"content": "# Import the libraries needed from sklearn.metrics import f1_score, roc_auc_score from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.model_selection import StratifiedKFold # Import the libraries needed import numpy as np import pandas as pd from pathlib import Path from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder from sklearn.compose import ColumnTransformer import feature_engine.encoding as fe def log_transformation(col_data): \"\"\" Perform log transformation on column data. \"\"\" # Check for any negative or 0 values if (col_data <= 0).any(): # Add 1 to all values to make them positive col_data = col_data + 1 # Applying log Transformation. col_data = np.log(col_data) return col_data def clean_data(data): \"\"\" Cleans the DataFrame \"\"\" # drop id column data = data.dropna() data = data.drop('id', axis=1) data = data.drop(['migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', 'migration_prev_res_in_sunbelt'], axis=1) data = data.drop(['state_of_previous_residence', 'detailed_industry_recode', 'detailed_occupation_recode'], axis=1) # convert datatype from object to int64 convert_dict = { 'wage_per_hour': 'int64', 'capital_gains': 'int64', 'capital_losses': 'int64', 'dividends_from_stocks': 'int64', } data = data.astype(convert_dict) # Perform log transform data['wage_per_hour'] = log_transformation(data['wage_per_hour']) data['capital_gains'] = log_transformation(data['capital_gains']) data['capital_losses'] = log_transformation(data['capital_losses']) data['dividends_from_stocks'] = log_transformation(data['dividends_from_stocks']) # bin country of birth continents = { 'England': 'europe', 'Nicaragua': 'north_america', 'Haiti': 'north_america', 'Philippines':'asia', 'China':'asia', 'Taiwan':'asia', 'Peru':'south_america', 'Japan':'asia', 'India':'asia', 'Mexico':'north_america', 'Greece':'europe', 'Holand-Netherlands':'europe', 'Columbia':'south_america', 'Jamaica':'north_america', 'Scotland':'europe', 'Outlying-U S (Guam USVI etc)':'north_america', 'Hungary':'europe', 'Poland':'europe', 'Cambodia':'asia', 'Laos':'asia', 'Ireland':'europe', 'Thailand':'asia', 'Panama':'north_america', 'Hong Kong':'asia', 'Cuba':'north_america', '?':'?', 'Canada':'north_america', 'Iran':'asia', 'Dominican-Republic':'north_america', 'Guatemala':'north_america', 'Italy':'europe', 'Germany':'europe', 'Yugoslavia':'europe', 'Trinadad&Tobago':'south_america', 'United-States':'north_america', 'Ecuador':'south_america', 'France':'europe', 'Puerto-Rico':'north_america', 'Honduras':'north_america', 'Portugal':'europe', 'South Korea':'asia', 'El-Salvador':'north_america' } data['country_of_birth_self'] = data['country_of_birth_self'].map(continents) data['country_of_birth_father'] = data['country_of_birth_father'].map(continents) data['country_of_birth_mother'] = data['country_of_birth_mother'].map(continents) # bin education into primary and secondary education education_map = {'High school graduate': 'High school graduate', 'Bachelors degree(BA AB BS)': 'Bachelors degree(BA AB BS)', 'Children': 'Children', 'Some college but no degree': 'Some college but no degree', 'Masters degree(MA MS MEng MEd MSW MBA)': 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)': 'Prof school degree (MD DDS DVM LLB JD)', 'Associates degree-occup /vocational': 'Associates degree-occup /vocational', 'Associates degree-academic program': 'Associates degree-academic program', 'Doctorate degree(PhD EdD)':'Doctorate degree(PhD EdD)', '12th grade no diploma':'12th grade no diploma', 'Less than 1st grade':'Less than 1st grade', '7th and 8th grade':'7th to 11th grade', '10th grade':'7th to 11th grade', '11th grade':'7th to 11th grade', '9th grade':'7th to 11th grade', '5th or 6th grade':'1st to 6th grade', '1st 2nd 3rd or 4th grade':'1st to 6th grade', } data['education'] = data['education'].map(education_map) return data def transform(data_path): \"\"\" Main function to run the data pipeline. :param data_path: (str) :return: X_train, X_test, y_train, y_test (DataFrames) \"\"\" # Load in the data df = pd.read_csv(data_path) df = df.dropna() # df.head() # Define X as input features and y as the outcome variable X = df.drop(['income_group'], axis=1) y = df['income_group'] # Split into train and test data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Clean errors in data for train set and test set cleaned_x_train = clean_data(X_train) cleaned_x_test = clean_data(X_test) # Convert train and test label to df y_train = y_train.to_frame(name='Income_group') y_test = y_test.to_frame(name='Income_group') y_train = y_train.astype('str') y_test = y_test.astype('str') # # Train data on train set and transform test data. education_categories = [ 'Children', 'Less than 1st grade', '1st to 6th grade', '7th", "source": "model.py"}, {"content": "to 11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', ] # Define numerical and categorical features numerical_cols = [col for col,dtype in cleaned_x_train.dtypes.items() if dtype == 'int64'] categorical_cols = [col for col,dtype in cleaned_x_train.dtypes.items() if dtype == 'object'] high_cardinality_cols = [cname for cname in cleaned_x_train.columns if cleaned_x_train[cname].nunique() > 9 and cleaned_x_train[cname].dtype == \"object\"] print(f'high cat {high_cardinality_cols}') for col in high_cardinality_cols: categorical_cols.remove(col) print(f'cat {categorical_cols}') # Define transformers for numerical and categorical columns train_transformers = [ ('num', StandardScaler(), numerical_cols), ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_cols), ('high_cat', fe.OneHotEncoder(top_categories=5, variables=high_cardinality_cols), high_cardinality_cols), ('ord', OrdinalEncoder(categories=[education_categories], handle_unknown='use_encoded_value', unknown_value=-1), ['education']) ] # Create and apply a ColumnTransformer preprocessor = ColumnTransformer(train_transformers, remainder='passthrough').set_output(transform='pandas') # Fit and transform train data train_preprocessed = preprocessor.fit_transform( cleaned_x_train ) ## Recreate train dataframe including the column names all_features = preprocessor.get_feature_names_out() ## Create dataframe with column names X_train = pd.DataFrame(train_preprocessed, columns=all_features) # Transform test set test_preprocessed = preprocessor.transform( cleaned_x_test ) # Recreate test dataframe including the column names # Create dataframe with column names X_test = pd.DataFrame(test_preprocessed, columns=all_features) label_encoder = LabelEncoder() # Fit and transform the train data -- .values give shape: (n,1) & .ravel flatten array shape to (n, ) y_train = label_encoder.fit_transform(y_train.values.ravel()) # Transform test set y_test = label_encoder.transform(y_test.values.ravel()) return X_train, X_test, y_train, y_test class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() def train(self, params, X_train, y_train): \"\"\" Trains the model on the provided training data. Parameters: :params: Model parameters. :X_train: (DataFrame) Feature matrix. :y_train: (Series) Target vector. :return: (float) Train f1 score \"\"\" self.model = RandomForestClassifier(**params) param_grid=[{ \"n_estimators\": np.arange(30, 50, 150), \"max_features\": np.arange(0.1, 1, 1.5), \"max_leaf_nodes\": [5, 7, 9], \"max_depth\": [3, 5, 7] }] grid_search = GridSearchCV(self.model, param_grid=param_grid, scoring='roc_auc', cv=StratifiedKFold(), verbose=1) grid_search.fit(X_train, y_train) # Report the best hyperparameters configuration print('--------------') print(f\"Best parameters: {grid_search.best_params_}\") print(f\"Best cross-validation score: {round(grid_search.best_score_, 2)}\") # create model with best params self.model = RandomForestClassifier(**grid_search.best_params_) # train model on provided training data self.model.fit(X_train, y_train) # get predictions for train data train_pred = self.model.predict(X_train) train_f1_score = f1_score(y_train, train_pred) train_auc_score = roc_auc_score(y_train, train_pred) print(f\"ROC AUC score: {train_auc_score}\") print(f\"f1 score: {train_f1_score}\") return def evaluate(self, X_test, y_test): \"\"\" Evaluates the model (predict the target) on the test dataset. :param X_test: (DataFrame) Feature matrix of the test dataset. :param y_test: (Series): Target vector of the test dataset. :return: Test data f1 score \"\"\" predictions = self.model.predict(X_test) test_f1_score = f1_score(y_test, predictions) test_auc_score = roc_auc_score(y_test, predictions) print(f\"ROC AUC score: {test_auc_score}\") print(f\"f1 score: {test_f1_score}\") return def get_default_params(self): \"\"\" Returns default parameters for the base model to be used for training model. :return: default parameters for the base model \"\"\" params = {\"random_state\": 2, \"n_jobs\": 4, \"class_weight\": \"balanced\"} return params mypath = Path.cwd() data_path = f\"{mypath}/data/raw/data.csv\" X_train, X_test, y_train, y_test = transform(data_path) model = Model() params = model.get_default_params() print(params) train_f1 = model.train(params, X_train, y_train) print(train_f1) test_f1 = model.evaluate(X_test, y_test) print(train_f1)", "source": "model.py"}, {"content": "import numpy as np from sklearn.tree import DecisionTreeClassifier from sklearn.utils import resample class RandomForest: \"\"\" An implementation of a Random Forest classifier using Decision Trees. Parameters: ----------- n_trees : int, optional (default=5) The number of trees in the forest. subsample_size : float, optional (default=1.0) The proportion of the dataset to include in each subsample used to train each tree. Should be between 0 and 1. sample_with_replacement : bool, optional (default=True) Whether to sample with replacement when creating subsamples. feature_proportion : float, optional (default=1.0) The proportion of features to use when training each tree. Should be between 0 and 1. Attributes: ----------- trees : list of DecisionTreeClassifier The collection of fitted decision trees in the forest. feature_indices : list of arrays The list of arrays where each array contains the indices of features used by each corresponding tree. \"\"\" def __init__(self, n_trees=5, subsample_size=1.0, sample_with_replacement=True, feature_proportion=1.0): self.n_trees = n_trees self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.trees = [] self.feature_indices = [] def fit(self, X_train, y_train): \"\"\" Fit the Random Forest model to the training data. Parameters: ----------- X_train : array-like of shape (n_samples, n_features) The training input samples. y_train : array-like of shape (n_samples,) The target values. Returns: -------- self : object Fitted RandomForest object. \"\"\" self.trees = [] self.feature_indices = [] n_samples, n_features = X_train.shape subsample_size = int(self.subsample_size * n_samples) # Calculate subsample size n_feature_subset = int(self.feature_proportion * n_features) # Number of features to use per tree for _ in range(self.n_trees): if self.sample_with_replacement: # Bootstrap sampling with replacement indices = np.random.choice(n_samples, size=subsample_size, replace=True) else: # Sampling without replacement if subsample_size > n_samples: raise ValueError(\"subsample_size exceeds the number of samples.\") indices = np.random.choice(n_samples, size=subsample_size, replace=False) # Select features randomly using np.random.choice feature_subset_indices = np.random.choice(n_features, size=n_feature_subset, replace=False) self.feature_indices.append(feature_subset_indices) # Create subsampled dataset X_resampled = X_train[indices][:, feature_subset_indices] y_resampled = y_train[indices] # Fit the decision tree tree = DecisionTreeClassifier() # can customize tree parameters here tree.fit(X_resampled, y_resampled) self.trees.append(tree) def predict(self, X): \"\"\" Predict the classes for a set of samples using the fitted Random Forest model. Parameters: ----------- X : array-like of shape (n_samples, n_features) The input samples. Returns: -------- array-like of shape (n_samples,) The predicted class labels. \"\"\" # Collect predictions from all trees tree_predictions = np.array([tree.predict(X[:, self.feature_indices[i]]) for i, tree in enumerate(self.trees)]) # Get the majority vote for each sample majority_votes = np.array([np.bincount(preds).argmax() for preds in tree_predictions.T]) return majority_votes", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from pathlib import Path import pandas as pd import numpy as np import scipy.stats as stats from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import StandardScaler from sklearn.compose import ColumnTransformer class Datapipeline(): def __init__(self): self.preprocessor = None self.label_encoder = None def data_column_transformer(self, numerical_cols): \"\"\" Create a transformer object. :param numerical columns: numerical df columns for scaling transformation. :return: ColumnTransformer preprocessor \"\"\" data_transformers = [ ('num', StandardScaler(), numerical_cols) ] # Create and apply a ColumnTransformer self.preprocessor = ColumnTransformer(data_transformers, remainder='passthrough') return self.preprocessor def transform_train_data(self, train_data_path): \"\"\" Process and transform train_data :param train_data_path: path to train data :return: processed X_train, y_train \"\"\" df = pd.read_csv(train_data_path) # drop duplicates df = df.drop_duplicates() # Define X as input features and y as the outcome variable X_train = df.drop(['Class'], axis=1) y_train = df['Class'] # Input X_train data into data_column_transformer() numerical_cols = [col for col,dtype in X_train.dtypes.items() if dtype == 'int64'] self.preprocessor = self.data_column_transformer(numerical_cols) # Fit and transform train data train_preprocessed = self.preprocessor.fit_transform( X_train ) all_features = self.preprocessor.get_feature_names() # Create dataframe with column names X_train = pd.DataFrame(train_preprocessed, columns=all_features) y_train = y_train.to_frame(name='Class_label') y_train = y_train.astype('str') self.label_encoder = LabelEncoder() y_train = self.label_encoder.fit_transform(y_train.values.ravel()) return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Process and transform test_data :param test_data_path: path to test data :return: processed X_test, y_test \"\"\" df = pd.read_csv(test_data_path) # drop duplicates df = df.drop_duplicates() # Define X as input features and y as the outcome variable X_test = df.drop(['Class'], axis=1) y_test = df['Class'] # Transform test set if self.preprocessor is not None: test_preprocessed = self.preprocessor.transform( X_test ) all_features = self.preprocessor.get_feature_names() # Create dataframe with column names X_test = pd.DataFrame(test_preprocessed, columns=all_features) y_test = y_test.to_frame(name='Class_label') y_test = y_test.astype('str') if self.label_encoder is not None: y_test = self.label_encoder.transform(y_test.values.ravel()) return X_test, y_test if __name__ == \"__main__\": train_data_path = \"./data/train_data.csv\" test_data_path = \"./data/test_data.csv\" data_pipeline = Datapipeline() X_train, y_train = data_pipeline.transform_train_data(train_data_path) X_test, y_test = data_pipeline.transform_test_data(test_data_path)", "source": "datapipeline.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): \"\"\" input size = num of features (4) hidden size = num of neurons in hidden layer output size = 3 labels (3) \"\"\" self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size self.learning_rate = 0.001 self.layer1_weights = np.random.normal(0, 1/np.sqrt(input_size+hidden_size), (hidden_size, input_size)) # variance = (std)**2 self.layer1_bias = np.zeros((hidden_size, 1)) self.layer2_weights = np.random.normal(0, 1/np.sqrt(hidden_size+output_size), (output_size, hidden_size)) # variance = (std)**2 self.layer2_bias = np.zeros((output_size, 1)) # print(self.layer1_weights.shape, self.layer1_bias.shape, self.layer2_weights.shape, self.layer2_bias.shape) def soft_max(self, x): \"\"\"Compute softmax values for each sets of scores in x.\"\"\" e_x = np.exp(x - np.max(x)) return e_x / e_x.sum(axis=0) def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" self.output_layer1 = np.dot(self.layer1_weights, features) + self.layer1_bias # tanh activation layer1 self.activation_layer1 = np.tanh(self.output_layer1) self.output_layer2 = np.dot(self.layer2_weights, self.activation_layer1) + self.layer2_bias # softmax activation layer self.activation_layer2 = self.soft_max(self.output_layer2) return self.activation_layer2 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" return -np.log(predictions[label]) def backward(self, X, y): \"\"\" Adjusts the internal weights/biases \"\"\" def tanh_derivative(z): return 1 - np.tanh(z)**2 # Number of examples m = 1 # get gradient of loss with respect to A2 dA2 = self.activation_layer2 - y # Gradients for layer2 Weights and bias dW2 = np.dot(dA2, self.activation_layer1) / m db2 = np.sum(dA2, axis=1, keepdims=True) / m # Compute gradient (softmax) dZ2 = dA2 # Compute gradient of loss with respect to A1 dA1 = np.dot(self.layer2_weights.T, dZ2) # Gradients for W1 and b1 dZ1 = dA1 * tanh_derivative(self.output_layer1) dW1 = np.dot(dZ1.reshape(1,4), X) / m db1 = np.sum(dZ1, axis=1, keepdims=True) / m gradients_layer2_weights = dW2 gradients_layer2_biases = db2 self.layer2_weights -= self.learning_rate * gradients_layer2_weights self.layer2_bias -= gradients_layer2_biases * self.learning_rate gradients_layer1_weights = dW1 gradients_layer1_biases = db1 self.layer1_weights -= self.learning_rate * gradients_layer1_weights self.layer1_bias -= self.learning_rate * gradients_layer1_biases", "source": "mlp.py"}, {"content": "import pymssql import pandas as pd from pathlib import Path from sklearn.preprocessing import StandardScaler, LabelEncoder from sklearn.compose import ColumnTransformer class Datapipeline(): def __init__(self): self.preprocessor = None self.label_encoder = None def data_column_transformer(self, numerical_cols): \"\"\" Create a transformer object. :param numerical columns: numerical df columns for scaling transformation. :return: ColumnTransformer preprocessor \"\"\" data_transformers = [ ('num', StandardScaler(), numerical_cols) ] # Create and apply a ColumnTransformer self.preprocessor = ColumnTransformer(data_transformers, remainder='passthrough') return self.preprocessor def transform_train_data(self, train_data_path): \"\"\" Process and transform train_data :param train_data_path: path to train data :return: processed X_train, y_train \"\"\" df = pd.read_csv(train_data_path) # drop duplicates df = df.drop_duplicates() df = df.drop('id', axis=1) # Define X as input features and y as the outcome variable X_train = df.drop(['y'], axis=1) y_train = df['y'] # Input X_train data into data_column_transformer() numerical_cols = [col for col,dtype in X_train.dtypes.items() if dtype == 'int64'] self.preprocessor = self.data_column_transformer(numerical_cols) # Fit and transform train data train_preprocessed = self.preprocessor.fit_transform( X_train ) all_features = self.preprocessor.get_feature_names() # Create dataframe with column names X_train = pd.DataFrame(train_preprocessed, columns=all_features) y_train = y_train.to_frame(name='y_label') y_train = y_train.astype('str') self.label_encoder = LabelEncoder() y_train = self.label_encoder.fit_transform(y_train.values.ravel()) return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Process and transform test_data :param test_data_path: path to test data :return: processed X_test, y_test \"\"\" df = pd.read_csv(test_data_path) # drop duplicates df = df.drop_duplicates() df = df.drop('id', axis=1) # Define X as input features and y as the outcome variable X_test = df.drop(['y'], axis=1) y_test = df['y'] # Transform test set if self.preprocessor is not None: test_preprocessed = self.preprocessor.transform( X_test ) all_features = self.preprocessor.get_feature_names() # Create dataframe with column names X_test = pd.DataFrame(test_preprocessed, columns=all_features) y_test = y_test.to_frame(name='y_label') y_test = y_test.astype('str') if self.label_encoder is not None: y_test = self.label_encoder.transform(y_test.values.ravel()) return X_test, y_test if __name__ == \"__main__\": data_path = Path.cwd() data_pipeline = Datapipeline() data_path = f\"{data_path}/balanced_credit.csv\" X_array, y_array = data_pipeline.transform(data_path)", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "import tensorflow as tf from tensorflow.keras.utils import image_dataset_from_directory import logging import os import subprocess import tarfile # Set up logging log_dir = \"logs\" if not os.path.exists(log_dir): os.makedirs(log_dir) logging.basicConfig(filename=os.path.join(log_dir, 'training.log'), level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') def download_and_extract(url, extract_to='.'): \"\"\" Download a file from the given URL and extract it to the specified directory. :param url: The URL of the file to download :param extract_to: The directory to extract the file to \"\"\" logging.info(f\"Downloading file from {url}\") subprocess.run(['curl', '-o', 'tensorfood.tar.gz', url], check=True) # Extract the tar.gz file logging.info(f\"Extracting file to {extract_to}\") with tarfile.open('tensorfood.tar.gz', 'r:gz') as tar_ref: tar_ref.extractall(extract_to) logging.info(\"Extraction completed\") return def load_image_datasets(self, base_dir, image_size=(450, 500), seed=2, validation_split=0.2, batch_size=32): \"\"\" Load the image datasets from the given directory and return the train and validation datasets. :return: train_dataset, val_dataset, class_names, num_classes :param base_dir: The base directory containing the extracted image data \"\"\" train_dataset = image_dataset_from_directory( base_dir, validation_split=validation_split, subset=\"training\", image_size=image_size, seed=seed, batch_size=batch_size ) val_dataset = image_dataset_from_directory( base_dir, validation_split=validation_split, subset='validation', image_size=image_size, seed=seed, batch_size=batch_size ) class_names = train_dataset.class_names num_classes = len(class_names) # One-hot encode the labels train_dataset = train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes))) val_dataset = val_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=num_classes))) # Print class names and number of classes logging.info(f'Class names: {class_names}') logging.info(f'Number of classes: {num_classes}') return train_dataset, val_dataset, class_names, num_classes if __name__ == \"__main__\": dataset_url = 'https://aiapstorage1.blob.core.windows.net/datasets/tensorfood.tar.gz' base_dir = './data/raw' download_and_extract(dataset_url, extract_to=base_dir) # train_dataset, val_dataset, class_names, num_classes = load_image_datasets(base_dir) # print(train_dataset.shape) # print(val_dataset.shape)", "source": "dataloader.py"}, {"content": "import os import logging import subprocess import tarfile import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.applications import resnet50, inception_v3, vgg16 from tensorflow.keras.utils import image_dataset_from_directory import mlflow import mlflow.tensorflow from sklearn.metrics import precision_score, recall_score, f1_score, classification_report # Set up logging log_dir = \"logs\" if not os.path.exists(log_dir): os.makedirs(log_dir) logging.basicConfig(filename=os.path.join(log_dir, 'training.log'), level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') class ImageClassifier: def __init__(self, base_model_name, image_height=450, image_width=450, img_channels=3,batch_size=32,seed=2,validation_split=0.2, lr=0.001, L1=0.01, L2=0.01, n_epochs=5, use_augmentation=False, use_cuda=False, experiment_name=\"model01\", model_name=\"model01\"): self.base_model_name = base_model_name self.image_height = image_height self.image_width = image_width self.img_channels = img_channels self.lr = lr self.L1 = L1 self.L2 = L2 self.n_epochs = n_epochs self.use_augmentation = use_augmentation self.use_cuda = use_cuda self.model = None self.batch_size = batch_size self.seed = seed self.validation_split = validation_split self.experiment_name = experiment_name self.model_name = model_name # Check for GPU availability and set CUDA device if specified if self.use_cuda: gpus = tf.config.list_physical_devices('GPU') if gpus: try: tf.config.experimental.set_memory_growth(gpus[0], True) tf.config.experimental.set_visible_devices(gpus[0], 'GPU') logging.info(\"CUDA is available and will be used for training.\") except RuntimeError as e: logging.info(f\"Error setting up CUDA: {e}\") else: logging.info(\"CUDA is not available. Training will use CPU.\") else: logging.info(\"CUDA usage is disabled. Training will use CPU.\") def load_image_datasets(self, base_dir, image_size=(450, 500)): \"\"\" Load the image datasets from the given directory and return the train and validation datasets. :return: train_dataset, val_dataset, class_names, num_classes :param base_dir: The base directory containing the extracted image data \"\"\" self.train_dataset = image_dataset_from_directory( base_dir, validation_split=self.validation_split, subset=\"training\", image_size=image_size, seed=self.seed, batch_size=self.batch_size ) self.val_dataset = image_dataset_from_directory( base_dir, validation_split=self.validation_split, subset='validation', image_size=image_size, seed=self.seed, batch_size=self.batch_size ) self.class_names = self.train_dataset.class_names self.num_classes = len(self.class_names) # One-hot encode the labels self.train_dataset = self.train_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=self.num_classes))) self.val_dataset = self.val_dataset.map(lambda x, y: (x, tf.one_hot(y, depth=self.num_classes))) # Print class names and number of classes logging.info(f'Class names: {self.class_names}') logging.info(f'Number of classes: {self.num_classes}') return self.train_dataset, self.val_dataset, self.class_names, self.num_classes def build_and_train_model(self): \"\"\" Build and train the image classification model using the specified base model. :return: model, history \"\"\" if self.base_model_name == 'vgg16': conv_base = vgg16.VGG16( weights=\"imagenet\", include_top=False, input_shape=(self.image_height, self.image_width, self.img_channels)) elif self.base_model_name == 'resnet50': conv_base = resnet50.ResNet50( weights=\"imagenet\", include_top=False, input_shape=(self.image_height, self.image_width, self.img_channels)) elif self.base_model_name == 'inception_v3': conv_base = inception_v3.InceptionV3( weights=\"imagenet\", include_top=False, input_shape=(self.image_height, self.image_width, self.img_channels)) else: raise ValueError(\"Base model is not supported.\") # Data Augmentation if self.use_augmentation: data_augmentation = tf.keras.Sequential([ layers.experimental.preprocessing.RandomRotation(0.2), layers.experimental.preprocessing.RandomZoom(0.1), # layers.experimental.preprocessing.RandomFlip(\"horizontal\"), # layers.experimental.preprocessing.RandomHeight(0.2), # layers.experimental.preprocessing.RandomWidth(0.2) ]) model_layers = [data_augmentation, conv_base] else: model_layers = [conv_base] model_layers.extend([ layers.Flatten(), layers.Dense(32, activation=\"relu\"), layers.Dropout(0.3), layers.Dense(self.num_classes, activation=\"softmax\") ]) self.model = models.Sequential(model_layers) # Compile model self.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=self.lr), loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics=['accuracy']) logging.info(self.model.summary()) # Define callbacks early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.0001) callbacks = [early_stopping, reduce_lr] # Set up an MLflow active experiment mlflow.set_experiment(self.experiment_name) # Enable MLflow autologging mlflow.autolog() logging.info(f\"experiment_name: {self.experiment_name}, experiment_id:\", self.experiment.experiment_id) # Start MLflow run with mlflow.start_run(experiment_id=self.experiment.experiment_id, run_name=self.experiment_name): # Log parameters mlflow.log_param(\"base_model_name\", self.base_model_name) mlflow.log_param(\"learning_rate\", self.lr) mlflow.log_param(\"epochs\", self.n_epochs) mlflow.log_param(\"batch_size\", self.train_dataset.batch_size) # Train the model self.history = self.model.fit(self.train_dataset, validation_data=self.val_dataset, epochs=self.n_epochs, callbacks=callbacks) # Save the entire model as a `.keras` zip archive. self.model.save(f\"{self.model_name}.keras\") # Log metrics for epoch in range(self.n_epochs): mlflow.log_metric(\"train_loss\", history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch) mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch) return self.model, self.history def evaluate(self): # Evaluate the model on the test dataset logging.info(\"Evaluating the model on the test dataset\") test_loss, test_accuracy = self.model.evaluate(self.val_dataset) logging.info(f\"Test Loss: {test_loss}\") logging.info(f\"Test Accuracy: {test_accuracy}\")", "source": "train.py"}, {"content": "# Predict on the test dataset test_predictions = self.model.predict(self.val_dataset) test_labels = [] for _, labels in self.val_dataset: test_labels.extend(labels.numpy()) test_labels = tf.argmax(test_labels, axis=1).numpy() test_predictions = tf.argmax(test_predictions, axis=1).numpy() report = classification_report(test_labels, test_predictions, target_names=self.val_dataset.class_names) logging.info(f\"Test Classification report:\\n{report}\") # Calculate additional metrics precision = precision_score(test_labels, test_predictions, average='weighted') recall = recall_score(test_labels, test_predictions, average='weighted') f1 = f1_score(test_labels, test_predictions, average='weighted') logging.info(f\"Test Precision: {precision:.4f}, Test Recall: {recall:.4f}, Test F1 Score: {f1:.4f}\") # Log additional metrics mlflow.log_metric(\"precision\", precision) mlflow.log_metric(\"recall\", recall) mlflow.log_metric(\"f1_score\", f1) # Log the model mlflow.tensorflow.log_model(self.model, \"model\") return self.test_loss, self.test_accuracy, self.precision, self.recall, self.f1 def download_and_extract(url, extract_to='.'): \"\"\" Download a file from the given URL and extract it to the specified directory. :param url: The URL of the file to download :param extract_to: The directory to extract the file to \"\"\" logging.info(f\"Downloading file from {url}\") subprocess.run(['curl', '-o', 'tensorfood.tar.gz', url], check=True) # Extract the tar.gz file logging.info(f\"Extracting file to {extract_to}\") with tarfile.open('tensorfood.tar.gz', 'r:gz') as tar_ref: tar_ref.extractall(extract_to) logging.info(\"Extraction completed\") return if __name__ == \"__main__\": dataset_url = 'https://aiapstorage1.blob.core.windows.net/datasets/tensorfood.tar.gz' base_dir = './data/raw' download_and_extract(dataset_url, extract_to=base_dir) BASE_MODEL_NAME = 'vgg16' # or 'resnet50' or 'inception_v3' classifier = ImageClassifier(BASE_MODEL_NAME, use_augmentation=False, use_cuda=True) train_dataset, val_dataset, class_names, num_classes = classifier.load_image_datasets(base_dir) model, history = classifier.build_and_train_model(train_dataset, val_dataset, class_names, num_classes) # Evaluate the model on the validation dataset test_loss, test_accuracy, precision, recall, f1 = classifier.evaluate(val_dataset) logging.info(f\"Final Test Loss: {test_loss}, Test Accuracy: {test_accuracy}, Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd import scipy.stats as stats import numpy as np import os class DataPipeline: def __init__(self, create_lag_features=False): self.create_lag_features = create_lag_features def clean_data(self, df): df = df.drop('No', axis=1) # Handle missing values using forward and backward fill df['pm2.5'] = df['pm2.5'].ffill() df['pm2.5'] = df['pm2.5'].bfill() # create new column 'datetime' from year, month, day, hour df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) # df['year'] = pd.to_datetime(df['year'], format='%Y') # df['month'] = pd.to_datetime(df['month'], format='%m') # df['day'] = pd.to_datetime(df['day'], format='%d') # df['hour'] = pd.to_datetime(df['hour'], format='%H') # Perform one-hot encoding for the 'cbwd' feature df_encoded = pd.get_dummies(df, columns=['cbwd'], drop_first=True, dtype=int) df_encoded = df_encoded.set_index('datetime') return df_encoded def create_lag_features(self, df): df['pm2.5_lag_1D'] = df['pm2.5'].shift(24) # 1 day lag df['pm2.5_lag_12H'] = df['pm2.5'].shift(12) # 12Hour / 0.5 day lag return df def run_data_pipeline(self, csv_path): # Load the dataset df = pd.read_csv(csv_path) cleaned_data = self.clean_data(df) if self.create_lag_features is True: cleaned_data = self.create_lag_features(cleaned_data) return cleaned_data if __name__ == \"__main__\": pipeline = DataPipeline() file_path = 'assignment6/data/PRSA_data_2010.1.1-2014.12.31.csv' abs_csv_path = os.path.abspath(file_path) cleaned_data = pipeline.run_data_pipeline(file_path) print(cleaned_data.head())", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import GridSearchCV from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score from sklearn.model_selection import TimeSeriesSplit import os import numpy as np # from datapipeline import DataPipeline class ForecastModel: def __init__(self): self.model = RandomForestRegressor(n_estimators=20, max_depth=7, random_state=2, n_jobs=-1) self.tscv = TimeSeriesSplit(n_splits=3) def create_train_test_sets(self, df, features, label, split=0.8): \"\"\" Creates the training and testing sets for prediction. returns: (dataFrames) X_train, X_test, y_train, y_test \"\"\" df = df.copy() train, test = np.split(df, [int(len(df) * split)]) return train[features], test[features], train[label], test[label] def scale_data(self, X_train, X_test, transform=True): scaler = StandardScaler() scaled_X_train = scaler.fit_transform(X_train) if transform==True: scaled_X_test = scaler.transform(X_test) return scaled_X_train, scaled_X_test def fine_tune(self, X, y): param_grid = { 'max_features': ['auto', 'sqrt', 'log2'], 'n_estimators': [100, 200, 300], 'max_depth': [10, 13, 15], 'min_samples_split': [6, 8, 10], 'min_samples_leaf': [5, 6, 7], } grid_search = GridSearchCV(self.model, param_grid, cv=self.tscv, scoring='r2', n_jobs=-1) grid_search.fit(X, y) # Report the best hyperparameters configuration print('--------------') print(f\"Best hyperparameters: {grid_search.best_params_}\") print(f\"Best cross-validation score: {round(grid_search.best_score_, 2)}\") # create model with best params self.model = RandomForestRegressor(**grid_search.best_params_) def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = model.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X) if __name__ == \"__main__\": pipeline = DataPipeline() file_path = 'assignment6/data/PRSA_data_2010.1.1-2014.12.31.csv' abs_csv_path = os.path.abspath(file_path) cleaned_data = pipeline.run_data_pipeline(file_path) print(cleaned_data.head()) # lag_features = ['pm2.5_lag_1D', 'pm2.5_lag_12H'] # data_features = cleaned_data.columns.difference(lag_features + ['pm2.5']).tolist() # features = data_features + lag_features features = cleaned_data.columns label = ['pm2.5'] model = ForecastModel() X_train, X_test, y_train, y_test = model.create_train_test_sets(cleaned_data, features, label, split=0.8) # Print the date range for train and test subsets print(f\"Train data range: {X_train.index.min()} to {X_train.index.max()}\") print(f\"Test data range: {X_test.index.min()} to {X_test.index.max()}\") print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) X_train, X_test = model.scale_data(X_train, X_test, transform=True) model.fine_tune(X_train, y_train) model.fit(X_train, y_train) train_error, test_error = model.evaluate(X_train, y_train, X_test, y_test) print(f\"Train Error: {train_error}, Test Error: {test_error}\") predictions_rf = model.predict(X_test) print(f'MSE: {mean_squared_error(y_test, predictions_rf):.3f}') print(f'MAE: {mean_absolute_error(y_test, predictions_rf):.3f}') print(f'R^2: {r2_score(y_test, predictions_rf):.3f}')", "source": "ml_model.py"}, {"content": "import torch import pandas as pd import numpy as np import matplotlib.pyplot as plt import torch.nn as nn from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size, predictions=[],y=[]): super().__init__() self.rnn = nn.LSTM(input_size, num_rnn, num_layers, batch_first=True) self.fc1 = nn.Linear(num_rnn, output_size) self.relu = nn.ReLU() self.fc2 = nn.Linear(output_size, 1) self.predictions = predictions self.y = y def forward(self, x): out, (hidden, cell) = self.rnn(x) out = hidden[-1, :, :] out = self.fc1(out) out = self.relu(out) out = self.fc2(out) return out def fit(self, train_dataloader): device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") loss_fn = nn.MSELoss() optimizer = torch.optim.Adam(self.parameters(), lr=0.003) self.train() total_loss = 0 for seq, label in train_dataloader: optimizer.zero_grad() pred = self(seq.to(device))[:, 0] loss = loss_fn(pred, label.to(device)) loss.backward() optimizer.step() total_loss += loss.item() * label.size(0) return total_loss / len(train_dataloader.dataset) def predict(self, test_dataloader): for seq, label in test_dataloader: with torch.no_grad(): # Adjust the reshaping operation to match the input tensor dimensions # print(seq.shape) # torch.Size([128, 6, 14]) batch_size, seq_len, num_features = seq.shape pred = self.cpu()(seq.view(batch_size, seq_len, num_features))[:, 0] self.predictions.append(pred) self.y.append(label) return def evaluate(self): predictions_flat = np.concatenate([p.flatten() for p in self.predictions]) y_flat = np.concatenate([y.flatten() for y in self.y]) print(y_flat.shape, predictions_flat.shape) print(f'R^2: {r2_score(y_flat, predictions_flat):.3f}') print(f'MSE: {mean_squared_error(y_flat, predictions_flat):.3f}') print(f'MAE: {mean_absolute_error(y_flat, predictions_flat):.3f}') return predictions_flat, y_flat", "source": "rnn_model.py"}, {"content": "import os import numpy as np import pandas as pd import torch import torch.nn as nn from torch.utils.data import Dataset # from ml_model import ForecastModel # from datapipeline import DataPipeline class WindowGenerator(Dataset): def __init__(self, X, y, lookback, lookahead): self.X = torch.FloatTensor(X) #convert scaled data and labels to tensor self.y = torch.FloatTensor(np.array(y)) self.lookback = lookback self.lookahead = lookahead def __len__(self): return len(self.X), len(self.y) def create_sequences(self): output_seq = [] for i in range(len(self.X) - self.lookback): seq = self.X[i : i+self.lookback] label = self.y[i+self.lookback-1] #[6] output_seq.append((seq, label)) return output_seq def __getitem__(self, idx): return self.output_seq[idx] if __name__ == \"__main__\": pipeline = DataPipeline() file_path = 'assignment6/data/PRSA_data_2010.1.1-2014.12.31.csv' abs_csv_path = os.path.abspath(file_path) cleaned_data = pipeline.run_data_pipeline(file_path) features = cleaned_data.columns label = ['pm2.5'] forecast_model = ForecastModel() X_train, X_test, y_train, y_test = forecast_model.create_train_test_sets(cleaned_data, features, label, split=0.8) print(f\"Train data range: {X_train.index.min()} to {X_train.index.max()}\") print(f\"Test data range: {X_test.index.min()} to {X_test.index.max()}\") print(X_train.shape, y_train.shape, X_test.shape, y_test.shape) X_train, X_test = forecast_model.scale_data(X_train, X_test, transform=True) win_gen = WindowGenerator(X_train, y_train, lookback=6, lookahead=1) sequence_train = win_gen.create_sequences() sequence_test = win_gen.create_sequences()", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53 0.23 0.68 0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "<<<<<<< HEAD import yaml import pandas as pd import numpy as np from sklearn.model_selection import train_test_split class DataPipeline: \"\"\"A class for processing and preparing data for machine learning models.\"\"\" def __init__(self): \"\"\"Initialize the DataPipeline\"\"\" self.df = None def load_config(self, config_path='config.yaml'): \"\"\"Load configuration from a YAML file.\"\"\" with open(config_path, 'r') as file: config = yaml.safe_load(file) return config def load_data(self, file_path): \"\"\" Load the dataset from a CSV file. :param file_path: Path to the CSV file. \"\"\" self.df = pd.read_csv(file_path) def convert_to_nan(self): \"\"\"Handle '?' values and convert to NaN for further processing.\"\"\" self.df.replace('?', np.nan, inplace=True) def eduyear(self): \"\"\"Label encodes education to years of education.\"\"\" config = self.load_config() education_to_eduyear = config['education_to_eduyear'] self.df['years_of_edu'] = self.df['education'].map(education_to_eduyear) def age_group(self): \"\"\"Binning age according to age groups\"\"\" self.df['age_group'] = pd.cut(self.df['age'], bins=[0, 18, 35, 50, 65, np.inf], labels=['Child', 'Youth', 'Adult', 'Senior', 'Elder']) def total_capital(self): \"\"\"Calculating the total capital from investment activities\"\"\" self.df['total_capital'] = self.df[['capital_gains', 'dividends_from_stocks']].sum(axis=1) - self.df['capital_losses'] def income_group(self): \"\"\"Label encoding income group.\"\"\" self.df['income_group'] = self.df['income_group'].map({'- 50000.': 0, '50000+.': 1}) def drop_columns(self): \"\"\"Dropping irrelevant and 'cleaned' columns\"\"\" self.df.drop(['id', 'education', 'age', 'capital_gains', 'dividends_from_stocks', 'capital_losses'], axis=1, inplace=True) def imp_missing_values(self): \"\"\"Impute missing values.\"\"\" for col in ['country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'hispanic_origin', 'state_of_previous_residence']: self.df[col] = self.df[col].fillna(self.df[col].mode()[0]) for col in ['migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', 'migration_prev_res_in_sunbelt']: self.df[col] = self.df[col].fillna('Nonmover') def one_hot_enc(self): \"\"\" One-hot encode categorical variables. :return: DataFrame with one-hot encoded variables. \"\"\" categorical_columns = self.df.select_dtypes(include='object').columns df_encoded = pd.get_dummies(self.df[categorical_columns], dtype=int, drop_first=True) return df_encoded def merge_tables(self, df_encoded): \"\"\" Merge encoded and numerical data into one DataFrame. :param df_encoded: DataFrame with one-hot encoded variables. \"\"\" numerical_columns = self.df.select_dtypes(include=[np.number]).columns merged_df = pd.concat([df_encoded, self.df[numerical_columns]],axis=1) self.df = merged_df def split_data(self): \"\"\" Split data for model training and testing. :return: Tuple of train and test splits (X_train, X_test, y_train, y_test). \"\"\" X = self.df.drop(columns=['income_group']) y = self.df['income_group'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, stratify=y, random_state=42) return X_train, X_test, y_train, y_test def transform(self, file_path): \"\"\" Run data processing steps and return train/test splits. :param file_path: Path to the CSV file. :return: Tuple of train and test splits (X_train, X_test, y_train, y_test). \"\"\" # calls transform directly self.load_data(file_path) self.convert_to_nan() self.eduyear() self.age_group() self.total_capital() self.income_group() self.drop_columns() self.imp_missing_values() encoded_df = self.one_hot_enc() self.merge_tables(encoded_df) return self.split_data() ======= def transform(data_path): \"\"\" Description of the function. :param data_path: ...... :return: ...... \"\"\" raise NotImplementedError >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028", "source": "datapipeline.py"}, {"content": "import numpy as np class DecisionTree: \"\"\" A simple implementation of a Decision Tree classifier. :param max_depth: Maximum depth of the tree. \"\"\" def __init__(self, max_depth=20): self.tree = None self.max_depth = max_depth def gini_one(self, y): \"\"\" Calculate the Gini impurity of a single group of y. :param y: Target values, a 1D numpy array. :return: Gini impurity as a float. \"\"\" # Calculate the Gini impurity of single group of y, lower better n = len(y) if n == 0: return 0 # Gini impurity formula: 1 - sum of the squared probabilities of each class (level) return 1.0 - sum((np.sum(y == c) / n) ** 2 for c in np.unique(y)) def gini(self, y1, y2): \"\"\" Calculate the average Gini impurity of two feature inputs. :param y1: First group of target values, a 1D numpy array. :param y2: Second group of target values, a 1D numpy array. :return: Average Gini impurity as a float. \"\"\" n_left = len(y1) n_right = len(y2) n_total = n_left + n_right gini_left = self.gini_one(y1) gini_right = self.gini_one(y2) # Weighted average of Gini impurities return (n_left / n_total) * gini_left + (n_right / n_total) * gini_right def split(self, X, y, index, value): \"\"\" Split the dataset into two groups based on a feature index and value. :param X: Feature matrix, a 2D numpy array. :param y: Target values, a 1D numpy array. :param index: Feature index to split on. :param value: Feature value to split on. :return: Tuple of split feature arrays and target arrays. \"\"\" # Split the dataset into 2 groups, based on given criteria of index and value to create 2 sets of new nodes left = np.where(X[:, index]<= value) right = np.where(X[:, index]> value) return X[left], X[right], y[left], y[right] def best_split(self, X, y): \"\"\" Find the best feature and value to split the dataset to minimize Gini impurity. :param X: Feature matrix, a 2D numpy array. :param y: Target values, a 1D numpy array. :return: Dictionary with the best index, value, and groups. \"\"\" # Find the best feature and value to split the dataset to minimise the gini impurity. # Initialising values best_index, best_value, best_score, best_groups = None, None, float('inf'), None # Loop through all columns for index in range(X.shape[1]): # Find the unique values in each column for value in np.unique(X[:, index]): # Calling split method based on each unique value X_left, X_right, y_left, y_right = self.split(X, y, index, value) if len(y_left) == 0 or len(y_right) == 0: continue # Calculating the gini_score of each instance of split gini_score = self.gini(y_left, y_right) # compare calculated gini_score with best score, to store the best split after all iterations of the index and values if gini_score< best_score: best_index, best_value, best_score, best_groups = index, value, gini_score, (X_left, X_right, y_left, y_right) return {'index': best_index, 'value': best_value, 'groups': best_groups} def build_tree(self, X, y, depth): \"\"\" Recursively build the decision tree. :param X: Feature matrix, a 2D numpy array. :param y: Target values, a 1D numpy array. :param depth: Current depth of the tree. :return: Tree node as a dictionary or leaf value. \"\"\" if depth >= self.max_depth", "source": "decision_tree.py"}, {"content": "or len(np.unique(y)) == 1: return np.unique(y)[0] node = self.best_split(X, y) if node['groups'] is None: return np.unique(y)[0] left = self.build_tree(node['groups'][0], node['groups'][2], depth + 1) right = self.build_tree(node['groups'][1], node['groups'][3], depth + 1) return {'index': node['index'], 'value': node['value'], 'left': left, 'right': right} def fit(self, X, y): \"\"\" Train the decision tree model. :param X: Training feature matrix, a 2D numpy array. :param y: Training target values, a 1D numpy array. \"\"\" # training model with initial depth 0, recursively to construct tree based model self.tree = self.build_tree(X, y, 0) def predict_one(self, X, tree): \"\"\" Predict the target for a single sample. :param X: Single sample feature array, a 1D numpy array. :param tree: Current tree or subtree. :return: Predicted target value. \"\"\" # Prediction of a single sample using decision tree, and recursively navigate left and right branches until leaf node is reached if not isinstance(tree, dict): return tree # Checks feature value, and determine which tree to traverse next if X[tree['index']] <= tree['value']: # recursively evaluate left subtree return self.predict_one(X, tree['left']) else: # recursively evaluate right subtree return self.predict_one(X, tree['right']) def predict(self, X): \"\"\" Predict the target for a dataset. :param X: Feature matrix, a 2D numpy array. :return: List of predicted target values. \"\"\" # prediction method on test dataset. Calling multiple predict_one method for each sample. return [self.predict_one(x, self.tree) for x in X]", "source": "decision_tree.py"}, {"content": "<<<<<<< HEAD from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self, params=None): \"\"\" A wrapper for the RandomForestClassifier model with training and evaluation methods. :param params: Dictionary of parameters for RandomForestClassifier. \"\"\" if params is None: params = self.get_default_params() self.params = params self.rf_clf = RandomForestClassifier(**params, random_state=42) def train(self, params, X_train, y_train): \"\"\" Train the RandomForestClassifier model and return the training F1 score. :param params: Dictionary of parameters for RandomForestClassifier. :param X_train: Training feature data, a 2D numpy array or similar structure. :param y_train: Training target data, a 1D numpy array or similar structure. :return: F1 score of the training data as a float. ======= class Model: def __init__(self): # init your model here pass def train(self, params, X_train, y_train): \"\"\" Description of the function. :param params: ...... :param X_train: ...... :param y_train: ...... :return: ...... >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028 \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it <<<<<<< HEAD self.rf_clf.set_params(**params) self.rf_clf.fit(X_train, y_train) y_train_pred = self.rf_clf.predict(X_train) train_f1_score = f1_score(y_train, y_train_pred, average= 'weighted') return train_f1_score def evaluate(self, X_test, y_test): \"\"\" Evaluate the trained model on test data and return the test F1 score. :param X_test: Test feature data, a 2D numpy array or similar structure. :param y_test: Test target data, a 1D numpy array or similar structure. :return: F1 score of the test data as a float. \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.rf_clf.predict(X_test) test_f1_score = f1_score(y_test, y_pred, average= 'weighted') return test_f1_score def get_default_params(self): \"\"\" Return the default parameters for RandomForestClassifier. :return: Dictionary of default parameters. ======= pass def evaluate(self, X_test, y_test): \"\"\" Description of the function. :param X_test: ...... :param y_test: ...... :return: ...... \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score pass def get_default_params(self): \"\"\" Description of the function. :return: ...... >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028 \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model <<<<<<< HEAD param_grid = { 'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2 } return param_grid ======= pass >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028", "source": "model.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree class RandomForest(): \"\"\" A simple implementation of a Random Forest classifier. :param n_trees: Number of trees in the forest. \"\"\" # initialisation of the random forest class: def __init__(self, n_trees = 5): self.n_trees = n_trees self.trees_list = [] def fit(self, X, y): \"\"\" Fits the Random Forest model to the training data. :param X: Training features, a 2D numpy array. :param y: Training labels, a 1D numpy array. \"\"\" for n in range(self.n_trees): # To bootstrap sample, replace = True, regenerate the integer used. indices = np.random.choice(len(X), len(X), replace = True) X_sample = X[indices] y_sample = y[indices] #Create a decision tree tree = DecisionTree() tree.fit(X_sample, y_sample) self.trees_list.append(tree) def predict(self, X): \"\"\" Predict using the Random Forest model. :param X: Features to predict, a 2D numpy array. :return: Predicted labels, a 1D numpy array. \"\"\" # Predictions from trees and get most common prediction for each sample predictions = np.array([tree.predict(X) for tree in self.trees_list]) return np.array([np.bincount(pred).argmax() for pred in predictions.T])", "source": "random_forest.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree class RandomForest1: \"\"\" A simple implementation of a Random Forest classifier. :param n_trees: Number of trees in the forest. :param subsample_size: Proportion of the dataset to sample for each tree. :param replace: Whether to sample with replacement. \"\"\" def __init__(self, n_trees=5, subsample_size=0.8, replace=True): self.n_trees = n_trees self.subsample_size = subsample_size self.replace = replace self.trees_list = [] def fit(self, X, y): \"\"\" Fits the Random Forest model to the training data. :param X: Training features, a 2D numpy array. :param y: Training labels, a 1D numpy array. \"\"\" for n in range(self.n_trees): # To bootstrap sample, replace = True, regenerate the integer used. indices = np.random.choice(len(X), int(len(X)*self.subsample_size), replace=self.replace) X_sample = X[indices] y_sample = y[indices] #Creates a decision tree tree = DecisionTree() tree.fit(X_sample, y_sample) self.trees_list.append(tree) def predict(self, X): \"\"\" Predict using the Random Forest model. :param X: Features to predict, a 2D numpy array. :return: Predicted labels, a 1D numpy array. \"\"\" # Predictions from trees and get most common prediction for each sample predictions = np.array([tree.predict(X) for tree in self.trees_list]) return np.array([np.bincount(pred).argmax() for pred in predictions.T])", "source": "random_forest_1.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree class RandomForest2: \"\"\" A simple implementation of a Random Forest classifier. :param n_trees: Number of trees in the forest. :param subsample_size: Proportion of the dataset to sample for each tree. :param feature_proportion: Proportion of features to consider when splitting. :param replace: Whether to sample with replacement. \"\"\" def __init__(self, n_trees=5, subsample_size=0.8, feature_proportion=0.8, replace=True): self.n_trees = n_trees self.subsample_size = subsample_size self.replace = replace self.feature_proportion = feature_proportion self.trees_list = [] def fit(self, X, y): \"\"\" Fits the Random Forest model to the training data. :param X: Training features, a 2D numpy array. :param y: Training labels, a 1D numpy array. \"\"\" n_features = X.shape[1] # Calculate number of features to use n_features_subset = int(n_features*self.feature_proportion) for n in range(self.n_trees): # To bootstrap sample, replace = True, regenerate the integer used. indices = np.random.choice(len(X), int(len(X)*self.subsample_size), replace=self.replace) X_sample = X[indices] y_sample = y[indices] # Set the number of features to use at random feature_indices = np.random.choice(n_features, n_features_subset, replace=False) X_subset = X_sample[:, feature_indices] #Creates a decision tree tree = DecisionTree() tree.fit(X_subset, y_sample) self.trees_list.append(tree) def predict(self, X): \"\"\" Predict using the Random Forest model. :param X: Features to predict, a 2D numpy array. :return: Predicted labels, a 1D numpy array. \"\"\" # Predictions from trees and get most common prediction for each sample predictions = np.array([tree.predict(X) for tree in self.trees_list]) # Vote for the majority class return np.array([np.bincount(pred).argmax() for pred in predictions.T])", "source": "random_forest_2.py"}, {"content": "from .datapipeline import DataPipeline from .model import Model from .decision_tree import DecisionTree from .random_forest import RandomForest from .random_forest_1 import RandomForest1 from .random_forest_2 import RandomForest2", "source": "__init__.py"}, {"content": "<<<<<<< HEAD from ..src import DataPipeline ======= from ..src import datapipeline >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028 from ..src import model def test_model(): <<<<<<< HEAD data_path = \"/data/A1_test.csv\" datapipeline = DataPipeline() ======= data_path = \"/data/A1.csv\" >>>>>>> 760e041562957d05bc83dd50bc4871f8c40a5028 X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd from sklearn.preprocessing import OneHotEncoder class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): \"\"\" Initialise the multi-layer perceptron with two layers. Parameters: input_size (int): The number of input neurons. hidden_size (int): The number of neurons in the hidden layer. output_size (int): The number of neurons in the output layer. The weights and biases are initialsed from a standard normal distribution. \"\"\" self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size np.random.seed(42) # Xavier Initialization for weights self.w1 = np.random.randn(self.input_size, self.hidden_size) * np.sqrt(2 / (self.input_size + self.hidden_size)) self.b1 = np.zeros((1, self.hidden_size)) self.w2 = np.random.randn(self.hidden_size, self.output_size) * np.sqrt(2 / (self.hidden_size + self.output_size)) self.b2 = np.zeros((1, self.output_size)) @staticmethod def sigmoid(x): \"\"\" The sigmoid function maps any real-valued number to a value between 0 and 1. It is often used in the output layer of a neural network when the task is a binary classification problem. Parameters: x (float): The input to the sigmoid function. Returns: float: The output of the sigmoid function. \"\"\" return 1 / (1 + np.exp(-x)) @staticmethod def sigmoid_derivative(x): \"\"\" The derivative of the sigmoid function is used in the backpropagation algorithm to compute the gradients of the weights and biases in the neural network. Parameters: x (float): The input to the sigmoid function. Returns: float: The derivative of the sigmoid function. \"\"\" return x * (1 - x) @staticmethod def softmax(x): \"\"\" The softmax function maps a vector of real numbers to a vector of values in the range [0, 1] that add up to 1. It is often used in the output layer of a neural network when the task is a multi-class classification problem. The softmax function is defined as the exponential of the input vector divided by the sum of the exponentials of all the elements in the input vector. Parameters: x (float): The input to the softmax function. Returns: float: The output of the softmax function. \"\"\" exp_x = np.exp(x - np.max(x, axis=1, keepdims=True)) return exp_x / exp_x.sum(axis=1, keepdims=True) def forward(self, X): \"\"\" Forward propagation through the neural network. Parameters: features (ndarray): Input features of shape (m, n) where m is the number of samples and n is the number of features. Returns: ndarray: Predictions of shape (m, output_size) where output_size is the number of classes in the output layer. \"\"\" # Forward propagation self.z1 = np.dot(X, self.w1) + self.b1 self.a1 = self.sigmoid(self.z1) self.z2 = np.dot(self.a1, self.w2) + self.b2 self.y_pred = self.softmax(self.z2) return self.y_pred def loss(self, y_pred, y): \"\"\" Computes the cross entropy loss between the predictions and the labels. Parameters: predictions (ndarray): Predictions of shape (m, output_size) where m is the number of samples and output_size is the number of classes in the output layer. label (ndarray): Labels of shape (m, output_size) where m is the number of samples and output_size is the number of classes in the output layer. Returns: float: Cross entropy loss between the predictions and the labels. \"\"\" m = y.shape[0] #Calculate cross entropy loss function loss = -np.sum(y * np.log(y_pred), axis=1) return loss def backward(self, X, y, y_pred,", "source": "mlp.py"}, {"content": "learning_rate=0.001): \"\"\" Computes the gradients of the loss with respect to the weights and biases of the neural network and updates the parameters using gradient descent. Parameters: X (ndarray): Input features of shape (m, n) where m is the number of samples and n is the number of features. y (ndarray): Labels of shape (m, output_size) where m is the number of samples and output_size is the number of classes in the output layer. y_pred (ndarray): Predictions of shape (m, output_size) where m is the number of samples and output_size is the number of classes in the output layer. learning_rate (float): Learning rate for gradient descent. Default is 0.001. Returns: None \"\"\" m = X.shape[0] dZ2 = y_pred - y dW2 = np.dot(self.a1.T, dZ2) / m db2 = np.sum(dZ2, axis=0, keepdims=True) / m dA1 = np.dot(dZ2, self.w2.T) dZ1 = dA1 * self.sigmoid_derivative(self.a1) dW1 = np.dot(X.T, dZ1) / m db1 = np.sum(dZ1, axis=0, keepdims=True) / m # Update weights and biases self.w1 -= learning_rate * dW1 self.b1 -= learning_rate * db1 self.w2 -= learning_rate * dW2 self.b2 -= learning_rate * db2", "source": "mlp.py"}, {"content": "import os import pandas as pd from sqlalchemy import create_engine from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import OneHotEncoder class Datapipeline: \"\"\" Data pipeline class to load, clean and transform data Parameters ---------- filepath : str Filepath to the raw data file Returns: train, validation and test datasets \"\"\" def __init__(self, filepath): self.filepath = filepath def load_data_from_csv(self): df = pd.read_csv(self.filepath) return df def transform(self): df = self.load_data_from_csv() scaler = StandardScaler() df['y']= scaler.fit_transform(df['y'].values.reshape(-1,1)) X = df.drop(['y','id'], axis=1).values y = df['y'].values encoder = OneHotEncoder(sparse_output=False) y_one_hot = encoder.fit_transform(y.reshape(-1, 1)) return X, y_one_hot # Example usage if __name__ == \"__main__\": filepath = '~/aiap18-Onn-Yun-Hui-595Z/all-assignments/assignment4/data/raw/data.csv' pipeline = Datapipeline(filepath) X, y = pipeline.transform() print(\"Data transformation complete.\")", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100] ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd class DataPipeline: def __init__(self): # Your code here pass def data_preprocessing(self,df): df['date_time'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) df.drop(columns=['year', 'month', 'day', 'hour'], inplace=True) df.set_index('date_time', inplace=True) #df = df[['DEWP','TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'pm2.5']].resample('D').mean() df = df.bfill() return df def run_data_pipeline(self, csv_path): df = pd.read_csv(csv_path) cleaned_data = self.data_preprocessing(df) return cleaned_data if __name__ == \"__main__\": pipeline = DataPipeline() csv_path = \"./data/BeijingPM20100101_20151231.csv\" df_eda = pipeline.run_data_pipeline(csv_path) print(df_eda.head())", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "class ForecastModel: def __init__(self): self.model = ml_model_of_your_choice def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = metric_of_your_choice y_test_pred = model.predict(X_test) test_error = metric_of_your_choice return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [[0.53, 0.23, 0.68, 0.45]]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "# Import the libraries needed import pandas as pd import numpy as np from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OrdinalEncoder, FunctionTransformer, OneHotEncoder from sklearn.impute import SimpleImputer from imblearn.over_sampling import SMOTE from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin from sklearn.model_selection import train_test_split def transform(data_path): df = pd.read_csv(data_path) # Drop columns with high percentage of non-informative values columns_to_drop = [ 'fill_inc_questionnaire_for_veteran_s_admin', 'reason_for_unemployment', 'enroll_in_edu_inst_last_wk', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_prev_res_in_sunbelt', 'family_members_under_18', 'member_of_a_labor_union', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'migration_code_move_within_reg', 'major_occupation_code', 'class_of_worker' ] df.drop(columns_to_drop, axis=1, inplace=True) # Convert columns to more appropriate category type columns_to_convert = [ 'detailed_industry_recode', 'detailed_occupation_recode', 'own_business_or_self_employed', 'year', 'veterans_benefits' ] df[columns_to_convert] = df[columns_to_convert].astype('object') # Define X as input features and y as the outcome variable X = df.drop(columns=['id', 'income_group']) y = (df['income_group'] == '50000+.').astype(int) # Define feature types numeric_features = X.select_dtypes(include=['int64']).columns nominal_features = X.select_dtypes(include=['object']).columns ordinal_features = ['education'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Define custom transformer for log transformation class LogTransformer(BaseEstimator, TransformerMixin): def __init__(self): pass def fit(self, X, y=None): return self def transform(self, X): return np.log1p(X) # Build a preprocessing step for numeric features numeric_pipeline = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')), ('log', LogTransformer()), ('scaler', StandardScaler()) ]) # Build a preprocessing step for nominal features nominal_pipeline = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore')) ]) # Build a preprocessing step for ordinal features ordinal_pipeline = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('ordinal', OrdinalEncoder( categories=[[ 'Children', 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-academic program', 'Associates degree-occup /vocational', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Doctorate degree(PhD EdD)', 'Prof school degree (MD DDS DVM LLB JD)']], handle_unknown='use_encoded_value', unknown_value=-1)) ]) # Define the full preprocessing pipeline preprocessor = ColumnTransformer( transformers=[ ('num', numeric_pipeline, numeric_features), ('cat', nominal_pipeline, nominal_features), ('ord', ordinal_pipeline, ordinal_features) ], remainder='passthrough' ) # Fit and transform the training data X_train_transformed = preprocessor.fit_transform(X_train) # Transform the test data X_test_transformed = preprocessor.transform(X_test) # Convert transformed data to DataFrame for inspection X_train_transformed_df = pd.DataFrame(X_train_transformed) X_test_transformed_df = pd.DataFrame(X_test_transformed) return X_train_transformed, X_test_transformed, y_train, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class DecisionTree: def __init__(self): pass def gini(self, label_list_0, label_list_1): \"\"\" Calculate the Gini impurity of a node. Parameters: label_list_0 (list): A list of labels representing the left node. label_list_1 (list): A list of labels representing the right node. Returns: float: The Gini impurity of the node. Returns 0.0 if there are no samples in the node. \"\"\" def gini_impurity(labels): \"\"\" Calculate the Gini impurity for a list of labels. Parameters: labels (list): A list of labels in the node. Returns: float: The Gini impurity of the node. \"\"\" if len(labels) == 0: return 0 # Initialize counts for each class class_counts = {0: 0, 1: 0} for label in labels: class_counts[label] += 1 # Calculate the Gini impurity total_labels = len(labels) impurity = 1.0 for count in class_counts.values(): prob = count / total_labels impurity -= prob ** 2 return impurity # Calculate total number of labels total_labels = len(label_list_0) + len(label_list_1) if total_labels == 0: return 0.0 # Compute Gini impurity for the left and right nodes gini_left = gini_impurity(label_list_0) gini_right = gini_impurity(label_list_1) # Calculate the weighted Gini impurity gini_weighted = (len(label_list_0) / total_labels) * gini_left + (len(label_list_1) / total_labels) * gini_right return gini_weighted", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score import numpy as np class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() pass def train(self, params, X_train, y_train): \"\"\" Trains the initialized model with the provided training data and parameters. :param params: Dictionary containing the hyperparameters for the model. :param X_train: Training data (features). :param y_train: Training data (labels). :return: Train F1 score as a single float. \"\"\" # Set the model parameters from the input dictionary self.model.set_params(**params) # Train the model on the training data self.model.fit(X_train, y_train) # Predict the training data y_pred_train = self.model.predict(X_train) # Calculate and return the F1 score on the training data train_f1 = f1_score(y_train, y_pred_train, average='macro') return train_f1 def evaluate(self, X_test, y_test): \"\"\" Uses the trained model to predict the target for the test data and returns the test F1 score. :param X_test: Test data (features). :param y_test: Test data (labels). :return: Test F1 score as a single float. \"\"\" # Predict on the test data y_pred_test = self.model.predict(X_test) # Calculate and return the F1 score on the test data test_f1 = f1_score(y_test, y_pred_test, average='macro') return test_f1 def get_default_params(self): \"\"\" Returns the optimized parameters for the model based on fine-tuning. :return: Dictionary of optimized model parameters. \"\"\" return { 'n_estimators': 300, 'max_depth': 30, 'min_samples_split': 15, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'bootstrap': True }", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import mlflow import os import sys import time import random # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from dotenv import load_dotenv from utils.data_utils import load_data from src.preprocessingpipeline import PreprocessingPipeline from src.modeltraining import ModelTraining class MLFlow: def __init__(self): load_dotenv() username = os.getenv('MLFLOW_TRACKING_USERNAME') password = os.getenv('MLFLOW_TRACKING_PASSWORD') mlflow_uri = os.getenv('MLFLOW_TRACKING_URI') # Construct the full tracking URI with authentication tracking_uri = f\"http://{username}:{password}@{mlflow_uri.split('://')[1]}\" mlflow.set_tracking_uri(tracking_uri) tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(os.getenv('MLFLOW_EXPERIMENT')) print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) # Start a run within the experiment context with mlflow.start_run(experiment_id=experiment.experiment_id): self.run_pipeline() def run_pipeline(self): # Load Data df = load_data() # Preprocess Data preprocessor = PreprocessingPipeline(df) X_train, y_train, X_val, y_val, X_test, y_test = preprocessor.overall_pipeline() # Train Model model_trainer = ModelTraining() metrics = model_trainer.overall_model_training(X_train, y_train, X_test, y_test, X_val, y_val) # Log metrics for metric_name, metric_value in metrics.items(): mlflow.log_metric(metric_name, metric_value) if __name__ == \"__main__\": mlflow_client = MLFlow()", "source": "mlflowtensor.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # Initialize weights for the input to hidden layer using Xavier initialization self.w1 = np.random.randn(input_size, hidden_size) * np.sqrt(1. / input_size) self.b1 = np.zeros((hidden_size,)) # Initialize weights for the hidden to output layer using Xavier initialization self.w2 = np.random.randn(hidden_size, output_size) * np.sqrt(1. / hidden_size) self.b2 = np.zeros((output_size,)) # Placeholder to store features and labels for backpropagation self.features = None self.labels = None self.learning_rate = 0.001 def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" if features.ndim == 1: features = features[None, :] # Store features for use in backpropagation self.features = features # Compute the hidden layer's activations self.z1 = np.dot(features, self.w1) + self.b1 # Tanh activation for hidden layer self.a1 = np.tanh(self.z1) # Compute the output layer's activation self.z2 = np.dot(self.a1, self.w2) + self.b2 # Sigmoid activation for binary output self.a2 = self.softmax(self.z2) return self.a2 def softmax(self, z): exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) return exp_z / exp_z.sum(axis=1, keepdims=True) def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" # Store labels for use in backpropagation self.labels = label return -np.sum(label * np.log(predictions + 1e-15)) def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" # Number of samples m = self.features.shape[0] # Calculate gradient of the loss dz2 = self.a2 - self.labels # Calculate gradient for output layer dw2 = np.dot(self.a1.T, dz2) / m db2 = np.sum(dz2, axis=0) / m # Calculate gradient for hidden layer da1 = np.dot(dz2, self.w2.T) dz1 = da1 * (1 - np.tanh(self.z1)**2) # Calculate gradients for the hidden layer weights and biases dw1 = np.dot(self.features.T, dz1) / m db1 = np.sum(dz1, axis=0) / m # Update weights and biases self.w1 -= self.learning_rate * dw1 self.b1 -= self.learning_rate * db1 self.w2 -= self.learning_rate * dw2 self.b2 -= self.learning_rate * db2", "source": "mlp.py"}, {"content": "import os import pyodbc import pandas as pd import numpy as np def standardize(X): mean = np.mean(X, axis=0) std_dev = np.std(X, axis=0) X_scaled = (X - mean) / std_dev return X_scaled class Datapipeline(): def __init__(self): pass def transform(self, data_path): conn = None try: # Establish connection conn = pyodbc.connect(data_path) # Query to fetch data from the specified table query = \"SELECT * FROM mlp\" # Read data into DataFrame df = pd.read_sql(query, conn) # Split DataFrame into features (X) and target (y) X_array = df.drop(columns=['id', 'y']).values y_array = df['y'].values y_array = y_array.reshape(-1, 1) X_scaled = standardize(X_array) return X_scaled, y_array except Exception as e: print(f\"An unexpected error occurred: {e}\") return None, None finally: # Ensure the connection is closed to avoid resource leaks if conn is not None: conn.close()", "source": "mlp_datapipeline.py"}, {"content": "import hydra import tensorflow as tf import numpy as np import matplotlib.pyplot as plt from tensorflow.keras import layers, models from omegaconf import DictConfig, OmegaConf from sklearn.metrics import (f1_score, confusion_matrix, recall_score, matthews_corrcoef, brier_score_loss, precision_recall_curve, auc) from imblearn.over_sampling import SMOTE from collections import Counter from utils.general_utils import initialize_hydra, close_hydra, stage_message, set_seed from utils.plot_utils import plot_training_history, plot_precision_recall_curve from src.preprocessingpipeline import PreprocessingPipeline, shared_config class ModelTraining: def __init__(self, config_name=\"config.yaml\", config_path=\"../conf\"): \"\"\" Initializes the ModelTraining class by loading the configuration. Args: config_name (str, optional): Name of the configuration file to use. Defaults to \"config.yaml\". config_path (str, optional): Path to the configuration directory. Defaults to \"../conf\". \"\"\" self.config = initialize_hydra(config_name=config_name, config_path=config_path) self.model = None self.input_dim = shared_config.variables.input_dim self.metrics = list(self.config.model.keras.metrics) # Set seed for reproducibility self.random_state = self.config.split.random_state set_seed(self.random_state) def create_model(self): \"\"\" Creates and compiles a Keras model based on the architecture defined in the configuration. Returns: tf.keras.Model: Compiled Keras model. \"\"\" # Initialize the Sequential model with input layer self.model = models.Sequential([layers.Input(shape=(self.input_dim,))]) # Build model architecture from configuration for layer in self.config.model.common.architecture: layer_type = layer.layer_type units = layer.units activation = layer.activation if layer_type == \"Dense\": self.model.add(layers.Dense(units=units, activation=activation)) # Compile the model with specified optimizer, loss, and metrics self.model.compile(optimizer=self.config.model.keras.optimizer, loss=self.config.model.keras.loss, metrics=self.metrics) stage_message(\"Model created with the following summary:\") self.model.summary() return self.model def train_model(self, X_train, y_train, X_val, y_val): \"\"\" Trains the model using the provided training data. Args: X_train (np.array): Training data features. y_train (np.array): Training data labels. X_val (np.array): Validation data features. y_val (np.array): Validation data labels. \"\"\" # Train the model using parameters from the flattened training configuration history = self.model.fit( X_train, y_train, epochs=self.config.training.base.epochs, batch_size=self.config.training.base.batch_size, validation_data=(X_val, y_val), verbose=self.config.training.base.verbose ) # Plot training history plot_training_history(history, \"keras\") stage_message(\"Model training completed successfully.\") return history def evaluate_model(self, X_test, y_test): \"\"\" Evaluates the performance of the model using various metrics. Args: X_test (np.array): Test data features. y_test (np.array): True labels for the test set. Returns: dict: A dictionary containing all calculated metrics. \"\"\" # Make probabilistic predictions y_pred_prob = self.model.predict(X_test) # Convert probabilities to binary predictions y_pred_classes = (y_pred_prob > 0.5).astype(int) # Calculate metrics f1 = f1_score(y_test, y_pred_classes) tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel() sensitivity = recall_score(y_test, y_pred_classes) # True Positive Rate (Recall) specificity = tn / (tn + fp) # True Negative Rate g_mean = np.sqrt(sensitivity * specificity) mcc = matthews_corrcoef(y_test, y_pred_classes) brier = brier_score_loss(y_test, y_pred_prob) precision, recall, _ = precision_recall_curve(y_test, y_pred_prob) pr_auc = auc(recall, precision) # Call the plotting function plot_precision_recall_curve(recall, precision, pr_auc) # Collect metrics in a dictionary metrics_dict = { 'F1 Score': round(f1, 2), 'G-Mean': round(g_mean, 2), 'MCC': round(mcc, 2), 'Brier Score': round(brier, 4), 'Precision-Recall AUC': round(pr_auc, 2) } stage_message(\"Model evaluation completed successfully.\") return metrics_dict def overall_model_training(self, X_train, y_train, X_test, y_test, X_val, y_val): \"\"\" Executes the entire model training workflow, including model creation, training, and evaluation. Args: X_train (np.array): Training data features. y_train (np.array): Training data labels. X_test (np.array): Test data features. y_test (np.array): True labels for the test set. Returns: dict: A dictionary containing all evaluation metrics. \"\"\" # Step 1: Create the model stage_message(\"Creating the model...\") model = self.create_model() if model is None: stage_message(\"Model creation failed.\", level=\"error\") return None # Step 2: Train the model stage_message(\"Training the", "source": "modeltraining.py"}, {"content": "model...\") history = self.train_model(X_train, y_train, X_val, y_val) if history is None: stage_message(\"Model training failed.\", level=\"error\") return None # Step 3: Evaluate the model stage_message(\"Evaluating the model...\") metrics_dict = self.evaluate_model(X_test, y_test) if metrics_dict is None: stage_message(\"Model evaluation failed.\", level=\"error\") return None # Step 4: Return the evaluation metrics stage_message(\"Overall model training and evaluation completed successfully.\") close_hydra() return metrics_dict class SMOTEModelTraining(ModelTraining): def __init__(self, config_name=\"config.yaml\", config_path=\"../conf\"): \"\"\" Initializes the SMOTEModelTraining class by loading the SMOTE configuration. Args: config_name (str, optional): Name of the configuration file to use. config_path (str, optional): Path to the configuration directory. \"\"\" # Call the parent constructor to initialize base configuration super().__init__(config_name=config_name, config_path=config_path) def apply_smote(self, X_train, y_train): \"\"\" Applies SMOTE to the training data. Args: X_train (np.array): Training data features. y_train (np.array): Training data labels. Returns: np.array, np.array: SMOTE-applied training features and labels. \"\"\" # Print class distribution before applying SMOTE stage_message(f\"Class distribution before SMOTE: {Counter(y_train)}\") # Create SMOTE object with random_state from configuration smote = SMOTE(random_state=self.random_state) # Apply SMOTE X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) # Print class distribution after applying SMOTE stage_message(f\"Class distribution after SMOTE: {Counter(y_train_resampled)}\") return X_train_resampled, y_train_resampled def train_model(self, X_train, y_train, X_val, y_val): \"\"\" Trains the model using the provided training data, applying SMOTE. Args: X_train (np.array): Training data features. y_train (np.array): Training data labels. X_val (np.array): Validation data features. y_val (np.array): Validation data labels. \"\"\" # Apply SMOTE to the training data X_train_resampled, y_train_resampled = self.apply_smote(X_train, y_train) # Train the model using parameters from the flattened training configuration history = self.model.fit( X_train_resampled, y_train_resampled, epochs=self.config.training.base.epochs, batch_size=self.config.training.base.batch_size, validation_data=(X_val, y_val), verbose=self.config.training.base.verbose ) # Plot training history plot_training_history(history, \"keras\") stage_message(\"Model training completed successfully.\") return history class IncreasedDepthModel(ModelTraining): def create_model(self): \"\"\" Creates and compiles a deeper Keras model by adding more layers. \"\"\" # Initialize the Sequential model with input layer self.model = models.Sequential([layers.Input(shape=(self.input_dim,))]) # Increase depth by adding more layers for _ in range(3): # Add 3 additional layers self.model.add(layers.Dense(units=64, activation='relu')) # Add remaining layers from configuration for layer in self.config.model.common.architecture: layer_type = layer.layer_type units = layer.units activation = layer.activation if layer_type == \"Dense\": self.model.add(layers.Dense(units=units, activation=activation)) # Compile the model self.model.compile(optimizer=self.config.model.keras.optimizer, loss=self.config.keras.default.loss, metrics=self.metrics) stage_message(\"Increased depth model created with the following summary:\") self.model.summary() return self.model class LeakyReLUModel(ModelTraining): def create_model(self): \"\"\" Creates and compiles a Keras model using Leaky ReLU activation in all Dense layers. \"\"\" # Initialize the Sequential model with input layer self.model = models.Sequential([layers.Input(shape=(self.input_dim,))]) # Add layers with Leaky ReLU activation for layer in self.config.model.common.architecture: layer_type = layer.layer_type units = layer.units activation = layer.activation if layer_type == \"Dense\": self.model.add(layers.Dense(units=units, activation=activation)) # If not last layer, add a layer for LeakyReLU if activation != \"sigmoid\": self.model.add(layers.LeakyReLU(alpha=0.1)) # Add Leaky ReLU activation with alpha parameter # Compile the model self.model.compile(optimizer=self.config.model.keras.optimizer, loss=self.config.keras.default.loss, metrics=self.metrics) stage_message(\"Leaky ReLU model created with the following summary:\") self.model.summary() return self.model class DropoutEarlyStoppingModel(ModelTraining): def create_model(self): \"\"\" Creates and compiles a Keras model with dropout layers. \"\"\" self.model = models.Sequential() self.model.add(layers.InputLayer(shape=(self.input_dim,))) # Add layers with dropout for layer in self.config.model.common.architecture: layer_type = layer.layer_type units = layer.units activation = layer.activation dropout_rate = layer.dropout if 'dropout' in layer else 0.0 # Check if dropout rate is specified if layer_type == \"Dense\": self.model.add(layers.Dense(units=units, activation=activation))", "source": "modeltraining.py"}, {"content": "# If not last layer, add a layer for dropout if activation != \"sigmoid\": if dropout_rate > 0: # Only add Dropout if rate is greater than 0 self.model.add(layers.Dropout(rate=dropout_rate)) # Compile the model self.model.compile(optimizer=self.config.model.keras.optimizer, loss=self.config.model.keras.loss, metrics=self.metrics) stage_message(\"Dropout model created with the following summary:\") self.model.summary() return self.model def train_model(self, X_train, y_train, X_val=None, y_val=None): \"\"\" Trains the model using the provided training data with early stopping. Args: X_train (np.array): Training data features. y_train (np.array): Training data labels. X_val (np.array, optional): Validation data features for early stopping. y_val (np.array, optional): Validation data labels for early stopping. Returns: tf.keras.callbacks.History: Training history. \"\"\" # Check if validation data is provided callbacks = [] if X_val is not None and y_val is not None: early_stopping = tf.keras.callbacks.EarlyStopping( monitor='val_loss', patience=self.config.training.early_stopping.patience, restore_best_weights=True ) callbacks.append(early_stopping) # Train the model with optional validation data and early stopping history = self.model.fit( X_train, y_train, validation_data=(X_val, y_val) if X_val is not None and y_val is not None else None, epochs=self.config.training.base.epochs, batch_size=self.config.training.base.batch_size, callbacks=callbacks, verbose=self.config.training.base.verbose ) # Plot training history plot_training_history(history, \"keras\") stage_message(\"Model training completed successfully with dropout and early stopping.\") return history", "source": "modeltraining.py"}, {"content": "import torch import torch.nn as nn import numpy as np import torch.optim as optim from torchsummary import summary from torch.utils.data import DataLoader, TensorDataset from sklearn.metrics import (f1_score, confusion_matrix, recall_score, matthews_corrcoef, brier_score_loss, precision_recall_curve, auc) from utils.general_utils import initialize_hydra, set_seed, stage_message, close_hydra from utils.plot_utils import plot_training_history, plot_precision_recall_curve from src.preprocessingpipeline import shared_config class ModelTrainingPyT(nn.Module): def __init__(self, config_name=\"config.yaml\", config_path=\"../conf\"): super(ModelTrainingPyT, self).__init__() self.config = initialize_hydra(config_name=config_name, config_path=config_path) self.input_dim = shared_config.variables.input_dim self.model = self.create_model() summary(self.model, (self.input_dim,)) # Set seed for reproducibility self.random_state = self.config.split.random_state set_seed(self.random_state) # Extract configuration values for loss and optimizer loss_name = self.config.model.pytorch.loss optimizer_name = self.config.model.pytorch.optimizer learning_rate = self.config.model.pytorch.learning_rate print(loss_name) print(optimizer_name) # Initialize loss function based on configuration if loss_name == \"bce\": self.criterion = nn.BCELoss() else: raise ValueError(f\"Unsupported loss function: {loss_name}\") # Initialize optimizer based on configuration if optimizer_name == \"adam\": self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate) else: raise ValueError(f\"Unsupported optimizer: {optimizer_name}\") def create_model(self): \"\"\" Create the model architecture based on the configuration. Returns: - nn.Sequential: A PyTorch Sequential model containing the architecture. \"\"\" layers = [] first_layer_units = self.config.model.common.first_layer_units # Input layer layers.append(nn.Linear(self.input_dim, first_layer_units)) layers.append(nn.ReLU()) # Build model architecture from configuration previous_units = first_layer_units for layer in self.config.model.common.architecture: layer_type = layer.layer_type units = layer.units activation = layer.activation if layer_type == \"Dense\": layers.append(nn.Linear(previous_units, units)) previous_units = units # Add activation function if activation == 'relu': layers.append(nn.ReLU()) elif activation == 'sigmoid': layers.append(nn.Sigmoid()) stage_message(\"Model created with the following summary:\") return nn.Sequential(*layers) def forward(self, x): return self.model(x) def train_model(self, X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor): \"\"\" Trains the model using the provided training data. Args: X_train_tensor (torch.Tensor): Training data features. y_train_tensor (torch.Tensor): Training data labels. X_val_tensor (torch.Tensor): Validation data features. y_val_tensor (torch.Tensor): Validation data labels. \"\"\" # Create DataLoader for batching train_dataset = TensorDataset(X_train_tensor, y_train_tensor) val_dataset = TensorDataset(X_val_tensor, y_val_tensor) train_loader = DataLoader(train_dataset, batch_size=self.config.training.base.batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=self.config.training.base.batch_size, shuffle=False) # Training Parameters epochs = self.config.training.base.epochs history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []} # Using Keras-like structure # Training Loop for epoch in range(epochs): self.model.train() train_loss = 0.0 correct_train = 0 total_train = 0 for X_batch, y_batch in train_loader: self.optimizer.zero_grad() y_pred = self.model(X_batch).squeeze() # Calculate loss and perform backward propagation loss = self.criterion(y_pred, y_batch) loss.backward() self.optimizer.step() # Accumulate training loss and accuracy train_loss += loss.item() * X_batch.size(0) correct_train += ((y_pred > 0.5).int() == y_batch.int()).sum().item() total_train += y_batch.size(0) # Calculate average training loss and accuracy train_loss /= len(train_loader.dataset) train_accuracy = correct_train / total_train # Validation loop self.model.eval() # Set model to evaluation mode val_loss = 0.0 correct_val = 0 total_val = 0 with torch.no_grad(): # No need to compute gradients for validation for X_batch, y_batch in val_loader: y_pred = self.model(X_batch).squeeze() # Forward pass loss = self.criterion(y_pred, y_batch) # Compute loss val_loss += loss.item() * X_batch.size(0) correct_val += ((y_pred > 0.5).int() == y_batch.int()).sum().item() total_val += y_batch.size(0) # Calculate average validation loss and accuracy val_loss /= len(val_loader.dataset) val_accuracy = correct_val / total_val # Save losses and accuracies for plotting history['loss'].append(train_loss) history['val_loss'].append(val_loss) history['accuracy'].append(train_accuracy) history['val_accuracy'].append(val_accuracy) # Print progress print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} - Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\") # Plot training history using the common utils function plot_training_history(history, \"pytorch\") stage_message(\"Model training completed successfully.\") def evaluate_model(self, X_test_tensor, y_test_tensor): \"\"\"", "source": "modeltrainingpyt.py"}, {"content": "Evaluates the performance of the PyTorch model using various metrics. Args: X_test_tensor (torch.Tensor): Test data features. y_test_tensor (torch.Tensor): True labels for the test set. Returns: dict: A dictionary containing all calculated metrics. \"\"\" # Set the model to evaluation mode self.model.eval() # Disable gradient computation with torch.no_grad(): # Forward pass: get model predictions y_pred_prob = self.model(X_test_tensor).squeeze() # Convert probabilities to binary predictions (0 or 1) y_pred_classes = (y_pred_prob > 0.5).int() # Convert tensors to numpy arrays for metrics calculation y_test = y_test_tensor.numpy() y_pred_prob = y_pred_prob.numpy() y_pred_classes = y_pred_classes.numpy() # Calculate metrics f1 = f1_score(y_test, y_pred_classes) tn, fp, fn, tp = confusion_matrix(y_test, y_pred_classes).ravel() sensitivity = recall_score(y_test, y_pred_classes) # True Positive Rate (Recall) specificity = tn / (tn + fp) # True Negative Rate g_mean = np.sqrt(sensitivity * specificity) mcc = matthews_corrcoef(y_test, y_pred_classes) brier = brier_score_loss(y_test, y_pred_prob) precision, recall, _ = precision_recall_curve(y_test, y_pred_prob) pr_auc = auc(recall, precision) # Call the plotting function (assuming it is compatible with PyTorch) plot_precision_recall_curve(recall, precision, pr_auc) # Collect metrics in a dictionary metrics_dict = { 'F1 Score': round(f1, 2), 'G-Mean': round(g_mean, 2), 'MCC': round(mcc, 2), 'Brier Score': round(brier, 4), 'Precision-Recall AUC': round(pr_auc, 2) } stage_message(\"Model evaluation completed successfully.\") return metrics_dict def overall_model_training(self, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor, X_val_tensor, y_val_tensor): \"\"\" Executes the entire model training workflow, including model creation, training, and evaluation. Args: X_train_tensor (torch.Tensor): Training data features. y_train_tensor (torch.Tensor): Training data labels. X_test_tensor (torch.Tensor): Test data features. y_test_tensor (torch.Tensor): True labels for the test set. X_val_tensor (torch.Tensor): Validation data features. y_val_tensor (torch.Tensor): Validation data labels. Returns: dict: A dictionary containing all evaluation metrics. \"\"\" # Step 1: Create the model stage_message(\"Creating the model...\") self.model = self.create_model() if self.model is None: stage_message(\"Model creation failed.\", level=\"error\") return None # Step 2: Train the model stage_message(\"Training the model...\") self.model.train() # Set the model to training mode self.train_model(X_train_tensor, y_train_tensor, X_val_tensor, y_val_tensor) # Step 3: Evaluate the model stage_message(\"Evaluating the model...\") metrics_dict = self.evaluate_model(X_test_tensor, y_test_tensor) if metrics_dict is None: stage_message(\"Model evaluation failed.\", level=\"error\") return None # Step 4: Return the evaluation metrics stage_message(\"Overall model training and evaluation completed successfully.\") close_hydra() return metrics_dict", "source": "modeltrainingpyt.py"}, {"content": "import hydra from omegaconf import DictConfig, OmegaConf from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, MinMaxScaler from utils.general_utils import initialize_hydra, close_hydra, stage_message shared_config = OmegaConf.create({ 'variables': { 'input_dim': None } }) class PreprocessingPipeline: def __init__(self, data, config_name=\"config.yaml\", config_path=\"../conf\"): \"\"\" Initialize the PreprocessingPipeline class. Args: data (pd.DataFrame): The input data for preprocessing. config_name (str): Name of the configuration file for Hydra. config_path (str): Path to the configuration directory for Hydra. \"\"\" self.pipeline = None self.data = data self.config = initialize_hydra(config_name, config_path) self.columns_to_drop = self.config.preprocessing.columns_to_drop self.target_column = self.config.variables.target_column self.X = None self.y = None self.X_train = None self.X_test = None self.X_val = None self.y_train = None self.y_test = None self.y_val = None def drop_duplicates(self): \"\"\" Drop duplicates from the DataFrame. \"\"\" initial_row_count = self.data.shape[0] self.data = self.data.drop_duplicates() rows_dropped = initial_row_count - self.data.shape[0] stage_message(f\"Dropped {rows_dropped} duplicate rows.\" if rows_dropped > 0 else \"No duplicate rows found.\") def drop_columns(self): \"\"\" Drop specified columns from the DataFrame and log which columns were dropped. \"\"\" # Record the initial columns initial_columns = set(self.data.columns) # Drop specified columns self.data = self.data.drop(columns=self.columns_to_drop, errors='ignore') # Determine which columns were dropped final_columns = set(self.data.columns) dropped_columns = initial_columns - final_columns # Log the dropped columns using stage_message if dropped_columns: stage_message(f\"Dropped columns: {', '.join(dropped_columns)}\") else: stage_message(\"No columns were dropped.\") def transform(self): \"\"\" Separate features and target columns and store input dimensions. The method updates the class attribute `input_dim` to reflect the number of features and logs the operation along with the target column name. \"\"\" # Separate features (X) and target (y) self.X = self.data.drop(columns=[self.target_column]) self.y = self.data[self.target_column] # Convert target column to integers self.y = self.y.astype(int) # Update input_dim dynamically shared_config.variables.input_dim = self.X.shape[1] # Log the transformation using stage_message stage_message(f\"Separated features and target column '{self.target_column}' with input dimension {shared_config.variables.input_dim}.\") def create_pipeline(self): \"\"\" Create a preprocessing pipeline with scaling for numerical features. \"\"\" # Select numerical columns only numerical_features = self.X.select_dtypes(include=['float64', 'int64']).columns # Check if numerical features are available if numerical_features.empty: stage_message(\"No numerical features found for scaling. Pipeline not created.\", level=\"error\") return # Initialize the scalers standard_scaler = StandardScaler() # Define the pipeline for numerical features numerical_pipeline_standard = Pipeline(steps=[ ('standard_scaler', standard_scaler) ]) # Create a column transformer with both pipelines self.pipeline = ColumnTransformer(transformers=[ ('standard', numerical_pipeline_standard, numerical_features) ]) # Log the created pipeline stage_message(\"Pipeline created.\") def data_split(self): \"\"\" Perform train-test split on the data using configuration parameters. \"\"\" # Check if X and y are defined if self.X is None or self.y is None: stage_message(\"Features and target are not defined. Ensure the 'transform' method is called before splitting the data.\", level=\"error\") return # Retrieve test_size and random_state from config test_size = self.config.split.test_size val_size = self.config.split.val_size random_state = self.config.split.random_state # Split the data into training + validation and test X_train_val, self.X_test, y_train_val, self.y_test = train_test_split( self.X, self.y, test_size=test_size, random_state=random_state ) # Calculate the adjusted validation size relative to the remaining training data adjusted_val_size = val_size / (1 - test_size) # Split the training + validation set into training and validation self.X_train, self.X_val, self.y_train, self.y_val = train_test_split( X_train_val, y_train_val, test_size=adjusted_val_size, random_state=random_state ) # Log the split information using stage_message", "source": "preprocessingpipeline.py"}, {"content": "stage_message( f\"Data split into training and testing sets: {len(self.X_train)} training samples, {len(self.X_test)} testing samples.\" ) def transform_train(self): \"\"\" Fit and transform the training data using the preprocessing pipeline. :return: Transformed training data (X_train) and corresponding labels (y_train). \"\"\" # Check if pipeline is defined if self.pipeline is None: stage_message(\"Pipeline is not defined. Ensure 'create_pipeline' method is called before fitting and transforming the data.\", level=\"error\") return None, None # Fit and transform the training data self.X_train_transformed = self.pipeline.fit_transform(self.X_train) # Log the transformation stage_message(\"Fitted and transformed the training data.\") return self.X_train_transformed, self.y_train def transform_test(self): \"\"\" Transform the test data using the fitted preprocessing pipeline. :return: Transformed test data (X_test) and corresponding labels (y_test). \"\"\" # Check if pipeline is defined if self.pipeline is None: stage_message(\"Pipeline is not defined. Ensure 'create_pipeline' method is called before transforming the test data.\", level=\"error\") return None, None # Transform the test data self.X_test_transformed = self.pipeline.transform(self.X_test) # Log the transformation stage_message(\"Transformed the test data.\") return self.X_test_transformed, self.y_test def overall_pipeline(self): \"\"\" Execute the entire preprocessing pipeline and return preprocessed data. :return: Transformed training and testing datasets (X_train, y_train, X_test, y_test). \"\"\" # Initial message indicating that the pipeline process has started stage_message(\"Preprocessing pipeline has started.\") # Step 1: Drop duplicates self.drop_duplicates() # Step 2: Drop unnecessary columns self.drop_columns() # Step 3: Separate features and target self.transform() # Step 4: Split the data into training and testing sets self.data_split() # Step 5: Create the preprocessing pipeline self.create_pipeline() # Step 6: Fit and transform the training data X_train_transformed, y_train = self.transform_train() if X_train_transformed is None or y_train is None: stage_message(\"Failed to transform training data. Check the pipeline creation and input data.\", level=\"error\") return None, None, None, None # Step 7: Transform the test data X_test_transformed, y_test = self.transform_test() if X_test_transformed is None or y_test is None: stage_message(\"Failed to transform test data. Check the pipeline creation and input data.\", level=\"error\") return None, None, None, None # Step 8: Return the transformed training and test data stage_message(\"Preprocessing pipeline completed successfully.\") close_hydra() return X_train_transformed, self.y_train, self.X_val, self.y_val, X_test_transformed, self.y_test", "source": "preprocessingpipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "import os import pandas as pd import torch import pyodbc import numpy as np from dotenv import load_dotenv from utils.general_utils import close_hydra, initialize_hydra, stage_message def convert_to_tensors(X_train, y_train, X_val, y_val, X_test, y_test): \"\"\" Convert numpy arrays or pandas DataFrames to PyTorch tensors. Parameters: - X_train (np.ndarray or pd.DataFrame): Training features. - y_train (np.ndarray or pd.Series): Training labels. - X_val (np.ndarray or pd.DataFrame): Validation features. - y_val (np.ndarray or pd.Series): Validation labels. - X_test (np.ndarray or pd.DataFrame): Test features. - y_test (np.ndarray or pd.Series): Test labels. Returns: - dict: A dictionary containing converted PyTorch tensors. \"\"\" # Put all inputs into a dictionary data_dict = { 'X_train': X_train, 'y_train': y_train, 'X_val': X_val, 'y_val': y_val, 'X_test': X_test, 'y_test': y_test } # Convert each item in the dictionary to the appropriate tensor type tensor_dict = {} for name, data in data_dict.items(): # Check if the data is a pandas DataFrame or Series and convert to numpy array if isinstance(data, (pd.DataFrame, pd.Series)): data = data.values # Convert numpy array to PyTorch tensor tensor_dict[name] = torch.tensor(data, dtype=torch.float32) return tensor_dict def load_data(): \"\"\" Load data from the database using the configuration. Args: config_name (str): Name of the configuration file. config_path (str): Path to the configuration directory. Returns: pd.DataFrame: DataFrame containing the loaded data. \"\"\" # Load environment variables from .env file load_dotenv() # Initialize Hydra and load configuration cfg = initialize_hydra() stage_message(\"Starting the data loading process\") conn = None try: # Retrieve username and password from environment variables username = os.getenv('DB_USERNAME') password = os.getenv('DB_PASSWORD') # Construct the connection string using the configuration details conn_str = ( f'DSN={cfg.database.driver};SERVER={cfg.database.server};' f'DATABASE={cfg.database.database};UID={username};' f'PWD={password}' ) stage_message(\"Connecting to the database...\") # Establish connection conn = pyodbc.connect(conn_str) stage_message(\"Database connection established successfully.\") # Query to fetch data from the specified table query = f\"SELECT * FROM {cfg.database.table}\" stage_message(f\"Executing query: {query}\") # Read data into DataFrame df = pd.read_sql(query, conn) stage_message(\"Data loaded into DataFrame successfully.\") # Define the CSV file path where the data will be saved csv_filepath = cfg.database.data_save_path stage_message(f\"Saving data to CSV at {csv_filepath}\") # Create the directory for saving raw data if it doesn't exist os.makedirs(os.path.dirname(csv_filepath), exist_ok=True) # Save DataFrame to CSV df.to_csv(csv_filepath, index=False) stage_message(f\"Data successfully saved to {csv_filepath}\") close_hydra() # Return the DataFrame return df except pyodbc.Error as e: stage_message(f\"Database error occurred: {e}\", level=\"error\") except Exception as e: stage_message(f\"An unexpected error occurred: {e}\", level=\"error\") finally: # Ensure the connection is closed to avoid resource leaks if conn is not None: conn.close() stage_message(\"Database connection closed.\")", "source": "data_utils.py"}, {"content": "import os import torch import hydra import random import numpy as np import tensorflow as tf from torchsummary import summary from hydra.core.global_hydra import GlobalHydra from sklearn.utils import shuffle def set_seed(seed): \"\"\" Set the random seed for reproducibility in Python, NumPy, and PyTorch. Args: seed (int): The seed value to use for random number generation. \"\"\" # Python's built-in random module random.seed(seed) # NumPy np.random.seed(seed) # PyTorch torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) # if you are using multi-GPU # Ensure deterministic behavior in CUDA (if applicable) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Set seed for hash-based operations os.environ['PYTHONHASHSEED'] = str(seed) def stage_message(message, level=\"info\"): \"\"\" Print a stage message with different colors based on the message level. Args: message (str): The message to be printed. level (str): The level of the message. Options are \"info\", \"error\". Default is \"info\". \"\"\" # Define color codes colors = { \"info\": \"\\033[1;34m\", # Blue color for general info \"error\": \"\\033[1;31m\", # Red color for errors \"end_color\": \"\\033[0m\" # Reset color } # Choose color based on the level color = colors.get(level, colors[\"info\"]) # Default to \"info\" if level is unknown # Print the formatted message print(f\"{color}*** {level.upper()}: {message} ***{colors['end_color']}\") # Initialize Hydra and obtain configuration def initialize_hydra(config_name=\"config.yaml\", config_path=\"../conf\"): \"\"\" Initialize Hydra and return the configuration object. Args: config_name (str): Name of the configuration file. config_path (str): Path to the configuration directory. Returns: DictConfig: The loaded configuration object. \"\"\" if GlobalHydra.instance().is_initialized(): GlobalHydra.instance().clear() hydra.initialize(config_path=config_path, version_base=None) cfg = hydra.compose(config_name=config_name) return cfg def close_hydra(): \"\"\"Clear the Hydra instance to free up resources.\"\"\" if GlobalHydra.instance().is_initialized(): GlobalHydra.instance().clear()", "source": "general_utils.py"}, {"content": "import matplotlib.pyplot as plt def plot_training_history(history, framework_type): \"\"\" Plots the training and validation loss and metrics from a Keras or PyTorch history object. Args: history (dict or History object): The history object returned by model training. framework_type (str): The framework type, either \"keras\" or \"pytorch\". \"\"\" # Extract loss and metrics from the history object if framework_type == \"keras\": history_dict = history.history elif framework_type == \"pytorch\": history_dict = history else: raise ValueError(\"Invalid framework_type. Expected 'keras' or 'pytorch'.\") # Plot training & validation loss values plt.figure(figsize=(12, 5)) # Check if there is validation data if 'val_loss' in history_dict: # Plot training and validation loss plt.subplot(1, 2, 1) plt.plot(history_dict['loss'], label='Training Loss') plt.plot(history_dict['val_loss'], label='Validation Loss') plt.title('Loss over Epochs') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() else: # Plot only training loss if no validation data is available plt.subplot(1, 2, 1) plt.plot(history_dict['loss'], label='Training Loss') plt.title('Training Loss over Epochs') plt.xlabel('Epoch') plt.ylabel('Loss') plt.legend() # Plot training & validation accuracy values (if accuracy is a metric) if 'accuracy' in history_dict: plt.subplot(1, 2, 2) plt.plot(history_dict['accuracy'], label='Training Accuracy') if 'val_accuracy' in history_dict: plt.plot(history_dict['val_accuracy'], label='Validation Accuracy') plt.title('Accuracy over Epochs') plt.xlabel('Epoch') plt.ylabel('Accuracy') plt.legend() # Display the plots plt.tight_layout() try: get_ipython() plt.show() except NameError: plt.close() def plot_precision_recall_curve(recall, precision, pr_auc, title=\"Precision-Recall Curve\"): \"\"\" Plots the precision-recall curve. Args: recall (np.array): Array of recall values. precision (np.array): Array of precision values. pr_auc (float): Precision-Recall AUC value. title (str): Title of the plot. Default is \"Precision-Recall Curve\". \"\"\" plt.figure(figsize=(8, 6)) plt.plot(recall, precision, marker='.', label=f'PR Curve (AUC = {pr_auc:.2f})') plt.xlabel('Recall') plt.ylabel('Precision') plt.title(title) plt.legend() plt.grid(True) try: get_ipython() plt.show() except NameError: plt.close()", "source": "plot_utils.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [ -128, -200, -24], [ -127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import os import sys import h5py import hydra import mlflow import numpy as np import logging import random import shutil import tensorflow as tf import matplotlib.pyplot as plt from sklearn.metrics import precision_score, recall_score from PIL import Image from omegaconf import DictConfig import torch.nn.functional as F import torch import torch.nn as nn import matplotlib.pyplot as plt import torch.optim as optim from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score from torchvision import datasets, transforms from torch.utils.data import DataLoader from torchvision.datasets import ImageFolder # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import setup_logging, mlflow_init, mlflow_log class CustomImageFolder(ImageFolder): \"\"\"Custom ImageFolder to retain original image file paths.\"\"\" def __init__(self, root, transform=None): super().__init__(root, transform) # Create a list of original image file paths self.image_paths = [img_path for img_path, _ in self.imgs] def __getitem__(self, index): # Get original image path img_path = self.image_paths[index] # Call the original __getitem__ to get the image and label image, label = super().__getitem__(index) return image, label, img_path # Return the image, label, and path class BinaryClassifier(nn.Module): def __init__(self): super(BinaryClassifier, self).__init__() # Define the layers self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5) self.batch_norm1 = nn.BatchNorm2d(num_features=6) self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5) self.batch_norm2 = nn.BatchNorm2d(num_features=16) # Calculate the flattened size after convolution and pooling self.flattened_size = 16 * 58 * 58 # This should match the output size from your convolutions self.fc1 = nn.Linear(self.flattened_size, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 1) # Single output for binary classification def forward(self, x): # Forward pass through the layers x = self.pool1(F.relu(self.batch_norm1(self.conv1(x)))) x = F.relu(self.batch_norm2(self.conv2(x))) # Flatten the output for the fully connected layers x = x.view(x.size(0), -1) # Flatten the tensor x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) output = self.fc3(x) # Logits output, no sigmoid here return output class MultiClassClassifier(BinaryClassifier): def __init__(self, num_classes): super(MultiClassClassifier, self).__init__() self.fc3 = nn.Linear(84, num_classes) def forward(self, x): x = self.pool1(F.relu(self.batch_norm1(self.conv1(x)))) x = F.relu(self.batch_norm2(self.conv2(x))) # Flatten the output for the fully connected layers x = x.view(x.size(0), -1) # Flatten the tensor x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) output = self.fc3(x) # Multi-class output logits return output class ImageClassifier: def __init__(self, config, classifier_type='multi'): self.config = config self.logger = logging.getLogger(__name__) if classifier_type == 'binary': self.model = BinaryClassifier() self.criterion = nn.BCEWithLogitsLoss() # Binary classification self.num_classes = len(self.config.classes_to_keep) elif classifier_type == 'multi': self.model = MultiClassClassifier(num_classes=self.config.num_classes) self.criterion = nn.CrossEntropyLoss() # Multi-class classification self.num_classes = self.config.num_classes else: raise ValueError(\"Invalid classifier type. Choose 'binary' or 'multi'.\") self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.model.to(self.device) # Move model to the appropriate device self.mlflow_init_status = None self.mlflow_run = None self.optimizer = None self.train_loader = None self.test_loader = None self.train_losses = [] self.val_losses = [] def initialize_mlflow(self): \"\"\"Initialize MLflow tracking.\"\"\" self.mlflow_init_status, self.mlflow_run = mlflow_init( self.config, setup_mlflow=self.config.setup_mlflow, autolog=self.config.mlflow_autolog ) mlflow_log( self.mlflow_init_status, \"log_params\", params={ \"learning_rate\": self.config.lr, \"gamma\": self.config.gamma, \"seed\": self.config.seed, \"epochs\": self.config.epochs, }, ) torch.manual_seed(self.config.seed) use_cuda = not self.config.no_cuda and torch.cuda.is_available() if use_cuda: device = torch.device(\"cuda\") elif not self.config.no_mps and torch.backends.mps.is_available(): device = torch.device(\"mps\") else: device = torch.device(\"cpu\") train_kwargs = {\"batch_size\": self.config.train_bs} test_kwargs = {\"batch_size\": self.config.test_bs} if use_cuda: cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True} train_kwargs.update(cuda_kwargs) test_kwargs.update(cuda_kwargs) return self.mlflow_init_status, self.mlflow_run", "source": "pytorch.py"}, {"content": "def prepare_data(self): # Dataset and Directory Paths dataset_dir = self.config.data_dir_path train_dir = self.config.train_dir_path test_dir = self.config.test_dir_path # Delete existing training and testing directories if they exist if os.path.exists(train_dir): shutil.rmtree(train_dir) if os.path.exists(test_dir): shutil.rmtree(test_dir) # Create training and testing directories if they do not exist os.makedirs(train_dir, exist_ok=True) os.makedirs(test_dir, exist_ok=True) random.seed(self.config.seed) resize_width = self.config.resize_width resize_height = self.config.resize_height normalization_mean = self.config.normalization.mean normalization_std = self.config.normalization.std transform = transforms.Compose([ transforms.Resize((resize_width, resize_height)), # Resize images to 128x128 transforms.ToTensor(), # Convert to tensor transforms.Normalize(mean=normalization_mean, std=normalization_std) # Normalize ]) if isinstance(self.model, MultiClassClassifier): classes_to_process = os.listdir(dataset_dir) else: classes_to_process = self.config.classes_to_keep logging.info(f'Classes to process: {classes_to_process}') # Iterate Through Each Class Directory for class_name in classes_to_process: class_path = os.path.join(dataset_dir, class_name) if os.path.isdir(class_path): # Create class-specific directories for train and test os.makedirs(os.path.join(train_dir, class_name), exist_ok=True) os.makedirs(os.path.join(test_dir, class_name), exist_ok=True) # Shuffle Files and Split Them files = os.listdir(class_path) random.shuffle(files) # Shuffle files to ensure random selection # Calculate split index based on validation split ratio split_index = int(len(files) * self.config.validation_split) train_files = files[:split_index] test_files = files[split_index:] # Move Files to Corresponding Directories for file_name in train_files: shutil.copy(os.path.join(class_path, file_name), os.path.join(train_dir, class_name, file_name)) for file_name in test_files: shutil.copy(os.path.join(class_path, file_name), os.path.join(test_dir, class_name, file_name)) # Create PyTorch ImageFolder Datasets train_dataset = CustomImageFolder(root=train_dir, transform=transform) test_dataset = CustomImageFolder(root=test_dir, transform=transform) return train_dataset, test_dataset def load_data(self, train_dataset, test_dataset): self.train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True) self.test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False) self.logger.info(\"Data loaders created successfully.\") return self.train_loader, self.test_loader def train_model(self): self.optimizer = optim.Adam(self.model.parameters(), lr=self.config.lr) scheduler = torch.optim.lr_scheduler.StepLR( self.optimizer, step_size=1, gamma=self.config.gamma ) for epoch in range(self.config.epochs): self.model.train() running_loss = 0.0 for images, labels, img_paths in self.train_loader: # Move inputs and labels to the device images, labels = images.to(self.device), labels.to(self.device) self.optimizer.zero_grad() outputs = self.model(images) # Adjust loss computation for multi-class if isinstance(self.model, MultiClassClassifier): loss = self.criterion(outputs, labels) else: # For BinaryClassifier loss = self.criterion(outputs, labels.float().unsqueeze(1)) loss.backward() self.optimizer.step() running_loss += loss.item() * labels.size(0) epoch_loss = running_loss / len(self.train_loader.dataset) self.train_losses.append(epoch_loss) self.logger.info(f\"Epoch [{epoch + 1}/{self.config.epochs}], Loss: {epoch_loss:.4f}\") val_loss, wrong_predictions = self.evaluate_model() self.val_losses.append(val_loss) if epoch % self.config.model_checkpoint_interval == 0: self.logger.info(\"Exporting the model for epoch %s.\", epoch + 1) model_checkpoint_path = os.path.join( self.config.model_checkpoint_dir_path, \"model.pt\" ) torch.save( { \"model_state_dict\": self.model.state_dict(), \"epoch\": epoch, \"optimiser_state_dict\": self.optimizer.state_dict(), \"train_loss\": epoch_loss, \"test_loss\": val_loss, }, model_checkpoint_path, ) # if self.mlflow_init_status: # mlflow_log( # self.mlflow_init_status, # \"log_artifact\", # local_path=model_checkpoint_path, # artifact_path=\"model\", # ) scheduler.step() return self.train_losses, self.val_losses def evaluate_model(self): self.model.eval() running_loss = 0.0 all_preds, all_labels = [], [] wrong_predictions = [] with torch.no_grad(): for images, labels, img_paths in self.test_loader: images, labels = images.to(self.device), labels.to(self.device) if isinstance(self.model, MultiClassClassifier): labels = labels.long() # Ensure labels are of type Long outputs = self.model(images) loss = self.criterion(outputs, labels) # Multi-class loss preds = torch.argmax(outputs, dim=1) # Get predicted classes for multi-class else: # For BinaryClassifier labels = labels.float().unsqueeze(1) # For binary classification outputs = self.model(images) loss = self.criterion(outputs, labels) # Binary loss preds = (torch.sigmoid(outputs).squeeze() > 0.5).float() # Convert to binary predictions running_loss += loss.item() * labels.size(0) # Convert logits to probabilities and predictions all_preds.extend(preds.cpu().numpy()) all_labels.extend(labels.cpu().numpy()) # Collect wrong predictions with paths for i in range(len(preds)): if preds[i] != labels[i]: wrong_predictions.append((labels[i], preds[i], img_paths[i])) # Calculate average validation loss val_loss = running_loss / len(self.test_loader.dataset) self.logger.info(f\"Validation Loss: {val_loss:.4f}\") # Calculate evaluation metrics accuracy = accuracy_score(all_labels, all_preds) if isinstance(self.model, MultiClassClassifier):", "source": "pytorch.py"}, {"content": "# Multi-class metrics precision = precision_score(all_labels, all_preds, average='macro', zero_division=0) recall = recall_score(all_labels, all_preds, average='macro', zero_division=0) f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0) else: # Binary metrics precision = precision_score(all_labels, all_preds, zero_division=0) recall = recall_score(all_labels, all_preds, zero_division=0) f1 = f1_score(all_labels, all_preds, zero_division=0) self.logger.info(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\") print(f\"Accuracy: {accuracy:.4f}\") # Print accuracy to console # Log metrics to MLflow if self.mlflow_init_status: mlflow.log_metric(\"val_loss\", val_loss) mlflow.log_metric(\"accuracy\", accuracy) mlflow.log_metric(\"precision\", precision) mlflow.log_metric(\"recall\", recall) mlflow.log_metric(\"f1_score\", f1) return val_loss, wrong_predictions def plot_history(self): plt.figure(figsize=(10, 5)) plt.plot(self.train_losses, label=\"Train Loss\") plt.plot(self.val_losses, label=\"Validation Loss\") plt.xlabel(\"Epoch\") plt.ylabel(\"Loss\") plt.title(\"Training and Validation Losses\") plt.grid(True) plt.legend() plt.show() def visualize_wrong_predictions(self, wrong_predictions): \"\"\"Visualize the wrong predictions with original images.\"\"\" num_wrong = len(wrong_predictions) if num_wrong == 0: self.logger.info(\"No wrong predictions to display.\") return # Calculate number of rows and columns for the grid num_cols = 5 # Set the number of columns you want num_rows = (num_wrong + num_cols - 1) // num_cols # Calculate rows needed fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 3)) axes = axes.flatten() # Flatten the axes array to easily iterate for ax, (true_label, pred_label, img_path) in zip(axes, wrong_predictions): # Open the original image original_image = Image.open(img_path) ax.imshow(original_image) ax.set_title(f'True: {true_label.item()}, Pred: {pred_label.item()}') ax.axis('off') # Hide any remaining empty subplots for ax in axes[num_wrong:]: ax.axis('off') plt.tight_layout() # Adjust layout to prevent overlap plt.show() # def save_model(self): # \"\"\"Save the model in .pth format.\"\"\" # output_directory = 'models' # Replace with your desired directory # os.makedirs(output_directory, exist_ok=True) # Create directory if it doesn't exist # model_file_name = 'tensorfood.pth' # Change to .pth for PyTorch # file_path = os.path.join(output_directory, model_file_name) # # Save the model state dict # torch.save(self.model.state_dict(), file_path) # self.logger.info(f\"Model saved to {file_path}\") def execute(self): \"\"\"Main function for executing the model pipeline.\"\"\" self.logger.info(\"Setting up logging configuration.\") setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) self.initialize_mlflow() train_dataset, test_dataset = self.prepare_data() train_loader, test_loader = self.load_data(train_dataset, test_dataset) # Train the model and get training and validation losses train_losses, val_losses = self.train_model() # Plot the training and validation losses self.plot_history() # # Save the model # self.save_model() # Log parameters and artifacts mlflow_log( self.mlflow_init_status, \"log_dict\", dictionary=self.config, artifact_file=\"train_model_config.json\", ) if self.mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() self.logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_log( self.mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) self.logger.info(\"Model training with MLflow run ID %s has completed.\", self.mlflow_run.info.run_id) mlflow.end_run() else: self.logger.info(\"Model training has completed.\") print(\"Execution complete.\") @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"config.yaml\") def main(config: DictConfig): \"\"\"Main function to execute the model pipeline.\"\"\" classifier_type = config.classifier_type print(f\"Using classifier type: {classifier_type}\") pipeline = ImageClassifier(config, classifier_type=classifier_type) pipeline.execute() if __name__ == \"__main__\": main()", "source": "pytorch.py"}, {"content": "\"\"\" Test end to end - mlflow - runai - model artifact (h5) - google cloud artifact \"\"\" import os import sys import h5py import hydra import mlflow import numpy as np import logging import torch import torch.nn as nn import torchvision import torchvision.transforms as transforms import omegaconf from PIL import Image from omegaconf import DictConfig # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import setup_logging, mlflow_init, mlflow_log # Define a simple model class SimpleModel(nn.Module): def __init__(self): super(SimpleModel, self).__init__() self.fc1 = nn.Linear(28 * 28, 128) # Input size for MNIST self.fc2 = nn.Linear(128, 10) # Output size (10 classes) def forward(self, x): x = x.view(-1, 28 * 28) # Flatten the input x = torch.relu(self.fc1(x)) return self.fc2(x) class ModelPipeline: def __init__(self, config: DictConfig): \"\"\"Initialize the model pipeline with configuration.\"\"\" self.config = config self.device = None self.mlflow_init_status = None self.mlflow_run = None self.logger = logging.getLogger(__name__) def execute(self): \"\"\"This is the main function for training the model. Parameters ---------- self.config : omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) mlflow_init_status, mlflow_run = mlflow_init( self.config, setup_mlflow=self.config[\"setup_mlflow\"], autolog=self.config[\"mlflow_autolog\"] ) mlflow_log( mlflow_init_status, \"log_params\", params={ \"learning_rate\": self.config[\"lr\"], \"gamma\": self.config[\"gamma\"], \"seed\": self.config[\"seed\"], \"epochs\": self.config[\"epochs\"], }, ) torch.manual_seed(self.config[\"seed\"]) use_cuda = not self.config[\"no_cuda\"] and torch.cuda.is_available() if use_cuda: device = torch.device(\"cuda\") elif not self.config[\"no_mps\"] and torch.backends.mps.is_available(): device = torch.device(\"mps\") else: device = torch.device(\"cpu\") train_kwargs = {\"batch_size\": self.config[\"train_bs\"]} test_kwargs = {\"batch_size\": self.config[\"test_bs\"]} if use_cuda: cuda_kwargs = {\"num_workers\": 1, \"pin_memory\": True, \"shuffle\": True} train_kwargs.update(cuda_kwargs) test_kwargs.update(cuda_kwargs) \"\"\"Testing\"\"\" # Load the image image_path = 'myfile.jpg' # Replace with your image path try: image = Image.open(image_path).convert('L') # Convert to grayscale print(f\"Image mode: {image.mode}, Image size: {image.size}\") except Exception as e: print(f\"Error loading image: {e}\") # Define transformations transform = transforms.Compose([ transforms.Resize((28, 28)), # Resize to 28x28 for MNIST transforms.ToTensor(), # Convert image to tensor transforms.Normalize((0.5,), (0.5,)) # Normalize ]) # Initialize image_tensor image_tensor = None # Apply transformations if the image is loaded successfully if image is not None: try: # Apply transformations using NumPy image_tensor = transform(image) # Add batch dimension except Exception as e: print(f\"Error during transformation: {e}\") # Check if image_tensor is valid before proceeding if image_tensor is None: print(\"Image tensor is not created. Exiting.\") exit() # Instantiate the model model = SimpleModel() # Run the image through the model with torch.no_grad(): # Disable gradient calculation output = model(image_tensor) # Print the output print(\"Model output:\", output) # Save the model in H5 format to a specific directory output_directory = 'models' # Replace with your desired directory os.makedirs(output_directory, exist_ok=True) # Create directory if it doesn't exist file_path = os.path.join(output_directory, 'simple_model.h5') with h5py.File(file_path, 'w') as h5file: model_group = h5file.create_group('model') for key, value in model.state_dict().items(): model_group.create_dataset(key, data=value.cpu().numpy()) print(f\"Model saved at: {file_path}\") \"\"\"End testing\"\"\" # train_dataset = amlo.data_prep.datasets.MNISTDataset( # self.config[\"data_dir_path\"], \"train.csv\", to_grayscale=True, to_tensor=True # ) # test_dataset = amlo.data_prep.datasets.MNISTDataset( # self.config[\"data_dir_path\"], \"test.csv\", to_grayscale=True, to_tensor=True # ) # train_loader = torch.utils.data.DataLoader(train_dataset, **train_kwargs) # test_loader = torch.utils.data.DataLoader(test_dataset, **test_kwargs) # model = amlo.modeling.models.Net().to(device) # optimiser = torch.optim.Adadelta(model.parameters(), lr=self.config[\"lr\"]) # scheduler =", "source": "test.py"}, {"content": "torch.optim.lr_scheduler.StepLR( # optimiser, step_size=1, gamma=self.config[\"gamma\"] # ) # for epoch in range(1, self.config[\"epochs\"] + 1): # curr_train_loss = amlo.modeling.utils.train( # self.config, model, device, train_loader, optimiser, epoch, mlflow_init_status # ) # curr_test_loss, curr_test_accuracy = amlo.modeling.utils.test( # model, device, test_loader, epoch, mlflow_init_status # ) # if epoch % self.config[\"model_checkpoint_interval\"] == 0: # logger.info(\"Exporting the model for epoch %s.\", epoch) # model_checkpoint_path = os.path.join( # self.config[\"model_checkpoint_dir_path\"], \"model.pt\" # ) # torch.save( # { # \"model_state_dict\": model.state_dict(), # \"epoch\": epoch, # \"optimiser_state_dict\": optimiser.state_dict(), # \"train_loss\": curr_train_loss, # \"test_loss\": curr_test_loss, # }, # model_checkpoint_path, # ) # amlo.general_utils.mlflow_log( # mlflow_init_status, # \"log_artifact\", # local_path=model_checkpoint_path, # artifact_path=\"model\", # ) # scheduler.step() mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(self.config, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") # Outputs for conf/train_model.yaml for hydra.sweeper.direction # return curr_test_loss, curr_test_accuracy print(\"1\") @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(config: DictConfig): \"\"\"Main function to execute the model pipeline.\"\"\" pipeline = ModelPipeline(config) pipeline.execute() if __name__ == \"__main__\": main()", "source": "test.py"}, {"content": "import os import sys import h5py import hydra import mlflow import numpy as np import logging import tensorflow as tf import matplotlib.pyplot as plt from tensorflow.keras.applications import VGG16 from tensorflow.keras.models import Sequential, Model from tensorflow.keras.layers import Dense, Flatten, Dropout, Input from sklearn.metrics import precision_score, recall_score from tensorflow.keras.metrics import Precision, Recall from PIL import Image from omegaconf import DictConfig # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import setup_logging, mlflow_init, mlflow_log class ModelPipeline: def __init__(self, config): \"\"\"Initialize the model pipeline with configuration.\"\"\" self.config = config self.logger = logging.getLogger(__name__) self.model = None self.train_ds = None self.val_ds = None self.test_ds = None self.history = None self.class_names = None self.mlflow_init_status = False def initialize_mlflow(self): \"\"\"Initialize MLflow tracking.\"\"\" self.mlflow_init_status = mlflow_init( self.config, setup_mlflow=self.config.setup_mlflow, autolog=self.config.mlflow_autolog ) return self.mlflow_init_status def load_data(self): \"\"\"Load datasets for training, validation, and testing using image_dataset_from_directory.\"\"\" # Set the random seed for reproducibility tf.random.set_seed(self.config.seed) # Load the full dataset first self.train_ds, self.test_ds = tf.keras.preprocessing.image_dataset_from_directory( directory=self.config.data_dir_path, label_mode=\"categorical\", image_size=(self.config.resize_width, self.config.resize_height), seed=self.config.seed, batch_size=self.config.batch_size, validation_split=self.config.validation_split, subset=\"both\" ) return self.train_ds, self.test_ds def plot_history(self, history): hist = history.history plt.figure(figsize=(10, 5)) plt.plot(hist[\"loss\"], label=\"train_loss\") plt.plot(hist[\"val_loss\"], label=\"val_loss\") plt.grid(True) plt.legend() plt.show() def train_model(self): inputs = Input(shape=(self.config.resize_width, self.config.resize_height, 3)) base_model = VGG16(input_tensor=inputs, weights=\"imagenet\", include_top=False, input_shape=(self.config.resize_width, self.config.resize_height, 3)) base_model.trainable = False x = Flatten()(base_model.output) x = Dense(256, activation=\"relu\")(x) x = Dropout(rate=0.4)(x) outputs = Dense(self.config.num_classes, activation=\"softmax\")(x) model = Model(inputs, outputs) model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=self.config.lr), loss=tf.keras.losses.CategoricalCrossentropy(), metrics=[\"accuracy\"] ) tf.keras.backend.clear_session() history = model.fit( self.train_ds, batch_size=self.config.batch_size, epochs=self.config.epochs, verbose=1, validation_data=self.test_ds ) self.plot_history(history) self.model = model self.history = history def save_model(self): \"\"\"Save the model in H5 format and log it with MLflow.\"\"\" # Define the output directory and file name output_directory = self.config.output_directory os.makedirs(output_directory, exist_ok=True) # Create directory if it doesn't exist # Define the model file path model_file_name = self.config.model_file_name file_path = os.path.join(output_directory, model_file_name) # Save the model self.model.save(file_path) # Log the model with MLflow mlflow.log_artifact(file_path, artifact_path=self.config.artifact_path) self.logger.info(f\"Model logged to MLflow at: {file_path}\") def evaluate_model(self): \"\"\"Evaluate the model on the test dataset.\"\"\" # Evaluate model test_loss, test_accuracy = self.model.evaluate(self.test_ds, verbose=2) # Calculate precision and recall predictions = np.argmax(self.model.predict(self.test_ds), axis=-1) true_labels = np.concatenate([y for x, y in self.test_ds], axis=0) precision = precision_score(true_labels, predictions, average='weighted') recall = recall_score(true_labels, predictions, average='weighted') self.logger.info(f\"Test Loss: {test_loss:.4f}\") self.logger.info(f\"Test Accuracy: {test_accuracy:.4f}\") self.logger.info(f\"Precision: {precision:.4f}\") self.logger.info(f\"Recall: {recall:.4f}\") return test_loss, precision, recall def execute(self): \"\"\"Main function for executing the model pipeline.\"\"\" self.logger.info(\"Setting up logging configuration.\") setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) self.initialize_mlflow() # Load the datasets self.load_data() # Train the model self.train_model() # Evaluate the model test_loss, precision, recall = self.evaluate_model() # Save the model self.save_model() # Log parameters and artifacts mlflow_log( self.mlflow_init_status, \"log_dict\", dictionary=self.config, artifact_file=\"train_model_config.json\", ) if self.mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() self.logger.info(\"Artifact URI: %s\", artifact_uri) mlflow_log( self.mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) self.logger.info(\"Model training with MLflow run ID %s has completed.\", mlflow.active_run().info.run_id) mlflow.end_run() else: self.logger.info(\"Model training has completed.\") print(\"Execution complete.\") @hydra.main(config_path=\"../conf\", config_name=\"config.yaml\") def main(config: DictConfig): \"\"\"Main function to execute the model pipeline.\"\"\" pipeline = ModelPipeline(config) pipeline.execute() if __name__ == \"__main__\": main()", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time import shutil from dotenv import load_dotenv logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: load_dotenv() username = os.getenv('MLFLOW_TRACKING_USERNAME') password = os.getenv('MLFLOW_TRACKING_PASSWORD') mlflow_uri = os.getenv('MLFLOW_TRACKING_URI') experiment_name = os.getenv('MLFLOW_EXPERIMENT') # Construct the full tracking URI with authentication tracking_uri = f\"http://{username}:{password}@{mlflow_uri.split('://')[1]}\" mlflow.set_tracking_uri(tracking_uri) mlflow.set_experiment(experiment_name) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") # logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) logger.debug(f\"Logging with MLflow function: {log_function}, Arguments: {kwargs}\") method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: # logger.error(error) logger.error(f\"Error during MLflow logging: {error}\") logger.error(\"Stack Trace:\", exc_info=True) # Log the stack trace def copy_data_to_pvc(): \"\"\" Copies all files from the assignment (source) folder to the pvc-data (destination) folder. For runai usage. \"\"\" try: load_dotenv() src_folder = os.getenv('PVC_SOURCE_FOLDER_DIR') dest_folder = os.getenv('PVC_DEST_FOLDER_DIR') # Check if the source folder exists if not os.path.exists(src_folder): print(f\"Source folder does not exist: {src_folder}\") return # Create the destination folder if it doesn't exist os.makedirs(dest_folder, exist_ok=True) # Copy files from source to destination for item in os.listdir(src_folder): s = os.path.join(src_folder, item) d = os.path.join(dest_folder, item) if os.path.isdir(s): shutil.copytree(s, d, False, None) # Recursively copy directories else: shutil.copy2(s, d) # Copy files print(f\"Data copied from {src_folder}", "source": "general_utils.py"}, {"content": "to {dest_folder}.\") except Exception as e: print(f\"An error occurred: {e}\")", "source": "general_utils.py"}, {"content": "import os import sys import logging import omegaconf import time import hydra import mlflow # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import setup_logging, mlflow_init, mlflow_log, set_random_seed from src.datapipeline import DataPipeline from src.pipeline import ModelPipeline def initialize_logging(config): \"\"\"Set up logging configuration.\"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) return logger def log_parameters(config, mlflow_init_status): \"\"\"Log hyperparameters to MLflow.\"\"\" mlflow_log( mlflow_init_status, \"log_params\", params={ \"n_estimators\": config.n_estimators, \"max_depth\": config.max_depth, \"min_samples_split\": config.min_samples_split }, ) def log_metrics(train_mse, test_mse, mlflow_init_status): \"\"\"Log evaluation metrics to MLflow.\"\"\" mlflow_log( mlflow_init_status, \"log_metrics\", metrics={\"train_mse\": train_mse, \"test_mse\": test_mse}, ) def save_and_log_model(model, config, mlflow_init_status): \"\"\"Save and log model to MLflow.\"\"\" model_name = f\"model-{time.time():.0f}.pkl\" model_checkpoint_path = os.path.join(config.model_checkpoint_dir_path, model_name) # Ensure the checkpoint directory exists os.makedirs(config.model_checkpoint_dir_path, exist_ok=True) mlflow.sklearn.save_model(model, model_checkpoint_path) mlflow_log( mlflow_init_status, \"log_artifact\", local_path=model_checkpoint_path, artifact_path=\"model\", ) artifact_uri = mlflow.get_artifact_uri() logging.info(\"Artifact URI: %s\", artifact_uri) mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) # pylint: disable = no-value-for-parameter @hydra.main(config_path=\"../assignment6/conf\", config_name=\"train_model.yaml\") def main(config): \"\"\"Main function for training the model.\"\"\" logger = initialize_logging(config) # Initialize MLflow mlflow_init_status, mlflow_run = mlflow_init( config, setup_mlflow=config.setup_mlflow, autolog=config.mlflow_autolog ) log_parameters(config, mlflow_init_status) # Set random seed for reproducibility set_random_seed(config.seed) # Run model pipeline pipeline = ModelPipeline(config) model, cv_scores, train_error, test_error = pipeline.run() # # Log metrics and model # logger.info(f\"Training MSE: {train_mse}\") # logger.info(f\"Test MSE: {test_mse}\") # log_metrics(train_mse, test_mse, mlflow_init_status) if mlflow_init_status: save_and_log_model(model, config, mlflow_init_status) logger.info(\"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id) mlflow.end_run() else: logger.info(\"Model training has completed without MLflow.\") # Return test MSE for Hydra sweeper return cv_scores if __name__ == \"__main__\": main()", "source": "main.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import os import sys import logging import pandas as pd from sklearn.preprocessing import LabelEncoder # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import load_data class DataPipeline: def __init__(self, config): self.config = config self.logger = logging.getLogger(__name__) self.data = self.load_data() self.X = None self.y = None def load_data(self): \"\"\"Load data from the specified path in the configuration.\"\"\" self.logger.info(f\"Loading data from {self.config.data_dir_path}\") data = load_data(self.config.data_dir_path) return data def combine_datetime(self): \"\"\"Combine Year, Month, Day, and Hour into a single DateTime column and set it as index.\"\"\" self.data['datetime'] = pd.to_datetime(self.data[['year', 'month', 'day', 'hour']]) self.data.set_index('datetime', inplace=True) def fill_na(self): \"\"\"Fill missing values in the specified column using forward fill and drop any remaining missing values.\"\"\" self.data = self.data.ffill() self.data.dropna(inplace=True) return self.data def encode_categorical(self): \"\"\"Encodes categorical features in the data.\"\"\" # Identify and encode categorical columns categorical_cols = self.data.select_dtypes(include=['object']).columns for col in categorical_cols: self.data[col] = LabelEncoder().fit_transform(self.data[col]) def add_lagged_features(self, lags=[1, 2, 3]): \"\"\"Add lagged features for selected columns.\"\"\" for lag in lags: for column in ['pm2.5', 'TEMP', 'PRES', 'DEWP']: self.data[f'{column}_lag{lag}'] = self.data[column].shift(lag) # Drop any rows with NaN values generated by the lagging process self.data.dropna(inplace=True) def prepare_features_and_target(self): \"\"\"Separate features and target variable after lagging.\"\"\" self.X = self.data.drop(columns=[self.config.target_column]) self.y = self.data[self.config.target_column] def run_data_pipeline(self): \"\"\"Run the full data pipeline process.\"\"\" self.combine_datetime() self.fill_na() self.encode_categorical() self.add_lagged_features() self.prepare_features_and_target() return self.X, self.y def run_data_pipeline_for_rnn(self): \"\"\"Run the data pipeline process without lagged features for RNNs.\"\"\" self.combine_datetime() self.fill_na() self.encode_categorical() self.prepare_features_and_target() return self.X, self.y", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "import matplotlib.pyplot as plt import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.base import BaseEstimator, RegressorMixin from sklearn.metrics import mean_squared_error class ForecastModel(BaseEstimator, RegressorMixin): def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, random_state=42, lag=None): self.n_estimators = n_estimators self.max_depth = max_depth self.min_samples_split = min_samples_split self.random_state = random_state self.model = RandomForestRegressor( n_estimators=self.n_estimators, max_depth=self.max_depth, min_samples_split=self.min_samples_split, random_state=self.random_state ) def fit(self, X, y): self.model.fit(X, y) return self # scikit-learn expects fit to return self def predict(self, X): return self.model.predict(X) def evaluate(self, X_train, y_train, X_test, y_test): # Predict on training data and calculate error y_train_pred = self.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) # Predict on testing data and calculate error y_test_pred = self.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def get_params(self, deep=True): return { \"n_estimators\": self.n_estimators, \"max_depth\": self.max_depth, \"min_samples_split\": self.min_samples_split, \"random_state\": self.random_state } def set_params(self, **params): for key, value in params.items(): setattr(self, key, value) self.model = RandomForestRegressor( n_estimators=self.n_estimators, max_depth=self.max_depth, min_samples_split=self.min_samples_split, random_state=self.random_state ) return self def plot_predictions(self, y_test, y_test_pred, index): # Convert to a DataFrame for easy plotting df = pd.DataFrame({'Actual': y_test, 'Predicted': y_test_pred}, index=index) # Plot actual and predicted values over time plt.figure(figsize=(14, 7)) plt.plot(df.index, df['Actual'], label='Actual PM2.5', color='blue', marker='o', linestyle='', alpha=0.7) plt.plot(df.index, df['Predicted'], label='Predicted PM2.5', color='orange', marker='x', linestyle='', alpha=0.7) plt.title('Actual vs Predicted PM2.5 Levels') plt.xlabel('Time') plt.ylabel('PM2.5') plt.legend() plt.show()", "source": "ml_model.py"}, {"content": "import os import sys import numpy as np import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from sklearn.model_selection import TimeSeriesSplit, cross_val_score # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from src.datapipeline import DataPipeline from src.ml_model import ForecastModel class ModelPipeline: def __init__(self, config): self.config = config # Extract model parameters from config and pass them to ForecastModel self.model = ForecastModel( n_estimators=config.get(\"n_estimators\", 100), max_depth=config.get(\"max_depth\", None), min_samples_split=config.get(\"min_samples_split\", 2), random_state=config.get(\"seed\", 42) ) self.cv_scores = None self.cv_predictions = [] self.y = None self.metrics_dict = {} # Initialize the metrics dictionary def run(self): # Run data pipeline pipeline = DataPipeline(self.config) X, y = pipeline.run_data_pipeline() self.y = y # Perform cross-validation with TimeSeriesSplit tscv = TimeSeriesSplit(n_splits=5) self.cv_scores = cross_val_score(self.model, X, y, cv=tscv, scoring='neg_mean_squared_error') self.cv_scores = -self.cv_scores # Generate predictions for each fold for train_index, test_index in tscv.split(X): X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index] y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index] self.model.fit(X_train_fold, y_train_fold) y_pred_fold = self.model.predict(X_test_fold) self.cv_predictions.append((y_test_fold.index, y_pred_fold)) # Store both index and predictions # Use the last fold for final train-test split evaluation last_train_index, last_test_index = list(tscv.split(X))[-1] X_train, X_test = X.iloc[last_train_index], X.iloc[last_test_index] y_train, y_test = y.iloc[last_train_index], y.iloc[last_test_index] # Train and evaluate on the last fold self.model.fit(X_train, y_train) train_error, test_error = self.model.evaluate(X_train, y_train, X_test, y_test) # Store the final fold\u2019s test and predictions for plotting self.final_y_test = y_test self.final_y_pred = self.model.predict(X_test) # Return the model, cross-validation scores, and single train-test errors return self.model, self.cv_scores, train_error, test_error def plot_model_outputs_vs_actuals(self): \"\"\"Plot the distribution of model outputs vs actual values for the last test fold.\"\"\" plt.figure(figsize=(12, 6)) plt.hist(self.final_y_test, bins=30, alpha=0.5, label=\"Actual\") plt.hist(self.final_y_pred, bins=30, alpha=0.5, label=\"Predicted\") plt.title(\"Distribution of Actual vs Predicted Values (Last Test Fold)\") plt.xlabel(\"Values\") plt.ylabel(\"Frequency\") plt.legend() plt.show() def plot_residuals_over_time(self): \"\"\"Plot residuals over time to check for non-homogeneous patterns in the last test fold.\"\"\" residuals = self.final_y_test - self.final_y_pred plt.figure(figsize=(12, 6)) plt.plot(self.final_y_test.index, residuals, label=\"Residuals\") plt.title(\"Residuals Over Time (Last Test Fold)\") plt.xlabel(\"Time\") plt.ylabel(\"Residual (Actual - Predicted)\") plt.show() def plot_model_outliers(self): \"\"\"Plot model handling of outliers for the last test fold.\"\"\" plt.figure(figsize=(12, 6)) plt.plot(self.final_y_test.index, self.final_y_test, label=\"Actual\", color=\"blue\") plt.plot(self.final_y_test.index, self.final_y_pred, label=\"Predicted\", color=\"orange\", alpha=0.7) plt.title(\"Outlier Analysis: Actual vs Predicted Values (Last Test Fold)\") plt.xlabel(\"Time\") plt.ylabel(\"Values\") plt.legend() plt.show()", "source": "pipeline.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import random import logging import logging.config import yaml import mlflow import time import shutil from dotenv import load_dotenv import pandas as pd import numpy as np logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: load_dotenv() username = os.getenv('MLFLOW_TRACKING_USERNAME') password = os.getenv('MLFLOW_TRACKING_PASSWORD') mlflow_uri = os.getenv('MLFLOW_TRACKING_URI') experiment_name = os.getenv('MLFLOW_EXPERIMENT') # Construct the full tracking URI with authentication tracking_uri = f\"http://{username}:{password}@{mlflow_uri.split('://')[1]}\" mlflow.set_tracking_uri(tracking_uri) mlflow.set_experiment(experiment_name) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error) def set_random_seed(seed): random.seed(seed) np.random.seed(seed) def copy_data_to_pvc(): \"\"\" Copies all files from the assignment (source) folder to the pvc-data (destination) folder. For runai usage. \"\"\" try: load_dotenv() src_folder = os.getenv('PVC_SOURCE_FOLDER_DIR') dest_folder = os.getenv('PVC_DEST_FOLDER_DIR') # Check if the source folder exists if not os.path.exists(src_folder): print(f\"Source folder does not exist: {src_folder}\") return # Create the destination folder if it doesn't exist os.makedirs(dest_folder, exist_ok=True) # Copy files from source to destination for item in os.listdir(src_folder): s = os.path.join(src_folder, item) d = os.path.join(dest_folder, item) if os.path.isdir(s): shutil.copytree(s, d, False, None) # Recursively copy directories else: shutil.copy2(s, d) # Copy files print(f\"Data copied from {src_folder} to", "source": "general_utils.py"}, {"content": "{dest_folder}.\") except Exception as e: print(f\"An error occurred: {e}\") def load_data(csv_path): return pd.read_csv(csv_path)", "source": "general_utils.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import re from bs4 import BeautifulSoup import unicodedata import inflect import contractions class TextCleaner: def __init__(self): self.inflect_engine = inflect.engine() def clean_text(self, text): # Expand contractions text = contractions.fix(text) # Normalize Unicode to NFKD text = unicodedata.normalize('NFKD', text) # Remove HTML tags text = BeautifulSoup(text, \"html.parser\").get_text() # Change to lower case text = text.lower() # Replace line breaks with a space text = re.sub(r'[\\r\\n]+', ' ', text) # Remove punctuation text = re.sub(r'[^\\w\\s]', ' ', text) # Convert numbers to words text = re.sub(r'\\b\\d+\\b', lambda x: self.inflect_engine.number_to_words(x.group()), text) # Remove duplicate phrases text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text) # Normalize whitespace text = re.sub(r'\\s+', ' ', text).strip() return text", "source": "data_cleaning.py"}, {"content": "import os import sys import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from src.data_cleaning import TextCleaner from src.lemmatisation import Lemmatizer class DataPipeline(): def __init__(self, config, df): self.config = config self.df = df self.count_vectorizer = CountVectorizer() self.tfidf_transformer = TfidfTransformer() self.X_train_bow = None self.X_train_tfidf = None self.feature_names = None def clean_data(self): \"\"\"Cleans the text data in the DataFrame.\"\"\" cleaner = TextCleaner() self.df['text'] = self.df['text'].apply(cleaner.clean_text) self.df = self.df.drop_duplicates() return self def process_data(self): \"\"\"Applies lemmatization to the text data in the DataFrame.\"\"\" lemmatizer = Lemmatizer() # self.df['text'] = self.df['text'].apply(lemmatizer.lemmatize_text) self.df.loc[:, 'text'] = self.df['text'].apply(lemmatizer.lemmatize_text) return self def split_data(self): \"\"\"Splits the DataFrame into training and testing sets.\"\"\" train, test = train_test_split( self.df, test_size=self.config.split_ratio, random_state=self.config.seed) # Convert to lists for features and labels train_text = train[self.config['text_column']].tolist() y_train = train[self.config['target_column']].tolist() test_text = test[self.config['text_column']].tolist() y_test = test[self.config['target_column']].tolist() return train_text, test_text, y_train, y_test def convert_bow(self, text): \"\"\"Convert train text to Bag of Words representation\"\"\" bow = self.count_vectorizer.fit_transform(text) self.feature_names = self.count_vectorizer.get_feature_names_out() return bow, self.feature_names def convert_tfidf(self, bow, for_first_doc=False): \"\"\"Convert Bag of Words to TF-IDF representation.\"\"\" self.X_train_tfidf = self.tfidf_transformer.fit_transform(bow) # If only the first document is needed, extract its TF-IDF values if for_first_doc: # Get the TF-IDF values for the first document tfidf_values = self.X_train_tfidf[0].toarray().flatten() non_zero_indices = self.X_train_tfidf[0].nonzero() # Create a DataFrame for non-zero entries for the first document non_zero_df = pd.DataFrame({ 'feature': np.array(self.feature_names)[non_zero_indices[1]], 'tfidf_value': tfidf_values[non_zero_indices[1]] }) else: # Create a DataFrame for all documents tfidf_df = pd.DataFrame(self.X_train_tfidf.toarray(), columns=self.feature_names) # Identify non-zero entries for all documents non_zero_indices = self.X_train_tfidf.nonzero() # Create a DataFrame for non-zero entries non_zero_df = pd.DataFrame({ 'feature': np.array(self.feature_names)[non_zero_indices[1]], 'tfidf_value': self.X_train_tfidf[non_zero_indices].A1 }) non_zero_df = non_zero_df.sort_values(by='tfidf_value', ascending=False) return non_zero_df def get_word_frequencies(self, bow, for_first_doc=False): \"\"\"Calculate word frequencies from the Bag of Words representation.\"\"\" # Convert the sparse matrix to a dense format dense_bow = bow.toarray() if for_first_doc: # If only the first document is needed, use the first row word_counts = dense_bow[0] df_word_frequencies = pd.DataFrame({ 'word': self.feature_names, 'frequency': word_counts }) else: # Create a DataFrame to hold word counts for all documents df_word_counts = pd.DataFrame(dense_bow, columns=self.feature_names) # Sum the counts across all documents to get total word frequencies word_counts = df_word_counts.sum() # Create DataFrame with word names as index df_word_frequencies = pd.DataFrame({ 'word': self.feature_names, 'frequency': word_counts }) df_word_frequencies = df_word_frequencies.sort_values(by='frequency', ascending=False) return df_word_frequencies.reset_index(drop=True)", "source": "data_pipeline.py"}, {"content": "import nltk from nltk.stem import WordNetLemmatizer from nltk.tokenize import word_tokenize class Lemmatizer: def __init__(self): self.lemmatizer = WordNetLemmatizer() nltk.download('wordnet') nltk.download('averaged_perceptron_tagger') def get_wordnet_pos(self, pos): \"\"\"Converts POS tag to a format recognized by WordNetLemmatizer.\"\"\" if pos.startswith('J'): return 'a' # Adjective elif pos.startswith('V'): return 'v' # Verb elif pos.startswith('N'): return 'n' # Noun elif pos.startswith('R'): return 'r' # Adverb else: return 'n' # Default to noun def lemmatize_text(self, text): \"\"\"Lemmatizes the input text based on POS tagging.\"\"\" tokens = word_tokenize(text) pos_tags = nltk.pos_tag(tokens) lemmatized_words = [ self.lemmatizer.lemmatize(token, pos=self.get_wordnet_pos(pos)) for token, pos in pos_tags ] return lemmatized_words", "source": "lemmatisation.py"}, {"content": "import os import sys import pickle import hydra import logging import matplotlib.pyplot as plt from omegaconf import DictConfig from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics import accuracy_score, roc_curve, auc # Add the parent directory to the Python path parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..')) if parent_dir not in sys.path: sys.path.append(parent_dir) from utils.general_utils import mlflow_init, load_data from src.data_pipeline import DataPipeline class SentimentPipeline: def __init__(self, config): self.config = config self.logger = logging.getLogger(__name__) self.df = None self.data_pipeline = None self.mlflow_init_status = False self.vectorizer = TfidfVectorizer() def initialize_mlflow(self): \"\"\"Initialize MLflow tracking.\"\"\" self.mlflow_init_status = mlflow_init( self.config, setup_mlflow=self.config.setup_mlflow, autolog=self.config.mlflow_autolog ) return self.mlflow_init_status def load_data(self): \"\"\"Load the dataset.\"\"\" self.df = load_data(self.config.database.raw_data_save_path) def process_data(self): \"\"\"Preprocess the text data.\"\"\" self.data_pipeline = DataPipeline(self.config, self.df) self.data_pipeline.clean_data() self.data_pipeline.process_data() def split_data(self): \"\"\"Split the data into training and testing sets.\"\"\" train_text, test_text, y_train, y_test = self.data_pipeline.split_data() return train_text, test_text, y_train, y_test def vectorize_data(self, train_text, test_text): \"\"\"Vectorize the training and testing text data.\"\"\" train_text_flat = [' '.join(doc) for doc in train_text] test_text_flat = [' '.join(doc) for doc in test_text] X_train = self.vectorizer.fit_transform(train_text_flat) X_test = self.vectorizer.transform(test_text_flat) return X_train, X_test def train_model(self, X_train, y_train): \"\"\"Train the logistic regression model.\"\"\" model = LogisticRegression() model.fit(X_train, y_train) return model def evaluate_model(self, model, X_test, y_test): \"\"\"Make predictions and evaluate the model's performance.\"\"\" preds = model.predict(X_test) probs = model.predict_proba(X_test) # Print accuracy self.logger.info(f\"Accuracy: {accuracy_score(y_test, preds)}\") # Convert string labels to binary format y_test_binary = [1 if label == 'pos' else 0 for label in y_test] # Convert 'pos' to 1 and 'neg' to 0 y_prob = probs[:, 1] return y_test_binary, y_prob def save_model(self, model, vectorizer): \"\"\"Save the model and vectorizer to a single file.\"\"\" directory = os.path.dirname(self.config.database.model_save_path) os.makedirs(directory, exist_ok=True) filename = self.config.database.model_save_path with open(filename, 'wb') as f: pickle.dump({'model': model, 'vectorizer': vectorizer}, f) self.logger.info(f\"Model and vectorizer saved to {filename}\") def load_model(self): \"\"\"Load the model and vectorizer from a single file.\"\"\" filename = self.config.database.model_save_path with open(filename, 'rb') as f: data = pickle.load(f) self.model = data['model'] self.vectorizer = data['vectorizer'] self.logger.info(f\"Model and vectorizer loaded from {filename}\") return self.model, self.vectorizer def log_results(self): if self.mlflow_init_status: # Log params, dict, artifact pass def plot_results(self, y_test_binary, y_prob): fpr, tpr, thresholds = roc_curve(y_test_binary, y_prob) roc_auc = auc(fpr, tpr) plt.figure() plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})') plt.plot([0, 1], [0, 1], color='gray', linestyle='--') plt.xlim([0.0, 1.0]) plt.ylim([0.0, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') plt.title('Receiver Operating Characteristic') plt.legend(loc='lower right') plt.show() def run_pipeline(self): \"\"\"Main function for executing the model pipeline.\"\"\" self.logger.info(\"Setting up logging configuration.\") # setup_logging( # logging_config_path=os.path.join( # hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" # ) # ) self.initialize_mlflow() self.load_data() self.process_data() train_text, test_text, y_train, y_test = self.split_data() X_train, X_test = self.vectorize_data(train_text, test_text) model = self.train_model(X_train, y_train) y_test_binary, y_prob = self.evaluate_model(model, X_test, y_test) self.save_model(model, self.vectorizer) # self.log_results() return y_test_binary, y_prob def load_and_split_data(self): \"\"\"Load the dataset and split it into training and testing sets.\"\"\" # self.logger.info(\"Setting up logging configuration.\") # self.initialize_mlflow() self.load_data() self.process_data() train_text, test_text, y_train, y_test = self.split_data() return train_text, test_text, y_train, y_test @hydra.main(config_path=\"../conf\", config_name=\"config.yaml\") def main(config: DictConfig): \"\"\"Main function to execute the model pipeline.\"\"\" pipeline = SentimentPipeline(config) pipeline.run_pipeline() if __name__ == \"__main__\": main()", "source": "sentiment_pipeline.py"}, {"content": "import nltk from nltk.stem import PorterStemmer class Stemmer: def __init__(self): nltk.download('punkt') self.porter = PorterStemmer() def stem_text(self, text): tokens = nltk.word_tokenize(text) return [self.porter.stem(token) for token in tokens]", "source": "stemming.py"}, {"content": "import torch from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments import numpy as np import evaluate from torch.utils.data import Dataset from datasets import load_dataset def tokenize(batch): return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt') emotions = load_dataset(\"emotion\") emotions_encoded = emotions.map(tokenize, batched=True) # Load tokenizer and model tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5) # Load metrics metric = evaluate.load(\"accuracy\") def compute_metrics(eval_pred): logits, labels = eval_pred predictions = np.argmax(logits, axis=-1) return metric.compute(predictions=predictions, references=labels) # Tokenization with padding and truncation tokenized_train = tokenizer( emotions_encoded[\"train\"][\"text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\" ) tokenized_validation = tokenizer( emotions_encoded[\"validation\"][\"text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\" ) # Create dataset objects class EmotionDataset(Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item['labels'] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = EmotionDataset(tokenized_train, emotions_encoded[\"train\"][\"label\"]) validation_dataset = EmotionDataset(tokenized_validation, emotions_encoded[\"validation\"][\"label\"]) # Define training arguments training_args = TrainingArguments( output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", learning_rate=2e-5, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_train_epochs=3, weight_decay=0.01, report_to=[], ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=validation_dataset, compute_metrics=compute_metrics )", "source": "trainer.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import random import logging import logging.config import yaml import mlflow import time import shutil from dotenv import load_dotenv import pandas as pd import numpy as np # import pyodbc from hydra.core.global_hydra import GlobalHydra import hydra logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: load_dotenv() username = os.getenv('MLFLOW_TRACKING_USERNAME') password = os.getenv('MLFLOW_TRACKING_PASSWORD') mlflow_uri = os.getenv('MLFLOW_TRACKING_URI') experiment_name = os.getenv('MLFLOW_EXPERIMENT') # Construct the full tracking URI with authentication tracking_uri = f\"http://{username}:{password}@{mlflow_uri.split('://')[1]}\" mlflow.set_tracking_uri(tracking_uri) mlflow.set_experiment(experiment_name) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error) def initialize_hydra(config_name=\"config.yaml\", config_path=\"../conf\"): \"\"\" Initialize Hydra and return the configuration object. Args: config_name (str): Name of the configuration file. config_path (str): Path to the configuration directory. Returns: DictConfig: The loaded configuration object. \"\"\" if GlobalHydra.instance().is_initialized(): GlobalHydra.instance().clear() hydra.initialize(config_path=config_path, version_base=None) config = hydra.compose(config_name=config_name) return config def close_hydra(): \"\"\"Clear the Hydra instance to free up resources.\"\"\" if GlobalHydra.instance().is_initialized(): GlobalHydra.instance().clear() def set_seed(seed): \"\"\" Set the random seed for reproducibility in Python, NumPy. Args: seed (int): The seed value to use for random number generation. \"\"\" # Python's built-in random module random.seed(seed) # NumPy np.random.seed(seed) def", "source": "general_utils.py"}, {"content": "copy_data_to_pvc(): \"\"\" Copies all files from the assignment (source) folder to the pvc-data (destination) folder. For runai usage. \"\"\" try: load_dotenv() src_folder = os.getenv('PVC_SOURCE_FOLDER_DIR') dest_folder = os.getenv('PVC_DEST_FOLDER_DIR') # Check if the source folder exists if not os.path.exists(src_folder): print(f\"Source folder does not exist: {src_folder}\") return # Create the destination folder if it doesn't exist os.makedirs(dest_folder, exist_ok=True) # Copy files from source to destination for item in os.listdir(src_folder): s = os.path.join(src_folder, item) d = os.path.join(dest_folder, item) if os.path.isdir(s): shutil.copytree(s, d, False, None) # Recursively copy directories else: shutil.copy2(s, d) # Copy files print(f\"Data copied from {src_folder} to {dest_folder}.\") except Exception as e: print(f\"An error occurred: {e}\") def download_data(): \"\"\" Download data from the database using the configuration. Args: config_name (str): Name of the configuration file. config_path (str): Path to the configuration directory. Returns: pd.DataFrame: DataFrame containing the loaded data. \"\"\" # Load environment variables from .env file load_dotenv() # Initialize Hydra and load configuration config = initialize_hydra() logger.info(\"Starting the data loading process\") conn = None try: # Retrieve username and password from environment variables username = os.getenv('DB_USERNAME') password = os.getenv('DB_PASSWORD') # Construct the connection string using the configuration details conn_str = ( f'DSN={config.database.driver};SERVER={config.database.server};' f'DATABASE={config.database.database};UID={username};' f'PWD={password}' ) # Establish connection conn = pyodbc.connect(conn_str) # Query to fetch data from the specified table query = f\"SELECT * FROM {config.database.table}\" # Read data into DataFrame df = pd.read_sql(query, conn) # Define the CSV file path where the data will be saved csv_filepath = config.database.data_save_path # Create the directory for saving raw data if it doesn't exist os.makedirs(os.path.dirname(csv_filepath), exist_ok=True) # Save DataFrame to CSV df.to_csv(csv_filepath, index=False) logger.info(f\"Data successfully saved to {csv_filepath}\") close_hydra() # Return the DataFrame return df except pyodbc.Error as e: logger.info(f\"Database error occurred: {e}\", level=\"error\") except Exception as e: logger.info(f\"An unexpected error occurred: {e}\", level=\"error\") finally: # Ensure the connection is closed to avoid resource leaks if conn is not None: conn.close() logger.info(\"Database connection closed.\") def load_data(csv_path): return pd.read_csv(csv_path) def save_data(df, directory, filename): os.makedirs(directory, exist_ok=True) csv_path = os.path.join(directory, filename) return df.to_csv(csv_path, index=False)", "source": "general_utils.py"}, {"content": "import os from dotenv import load_dotenv from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage from langchain.prompts import PromptTemplate, FewShotPromptTemplate # Load environment variables from .env file load_dotenv() class PromptingClassifier: def __init__(self, temperature=0.0): self.llm = AzureChatOpenAI( azure_endpoint=os.getenv('OPEN_API_BASE'), openai_api_version=os.getenv('OPEN_API_VERSION'), deployment_name=os.getenv('DEPLOYMENT_NAME'), openai_api_key=os.getenv('OPEN_API_KEY'), openai_api_type=os.getenv('OPEN_API_TYPE'), temperature=temperature ) def zero_shot_classify(self, cases, prompt, use_cot=False): prompt_template = PromptTemplate(input_variables=[\"case\"], template=prompt) results = [] for case in cases: formatted_prompt = prompt_template.format(case=case) if use_cot: formatted_prompt += \"\\n\\nLet's think step by step.\" message = HumanMessage(content=formatted_prompt) category = self.llm([message]).content.strip() results.append((case, category)) return results def few_shot_classify(self, cases, prompt, examples, use_cot=False): # Create the example dictionary from provided examples example_dicts = [{\"input\": ex.split(' (')[0], \"output\": ex.split(' (')[1][:-1]} for ex in examples] # Build FewShotPromptTemplate few_shot_prompt = FewShotPromptTemplate( examples=example_dicts, example_prompt=PromptTemplate( input_variables=[\"input\", \"output\"], template=\"Case: {input}\\nClassification: {output}\" ), suffix=prompt, input_variables=[\"case\"] ) results = [] for case in cases: # Format the prompt dynamically with the case and examples formatted_prompt = few_shot_prompt.format(case=case) if use_cot: formatted_prompt += \"\\n\\nLet's think step by step.\" message = HumanMessage(content=formatted_prompt) category = self.llm([message]).content.strip() results.append((case, category)) return results", "source": "prompt.py"}, {"content": "# def transform(data_path): # \"\"\" # Description of the function. # :param data_path: ...... # :return: ...... # \"\"\" # raise NotImplementedError # Import necessary libraries import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder from sklearn.pipeline import Pipeline def transform(data_path): \"\"\" Loads the dataset, preprocesses the data, and splits it into training and testing sets. :param data_path: Path to the CSV file containing the dataset. :return: Four numpy arrays: X_train, X_test, y_train, y_test \"\"\" # Load the data df = pd.read_csv(data_path) # Replace placeholder values with pd.NA df.replace({'Not in universe under 1 year old': pd.NA, 'Not in universe': pd.NA, '?': pd.NA}, inplace=True) # Drop columns with a high percentage of missing values df.drop(columns=[ 'enroll_in_edu_inst_last_wk', 'member_of_a_labor_union', 'reason_for_unemployment', 'family_members_under_18', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt' ], inplace=True) # Impute missing values in categorical columns with a low percentage of missing data using the mode (most frequent value) df['country_of_birth_father'].fillna(df['country_of_birth_father'].mode()[0], inplace=True) df['country_of_birth_mother'].fillna(df['country_of_birth_mother'].mode()[0], inplace=True) df['country_of_birth_self'].fillna(df['country_of_birth_self'].mode()[0], inplace=True) df['hispanic_origin'].fillna(df['hispanic_origin'].mode()[0], inplace=True) # For columns with moderate missing data (5%-30%), impute categorical columns with the mode, especially when missing values are likely to indicate a common category. df['class_of_worker'].fillna(df['class_of_worker'].mode()[0], inplace=True) df['major_occupation_code'].fillna(df['major_occupation_code'].mode()[0], inplace=True) # Remove duplicates values in the rows df.drop_duplicates(inplace=True) df= df.reset_index(drop=True) # Reset the index # Check the first few rows of potential numeric columns to see if they contain numeric data stored as strings potential_numeric_columns = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'weeks_worked_in_year'] # Convert columns to numeric if they are indeed numeric, coercing errors to NaN for column in potential_numeric_columns: df[column] = pd.to_numeric(df[column], errors='coerce') # List of numeric and categorical features based on the dataset numeric_features = ['id', 'age', 'wage_per_hour', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] categorical_features = ['education', 'marital_stat', 'race', 'sex', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'detailed_industry_recode', 'detailed_occupation_recode', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'year', 'tax_filer_stat', 'veterans_benefits'] # Convert the categorical features from int64 to string/object if they are not already df[categorical_features] = df[categorical_features].astype(str) # Define numeric, nominal, and ordinal features numeric_features = ['id', 'age', 'wage_per_hour', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] nominal_features = ['marital_stat', 'race', 'sex', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'detailed_industry_recode', 'detailed_occupation_recode', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'year', 'tax_filer_stat', 'veterans_benefits'] ordinal_features = ['education'] # Define preprocessing for numeric features numeric_transformer = Pipeline(steps=[ ('scaler', StandardScaler()) ]) # Define preprocessing for nominal features nominal_transformer = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Define the order of education levels categories = [ [ 'Children', 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', ] ] # Define the ordinal transformer with specified categories ordinal_transformer = Pipeline(steps=[ ('ordinal', OrdinalEncoder(categories=categories)) ]) # Combine preprocessing pipelines with ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('nom', nominal_transformer, nominal_features), ('ord', ordinal_transformer, ordinal_features) ] ) # Define Features and Target # Define X as input features", "source": "datapipeline.py"}, {"content": "and y as the outcome variable X = df.drop(columns='income_group') y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Preprocess the training data X_train_preprocessed = preprocessor.fit_transform(X_train) X_test_preprocessed = preprocessor.transform(X_test) # Assign the preprocessed data back to X_train and X_test X_train = X_train_preprocessed.toarray() X_test = X_test_preprocessed.toarray() y_train = np.array(y_train) y_test = np.array(y_test) # Print the types of the training and testing sets print(\"Type of X_train:\", type(X_train)) print(\"Type of X_test:\", type(X_test)) print(\"Type of y_train:\", type(y_train)) print(\"Type of y_test:\", type(y_test)) return X_train, X_test, y_train, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class DecisionTree: def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'): \"\"\" Initializes the DecisionTree classifier. Parameters: max_depth (int, optional): The maximum depth of the tree. If None, nodes are expanded until all leaves are pure. min_samples_split (int, optional): The minimum number of samples required to split an internal node. criterion (str, optional): The function to measure the quality of a split ('gini' or 'entropy'). \"\"\" self.max_depth = max_depth self.min_samples_split = min_samples_split self.criterion = criterion self.tree = None # Placeholder for the tree structure def gini(self, y): \"\"\"Calculate Gini Impurity.\"\"\" m = len(y) return 1.0 - sum((np.sum(y == c) / m) ** 2 for c in np.unique(y)) def entropy(self, y): \"\"\"Calculate Entropy.\"\"\" m = len(y) return -sum((np.sum(y == c) / m) * np.log2(np.sum(y == c) / m + 1e-9) for c in np.unique(y)) def split(self, X, y, feature_index, threshold): \"\"\"Split the dataset based on a feature and a threshold.\"\"\" left_idx = X.iloc[:, feature_index] <= threshold right_idx = X.iloc[:, feature_index] > threshold return X[left_idx], X[right_idx], y[left_idx], y[right_idx] def best_split(self, X, y): \"\"\"Find the best split for the dataset.\"\"\" best_feature, best_threshold = None, None best_criterion_value = float('inf') for feature_index in range(X.shape[1]): # Use .iloc to access the column by index thresholds = np.unique(X.iloc[:, feature_index]) # Change this line for threshold in thresholds: X_left, X_right, y_left, y_right = self.split(X, y, feature_index, threshold) if len(y_left) > 0 and len(y_right) > 0: if self.criterion == 'gini': criterion_value = self.gini(y_left) * (len(y_left) / len(y)) + self.gini(y_right) * (len(y_right) / len(y)) elif self.criterion == 'entropy': criterion_value = self.entropy(y_left) * (len(y_left) / len(y)) + self.entropy(y_right) * (len(y_right) / len(y)) if criterion_value < best_criterion_value: best_criterion_value = criterion_value best_feature = feature_index best_threshold = threshold return best_feature, best_threshold def build_tree(self, X, y, depth=0): \"\"\"Recursively build the decision tree.\"\"\" num_samples, num_features = X.shape num_labels = len(np.unique(y)) if depth >= self.max_depth or num_samples < self.min_samples_split or num_labels == 1: leaf_value = self.most_common_label(y) return {\"leaf\": leaf_value} feature_index, threshold = self.best_split(X, y) if feature_index is None: leaf_value = self.most_common_label(y) return {\"leaf\": leaf_value} X_left, X_right, y_left, y_right = self.split(X, y, feature_index, threshold) left_subtree = self.build_tree(X_left, y_left, depth + 1) right_subtree = self.build_tree(X_right, y_right, depth + 1) return {\"feature_index\": feature_index, \"threshold\": threshold, \"left\": left_subtree, \"right\": right_subtree} def fit(self, X, y): \"\"\"Fit the decision tree model to the training data.\"\"\" self.tree = self.build_tree(X, y) def predict(self, X): \"\"\"Predict class labels for the provided data.\"\"\" return np.array([self._predict(inputs, self.tree) for inputs in X]) def _predict(self, inputs, tree): \"\"\"Predict class label for a single input.\"\"\" if \"leaf\" in tree: return tree[\"leaf\"] feature_index = tree[\"feature_index\"] threshold = tree[\"threshold\"] if inputs[feature_index] <= threshold: return self._predict(inputs, tree[\"left\"]) else: return self._predict(inputs, tree[\"right\"]) def most_common_label(self, y): \"\"\"Return the most common label in the target array.\"\"\" return np.bincount(y).argmax()", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # Initialize your model here self.model = None def train(self, params, X_train, y_train): \"\"\" Trains the model using the provided training data and parameters. :param params: Dictionary of model parameters to use for training. :param X_train: Training features as a NumPy array or DataFrame. :param y_train: Training labels as a NumPy array or Series. :return: f1_score (float) on the training data. \"\"\" # Initialize the RandomForestClassifier with the provided parameters self.model = RandomForestClassifier(**params) # Fit the model to the training data self.model.fit(X_train, y_train) # Predict on the training data y_train_pred = self.model.predict(X_train) # Calculate and return the F1 score for training data train_f1 = f1_score(y_train, y_train_pred, average=\"weighted\") return train_f1 def evaluate(self, X_test, y_test): \"\"\" Evaluates the trained model using the provided test data. :param X_test: Test features as a NumPy array or DataFrame. :param y_test: Test labels as a NumPy array or Series. :return: f1_score (float) on the test data. \"\"\" if self.model is None: raise ValueError( \"Model has not been trained yet. Please call the train method first.\" ) # Predict on the test data y_test_pred = self.model.predict(X_test) # Calculate and return the F1 score for test data test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\") return test_f1 # def get_default_params(self): # \"\"\" # Returns the default parameters for the RandomForestClassifier. # :return: Dictionary of default parameters. # \"\"\" # # Default parameters for RandomForestClassifier # default_params = { # 'n_estimators': 100, # 'max_depth': None, # 'min_samples_split': 2, # 'min_samples_leaf': 1, # 'random_state': 42 # } # return default_params def get_default_params(self): \"\"\" Returns the optimal parameters for the RandomForestClassifier. :return: Dictionary of optimal parameters. \"\"\" # Default parameters for RandomForestClassifier default_params = { \"n_estimators\": 200, \"max_depth\": None, \"min_samples_split\": 10, \"min_samples_leaf\": 1, \"random_state\": 42, } return default_params", "source": "model.py"}, {"content": "import requests import zipfile import os import pandas as pd from sklearn.preprocessing import LabelEncoder def download_and_extract_zip(url, extract_to='.'): \"\"\" Downloads a zip file from the given URL and extracts it to the specified directory. Parameters: - url (str): The URL of the zip file to download. - extract_to (str): The directory where the zip file will be extracted. \"\"\" # Step 1: Download the zip file local_filename = url.split('/')[-1] with requests.get(url, stream=True) as r: r.raise_for_status() with open(local_filename, 'wb') as f: for chunk in r.iter_content(chunk_size=8192): f.write(chunk) # Step 2: Extract the zip file with zipfile.ZipFile(local_filename, 'r') as zip_ref: zip_ref.extractall(extract_to) # Step 3: remove the downloaded zip file os.remove(local_filename) def load_census_data(train_filepath, test_filepath): # Define column names based on the dataset description column_names = [ 'age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income' ] # Load the training data train_data = pd.read_csv(train_filepath, header=None, names=column_names, na_values=' ?') # Load the test data, skip the first row because it contains extra metadata test_data = pd.read_csv(test_filepath, skiprows=1, header=None, names=column_names, na_values=' ?') # Drop rows with missing values in both datasets train_data.dropna(inplace=True) test_data.dropna(inplace=True) # Clean the target labels by stripping leading/trailing spaces and removing periods train_data['income'] = train_data['income'].str.strip() test_data['income'] = test_data['income'].str.strip().str.replace('.', '', regex=False) # Encode categorical variables in both datasets for col in train_data.columns: if train_data[col].dtype == 'object': le = LabelEncoder() train_data[col] = le.fit_transform(train_data[col]) test_data[col] = le.transform(test_data[col]) # Apply the same encoding to the test set # Separate features (X) and target (y) for training and testing data X_train = train_data.drop('income', axis=1) y_train = train_data['income'] X_test = test_data.drop('income', axis=1) y_test = test_data['income'] return X_train, X_test, y_train, y_test", "source": "utils.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "# class Datapipeline(): # def transform_train_data(self, train_data_path): # \"\"\" # Description of the function. # :param train_data_path: ...... # :return: ...... # \"\"\" # return X_train, y_train # def transform_test_data(self, test_data_path): # \"\"\" # Description of the function. # :param test_data_path: ...... # :return: ...... # \"\"\" # return X_test, y_test import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler class Datapipeline: def transform_train_data(self, train_data_path): \"\"\" Loads and processes training data from the provided path. :param train_data_path: Path to the CSV file containing training data :return: Scaled training features and labels \"\"\" # Step 1: Load the dataset df = pd.read_csv(train_data_path) # Step 2: Check for missing values (optional, could be removed in production) print(\"Checking for missing values:\") print(df.isnull().sum()) # Check for missing values in the DataFrame # Step 3: Remove duplicate rows # Check and print the number of duplicate rows before removal duplicate_count = df.duplicated().sum() print(f\"Number of duplicate transactions before removal: {duplicate_count}\") df_clean = df.drop_duplicates() # Check and print the number of duplicate rows after removal duplicate_count_after = df_clean.duplicated().sum() print( f\"Number of duplicate transactions after removal: {duplicate_count_after}\" ) # Optional: Check the shape of the DataFrame before and after to confirm the number of rows removed print(f\"Original shape: {df.shape}\") print(f\"New shape after removing duplicates: {df_clean.shape}\") # Step 4: Separate features (X) and target variable (y) X = df_clean.drop( columns=[\"Class\"] ) # Drop the target 'Class' column from features y = df_clean[\"Class\"].astype(int) # Convert 'Class' column to integers (0 or 1) # Step 5: Split the dataset into training and validation sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42, stratify=y ) # Step 6: Feature Scaling scaler = StandardScaler() X_train_scaled = scaler.fit_transform( X_train ) # Fit the scaler on training data and transform it X_test_scaled = scaler.transform( X_test ) # Use the same scaler for validation data (do not fit again) return X_train_scaled, y_train, X_test_scaled, y_test def transform_test_data(self, test_data_path): \"\"\" Loads and processes test data from the provided path. :param test_data_path: Path to the CSV file containing test data :return: Scaled test features and labels \"\"\" # Implement this similarly if you need a separate method for test data pass", "source": "datapipeline.py"}, {"content": "# class MLPTwoLayers: # # DO NOT adjust the constructor params # def __init__(self, input_size=3072, hidden_size=100, output_size=10): # raise NotImplementedError # def forward(self, features): # \"\"\" # Takes in the features # returns the prediction # \"\"\" # raise NotImplementedError # def loss(self, predictions, label): # \"\"\" # Takes in the predictions and label # returns the training loss # \"\"\" # raise NotImplementedError # def backward(self): # \"\"\" # Adjusts the internal weights/biases # \"\"\" # raise NotImplementedError import matplotlib.pyplot as plt from sklearn.metrics import ( classification_report, precision_score, recall_score, f1_score, ) from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Input, Dense from tensorflow.keras.optimizers import Adam class MLPClassifier: def __init__( self, input_size, hidden_sizes=[64, 128, 64, 32], output_size=1, learning_rate=0.001, ): \"\"\" Initializes a multi-layer perceptron (MLP) using Keras Sequential API. :param input_size: Number of input features :param hidden_sizes: List containing the number of units for hidden layers :param output_size: Number of output units (1 for binary classification) :param learning_rate: Learning rate for optimizer \"\"\" self.input_size = input_size self.hidden_sizes = hidden_sizes self.output_size = output_size self.learning_rate = learning_rate # Build the model self.model = Sequential() self.model.add(Input(shape=(self.input_size,))) # Hidden layers for units in self.hidden_sizes: self.model.add(Dense(units=units, activation=\"relu\")) # Output layer self.model.add(Dense(units=self.output_size, activation=\"sigmoid\")) # Compile the model self.model.compile( optimizer=Adam(learning_rate=self.learning_rate), loss=\"binary_crossentropy\", metrics=[\"accuracy\"], ) def summary(self): \"\"\"Prints the model architecture.\"\"\" self.model.summary() def train(self, X_train, y_train, X_val, y_val, epochs, batch_size): \"\"\"Train the model.\"\"\" history = self.model.fit( X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), ) return history def plot_training_history(self, history): \"\"\"Visualize the model's training and validation loss and accuracy across epochs.\"\"\" history_dict = history.history fig, (p1, p2) = plt.subplots( 1, 2, figsize=(15, 5) ) # Create two side-by-side plots # Get the number of epochs EPOCHS = len(history_dict[\"loss\"]) x = range(1, EPOCHS + 1) # Epochs range # Plot the loss values p1.plot(x, history_dict[\"loss\"], \"r\", label=\"Training Loss\") p1.plot(x, history_dict[\"val_loss\"], \"b\", label=\"Validation Loss\") p1.set_title(\"Training and Validation Loss\") p1.set_xlabel(\"Epochs\") p1.set_ylabel(\"Loss\") p1.legend() # Plot the accuracy values p2.plot(x, history_dict[\"accuracy\"], \"r\", label=\"Training Accuracy\") p2.plot(x, history_dict[\"val_accuracy\"], \"b\", label=\"Validation Accuracy\") p2.set_title(\"Training and Validation Accuracy\") p2.set_xlabel(\"Epochs\") p2.set_ylabel(\"Accuracy\") p2.legend() # Display the plots plt.show() def evaluate(self, X_test, y_test): \"\"\"Evaluate the model on the test set.\"\"\" return self.model.evaluate(X_test, y_test) def predict(self, X): \"\"\"Make predictions using the trained model.\"\"\" return self.model.predict(X) def evaluate_metrics(self, X_test, y_test): \"\"\"Generate evaluation metrics including classification report.\"\"\" y_pred = self.predict(X_test) # Get predictions # Convert probabilities to binary labels (0 or 1) y_pred_classes = (y_pred > 0.5).astype(\"int32\") # Classification report which includes precision, recall, F1-score print(\"Classification Report:\") print(classification_report(y_test, y_pred_classes)) # Additional individual metrics if needed precision = precision_score(y_test, y_pred_classes) recall = recall_score(y_test, y_pred_classes) f1 = f1_score(y_test, y_pred_classes) print(f\"Precision: {precision}\") print(f\"Recall: {recall}\") print(f\"F1-Score: {f1}\") import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset import numpy as np import matplotlib.pyplot as plt import time # Import time module for measuring epoch runtime # Define the neural network class in PyTorch class MLPClassifierPyTorch(nn.Module): def __init__(self, input_size, hidden_sizes=[64, 128, 64, 32], output_size=1): super(MLPClassifierPyTorch, self).__init__() self.hidden_layers = nn.Sequential( nn.Linear(input_size, hidden_sizes[0]), # Input layer nn.BatchNorm1d(hidden_sizes[0]), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_sizes[0], hidden_sizes[1]), # First hidden layer nn.BatchNorm1d(hidden_sizes[1]), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_sizes[1], hidden_sizes[2]), # Second hidden layer nn.BatchNorm1d(hidden_sizes[2]), nn.ReLU(), nn.Dropout(0.3), nn.Linear(hidden_sizes[2], hidden_sizes[3]), # Third hidden", "source": "mlp.py"}, {"content": "layer nn.BatchNorm1d(hidden_sizes[3]), nn.ReLU(), nn.Dropout(0.3), ) # Output layer self.output_layer = nn.Linear(hidden_sizes[3], output_size) self.sigmoid = nn.Sigmoid() # Sigmoid for binary classification def forward(self, x): x = self.hidden_layers(x) x = self.output_layer(x) return self.sigmoid(x) def train_model( self, X_train, y_train, X_val, y_val, batch_size, epochs, learning_rate=0.001, device=\"cpu\", # Add device parameter (for CPU or GPU) ): # Convert the training and validation data to PyTorch tensors X_train = torch.tensor(X_train, dtype=torch.float32).to(device) y_train = torch.tensor(y_train.values, dtype=torch.float32).to( device ) # Convert y_train to NumPy array first X_val = torch.tensor(X_val, dtype=torch.float32).to(device) y_val = torch.tensor(y_val.values, dtype=torch.float32).to( device ) # Convert y_val to NumPy array first train_dataset = TensorDataset(X_train, y_train) val_dataset = TensorDataset(X_val, y_val) # Create DataLoader for batching train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_loader = DataLoader(val_dataset, batch_size=batch_size) # Define the loss function and optimizer criterion = nn.BCELoss() # Binary cross-entropy for binary classification optimizer = optim.Adam(self.parameters(), lr=learning_rate) # Move model to device (GPU or CPU) self.to(device) # Training loop train_loss_history = [] val_loss_history = [] train_acc_history = [] val_acc_history = [] for epoch in range(epochs): start_time = time.time() # Start time for epoch self.train() # Set model to training mode running_loss = 0.0 correct_train = 0 total_train = 0 # Training step for inputs, labels in train_loader: optimizer.zero_grad() # Zero the gradients outputs = self(inputs).squeeze() loss = criterion(outputs, labels) loss.backward() optimizer.step() running_loss += loss.item() # Calculate accuracy during training preds = (outputs > 0.5).float() correct_train += (preds == labels).sum().item() total_train += labels.size(0) train_accuracy = correct_train / total_train # Validation step self.eval() with torch.no_grad(): val_loss = 0.0 correct_val = 0 total_val = 0 for inputs, labels in val_loader: outputs = self(inputs).squeeze() loss = criterion(outputs, labels) val_loss += loss.item() preds = (outputs > 0.5).float() correct_val += (preds == labels).sum().item() total_val += labels.size(0) val_accuracy = correct_val / total_val avg_train_loss = running_loss / len(train_loader) avg_val_loss = val_loss / len(val_loader) train_loss_history.append(avg_train_loss) val_loss_history.append(avg_val_loss) train_acc_history.append(train_accuracy) val_acc_history.append(val_accuracy) end_time = time.time() # End time for epoch epoch_runtime = end_time - start_time # Calculate runtime for this epoch # Print the progress with runtime included print( f\"Epoch [{epoch+1}/{epochs}] - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}, \" f\"Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}, \" f\"Time: {epoch_runtime:.2f} seconds\" ) return train_loss_history, val_loss_history, train_acc_history, val_acc_history def evaluate(self, X_test, y_test): \"\"\"Evaluate the model on the test set.\"\"\" # Convert y_test to a NumPy array before converting to a tensor X_test = torch.tensor(X_test, dtype=torch.float32) y_test = torch.tensor( y_test.values, dtype=torch.float32 ) # Convert to NumPy array first # Create a test dataset and dataloader test_dataset = TensorDataset(X_test, y_test) test_loader = DataLoader(test_dataset, batch_size=128) criterion = nn.BCELoss() self.eval() # Set model to evaluation mode test_loss = 0.0 correct = 0 total = 0 with torch.no_grad(): # Disable gradient computation for evaluation for inputs, labels in test_loader: outputs = self(inputs).squeeze() loss = criterion(outputs, labels) test_loss += loss.item() preds = (outputs > 0.5).float() correct += (preds == labels).sum().item() total += labels.size(0) accuracy = correct / total avg_test_loss = test_loss / len(test_loader) print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.4f}\") return avg_test_loss, accuracy def predict(self, X): \"\"\"Make predictions using the trained model.\"\"\" X_tensor = torch.tensor(X, dtype=torch.float32) self.eval() with torch.no_grad(): outputs = self(X_tensor) return (outputs.squeeze() > 0.5).float().numpy() def plot_training_history(self, train_loss, val_loss, train_acc, val_acc): \"\"\"Plot the training", "source": "mlp.py"}, {"content": "and validation loss and accuracy.\"\"\" epochs_range = range(1, len(train_loss) + 1) fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6)) # Plot Loss ax1.plot(epochs_range, train_loss, label=\"Training Loss\") ax1.plot(epochs_range, val_loss, label=\"Validation Loss\") ax1.set_title(\"Training and Validation Loss\") ax1.set_xlabel(\"Epochs\") ax1.set_ylabel(\"Loss\") ax1.legend() # Set custom y-axis limits for Loss (adjust the numbers as per your needs) ax1.set_ylim(0, 0.1) # Example: Loss range from 0 to 0.1 # Plot Accuracy ax2.plot(epochs_range, train_acc, label=\"Training Accuracy\") ax2.plot(epochs_range, val_acc, label=\"Validation Accuracy\") ax2.set_title(\"Training and Validation Accuracy\") ax2.set_xlabel(\"Epochs\") ax2.set_ylabel(\"Accuracy\") ax2.legend() # Set custom y-axis limits for Accuracy (adjust the numbers as per your needs) ax2.set_ylim(0.9990, 1.0000) # Example: Accuracy range from 0.9990 to 1.0 plt.show() def evaluate_metrics(self, X_test, y_test): \"\"\"Generate evaluation metrics including classification report.\"\"\" y_pred = self.predict(X_test) # Get predictions # Convert probabilities to binary labels (0 or 1) y_pred_classes = (y_pred > 0.5).astype(\"int32\") # Classification report which includes precision, recall, F1-score print(\"Classification Report:\") print(classification_report(y_test, y_pred_classes)) # Additional individual metrics if needed precision = precision_score(y_test, y_pred_classes) recall = recall_score(y_test, y_pred_classes) f1 = f1_score(y_test, y_pred_classes) print(f\"Precision: {precision}\") print(f\"Recall: {recall}\") print(f\"F1-Score: {f1}\")", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import sys import matplotlib.pyplot as plt from sklearn.metrics import ( classification_report, precision_score, recall_score, f1_score, ) import torch # Ensure PyTorch is imported from torchinfo import summary # Import torchinfo for summary from datapipeline import Datapipeline from mlp import MLPClassifierPyTorch # Ensure this import is correct # Step 1: Use the Datapipeline class to preprocess the data data_pipeline = Datapipeline() # Assuming 'train_data_path' is the path to your training dataset train_data_path = ( \"/pvc-data/workspaces/soh-sze-han/all-assignments/assignment4/data/raw/data.csv\" ) # Preprocess the training data using the pipeline X_train_scaled, y_train, X_test_scaled, y_test = data_pipeline.transform_train_data( train_data_path ) # Step 2: Initialize the MLPClassifier (PyTorch) # Get the input size from the preprocessed X_train_scaled input_size = X_train_scaled.shape[1] batch_size = 128 # Define the batch size here # Instantiate the MLPClassifier with the correct input size mlp_classifier = MLPClassifierPyTorch(input_size=input_size) # Step 3: Print the summary of the model architecture summary( mlp_classifier, input_size=(batch_size, input_size) ) # Display model architecture # Step 4: Train the model using the preprocessed data # Determine if CUDA is available for GPU usage, else use CPU device = \"cuda\" if torch.cuda.is_available() else \"cpu\" train_loss_history, val_loss_history, train_acc_history, val_acc_history = ( mlp_classifier.train_model( X_train_scaled, y_train, X_test_scaled, y_test, batch_size=128, epochs=30, device=device, ) ) # Step 5: Evaluate the model on the test set test_loss, test_accuracy = mlp_classifier.evaluate(X_test_scaled, y_test) print( f\"Test set evaluation metrics: Loss={test_loss:.4f}, Accuracy={test_accuracy:.4f}\" ) # Step 6: Plot training and validation loss/accuracy mlp_classifier.plot_training_history( train_loss_history, val_loss_history, train_acc_history, val_acc_history ) # Step 7: Generate the classification report mlp_classifier.evaluate_metrics(X_test_scaled, y_test)", "source": "train_model.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import os import numpy as np import tensorflow as tf from tensorflow.keras.applications import MobileNetV3Large from tensorflow.keras.applications.mobilenet_v3 import preprocess_input from tensorflow.keras.layers import ( Dense, GlobalAveragePooling2D, BatchNormalization, Dropout, ) from tensorflow.keras.models import Model from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau from sklearn.utils import class_weight import matplotlib.pyplot as plt from PIL import Image, ImageFile from collections import defaultdict import warnings # Suppress PIL warnings for sRGB profiles and truncated images ImageFile.LOAD_TRUNCATED_IMAGES = True warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"PIL\") from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay # Define directory paths base_dir = \"/pvc-data/workspaces/soh-sze-han/data/img-classification/tensorfood\" processed_dir = \"/pvc-data/workspaces/soh-sze-han/data/img-classification/processed\" models_dir = \"/pvc-data/workspaces/soh-sze-han/models\" plots_dir = os.path.join(models_dir, \"plots\") # Create the directories if they do not exist os.makedirs(models_dir, exist_ok=True) os.makedirs(plots_dir, exist_ok=True) os.makedirs(processed_dir, exist_ok=True) # Custom function to preprocess images and save them to processed_dir def preprocess_and_save_image(image_path, save_dir): try: # Load the image using PIL pil_image = Image.open(image_path) # Convert to RGB if necessary if pil_image.mode != \"RGB\": pil_image = pil_image.convert(\"RGB\") # # Resize image to (224, 224) pil_image = pil_image.resize((224, 224)) # Save the processed image as JPEG filename = os.path.splitext(os.path.basename(image_path))[0] + \".jpg\" save_path = os.path.join(save_dir, filename) pil_image.save(save_path, \"JPEG\") except Exception as e: print(f\"Error processing image {image_path}: {e}\") # Preprocess and save all images from base_dir to processed_dir def preprocess_images_in_directory(source_dir, target_dir): for root, _, files in os.walk(source_dir): relative_path = os.path.relpath(root, source_dir) target_subdir = os.path.join(target_dir, relative_path) os.makedirs(target_subdir, exist_ok=True) for file in files: if file.startswith(\".\"): continue image_path = os.path.join(root, file) preprocess_and_save_image(image_path, target_subdir) # Function to count images by format and mode def count_images_by_format_and_mode(processed_dir): format_mode_count = defaultdict(int) for subdir, _, files in os.walk(processed_dir): for filename in files: image_path = os.path.join(subdir, filename) try: with Image.open(image_path) as img: image_format = img.format image_mode = img.mode format_mode_count[(image_format, image_mode)] += 1 except Exception as e: print(f\"Could not open {filename}: {e}\") print(f\"{'Count':<10} {'Format':<10} {'Mode':<10}\") print(\"-\" * 30) for (image_format, image_mode), count in format_mode_count.items(): print(f\"{count:<10} {image_format:<10} {image_mode:<10}\") # Preprocess and save the images preprocess_images_in_directory(base_dir, processed_dir) count_images_by_format_and_mode(processed_dir) # Load the dataset using the processed_dir train_dataset = tf.keras.preprocessing.image_dataset_from_directory( processed_dir, validation_split=0.2, subset=\"training\", seed=123, image_size=(224, 224), batch_size=32, label_mode=\"categorical\", ) validation_dataset = tf.keras.preprocessing.image_dataset_from_directory( processed_dir, validation_split=0.2, subset=\"validation\", seed=123, image_size=(224, 224), batch_size=32, label_mode=\"categorical\", ) # Extract class names before applying the map function class_names = train_dataset.class_names # Preprocess images for MobileNetV3 def preprocess_function(image, label): image = preprocess_input(image) return image, label # Apply preprocessing function using map train_dataset = train_dataset.map(preprocess_function) validation_dataset = validation_dataset.map(preprocess_function) # Apply data augmentation only to the training dataset def random_flip(image, label): image = tf.image.random_flip_left_right(image) return image, label def random_rotation(image, label): image = tf.image.rot90( image, k=tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32) ) return image, label def random_zoom(image, label): scales = [0.8, 1.0] # Scale down to 80% (zoom in) and up to 100% (no zoom) scale = tf.random.uniform([], minval=scales[0], maxval=scales[1], dtype=tf.float32) # Apply central crop for zoom effect image = tf.image.central_crop(image, scale) image = tf.image.resize(image, [224, 224]) # Resize back to the target size return image, label def augment_data(image, label): image, label = random_flip(image, label) image, label = random_rotation(image, label) image, label = random_zoom(image, label) return image, label # Apply augmentations using map to the training dataset train_dataset = train_dataset.map(augment_data) # Shuffle and optimize data pipeline train_dataset = ( train_dataset.shuffle(buffer_size=1000, seed=123) .cache() .prefetch(buffer_size=tf.data.AUTOTUNE) ) validation_dataset = validation_dataset.cache().prefetch(buffer_size=tf.data.AUTOTUNE) # Calculate class weights train_labels =", "source": "train.py"}, {"content": "np.concatenate([y.numpy() for x, y in train_dataset], axis=0) train_labels = np.argmax(train_labels, axis=1) class_weights = dict( enumerate( class_weight.compute_class_weight( \"balanced\", classes=np.unique(train_labels), y=train_labels ) ) ) # Load the pre-trained MobileNetV3 model base_model = MobileNetV3Large( input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\" ) # Build the model x = GlobalAveragePooling2D()(base_model.output) x = BatchNormalization()(x) x = Dense(512, activation=\"relu\")(x) x = BatchNormalization()(x) x = Dropout(0.5)(x) x = Dense(256, activation=\"relu\")(x) # New Dense layer with 256 units # x = BatchNormalization()(x) # Add batch normalization for the new layer # x = Dense(128, activation=\"relu\")(x) # New Dense layer with 128 units predictions = Dense(len(class_names), activation=\"softmax\")(x) model = Model(inputs=base_model.input, outputs=predictions) # Freeze the base model layers for layer in base_model.layers: layer.trainable = False # Compile the model with the specified learning rate model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()], ) # Display model summary model.summary() # Callbacks checkpoint_cb = ModelCheckpoint( filepath=os.path.join(models_dir, \"tensorfood.h5\"), save_best_only=True, monitor=\"val_accuracy\", mode=\"max\", verbose=1, ) early_stopping_cb = EarlyStopping( monitor=\"val_loss\", patience=5, verbose=1, restore_best_weights=True ) lr_scheduler = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=3, verbose=1) # Fit the model history = model.fit( train_dataset, validation_data=validation_dataset, epochs=15, callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler], class_weight=class_weights, ) # Fine-tuning for layer in base_model.layers[-50:]: layer.trainable = True model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=1e-6), loss=\"categorical_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()], ) history_fine = model.fit( train_dataset, validation_data=validation_dataset, epochs=5, callbacks=[checkpoint_cb, early_stopping_cb, lr_scheduler], class_weight=class_weights, ) # Evaluation val_loss, val_acc, val_precision, val_recall = model.evaluate(validation_dataset) print(f\"Validation accuracy: {val_acc:.2f}\") print(f\"Validation precision: {val_precision:.2f}\") print(f\"Validation recall: {val_recall:.2f}\") # Combine the history from initial training and fine-tuning acc = history.history[\"accuracy\"] + history_fine.history[\"accuracy\"] val_acc = history.history[\"val_accuracy\"] + history_fine.history[\"val_accuracy\"] loss = history.history[\"loss\"] + history_fine.history[\"loss\"] val_loss = history.history[\"val_loss\"] + history_fine.history[\"val_loss\"] # history from initial training acc = history.history[\"accuracy\"] val_acc = history.history[\"val_accuracy\"] loss = history.history[\"loss\"] val_loss = history.history[\"val_loss\"] epochs_range = range(len(acc)) # Total number of epochs including both phases # Plot training and validation accuracy plt.figure(figsize=(12, 8)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label=\"Training Accuracy\") plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\") plt.legend(loc=\"lower right\") plt.title(\"Training and Validation Accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Accuracy\") # Plot training and validation loss plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label=\"Training Loss\") plt.plot(epochs_range, val_loss, label=\"Validation Loss\") plt.legend(loc=\"upper right\") plt.title(\"Training and Validation Loss\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") # Save and show the training and validation plot training_validation_plot_path = os.path.join(plots_dir, \"training_validation_plot.png\") plt.savefig(training_validation_plot_path) plt.show() plt.close() # Clear the plot # Get predictions on the validation dataset y_pred = [] y_true = [] for images, labels in validation_dataset: preds = model.predict(images) y_pred.extend(np.argmax(preds, axis=1)) y_true.extend(np.argmax(labels.numpy(), axis=1)) # Generate the confusion matrix cm = confusion_matrix(y_true, y_pred) cmd = ConfusionMatrixDisplay(cm, display_labels=class_names) # Plot the confusion matrix plt.figure(figsize=(12, 10)) cmd.plot( cmap=\"Blues\", values_format=\"d\", ax=plt.gca(), colorbar=True ) # Add colorbar here # Title and labels plt.title(\"Confusion Matrix\", fontsize=16) plt.xlabel(\"Predicted Class\", fontsize=14) plt.ylabel(\"Actual Class\", fontsize=14) # Rotate the x-axis labels (Predicted Class) to be vertical plt.xticks(rotation=90) # Save the confusion matrix plot confusion_matrix_plot_path = os.path.join(plots_dir, \"confusion_matrix.png\") plt.savefig(confusion_matrix_plot_path, bbox_inches=\"tight\") plt.show() plt.close() # Clear the plot # Visualize all misclassified images misclassified_indices = [i for i in range(len(y_true)) if y_true[i] != y_pred[i]] num_images_to_show = len( misclassified_indices ) # Set to total number of misclassified images # Calculate the grid size for subplots num_cols = 5 num_rows = (num_images_to_show // num_cols) + int(num_images_to_show % num_cols != 0) # Create a figure to display images plt.figure( figsize=(num_cols * 4, num_rows * 4) ) # Adjust", "source": "train.py"}, {"content": "the figure size for better clarity for i, index in enumerate(misclassified_indices): try: # Get the specific image and label image_label = list(validation_dataset.unbatch().skip(index).take(1)) if len(image_label) == 1: image, true_label = image_label[0] else: print(f\"Could not extract image and label at index {index}\") continue # Convert the true label from one-hot encoded to a scalar index true_label = np.argmax(true_label) # Convert image to numpy format for display image = image.numpy().astype(\"uint8\") predicted_label = y_pred[index] plt.subplot(num_rows, num_cols, i + 1) plt.imshow(image) # Set the title with increased font size and wrap text for better readability plt.title( f\"True: {class_names[true_label]}\\nPred: {class_names[predicted_label]}\", fontsize=12, # Increase the font size wrap=True, # Wrap text if it's too long ) plt.axis(\"off\") except Exception as e: print(f\"Error processing image at index {index}: {e}\") # Adjust layout to prevent overlap plt.tight_layout() # Save the misclassified images plot misclassified_images_plot_path = os.path.join(plots_dir, \"all_misclassified_images.png\") plt.savefig(misclassified_images_plot_path, bbox_inches=\"tight\") plt.show() plt.close() # Clear the plot", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "\"\"\" This module contains the DataPipeline class which processes data for time series forecasting. \"\"\" import pandas as pd class DataPipeline: \"\"\" DataPipeline processes data for time series forecasting. \"\"\" def __init__(self): pass def preprocess_data(self, df): \"\"\" Preprocess the data by handling missing values and selecting numeric columns. \"\"\" # Handle missing values with forward fill and backward fill df = df.ffill().bfill() # Select only numeric columns for further processing df = df.select_dtypes(include=[\"float64\", \"int64\"]) return df def run_data_pipeline(self, csv_path, for_neural_network=False): \"\"\" Run the data pipeline to preprocess and engineer features for time series forecasting. Parameters: csv_path (str): The path to the CSV file containing the data. for_neural_network (bool): Flag to indicate if the data is for a neural network model. Returns: pd.DataFrame: The processed DataFrame. \"\"\" # 1. Load the data from a CSV file df = pd.read_csv(csv_path) # 2. Convert relevant columns to integer to prevent issues in datetime conversion for col in [\"year\", \"month\", \"day\", \"hour\"]: if col in df.columns: df[col] = df[col].astype(int) else: raise KeyError(f\"Missing required column: {col}\") # 3. Create a datetime column from year, month, day, hour df[\"datetime\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]]) # 4. Set the datetime column as the index df = df.set_index(\"datetime\") df = df.sort_index() # Sort the data by the datetime index # 5. Remove redundant columns df = df.drop(columns=[\"year\", \"month\", \"day\", \"hour\"]) # 6. Handle missing values with forward fill and backward fill df = df.ffill().bfill() # 7. Select only numeric columns for further processing df = df.select_dtypes(include=[\"float64\", \"int64\"]) if not for_neural_network: # Feature Engineering: Create Lagged Features for Time Series Forecasting n_lags = 6 for lag in range(1, n_lags + 1): df[f\"pm2.5_lag_{lag}\"] = df[\"pm2.5\"].shift(lag) df[f\"TEMP_lag_{lag}\"] = df[\"TEMP\"].shift(lag) df[f\"DEWP_lag_{lag}\"] = df[\"DEWP\"].shift(lag) # Shift target variable to represent one hour ahead forecast df[\"pm2.5_target\"] = df[\"pm2.5\"].shift(-1) # Drop rows with NaN values (introduced due to lagging and target shifting) df.dropna(inplace=True) return df", "source": "datapipeline.py"}, {"content": "\"\"\" This module contains the implementation of the run_experiment function which performs forecasting experiments using different lag values and forecast horizons. \"\"\" import matplotlib.pyplot as plt from sklearn.metrics import mean_squared_error from datapipeline import DataPipeline from ml_model import ForecastModel def run_experiment(dataset_path, lags=None, horizons=None): \"\"\" Run the forecasting experiment with given data, forecast horizons, and lags. Parameters: dataset_path (str): Path to the dataset. lags (list, optional): List of lag values. Defaults to [6]. horizons (list, optional): List of forecast horizons. Defaults to [1, 2, 3, 6, 12, 24]. Returns: dict: Dictionary of trained models for each horizon. dict: Dictionary of metrics for each horizon. \"\"\" if lags is None: lags = [6] if horizons is None: horizons = [1, 2, 3, 6, 12, 24] # 1. Initialize data pipeline and run it pipeline = DataPipeline() cleaned_data = pipeline.run_data_pipeline(dataset_path) # Dictionary to store errors for different horizons metrics_dict = {} models_dict = {} def process_lag_horizon(lag, horizon): data = create_lagged_features(cleaned_data, lag) data_horizon = create_target_column(data, horizon) train_data, test_data = split_data(data_horizon) features, target = define_features_and_target(data_horizon, horizon) model, train_mse, test_mse = train_and_evaluate_model( train_data, test_data, features, target ) # Store errors and model in metrics_dict and models_dict metrics_dict[f\"lag_{lag}_horizon_{horizon}\"] = { \"train_error\": train_mse, \"test_error\": test_mse, } models_dict[f\"lag_{lag}_horizon_{horizon}\"] = model print(f\"Forecast Horizon: {horizon} hour(s) with Lag: {lag}\") print(f\"Training MSE: {train_mse}\") print(f\"Testing MSE: {test_mse}\\n\") for lag in lags: for horizon in horizons: process_lag_horizon(lag, horizon) visualize_errors(lags, horizons, metrics_dict) return models_dict, metrics_dict def create_lagged_features(data, lag): \"\"\" Create lagged features for the given data. Parameters: data (pd.DataFrame): The input data. lag (int): The number of lag periods. Returns: pd.DataFrame: DataFrame with lagged features. \"\"\" data = data.copy() for i in range(1, lag + 1): data[f\"pm2.5_lag_{i}\"] = data[\"pm2.5\"].shift(i) data[f\"TEMP_lag_{i}\"] = data[\"TEMP\"].shift(i) data[f\"DEWP_lag_{i}\"] = data[\"DEWP\"].shift(i) data.dropna(inplace=True) return data def create_target_column(data, horizon): \"\"\" Create the target column for the given forecast horizon. Parameters: data (pd.DataFrame): The input data. horizon (int): The forecast horizon. Returns: pd.DataFrame: DataFrame with the target column. \"\"\" data_horizon = data.copy() data_horizon[f\"pm2.5_target_h{horizon}\"] = data_horizon[\"pm2.5\"].shift(-horizon) data_horizon.dropna(inplace=True) return data_horizon def split_data(data_horizon): \"\"\" Split the data into training and testing sets. Parameters: data_horizon (pd.DataFrame): The input data with target column. Returns: tuple: A tuple containing the training data and testing data. \"\"\" train_data = data_horizon.iloc[:-24] # Use all but the last 24 hours for training test_data = data_horizon.iloc[-24:] # Use the last 24 hours for testing return train_data, test_data def define_features_and_target(data_horizon, horizon): \"\"\" Define the features and target for the model. Parameters: data_horizon (pd.DataFrame): The input data with lagged features and target column. horizon (int): The forecast horizon. Returns: tuple: A tuple containing the list of feature columns and the target column name. \"\"\" features = [col for col in data_horizon.columns if \"lag\" in col] target = f\"pm2.5_target_h{horizon}\" return features, target def train_and_evaluate_model(train_data, test_data, features, target): \"\"\" Train the model and evaluate it on the training and testing data. Parameters: train_data (pd.DataFrame): The training data. test_data (pd.DataFrame): The testing data. features (list): List of feature column names. target (str): The target column name. Returns: ForecastModel: The trained model. float: The mean squared error on the training data. float: The mean squared error on the testing data. \"\"\" x_train =", "source": "ml_experiment.py"}, {"content": "train_data[features] y_train = train_data[target] x_test = test_data[features] y_test = test_data[target] model = ForecastModel() model.fit(x_train, y_train) y_train_pred = model.predict(x_train) y_test_pred = model.predict(x_test) train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) return model, train_mse, test_mse def visualize_errors(lags, horizons, metrics_dict): \"\"\" Visualize the training and testing errors for different lags and forecast horizons. Parameters: lags (list): List of lag values. horizons (list): List of forecast horizons. metrics_dict (dict): Dictionary containing training and testing errors for each lag and horizon. \"\"\" for lag in lags: train_errors = [ metrics_dict[f\"lag_{lag}_horizon_{h}\"][\"train_error\"] for h in horizons ] test_errors = [ metrics_dict[f\"lag_{lag}_horizon_{h}\"][\"test_error\"] for h in horizons ] plt.figure(figsize=(10, 6)) plt.plot( horizons, train_errors, marker=\"o\", label=f\"Training MSE (Lag {lag})\", color=\"blue\", ) plt.plot( horizons, test_errors, marker=\"o\", label=f\"Testing MSE (Lag {lag})\", color=\"red\", ) plt.xlabel(\"Forecast Horizon (Hours)\") plt.ylabel(\"Mean Squared Error (MSE)\") plt.title(f\"Prediction Error vs. Forecast Horizon (Lag {lag})\") plt.legend() plt.grid(True) plt.show() if __name__ == \"__main__\": # Define the data path DATA_PATH = \"beijing_pm2_5_data.csv\" # Run the experiment with the specified lags and horizons run_experiment(DATA_PATH, lags=[6], horizons=[1, 2, 3, 6, 12, 24])", "source": "ml_experiment.py"}, {"content": "\"\"\" This module contains the ForecastModel class which uses a RandomForestRegressor for forecasting tasks. It includes methods for fitting the model, evaluating its performance, and making predictions. \"\"\" # class ForecastModel: # def __init__(self): # self.model = ml_model_of_your_choice # def fit(self, X, y): # self.model.fit(X, y) # def evaluate(model, X_train, y_train, X_test, y_test): # y_train_pred = model.predict(X_train) # train_error = metric_of_your_choice # y_test_pred = model.predict(X_test) # test_error = metric_of_your_choice # return train_error, test_error # def predict(self, X): # return self.model.predict(X) from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error class ForecastModel: \"\"\" A forecasting model using a RandomForestRegressor. \"\"\" def __init__(self): \"\"\" Initialize the ForecastModel with a RandomForestRegressor. \"\"\" self.model = RandomForestRegressor(n_estimators=100, random_state=42) def fit(self, x, y): \"\"\" Fit the model to the training data. Parameters: x (array-like): Training data. y (array-like): Target values. \"\"\" self.model.fit(x, y) def evaluate(self, x_train, y_train, x_test, y_test): \"\"\" Evaluate the model on training and testing data. Parameters: x_train (array-like): Training data. y_train (array-like): Training target values. x_test (array-like): Testing data. y_test (array-like): Testing target values. Returns: tuple: Training and testing mean squared errors. \"\"\" # Predictions for training and testing sets y_train_pred = self.model.predict(x_train) y_test_pred = self.model.predict(x_test) # Calculate evaluation metrics train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) train_mae = mean_absolute_error(y_train, y_train_pred) test_mae = mean_absolute_error(y_test, y_test_pred) # Print or return all metrics print(f\"Training MSE: {train_mse}\") print(f\"Testing MSE: {test_mse}\") print(f\"Training MAE: {train_mae}\") print(f\"Testing MAE: {test_mae}\") # Return only the MSEs for simplicity, you can adjust as needed return train_mse, test_mse def predict(self, x): \"\"\" Predict using the trained model. Parameters: x (array-like): Data to predict. Returns: array: Predicted values. \"\"\" return self.model.predict(x)", "source": "ml_model.py"}, {"content": "\"\"\" This module defines an RNN model for regression tasks using PyTorch. \"\"\" import torch from torch import nn import torch.optim as optim import numpy as np class RNNModel(nn.Module): \"\"\" A Recurrent Neural Network (RNN) model for regression tasks. Args: in_size (int): The number of input features. hid_size (int): The number of features in the hidden state. n_layers (int): The number of recurrent layers. out_size (int): The number of output features. \"\"\" def __init__(self, in_size, hid_size, n_layers, out_size, rnn_type=\"rnn\"): super(RNNModel, self).__init__() # Choose between vanilla RNN, LSTM, or GRU if rnn_type == \"rnn\": self.rnn = nn.RNN(in_size, hid_size, n_layers, batch_first=True) elif rnn_type == \"lstm\": self.rnn = nn.LSTM(in_size, hid_size, n_layers, batch_first=True) elif rnn_type == \"gru\": self.rnn = nn.GRU(in_size, hid_size, n_layers, batch_first=True) else: raise ValueError(\"rnn_type must be 'rnn', 'lstm', or 'gru'\") self.fc = nn.Linear(hid_size, out_size) def forward(self, x): \"\"\" Defines the forward pass of the RNN model. Args: x (torch.Tensor): The input tensor of shape (batch_size, sequence_length, input_size). Returns: torch.Tensor: The output tensor of shape (batch_size, output_size). \"\"\" rnn_out, _ = self.rnn(x) rnn_out = rnn_out[ :, -1, : ] # Get the last output for each sequence in the batch output = self.fc(rnn_out) # Output shape: (batch_size, out_size) return output def fit(self, train_loader, val_loader, epochs=20, lr=0.001, device=None): \"\"\" Trains the RNN model. Args: train_loader (DataLoader): DataLoader for the training data. val_loader (DataLoader): DataLoader for the validation data. epochs (int, optional): Number of epochs to train. Defaults to 20. lr (float, optional): Learning rate. Defaults to 0.001. device (torch.device, optional): The device to train the model on. \"\"\" if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.to(device) optimizer = optim.Adam(self.parameters(), lr=lr) scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5) criterion = nn.MSELoss() train_losses = [] val_losses = [] for epoch in range(epochs): train_loss = self._train_one_epoch( train_loader, optimizer, criterion, device ) val_loss = self._validate_one_epoch(val_loader, criterion, device) train_losses.append(train_loss) val_losses.append(val_loss) scheduler.step() # Adjust learning rate print( f\"Epoch [{epoch+1}/{epochs}], \" f\"Train Loss: {train_losses[-1]:.4f}, \" f\"Val Loss: {val_losses[-1]:.4f}\" ) return train_losses, val_losses def _train_one_epoch(self, train_loader, optimizer, criterion, device): self.train() train_loss = 0.0 for features, labels in train_loader: features, labels = features.to(device), labels.to(device) optimizer.zero_grad() outputs = self(features) loss = criterion(outputs, labels) loss.backward() optimizer.step() train_loss += loss.item() return train_loss / len(train_loader) def _validate_one_epoch(self, val_loader, criterion, device): self.eval() val_loss = 0.0 with torch.no_grad(): for features, labels in val_loader: features, labels = features.to(device), labels.to(device) outputs = self(features) loss = criterion(outputs, labels) val_loss += loss.item() return val_loss / len(val_loader) def evaluate(self, dataloader, device=None): \"\"\" Evaluates the model on the given dataloader. Args: dataloader (DataLoader): DataLoader for the evaluation data. device (torch.device, optional): The device to run the evaluation on. Returns: tuple: Average MSE and MAE losses. \"\"\" if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.to(device) self.eval() criterion_mse = nn.MSELoss() criterion_mae = nn.L1Loss() total_mse_loss = 0.0 total_mae_loss = 0.0 with torch.no_grad(): for features, labels in dataloader: features, labels = features.to(device), labels.to(device) outputs = self(features) mse_loss = criterion_mse(outputs, labels) mae_loss = criterion_mae(outputs, labels) total_mse_loss += mse_loss.item() total_mae_loss += mae_loss.item() avg_mse = total_mse_loss / len(dataloader) avg_mae = total_mae_loss / len(dataloader) return avg_mse, avg_mae def predict(self, dataloader, device=None): \"\"\" Generates predictions for the given dataloader. Args:", "source": "rnn_model.py"}, {"content": "dataloader (DataLoader): DataLoader for the data to predict. device (torch.device, optional): The device to run the prediction on. Returns: np.ndarray: Array of predictions. \"\"\" if device is None: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") self.to(device) self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(device) outputs = self(features) predictions.append(outputs.cpu().numpy()) return np.vstack(predictions)", "source": "rnn_model.py"}, {"content": "\"\"\" This module contains the WindowGenerator class, which is a custom PyTorch Dataset for generating sliding windows of data for time series forecasting. \"\"\" import numpy as np import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): \"\"\" A custom PyTorch Dataset for generating sliding windows of data for time series forecasting. Attributes: data (np.ndarray): The input data. lookback (int): The number of past time steps to include in each window. lookahead (int): The number of future time steps to predict. \"\"\" def __init__(self, data, lookback, lookahead): self.data = ( data if isinstance(data, np.ndarray) else data.values ) # Store the data as a NumPy array; convert from DataFrame if necessary self.lookback = lookback # Number of past time steps to use as input features self.lookahead = lookahead # Number of future time steps to predict self.length = ( len(self.data) - lookback - lookahead + 1 ) # Calculate the number of sliding windows that can be generated def __len__(self): return self.length # Return the total number of windows available def __getitem__(self, idx): start = idx # Starting index for the current window end = ( idx + self.lookback ) # Ending index for the input features of the current window features = self.data[ start:end ] # Extract the input features (lookback period) from the data labels = self.data[ end : end + self.lookahead, 0 ] # Extract the target values (lookahead period) from the first column of the data features = torch.tensor( features, dtype=torch.float32 ) # Convert the features to a PyTorch tensor with float32 data type labels = torch.tensor( labels, dtype=torch.float32 ) # Convert the labels to a PyTorch tensor with float32 data type return features, labels # Return the features and labels as tensors", "source": "windowing.py"}, {"content": "\"\"\" This module provides the ContextVector class for handling context vectors. \"\"\" import numpy as np class ContextVector: \"\"\" A class used to represent a Context Vector. \"\"\" def __init__(self): \"\"\" Initializes the ContextVector with a default vector. \"\"\" self.vector = self.compute_context_vector() def get_vector(self): \"\"\" Returns the context vector. \"\"\" return self.vector def set_vector(self, new_vector): \"\"\" Sets the context vector to a new value. Parameters: new_vector (list): A new context vector to be set. \"\"\" self.vector = new_vector def compute_context_vector(self): \"\"\" Computes the context vector based on the given encoder hidden states and decoder hidden state. Returns: list: A list representing the context vector rounded to 2 decimal places. \"\"\" # Given encoder hidden states (S0, S1, S2) and decoder hidden state (T1) s0 = np.array([0.3, 0.11, 0.9, 0.5]) s1 = np.array([0.8, 0.3, 0.7, 0.1]) s2 = np.array([0.5, 0.3, 0.4, 0.8]) t1 = np.array([0.2, 0.7, 0.9, 0.3]) # Compute alignment scores using dot product e0 = np.dot(s0, t1) e1 = np.dot(s1, t1) e2 = np.dot(s2, t1) # Calculate attention weights using softmax with stability improvement max_e = np.max([e0, e1, e2]) exp_e = np.exp([e0, e1, e2] - max_e) attention_weights = exp_e / np.sum(exp_e) # Compute the context vector as the weighted sum of encoder hidden states context_vector = ( attention_weights[0] * s0 + attention_weights[1] * s1 + attention_weights[2] * s2 ) # Round the context vector to 2 decimal places context_vector_rounded = np.round(context_vector, 2) return [context_vector_rounded.tolist()] # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [[0.53, 0.23, 0.68, 0.45]] if __name__ == \"__main__\": # Create an instance of ContextVector context_vector_instance = ContextVector() # Get the computed context vector result = context_vector_instance.get_vector() # Print the result print(\"Computed Context Vector:\", result)", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder from sklearn.pipeline import Pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.compose import ColumnTransformer def transform(data_path): \"\"\" Transforms and splits the 94/95 US Census into training and test sets :param data_path: The path to the .csv file which the data is in :return: 4 numpy arrays: X_train, X_test, y_train, y_test \"\"\" def tf_income(df:pd.DataFrame): df[\"income_group\"] = df[\"income_group\"].replace({'- 50000.':0, '50000+.':1}).astype(int) return df def keep_cols(df:pd.DataFrame): ''' Defines the columns to keep :param df: The dataframe of the Census dataset :return: The dataframe with the kept features ''' to_keep = [ \"income_group\", \"age\", \"education\", \"marital_stat\", \"race\", \"sex\", \"country_of_birth_self\", \"citizenship\", \"class_of_worker\", \"major_industry_code\", \"major_occupation_code\", \"full_or_part_time_employment_stat\", \"detailed_household_summary_in_household\", \"num_persons_worked_for_employer\", \"own_business_or_self_employed\", \"weeks_worked_in_year\", \"tax_filer_stat\"] new_df = df[to_keep] return new_df def proc_nom(df:pd.DataFrame): ''' Maps values of the nominal features :param df: The dataframe of the Census dataset :return: The dataframe with the transformed features ''' new_df = df changed_feat = [\"marital_stat\", \"country_of_birth_self\", \"full_or_part_time_employment_stat\"] # Binned to Married & Single new_df[\"is_married\"] = new_df[\"marital_stat\"].replace({ 'Never married': 0, 'Separated': 0, 'Divorced': 0, 'Widowed': 0, 'Married-civilian spouse present': 1, 'Married-spouse absent': 1, 'Married-A F spouse present': 1, }).astype(int) new_df[\"birth_region\"] = new_df[\"country_of_birth_self\"].replace({ 'United-States': 0, 'Canada': 0, 'Outlying-U S (Guam USVI etc)': 0, 'Mexico': 1, 'Cuba': 1, 'Puerto-Rico': 1, 'El-Salvador': 1, 'Dominican-Republic': 1, 'Columbia': 1, 'Jamaica': 1, 'Guatemala': 1, 'Ecuador': 1, 'Peru': 1, 'Haiti': 1, 'Nicaragua': 1, 'Honduras': 1, 'Trinadad&Tobago': 1, 'Panama': 1, 'Taiwan': 2, 'Philippines': 2, 'India': 2, 'China': 2, 'Japan': 2, 'South Korea': 2, 'Vietnam': 2, 'Thailand': 2, 'Hong Kong': 2, 'Laos': 2, 'Cambodia': 2, 'Holand-Netherlands': 2, 'Iran': 2, 'Germany': 3, 'England': 3, 'Italy': 3, 'Poland': 3, 'Portugal': 3, 'Ireland': 3, 'Greece': 3, 'France': 3, 'Scotland': 3, 'Hungary': 3, 'Yugoslavia': 3, '?': 4 }).astype(int) new_df[\"employment\"] = new_df[\"full_or_part_time_employment_stat\"].replace({ 'Children or Armed Forces': 0, 'Not in labor force': 1, 'Unemployed full-time': 1, 'Full-time schedules': 2, 'PT for non-econ reasons usually FT': 3, 'PT for econ reasons usually PT': 3, 'Unemployed part- time': 3, 'PT for econ reasons usually FT': 3 }).astype(int) return new_df.drop(changed_feat, axis=1) def proc_ord(df:pd.DataFrame): ''' Maps values of the ordinal features (education) :param df: The dataframe of the Census dataset :return: The dataframe with the transformed features ''' new_df = df changed_feat = [\"education\"] new_df[\"edu_bin\"] = new_df[\"education\"].replace( {'Children':0, 'Less than 1st grade': 1, '1st 2nd 3rd or 4th grade': 1, '5th or 6th grade': 1, '7th and 8th grade': 1, '9th grade': 1, '10th grade': 1, '11th grade': 1, '12th grade no diploma': 1, 'High school graduate': 2, 'Associates degree-occup /vocational': 3, 'Associates degree-academic program': 3, 'Some college but no degree': 3, 'Bachelors degree(BA AB BS)': 4, 'Masters degree(MA MS MEng MEd MSW MBA)': 5, 'Prof school degree (MD DDS DVM LLB JD)': 5, 'Doctorate degree(PhD EdD)': 5 }).astype(int) return new_df.drop(changed_feat, axis=1) # Load in the data df = pd.read_csv(data_path) df = tf_income(df) df = keep_cols(df) df = proc_ord(df) df = proc_nom(df) # Define X as input features and y as the outcome variable X = df.drop(\"income_group\", axis=1) y = df[\"income_group\"] num_feats = [\"age\", \"weeks_worked_in_year\"] ord_feats = [\"edu_bin\", \"num_persons_worked_for_employer\"] nom_feats = [\"is_married\", \"race\", \"sex\", \"birth_region\", \"citizenship\", \"class_of_worker\", \"major_industry_code\", \"major_occupation_code\", \"employment\", \"detailed_household_summary_in_household\",", "source": "datapipeline.py"}, {"content": "\"own_business_or_self_employed\", \"tax_filer_stat\"] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) num_tf = Pipeline(steps=[ ('std', StandardScaler()) ]) ord_tf = Pipeline(steps=[ ('ordinal', OrdinalEncoder(categories=[ord_feats])) ]) nom_tf = Pipeline(steps=[ ('ohe', OneHotEncoder(sparse=False, drop='first', handle_unknown='ignore')) ]) preprocessor = ColumnTransformer( transformers=[ ('num', num_tf, num_feats), ('nom', nom_tf, nom_feats) ], remainder='passthrough' ) X_train = preprocessor.fit_transform(X_train, y_train) X_test = preprocessor.transform(X_test) return X_train, X_test, y_train, y_test, preprocessor", "source": "datapipeline.py"}, {"content": "import numpy as np import pandas as pd class DecisionTree: \"\"\" A custom model that works as a DecisionTree classifier. When trained, it will have a list of Nodes to traverse through. Nodes \"\"\" class Node: \"\"\" A represenation of a split in a decision tree. Written to when training the dataset Read from when predicting \"\"\" def __init__(self, depth=0): self.is_leaf = False \"\"\"Quick way to find if this is a leaf\"\"\" self.proba = 0 \"\"\"The probability of this leaf being True\"\"\" self.value = False \"\"\"The majority of values on this leaf\"\"\" self.feature_id = 0 \"\"\"The feature this node corresponds to\"\"\" self.threshold = 0 \"\"\"The threshold to split this node\"\"\" self.impurity = 0 \"\"\"The impurity of this node - will be 0 if this is a leaf\"\"\" self.left_id = 0 \"\"\" The id of the left node; will be 0 if this is a leaf since you can't connect to the root \"\"\" self.right_id = 0 \"\"\" The id of the right node; will be 0 if this is a leaf since you can't connect to the root \"\"\" self.depth = depth \"\"\"The level this node is on\"\"\" def set_node(self, **kwargs): \"\"\" Set the attributes of this node \"\"\" for key, value in kwargs.items(): setattr(self, key, value) def split_data(self, X:np.ndarray, indices:np.array): \"\"\" Takes in a dataframe of features as well as the indices we will use Splits it into two lists based off the threshold and feature_id :param df: the dataframe :param indices: the indices to split at :returns: 2 lists of indices, one for the left node and one for the right node \"\"\" under_thresh = (X[indices, self.feature_id] <= self.threshold) return indices[under_thresh], indices[~under_thresh] def __init__(self): self.nodes_ = [] \"\"\"A list of Nodes\"\"\" #Stopping Conditions self.max_depth = 10 \"\"\"Stop splitting if the tree is too deep\"\"\" self.min_samples_per_node = 1 \"\"\"Stop splitting if there are too few samples\"\"\" self.min_impurity_change_on_split = 0.0001 \"\"\" If the max impurity change of the current split is too low, stop \"\"\" pass def gini(self, left:list, right:list): \"\"\" Calculates the gini impurity of the current split, given the label values of its left and right lists. :param left: A list of 0s or 1s :param right: A list of 0s or 1s :returns: A float representing the split's gini impurity. \"\"\" def calc_impurity(node:list): \"\"\" Gets the impurity of the current node Assumes the list only has zeros or ones (label) :param node: A list of 0s or 1s :returns: The impurity of the current list \"\"\" one = sum(node)/len(node) zero = (len(node) - sum(node))/len(node) return 1 - zero * zero - one * one return (calc_impurity(left) + calc_impurity(right))/2 def fit(self, X:np.ndarray, y:np.ndarray): \"\"\" What does a fit method do? Given a dataset of features and a dataset of labels Generate a list of nodes :param X: An ndarray of feature values :param y: An ndarray of labels :return: \"\"\" # Instantiate the indices indices = np.array(range(0, len(y))) # Commence splitting root_idx = self.build_tree(X, y, indices, 0) pass def get_threshold_list(self, X:np.ndarray, indices:np.array, feat_ind:int)-> list: \"\"\" :param X: An ndarray of feature values :param indices: An array of indices to analyse", "source": "decision_tree.py"}, {"content": ":returns: A list of values to serve as potential thresholds \"\"\" feat_values = np.array(X[indices, feat_ind]) thresholds = np.unique(feat_values) num_thresholds = len(thresholds) if num_thresholds <= 100: return thresholds if num_thresholds <= 1000: num_thresholds = 100 quantiles = np.linspace(0,1, num_thresholds) values = [] for i in quantiles[:1]: values.append(thresholds.iloc[int(i*len(thresholds))]) values.append(thresholds.iloc[-1]) return values def find_best_split(self, X:np.ndarray, y:np.array, indices): \"\"\" Find the best point to split :returns: a tuple of (feat_idx, threshold, impurity) \"\"\" best_impurity = float('inf') \"\"\"We choose a lower impurity to split at\"\"\" best_feat_id = 0 best_thresh_val = 0 # Generate a list of (feature_idx, list_threshold) # list_threshold are the potential thresholds per feature feature_ids = np.arange(X.shape[1]) # iterate through the feature list for feat_id in feature_ids: thresh_list = self.get_threshold_list(X, indices, feat_id) #get the feature values feat_vals = X[indices, feat_id] # for each (feature, threshold_list), # iterate through each threshold for thresh in thresh_list: # get the indices that correspond to each potential node l_ind = feat_vals <= thresh r_ind = ~l_ind # get the label list at the indices l_y = y.iloc[indices][l_ind] r_y = y.iloc[indices][r_ind] if len(l_y) == 0 or len(r_y) == 0: #invalid split! continue # get the impurity impurity = self.gini(l_y,r_y) if impurity < best_impurity: #print(f\"Replacing {best_feat_id}, {best_thresh_val} with {feat_id}, {thresh}: {best_impurity} > {impurity}\") best_impurity = impurity best_feat_id = feat_id best_thresh_val = thresh return best_feat_id, best_thresh_val, best_impurity def build_tree(self, X:np.ndarray, y:np.array, indices:np.array, depth=0) -> int: \"\"\" The main recursive function \"\"\" node = self.Node(depth) self.nodes_.append(node) node_idx = len(self.nodes_) - 1 #Stopping Condition 1: too few samples #Stopping Condition 2: max depth achieved if len(indices) < self.min_samples_per_node or depth >= self.max_depth: if len(indices) < self.min_samples_per_node: print(f\"Stopping {node_idx} at {len(indices)} samples\") else: print(f\"Stopping {node_idx} at {depth} depth\") node.is_leaf = True node.proba = np.mean(y) node.value = node.proba >= 0.5 return node_idx # Scour through all and find the best split best_feat_id, best_thresh_val, impurity = self.find_best_split(X, y, indices) #Stopping Condition 3: min impurity has been hit if impurity < self.min_impurity_change_on_split: # print(y.iloc[indices]) print(f\"Stopping {node_idx} at {impurity} impurity - {y[indices][y == True]}\") node.is_leaf = True node.proba = np.mean(y) node.value = node.proba >= 0.5 return node_idx node.feature_id = best_feat_id node.threshold = best_thresh_val node.impurity = impurity l_ind_list, r_ind_list = node.split_data(X, indices) node.left_id = self.build_tree(X, y, l_ind_list, depth + 1) node.right_id = self.build_tree(X, y, r_ind_list, depth + 1) return node_idx def predict(self, X:np.ndarray, y:np.ndarray): pass def __str__(self): if len(self.nodes_) == 0: return f\"Uninitialized!\" out = \"\" for idx, node in enumerate(self.nodes_): out = out + f\"---\\nNode {idx} - {'(leaf)' if node.is_leaf else ''}\\n Feature: {node.feature_id}\\n Depth: {node.depth}\\n Left: {node.left_id}\\n Right: {node.right_id}\\n Impurity: {node.impurity}\\n\" return out", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.rf_clf = RandomForestClassifier(criterion='gini', random_state=42) pass def train(self, params, X_train, y_train): \"\"\" Trains a Random Forest Classifier on the given parameters and X and y dataset values. Returns the F1-score to show the weighted mean of the precision and recall for evaluation. :param params: The parameters to train the random forest classifier on :param X_train: The feature values of the training set :param y_train: The corresponding labels of the training set :return: The f1 score of the RF rlassifier on the \"\"\" # For our case, this function should train the initialised model and return the train f1 score self.rf_clf.set_params(**params) self.rf_clf.fit(X_train, y_train) y_pred = self.rf_clf.predict(X_train) # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it return f1_score(y_train, y_pred) def evaluate(self, X_test, y_test): \"\"\" Evaluates the f1-score of the trained random forest classifier after being fitted using train(). :param X_test: The feature values of the test set :param y_test: The label values of the test set :return: The f1 score of the RF classifier on the test set \"\"\" y_pred = self.rf_clf.predict(X_test) # This function should use the trained model to predict the target for the test data and return the test f1 score return f1_score(y_test, y_pred) def get_default_params(self): \"\"\" Returns the default best parameters to be used :return: The best parameters discovered by me! \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return {\"n_estimators\": 500, \"max_depth\": 75, \"min_samples_split\": 10}", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from torch.utils.data import DataLoader, TensorDataset, random_split from torch import tensor, float32, Generator import pandas as pd def get_dataset(X,y): \"\"\"Generates a dataset from 2 array-likes of features and labels Args: X (array-like): features for this dataset. Must have same number of rows as y. y (array-like): targets of this dataset. Must have same number of rows as x. \"\"\" X_tens = tensor(X) y_tens = tensor(pd.array(y)) return TensorDataset(X_tens.to(float32), y_tens.to(float32)) def get_dataloader_from_dataset(ds, batch_size): \"\"\"Generates a Dataloader from a dataset Args: ds (torch.utils.data.Dataset): the dataset to be fed into a loader batch_size (int): batch size when training/testing with this loader. Returns: Dataloader: A dataloader with the features and targets \"\"\" return DataLoader(ds, batch_size) def get_dataloader(X,y, batch_size): \"\"\"Generates a Dataloader from features and labels, combining get_dataset and get_dataloader_from_dataset Args: X (array-like): features for this dataset. Must have same number of rows as y. y (array-like): targets of this dataset. Must have same number of rows as x. batch_size (int): batch size when training/testing with this loader. Returns: Dataloader: A dataloader with the features and targets \"\"\" ds = get_dataset(X, y) return get_dataloader_from_dataset(ds, batch_size) def split_dataset(ds, size_list, random_state=-1): \"\"\"Splits the dataset according to the indicated sizes Args: ds (torch.utils.data.Dataset): a dataset to be split size_list (array-like of Numbers): the proportions to split the dataset into. random_state (int, optional): an int to fix the random state. Defaults to None. Returns: List(Subset[T]): the original dataset, split into a list \"\"\" if random_state != -1: gen = Generator().manual_seed(random_state) split = random_split(ds, size_list, gen) else: split = random_split(ds, size_list) return split", "source": "a4p2_dataset.py"}, {"content": "import matplotlib.pyplot as plt from sklearn.metrics import classification_report, ConfusionMatrixDisplay def live_update_chart(val, title='Loss chart', x_lbl='Epochs', y_lbl='Loss'): \"\"\"plots a chart of loss values over epochs, live Args: val (List of floats): values of the epoch x_name (string): the label of the x-axis \"\"\" plt.plot(val) plt.title(title) plt.xlabel(x_lbl) plt.ylabel(y_lbl) plt.show() def chart_t_v_loss(train, val, title='Loss chart', x_lbl='Epochs', y_lbl='Loss'): \"\"\"plots a chart of training and validation losses over epochs Args: train (List of floats): training vals of the epoch val (List of floats): validation vals of the epoch x_lbl (string): the label of the x-axis \"\"\" plt.plot(train, label = 'training loss') plt.plot(val, label = 'validation loss') plt.title(title) plt.legend() plt.xlabel(x_lbl) plt.ylabel(y_lbl) plt.show() def chart_loss(val, title='Loss chart', x_lbl='Epochs', y_lbl='Loss'): \"\"\"plots a chart of loss values over epochs Args: val (List of floats): values of the epoch x_name (string): the label of the x-axis \"\"\" plt.plot(val) plt.title(title) plt.xlabel(x_lbl) plt.ylabel(y_lbl) plt.show() def class_report(y_true, y_pred, labels=None): report = classification_report(y_true, y_pred) print(report) ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=[\"True\",\"False\"]) plt.show()", "source": "a4p2_log.py"}, {"content": "from torch import nn import torch from .a4p2_dataset import split_dataset,get_dataloader_from_dataset from .a4p2_log import class_report, chart_loss, chart_t_v_loss from datetime import datetime import numpy as np from sklearn.metrics import f1_score from collections import Counter device = 'cpu' class CustomNet(nn.Module): \"\"\"A custom sequential neural network made of dense layers.\"\"\" def __init__(self, input_dim:int, layer_sizes=None): \"\"\"Constructor for the CustomNet sequential neural network Args: input_dim (int): The number of input features. batch_size (int, optional): The batch size of the input. Defaults to 32. node_list (list(int), optional): A list of nodes per layer. Defaults to None. \"\"\" super(CustomNet, self).__init__() self.flatten = nn.Flatten() self.input_dim = input_dim if layer_sizes: self.form_stack(input_dim, layer_sizes) def form_stack(self, input_dim, layer_sizes): \"\"\"Forms a stack of linear layers according to a list of passed integers, appends a Sigmoid layer to the end, sets it to a Sequential object and saves it under linear_relu_stack Args: layer_sizes (list(int)): A list of integers representing the neurons per HIDDEN layer. \"\"\" layer_dict = [] layer_dict.append(nn.Linear(input_dim, layer_sizes[0])) layer_dict.append(nn.ReLU()) for i in range(len(layer_sizes)-1): input_size = layer_sizes[i] output_size = layer_sizes[i+1] layer_dict.append(nn.Linear(input_size, output_size)) if i < len(layer_sizes) - 2: layer_dict.append(nn.ReLU()) layer_dict.append(nn.Sigmoid()) self.linear_relu_stack = nn.Sequential(*layer_dict) def forward(self,x): \"\"\"Defines a forward pass through the layers Args: x (_type_): _description_ \"\"\" x = self.flatten(x) logits = self.linear_relu_stack(x) return logits class TrainConfig(): def __init__(self, optimizer, loss_fn, epochs=25, batch_size=32, class_weight=None, model_name=None): \"\"\"A class to hold all the config info. TODO: use hydra? Args: loss_fn (torch.nn.loss): The loss function to be used optimizer (torch.nn.optimizer): An optimiser for backpropagation training epochs (int, optional): Number of epochs. Defaults to 25. batch_size (int, optional): _description_. Defaults to 32. class_weight_mod (Any, optional): how much influence class weights have. Defaults to None. \"\"\" self.optimizer = optimizer self.loss_fn = loss_fn self.epochs = epochs self.batch_size = batch_size self.class_weight_mod = class_weight self.model_name = model_name def get_name(self): return self.model_name if self.model_name is not None else 'model' def train(dataset, model, cfg:TrainConfig): \"\"\"Trains the model based on dataset and other parameters over the specified number of epochs Args: dataset (Dataset): A pytorch dataset with features and labels model (Model): A neural network model to be trained config (TrainConfig): A config with all the necessary information Returns: model: Returns the trained model. \"\"\" print(f\"- Training {cfg.get_name()} -\") timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') best_vloss = 1_000_000 loss_list = [] vloss_list = [] vf1_list = [] for epoch in range(cfg.epochs): print(f' Epoch {epoch+1}:') split = split_dataset(dataset, [0.8, 0.2]) train_ds = split[0] val_ds = split[1] train_dl = get_dataloader_from_dataset(train_ds, cfg.batch_size) val_dl = get_dataloader_from_dataset(val_ds, cfg.batch_size) avg_loss = train_one_epoch(train_dl, model, cfg) loss_list.append(avg_loss) running_vloss = 0.0 max_batches = 0 model.eval() vcorrect = 0 v_pred_t = 0 v_pred_f = 0 v_true_t = 0 v_true_f = 0 vtotal = 0 with torch.no_grad(): all_vpred = [] all_vlabels = [] for i, vdata in enumerate(val_dl): vinputs, vlabels = vdata voutputs = model(vinputs) vpred = torch.round(voutputs.data) # print(voutputs.data, vpred) vtotal += vlabels.size(0) vcorrect += (vpred == vlabels).sum().item() v_pred_t += (vpred == 1).sum().item() v_pred_f += (vpred == 0).sum().item() v_true_t += (vlabels == 1).sum().item() v_true_f += (vlabels == 0).sum().item() all_vpred.extend(vpred.numpy()) all_vlabels.extend(vlabels.numpy()) vloss = cfg.loss_fn(voutputs, vlabels.unsqueeze(1)) running_vloss += vloss max_batches = i vacc = vcorrect/vtotal * 100 vf1 = f1_score(all_vlabels, all_vpred) vf1_list.append(vf1) avg_vloss = running_vloss / (max_batches+1) vloss_list.append(avg_vloss) print('ACC valid {}'.format(vacc))", "source": "a4p2_nn.py"}, {"content": "print('P_T {}, P_F {}'.format(v_pred_t, v_pred_f)) print('T_T {}, T_F {}'.format(v_true_t, v_true_f)) print('F1 valid {}'.format(vf1)) print('LOSS train {} valid {}\\n'.format(avg_loss, avg_vloss)) if avg_vloss < best_vloss: best_vloss = avg_vloss model_path = './models/{}_{}_{}'.format(cfg.get_name(), timestamp, epoch) torch.save(model.state_dict(), model_path) chart_loss(vf1_list, cfg.get_name() + \" f1 scores\") chart_t_v_loss(loss_list, vloss_list, cfg.get_name() + \" performance\") return model def train_one_epoch(dataloader, model, cfg:TrainConfig): \"\"\"Trains one epoch on the dataset Args: dataloader (torch.utils.data.DataLoader): a dataloader with the training set model (torch.nn.model): the model to train on the data cfg (TrainConfig): the config holding all the necessary info (loss fn, optimizer, batch size) Return: float: The final loss of this epoch \"\"\" size = len(dataloader.dataset) model.train() running_loss = 0. last_loss = 0. correct = 0 # get the positive class weight for this current dataloader pos_weight = get_pos_weight(dataloader) if cfg.class_weight_mod is not None: cfg.loss_fn.pos_weight = pos_weight * cfg.class_weight_mod else : cfg.loss_fn.pos_weight = None print(f\" Positive class weight: {cfg.loss_fn.pos_weight}\") for batch,(X,y) in enumerate(dataloader): X, y = X.to(device), y.to(device) # Passes X into forward # returns predictions for the batch X pred = model(X) #compute prediction error loss = cfg.loss_fn(pred,y.unsqueeze(1).to(torch.float32)) #zero the gradients for the next round cfg.optimizer.zero_grad() #Backpropagation loss.backward() #update weights cfg.optimizer.step() running_loss += loss.item() predicted = torch.round(pred.data) #print(pred.mean().item(), predicted.mean().item()) correct += (predicted == y).sum().item() # print(f\"batch {batch}: {pred.size()}\") if batch % 1000 == 999: loss, current = loss.item(), (batch + 1) * len(X) * 32 acc = correct/current * 100 last_loss = running_loss/1000 running_loss = 0 print(f\" loss: {loss:>7f} acc: {correct}/{current} {acc:.1f}% [{current:>5d}/{size:>5d}]\") return last_loss def test(dataloader, model, loss_fn): \"\"\"A function to test model against the test dataset in dataloader Args: dataloader (dataloader): a dataloader referencing the test dataset model (torch.nn): the model to be bested loss_fn (torch.nn.loss): the loss function to calculate this model's error rate \"\"\" size = len(dataloader.dataset) num_batches = len(dataloader) model.eval() test_loss, correct = 0, 0 y_pred = [] y_true = [] with torch.no_grad(): for X, y in dataloader: pred = model(X) y_pred.extend(pred.argmax(1).cpu().numpy()) y_true.extend(y.cpu().numpy()) test_loss += loss_fn(pred, y.unsqueeze(1)).item() correct += (pred.argmax(1) == y).type(torch.float).sum().item() test_loss /= num_batches correct /= size class_report(y_pred, y_true, [\"True\", \"False\"]) print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\") print(Counter(y_pred)) print(Counter(y_true)) def get_pos_weight(y, mod=1): \"\"\"Returns a class weight tensor for use in a loss function Args: y (dataloader): a dataloader with the target values Returns: torch.tensor: a tensor of class weights \"\"\" all_labels = [] for _, labels in y: all_labels.extend(labels.numpy()) all_labels = np.array(all_labels) num_pos = (all_labels == 1).sum() num_neg = (all_labels == 0).sum() pos_weight_val = num_neg/num_pos return torch.tensor([pos_weight_val], dtype=torch.float32)", "source": "a4p2_nn.py"}, {"content": "class Datapipeline(): def transform_train_data(self, train_data_path): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" return X_train, y_train def transform_test_data(self, test_data_path): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" return X_test, y_test", "source": "datapipeline.py"}, {"content": "import tensorflow as tf import keras from keras.models import Sequential from keras.layers import Dense from sklearn.model_selection import train_test_split import imblearn as imb from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler data_path = './resources/local_data.csv' print(tf.__version__) print(data_path)", "source": "ex3_5.py"}, {"content": "import imblearn as imb import pandas as pd from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler def feat_engin_pipeline(df:pd.DataFrame, norm_list:list, drop_list:list, label:str): \"\"\"Performs all of the feature transformations in one go Args: df (pandas.Dataframe): The raw data which is to be transformed norm_list (list(string)): a list of features to apply standard scaling to drop_list (list(string)): a lit of features to drop label (string): The label which we are interested in Returns: list(np.ndarray): a group of 4 ndarrays, X_train, X_test, y_train, y_test \"\"\" new_df = df.drop(drop_list, axis=1) X = new_df.drop(label, axis=1) y = new_df[label] print(X.shape, y.shape) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y) scale_col_tf = ColumnTransformer([ ('norm_cols', StandardScaler(), norm_list) ]) X_train_fit = scale_col_tf.fit_transform(X_train, y_train) X_test_fit = scale_col_tf.transform(X_test) return X_train_fit, X_test_fit, y_train, y_test def apply_smote(X, y): \"\"\"Applies BorderlineSMOTE Args: X (array-like): The feature values of the dataset y (array-like): The label values of the dataset Returns: Pair: X and y values resampled after applying SMOTE \"\"\" smote = imb.over_sampling.BorderlineSMOTE(random_state=42) X, y = smote.fit_resample(X, y) return X, y", "source": "feat_engin.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import pandas as pd import os import pyodbc import matplotlib.pyplot as plt import numpy as np import seaborn as sns import math import yaml def load_config(yml_path, verbose=False): \"\"\"Loads the config from a yaml file Args: yml_path (string): Path of the yaml file. Should contain driver, server, database, username and password fields. verbose (bool): Whether to print the info_dict Returns: Dict(string, string): A dict containing driver, server, database, username and password fields. \"\"\" with open(yml_path) as info: info_dict = yaml.safe_load(info) if verbose: print(info_dict) return info_dict def get_raw_data(table, csv_path, config_dict): if(os.path.exists(csv_path)): print(\"File already exists! Continuing...\") raw_df = pd.read_csv(csv_path) else: print(\"File not found! Downloading...\") raw_df = download_table(table, csv_path, config_dict) return raw_df def download_table(table, csv_path, config_dict): \"\"\"Gets the raw data from an ODBC server Args: csv_path (string): the path to save the csv config_dict (dict(string,string)): A dictionary containing driver, server, database, username and password fields. Returns: Dataframe: A dataframe of the table at the given database. \"\"\" driver = config_dict['driver'] server = config_dict['server'] database = config_dict['database'] username = config_dict['username'] password = config_dict['password'] conn = pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) database = pd.read_sql_query(f\"SELECT * FROM {table}\", conn) final = database os.makedirs(os.path.dirname(csv_path), exist_ok=True) final.to_csv(csv_path, header=True) print(\"Downloaded and written to \" + csv_path + \" successfully.\") return final", "source": "tools.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # Working https://docs.google.com/spreadsheets/d/1zjEXeZHLMVF8YVsAqVADdkufbDfQbQUEQz96mkpW_Yk/edit?usp=sharing # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [-618, 573, -126], [153, -257, -100] ] # Replace below with your response matrix_2 = [ [ -128 -562, -200, -6], [480, 80, -685, -1028], [-127, -618, 573, -126], [924, 265, 391, -100] ] # Replace below with your response matrix_3 = [ [ -128, -200], [ -127, 573] ]", "source": "convolved_matrices.py"}, {"content": "from torch.utils.data import Dataset import pandas as pd #from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from tqdm import tqdm class DatasetFromSubset(Dataset): def __init__(self, subset, transform=None): self.subset = subset self.transform = transform def __getitem__(self, index): x, y = self.subset[index] if self.transform: x = self.transform(x) return x, y def __len__(self): return len(self.subset) def dataloader_info(dl): labels_list = [] print(f\"Loading {dl}\") for batch in tqdm(dl): X, y= batch labels_list.extend(y.numpy()) df = pd.DataFrame(labels_list, columns=['class']) label_distribution = df['class'].value_counts().sort_index() print(label_distribution) print(dl.dataset.subset.dataset.classes) label_distribution.plot(kind='bar') plt.title('Label Distribution') plt.xlabel('Labels') plt.ylabel('Count') plt.show()", "source": "data.py"}, {"content": "import matplotlib.pyplot as plt def chart_loss(val, title='Loss chart', x_lbl='Epochs', y_lbl='Loss'): \"\"\"plots a chart of loss values over epochs Args: val (List of floats): values of the epoch x_name (string): the label of the x-axis \"\"\" f, ax = plt.subplots(1) plt.plot(val) plt.title(title) plt.xlabel(x_lbl) plt.ylabel(y_lbl) ax.set_xlim(xmin=0) ax.set_ylim(ymin=0) plt.show() def chart_t_v_loss(train, val, title='Loss chart', x_lbl='Epochs', y_lbl='Loss'): \"\"\"plots a chart of training and validation losses over epochs Args: train (List of floats): training vals of the epoch val (List of floats): validation vals of the epoch x_lbl (string): the label of the x-axis \"\"\" plt.plot(train, label = 'training loss') plt.plot(val, label = 'validation loss') plt.title(title) plt.legend() plt.xlabel(x_lbl) plt.ylabel(y_lbl) plt.show()", "source": "log.py"}, {"content": "from torch import nn def example_func(): return \"Hello\" class BinaryClassifier(nn.Module): def __init__(self): super(BinaryClassifier, self).__init__() # INPUT SIZE = 512x512 # 512 -> 6x CONV2D with kernel 5 -> 512-5+1 = 508 # 254 -> 16x CONV2D with kernel 5 -> 254-5+1 = 250 conv_layers = [nn.Conv2d(3, 6, 5), # output 508*508*6 = 1548384 nn.BatchNorm2d(6), # doesn't change params nn.ReLU(), nn.MaxPool2d(2, 2), # divides params by 2 = 254*254*6 nn.Conv2d(6, 16, 5), # output 250*250*16 = 1000000 nn.BatchNorm2d(16)] fc_list = [nn.Linear(250*250*16, 120), nn.ReLU(), nn.Linear(120, 84), nn.ReLU(), nn.Linear(84, 1)] self.conv_stack = nn.Sequential(*conv_layers) self.flatten = nn.Flatten() self.linear_stack = nn.Sequential(*fc_list) #self.sigmoid = nn.Sigmoid() #don't use sigmoid as we are using BCEWithLogitsLoss def forward(self, x): conv = self.conv_stack(x) flat = self.flatten(conv) result = self.linear_stack(flat) # result = self.sigmoid(logits) return result class FoodClassifier(nn.Module): def __init__(self, input_dim=512): super(FoodClassifier, self).__init__() fc_input_dim = (input_dim//2-6) * (input_dim//2-6) * 16 # INPUT SIZE = 224*224 # 224 -> 6x CONV2D with kernel 5 -> 224-5+1 = 220 # 110 -> 16x CONV2D with kernel 5 -> 110-5+1 = 106 # 512 -> 508 -> 254 -> 250 # 224 -> 220 -> 110 -> 106 conv_layers = [nn.Conv2d(3, 6, 5), # output 220*220*6 nn.BatchNorm2d(6), # doesn't change params nn.ReLU(), nn.MaxPool2d(2, 2), # divides params by 2 = 110*110*6 nn.Conv2d(6, 16, 5), # output 106*106*16 nn.BatchNorm2d(16)] fc_list = [nn.Linear(fc_input_dim, 120), nn.ReLU(), nn.Dropout(0.1), nn.Linear(120, 84), nn.ReLU(), nn.Dropout(0.1), nn.Linear(84, 12)] self.conv_stack = nn.Sequential(*conv_layers) self.flatten = nn.Flatten() self.linear_stack = nn.Sequential(*fc_list) #self.sigmoid = nn.Sigmoid() def forward(self, x): conv = self.conv_stack(x) flat = self.flatten(conv) result = self.linear_stack(flat) # result = self.sigmoid(logits) return result", "source": "model.py"}, {"content": "from tqdm import tqdm from torch.utils.data import DataLoader, random_split #from torch import nn import torch from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score from .log import chart_t_v_loss, chart_loss from datetime import datetime class TrainModule(object): EPOCHS = 50 DEBUG = False MIN_EPOCHS = 20 PATIENCE = 5 @classmethod def reset_vars(cls, epochs=50, debug=False, min_epochs = 20): cls.EPOCHS = epochs cls.DEBUG = debug cls.MIN_EPOCHS = min_epochs @classmethod def display(cls): \"\"\"Displays the current values of class variables.\"\"\" print(f\"EPOCHS: {cls.EPOCHS}, DEBUG: {cls.DEBUG}, MIN_EPOCHS: {cls.MIN_EPOCHS}\") @classmethod def get_params(cls): \"\"\"Returns current training parameters based on class variables.\"\"\" return { \"epochs\": cls.EPOCHS, \"debug\": cls.DEBUG, \"min_epochs\": cls.MIN_EPOCHS } @classmethod def train(cls, dl, model, opt, crit, model_name='model', bin_cls = False): epochs_since_lowest_val_loss = 0 print(\"TRAINING!: \") min_loss = -1 losses = [] val_losses = [] accs = [] min_val_loss = -1 for epoch in range(cls.EPOCHS): print(f\"EPOCH: {epoch+1}/{cls.EPOCHS}:\") train_ss, val_ss = random_split(dl.dataset, [0.9, 0.1]) train_dl = DataLoader(train_ss, batch_size=dl.batch_size) val_dl = DataLoader(val_ss, batch_size=dl.batch_size) avg_loss, avg_acc = cls.train_one_epoch(train_dl, model, opt, crit, bin_cls) val_loss, val_acc = cls.test(val_dl, model, crit, True, bin_cls) print(\" Loss: \", avg_loss, \"| Accuracy: \", avg_acc) print(\"Val Loss: \", val_loss, \"| Val Accuracy: \", val_acc) losses.append(avg_loss) val_losses.append(val_loss) accs.append(avg_acc*100) # Early Stopping if min_val_loss == -1 or min_val_loss > val_loss: print(\"NEW min val loss!\\n\") min_val_loss = val_loss epochs_since_lowest_val_loss = 0 else: epochs_since_lowest_val_loss += 1 print(f\"Epochs since lowest loss: {epochs_since_lowest_val_loss}\\n\") if (epochs_since_lowest_val_loss >= cls.PATIENCE) and (epoch > cls.MIN_EPOCHS): print(f\"Training aborted at Epoch {epoch}!\") break #Update the min loss and save to models if min_loss == -1 or min_loss > avg_loss: min_loss = avg_loss timestamp = datetime.now().strftime('%Y%m%d_%H%M%S') model_path = './models/{}_{}_{}'.format(model_name, timestamp, epoch) torch.save(model.state_dict(), model_path) chart_t_v_loss(losses, val_losses) chart_loss(accs, title='Accuracy', y_lbl='Accuracy') @classmethod def train_one_epoch(cls, dl:DataLoader, model, optim, crit, bin_cls = False): size = len(dl.dataset) model.train() total = 0 running_loss = 0. corr = 0 for batch, (X,y) in tqdm(enumerate(dl), total=len(dl), unit='batches'): image = X['image'] labels = y image = image.to('cpu') labels = labels.to('cpu') pred = model(image) if bin_cls: lbl_val = y.unsqueeze(1).to(torch.float32) loss = crit(pred, lbl_val) end_pred = torch.sigmoid(pred) pred_class = (end_pred > 0.5).int() pred_class = pred_class.squeeze(1) else: lbl_val = y.to(torch.long) loss = crit(pred, lbl_val) pred_class = pred.argmax(1) correct = (pred_class == labels).sum().item() corr += correct total += labels.size(0) if cls.DEBUG: print(\"Labels: \", lbl_val) print(\"Loss: \", loss) print(pred.size(), labels.size()) print(pred_class.size(), pred_class) print(labels) print((pred_class == labels).sum().item(), labels.size(0)) print(f\"Correct: {correct}/{labels.size(0)} ({corr}/{(batch+1) * dl.batch_size})\") optim.zero_grad() loss.backward() optim.step() running_loss += loss.item() total += labels.size(0) print(f\" loss: {loss:>7f}\") return running_loss/total, corr/total @classmethod def test(cls, dataloader, model, loss_fn, val=True, bin_cls=False): \"\"\"A function to test model against the test dataset in dataloader Args: dataloader (dataloader): a dataloader referencing the test dataset model (torch.nn): the model to be bested loss_fn (torch.nn.loss): the loss function to calculate this model's error rate \"\"\" size = len(dataloader.dataset) num_batches = len(dataloader) if not val: print(f\"Testing on dataset of size {size}\") model.eval() test_loss, correct, total = 0, 0, 0 y_pred = [] y_true = [] with torch.no_grad(): # For the images and labels from dataloader for batch, (X, y) in enumerate(dataloader): image = X['image'] labels = y image = image.to('cpu') labels = labels.to('cpu') #predict... pred = model(image) if bin_cls: # Convert y into a tensor of class values # Calculate the loss test_loss +=", "source": "train.py"}, {"content": "loss_fn(pred, y.unsqueeze(1).to(torch.float32)).item() end_pred = torch.sigmoid(pred) preds = (end_pred > 0.5).int() #convert predictions to 0 or 1 preds = preds.squeeze(1) correct += (preds == labels).sum().item() else: # Convert y into a tensor of class values # Calculate the loss test_loss += loss_fn(pred, y.to(torch.long)).item() preds = pred.argmax(dim=1) correct += (preds == labels).sum().item() if not val: print(y) print(preds) total += labels.size(0) if not val: print(f\"Matches: {(preds == labels).sum().item()}\") y_true.extend(y.unsqueeze(1).int().cpu().numpy()) y_pred.extend(preds.cpu().numpy()) avg_loss = test_loss/len(dataloader) if not val: print(f\"Accuracy: {correct}/{total} - {accuracy_score(y_true, y_pred) * 100:>5f}%\") print(confusion_matrix(y_true, y_pred)) print(classification_report(y_true, y_pred)) print(f\"Test Error: \\n Avg loss: {avg_loss:>8f} \\n\") return avg_loss, correct/total", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "from urllib.request import urlretrieve from zipfile import ZipFile import os import pandas as pd def download_and_unzip(url, file_name, file_path): fp = os.path.join(file_path, file_name) urlretrieve(url, fp) with ZipFile(fp, 'r') as zip_ref: zip_ref.extractall(path=file_path) def get_data(path, zip_name, link): fullpath = os.path.join(path, zip_name) if os.path.exists(fullpath) is False: print(f'Downloading and unzipping {zip_name} to {path}') download_and_unzip(link, zip_name, path) else: print(f'Skipping, {fullpath} exists!') def read_csv(csv_path): return pd.read_csv(csv_path)", "source": "data.py"}, {"content": "from .data import read_csv import pandas as pd from sklearn.preprocessing import OneHotEncoder, StandardScaler from sklearn.model_selection import TimeSeriesSplit from statsmodels.tsa.stattools import adfuller class DataPipeline: # ==================== # Major Functions # ==================== def __init__(self): self.raw_df = pd.DataFrame() self.df = pd.DataFrame() self.test_df = pd.DataFrame() self.train_df = pd.DataFrame() self.verbose = False def run_data_pipeline(self, csv_path, verbose=False): # This pipeline should # 1) take in a csvpath, # 2) clean the data # 3) engineer the dataframe accordingly # And return the cleaned df. self.verbose = verbose self.raw_df = read_csv(csv_path) self.df = self.data_clean(self.raw_df) self.df = self.data_engin(self.df) return self.df # ==================== # Data Cleaning # ==================== def data_clean(self, df:pd.DataFrame): \"\"\"Performs data cleaning: - creates date index - selects only the necessary features - backfills the pm2.5 feature Args: df (pd.DataFrame): _description_ Returns: _type_: _description_ \"\"\" feat_list = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd'] if self.verbose: self.print_adf(df, ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir']) self.df = df.copy() # Make date and set it to be the index self.df['date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) self.df.set_index('date', inplace=True) self.df.index.freq = 'h' # Selects Data self.df = self.df[feat_list] # Fills missing data self.df['pm2.5'] = self.fillna_ytd(self.df['pm2.5']) self.df.dropna(inplace=True) return self.df def print_adf(self, df, feat_list): for feat in feat_list: adf_results = self.adf_test(df[feat].dropna().values, feat) print(adf_results['Feature'] + \": \" + str(adf_results['Stationary'])) def adf_test(self, series, name): result = adfuller(series) return { 'Feature': name, 'ADF Statistic': result[0], 'p-value': result[1], 'Critical Values': result[4], 'Stationary': result[1] < 0.05 # True if p-value is less than 0.05 } def fillna_ytd(self, data: pd.Series): \"\"\"Fills the chosen feature using yesterday's data Thanks Zhang Zhou! Args: data (pd.Series): A series of the feature you want to fill \"\"\" filled_data = data.copy() ytd_data = filled_data.shift(24) filled_data = filled_data.fillna(ytd_data) return filled_data # ==================== # Feature Engineering # ==================== def data_engin(self, data:pd.DataFrame): \"\"\"Performs data engineering on the dataframe Args: data (pd.DataFrame): the dataframe to perform engineering on \"\"\" d_l_feat_list = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir'] # Generate diffs for all feats eng_data = self.gen_diff_val_feats(data, d_l_feat_list) # Generate lags for selected feats (based off ccf) eng_data = self.gen_lag_val(eng_data, 'pm2.5', [1]) eng_data = self.gen_lag_val(eng_data, 'pm2.5_diff', [1, 2, 8, 24]) eng_data = self.gen_lag_val(eng_data, 'Iws_diff', [1]) eng_data = self.gen_lag_val(eng_data, 'Is_diff', [1]) eng_data = self.gen_lag_val(eng_data, 'Ir_diff', [1, 12]) eng_data = self.gen_lag_val(eng_data, 'DEWP_diff', [1, 8]) eng_data = self.gen_lag_val(eng_data, 'TEMP_diff', [1, 10]) eng_data = self.gen_lag_val(eng_data, 'PRES_diff', [7, 24]) # OHE that cbwd! cbwd_ohe = OneHotEncoder(sparse_output=False) cbwd_ohe.fit(data[['cbwd']]) eng_data[cbwd_ohe.get_feature_names_out()] = cbwd_ohe.transform(data[['cbwd']]) eng_data.dropna(axis=0, inplace=True) return eng_data def std_vals(self, X_train, X_test, y_train, y_test): \"\"\"Standardly scales the train test values Args: X_train (_type_): _description_ X_test (_type_): _description_ y_train (_type_): _description_ y_test (_type_): _description_ Returns: ndarray, ndarray: scaled X train and test values \"\"\" ss = StandardScaler() std_train = ss.fit_transform(X_train) std_test = ss.transform(X_test) return std_train, std_test def drop_feats(self, data:pd.DataFrame): return data.drop(['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir', 'cbwd'], axis=1) def gen_lag_val(self, data, feat, lag_list): \"\"\"Generate a feature from the specified feature using specified lag values Args: data (pd.Series): series to generate lag values from feat (string): the feature to engineer lags lag_list (list): a list containing lag values of 1 hour units Returns: series: a series with the new lagged feature \"\"\" lag_df = data.copy() for lag in lag_list: lag_df[f\"{feat}_{str(lag)}H\"]", "source": "datapipeline.py"}, {"content": "= lag_df[feat].shift(lag) return lag_df def gen_diff_val_feats(self, data, feat_list): \"\"\"Generate diff()s for the specified features Args: df (pd.DataFrame): dataframe to perform diffing on feat_list (list): list of features to diff Returns: df: a dataframe with new features, named in the format [feat + '_diff'] \"\"\" diff_df = data.copy() for feat in feat_list: diff_df[feat + '_diff'] = diff_df[feat].diff() return diff_df def v_log(self, message:str): if self.verbose: print(message) # ==================== # Data Splitting # ==================== def split_ts(self, df, target='pm2.5', sliding=True, split=None, n_splits=5, max_train_size=None, test_size=None): \"\"\"Simply splits the data based on the split described in split Args: df (pd.DataFrame): The data to perform the split on. target (string): The target feature. Needs to exist in *df*. Defaults to 'pm2.5'. split (list, optional): The split you want. Defaults to [0.8, 0.2] Returns: Iterator[tuple[ndarray, ndarray]] \"\"\" if split is None: split = [0.8,0.2] if sliding: tscv = TimeSeriesSplit(n_splits=n_splits) else: tscv = TimeSeriesSplit(n_splits=n_splits, max_train_size=max_train_size, test_size=test_size) X = df.drop([target], axis=1) y = df[target] return X, y, tscv.split(X) def get_split(self, train_index, test_index, X, y): X_train, X_test = X.iloc[train_index], X.iloc[test_index] y_train, y_test = y.iloc[train_index], y.iloc[test_index] return X_train[:-1], X_test, y_train[:-1], y_test", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_absolute_error as MAE from sklearn.metrics import root_mean_squared_error as RMSE from sklearn.metrics import mean_absolute_percentage_error as MAPE import matplotlib.pyplot as plt import pandas as pd class ForecastModel: class ErrorBundle: def __init__(self): self.rmse = 0. self.mae = 0. self.mape = 0. def get_errors(self, y_true, y_pred): self.rmse = RMSE(y_true, y_pred) self.mae = MAE(y_true, y_pred) self.mape = MAPE(y_true, y_pred) return self def __str__(self): return f'\\n RMSE: {self.rmse}\\n MAE: {self.mae}\\n MAPE: {self.mape}' def __init__(self): self.model = RandomForestRegressor() def fit(self, X, y): # print(type(y), y) self.model.fit(X, y) def evaluate(self, X_train, X_test, y_train, y_test): train_er = self.ErrorBundle() test_er = self.ErrorBundle() y_train_pred = self.model.predict(X_train) train_error = train_er.get_errors(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = test_er.get_errors(y_test, y_test_pred) return train_error, test_error, y_test_pred def predict(self, X): \"\"\"Predicts future values from the model using given data X Args: X (array-like): The data to predict on Returns: array-like: The predicted values \"\"\" return self.model.predict(X) def chart_preds(self, true, pred, split_count=-1): pred = pd.Series(pred, index=true.index) splt = '' if split_count != -1: splt = f'#{split_count}: ' plt.plot(true, label = 'true') plt.plot(pred, label = 'pred') plt.title( splt + \"True vs Pred\") plt.legend() plt.show() def multi_chart(self, train_er, test_er): \"\"\" Plot the training and test errors for RMSE, MAE, and MAPE across multiple iterations. :param train_er: List of ErrorBundle objects containing training errors :param test_er: List of ErrorBundle objects containing test errors \"\"\" train_rmse = [x.rmse for x in train_er] train_mae = [x.mae for x in train_er] train_mape = [x.mape for x in train_er] test_rmse = [x.rmse for x in test_er] test_mae = [x.mae for x in test_er] test_mape = [x.mape for x in test_er] print(train_rmse, test_rmse) print(train_mae, test_mae) print(train_mape, test_mape) self.chart_loss(train_rmse, test_rmse, 'RMSE') self.chart_loss(train_mae, test_mae, 'MAE') self.chart_loss(train_mape, test_mape, 'MAPE') def chart_loss(self, train, test, title='Loss chart', x_lbl='Time', y_lbl='Loss'): \"\"\"plots a chart of training and test losses over time Args: train (List of floats): training vals of the epoch test (List of floats): test vals of the epoch x_lbl (string): the label of the x-axis \"\"\" plt.plot(train, label = 'training loss') plt.plot(test, label = 'test loss') plt.title(title) plt.legend() plt.xlabel(x_lbl) plt.ylabel(y_lbl) plt.show()", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import nltk import numpy as np import pandas as pd import re import string from nltk.stem import WordNetLemmatizer, PorterStemmer from nltk.tokenize import word_tokenize from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer import logging class DataEngin: def __init__(self): self.lemmatizer = WordNetLemmatizer() nltk.download(\"wordnet\") nltk.download(\"omw-1.4\") logging.info(\"WordNetLemmatizer initialized\") self.ps = PorterStemmer() nltk.download('punkt_tab') logging.info(\"PorterStemmer initialized\") def tokenize(self, text): \"\"\" Takes in a string, splits it into a list of word tokens, stems them, and returns a list of cleaned tokens Parameters ---------- text : string a single string of text Returns ------- list of cleaned tokens \"\"\" text_cleaned = clean_input(text) tokens = word_tokenize(text_cleaned) lem_tok = [self.lemmatizer.lemmatize(x) for x in tokens] return lem_tok def clean_input(input): text = remove_tags(input) text = remove_punc(text) return text def remove_tags(input): \"\"\"Removes HTML tags from a string input. Args: input (str): A string containing HTML tags. Returns: str: The input string with all HTML tags removed. \"\"\" tag_regex = '<[^<>]+>' return re.sub(tag_regex, ' ', input) def remove_punc(input): #Replaces special characters except for single quote in_str = re.sub(r'[^\\w\\s\\']', ' ', input) # Replaces multiple spaces with single space in_str = re.sub(r'[\\s]+', ' ', in_str) # Removes 's and ', but not 't or 'll etc return re.sub(r'\\'s|\\'', '', in_str) def baggify(data, col=None, vocab=None): \"\"\"Generates a Series from the vocab in col based on Bag of Words Args: df (pd.DataFrame): The dataframe to be used col (string, optional): The column to base the bag of words from. Defaults to None. vocab (dict, optional): The dict to use. Defaults to None. Returns: pd.Series: The bag of words \"\"\" vectorizer = CountVectorizer() info = data[col] if isinstance(data, pd.DataFrame) else data vectorizer.fit(info) print(vectorizer.vocabulary_) # logging.info(vectorizer.vocabulary_) return vectorizer.transform(info).toarray() def tfidfify(data, col): \"\"\"Generates a Series from the vocab in col based on TF-IDF Args: df (pd.DataFrame): The dataframe to be used col (string): The column to base the bag of words from Return: pd.Series: The TF-IDF Series \"\"\" vectorizer = TfidfVectorizer() info = data[col] if isinstance(data, pd.DataFrame) else data vectorizer.fit(info) return vectorizer.transform(info).toarray()", "source": "data_engin.py"}, {"content": "from transformers import TFAutoModelForCausalLM, AutoTokenizer, DataCollatorForLanguageModeling from transformers import create_optimizer, AdamWeightDecay from datasets import load_dataset, load_dataset_builder import evaluate from transformers import pipeline import tensorflow as tf device = 'cuda' MODEL = \"gpt2\" TOKENIZER_NAME = \"distilbert/distilgpt2\" # adapted from https://huggingface.co/docs/transformers/tasks/language_modeling class DecoderTools(): def __init__(self): self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME) self.tokenizer.pad_token = self.tokenizer.eos_token self.model = TFAutoModelForCausalLM.from_pretrained(MODEL) self.data_collator = DataCollatorForLanguageModeling(tokenizer=self.tokenizer, mlm=False, return_tensors='tf') self.optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01) self.model.compile(optimizer=self.optimizer) def download_dataset(self, DATASET_NAME, split=\"[:5000]\"): return load_dataset(DATASET_NAME, split=split) def preprocess_fn(self, data, feat = 'text'): return self.tokenizer(data[feat], truncation=True, padding=True, return_tensors='tf') def preprocess_dataset(self, dataset:tf.data.Dataset): return dataset.map(self.preprocess_fn, batched=True, remove_columns=dataset.column_names) def fit_model(self, x_train, val_set=None, epochs=3): print(x_train) self.model.fit(x_train, validation_data=val_set, epochs=epochs) print(\"Model Trained!\") def get_generator(self): self.generator = pipeline(\"text-generation\", model = self.model, tokenizer = self.tokenizer) def predict_model(self, text): if self.generator is None: self.get_generator() print(self.generator(text))", "source": "decoder_tools.py"}, {"content": "import numpy as np from datasets import load_dataset, load_dataset_builder from transformers import AutoTokenizer, DataCollatorWithPadding from transformers import TFAutoModelForSequenceClassification, create_optimizer, pipeline from transformers.keras_callbacks import KerasMetricCallback import evaluate from sklearn.preprocessing import StandardScaler from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import classification_report DATASET_NAME = \"dair-ai/emotion\" TOKENIZER_NAME = \"distilbert/distilbert-base-uncased\" id2label = {0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"} label2id = {v: k for k, v in id2label.items()} id2label_bin = {0: False, 1: True, 2: True, 3: False, 4: False, 5: True} label2id_bin = {v: k for k, v in id2label_bin.items()} class HuggingTools(): def __init__(self): self.tokenizer = None self.get_tokenizer() self.accuracy = evaluate.load(\"accuracy\") self.f1 = evaluate.load(\"f1\") self.cm = evaluate.load(\"confusion_matrix\") def inspect_emotion(self): \"\"\"Prints the description and features of the emotion dataset.\"\"\" ds_builder = load_dataset_builder(DATASET_NAME) print(ds_builder.info.description) print(ds_builder.info.features) def load_dataset(self, split_amt=None): print(split_amt) return load_dataset(DATASET_NAME, split=split_amt) def split_dataset(self, dataset, split=0.2): return dataset.train_test_split(test_size=split) def load_split_dataset(self, split_amt=''): \"\"\"Loads the emotion dataset. Args: type (str, optional): The type of dataset to load. Defaults to 'ds'. Returns: Tuple: training, validation and test datasets \"\"\" train_ds = load_dataset(DATASET_NAME, split=f\"train{split_amt}\") val_ds = load_dataset(DATASET_NAME, split=f\"validation{split_amt}\") test_ds = load_dataset(DATASET_NAME, split=f\"test{split_amt}\") return train_ds, val_ds, test_ds def load_dataset_as_df(self): \"\"\"Loads the dataset and converts them to their pandas equivalent Returns: Tuple: training, validation and test dataframes \"\"\" train, val, test = self.load_split_dataset() return train.to_pandas(), val.to_pandas(), test.to_pandas() def get_tokenizer(self, max_len=None): if self.tokenizer is None: self.tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, truncation=True) if max_len is not None: self.tokenizer.model_max_length = max_len print(f'Using tokenizer {self.tokenizer.name_or_path} of max_len {self.tokenizer.model_max_length} with truncation {self.tokenizer.truncate_sequences}') return self.tokenizer def set_max_len(self, max_len): self.tokenizer.model_max_length = max_len print(f'Tokenizer {self.tokenizer.name_or_path} has max_len {self.tokenizer.model_max_length}') def preprocess_func(self, examples, feat='text'): \"\"\"Preprocesses a single example in a dataset. Args: examples (dict): A dictionary containing a single example in the dataset. feat (str, optional): The key of the feature to preprocess. Defaults to 'text'. Returns: dict: A dictionary containing the preprocessed example. \"\"\" return self.tokenizer(examples[feat], truncation=True) def preprocess_dataset(self, dataset, feat='text', split=None): \"\"\"Preprocesses a dataset. Args: dataset (dict): A dictionary containing the dataset. feat (str, optional): The key of the feature to preprocess. Defaults to 'text'. Returns: dict: A dictionary containing the preprocessed dataset. \"\"\" ds = dataset if split is not None: ds = dataset.shard(num_shards=split, index=0) ds = ds.map(self.preprocess_func, batched=True) return ds def get_class_weights(self, dataset, pad=0.1): class_counts = dataset['label'].value_counts() class_weights = 1 / class_counts class_weights = class_weights[class_weights > 0] #if pad * class_counts is more than 1, don't pad if sum(class_weights) > 1: print('pad is too large - retaining original class weights') elif (pad > 0) & (pad < 1): # multiply class weights by 1/class_counts class_weights = class_weights * (len(class_counts)/pad) + pad # pad class weights return dict(zip(class_weights.index, class_weights)) def set_data_collator(self): \"\"\"Returns the data collator to create batches Returns: DataCollator: A datacollator object that batches data \"\"\" self.data_collator = DataCollatorWithPadding(tokenizer=self.get_tokenizer(), return_tensors=\"tf\") return self.data_collator def gen_tf_dataset(self, dataset, shuffle, batch_size): \"\"\"Generates a TensorFlow dataset from a huggingface dataset Args: dataset (dict): A dictionary containing the dataset. shuffle (bool): Whether to shuffle the dataset batch_size (int): The batch size Returns: tf.data.Dataset: A TensorFlow dataset \"\"\" tf_dataset = self.model.prepare_tf_dataset( dataset, shuffle=shuffle, batch_size=batch_size, collate_fn=self.data_collator, ) return tf_dataset def evaluate(self, test_data): test_preds = np.round(self.model.predict(test_data)) test_labels = test_data['label'] print('Test accuracy: ', self.accuracy.compute(test_preds,", "source": "hf_tools.py"}, {"content": "test_labels)) print('Test F1: ', self.f1.compute(predictions=test_preds, references=test_labels)) print('Test confusion matrix: ', self.cm.compute(predictions=test_preds, references=test_labels)) print(classification_report(test_labels, test_preds, target_names=id2label.values())) ### Part I: Feature Extraction ### https://huggingface.co/docs/transformers/tasks/feature_extraction def set_feature_extractor(self, framework='tf'): \"\"\"Gets the feature extractor\"\"\" self.feature_extractor = pipeline('feature-extraction', framework=framework, model=TOKENIZER_NAME, tokenizer=self.tokenizer) return self.feature_extractor def set_custom_model(self, class_weights=None): \"\"\"Generates a logreg model based on logistic regression Returns: _type_: _description_ \"\"\" self.cust_model = LogisticRegression(solver='liblinear', random_state=42, class_weight=class_weights) # self.cust_model = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state=42, class_weight=class_weights) return self.cust_model # TODO: add kfold cv def fit_custom_model(self, train_data, val_data): \"\"\"Fits the custom model to the features of the train and test data Args: train_data (_type_): _description_ val_data (_type_): _description_ features (list of tensors): A list of tensors of dimension (1, sentence length, 768) Returns: _type_: _description_ \"\"\" print(\"Generating features and pooling...\") feat_vals = [self.gen_pool_features(f) for f in self.gen_features(train_data)] print(\"Fitting Model...\") self.cust_model.fit(feat_vals, train_data['label']) print(\"Fitted!\") return self.cust_model def gen_features(self, data): # features are a (1, sentence length, 768) tensor # we should get the mean of every word in the sentence to pool the values features = self.feature_extractor(data['text'], return_tensors='np') return features def gen_pool_features(self, features): \"\"\"Pools the features of a document using the mean of each word Args: features (Tensor): _description_ Returns: _type_: _description_ \"\"\" pool_features = np.mean(features, axis=1)[0] return pool_features def predict_custom_model(self, test_data): \"\"\"Predicts the labels of the test data Args: test_data (_type_): _description_ Returns: _type_: _description_ \"\"\" features = self.gen_features(test_data) pooled_features = [self.gen_pool_features(f) for f in features] preds = self.cust_model.predict(pooled_features) print(test_data['text'][0], features[0], pooled_features[0], preds[0]) print(classification_report(test_data['label'], preds, target_names=id2label.values())) return preds ### Part II: Fine-tuning DistillBERT ### https://huggingface.co/docs/transformers/tasks/sequence_classification def set_model(self): \"\"\"Gets the DistillBERT model\"\"\" self.model = TFAutoModelForSequenceClassification.from_pretrained( TOKENIZER_NAME, num_labels=6, id2label=id2label, label2id=label2id) return self.model def set_optimizer(self, dataset_len, batch_size, epochs): \"\"\"Returns the optimizer to be used in the model Args: dataset_len (int): The length of the dataset batch_size (int): The batch size epochs (int): The number of epochs Returns: Optimizer: The optimizer to be used in the model \"\"\" batches_per_epoch = dataset_len // batch_size total_train_steps = int(batches_per_epoch * epochs) optimizer, _ = create_optimizer( init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps) self.optimizer = optimizer return optimizer def get_func(self, function, eval_pred, kwargs=None): \"\"\"Returns relevant metrics for the model, to be run with Keras callbacks Args: function (evaluate.load): The metric to be used eval_pred (tuple of array-like): The predictions and labels from the model kwargs (dict, optional): Additional arguments to be passed to the metric Returns: float: The metric value \"\"\" if kwargs is None: kwargs = {} predictions, labels = eval_pred predictions = np.argmax(predictions, axis=1) return function.compute(predictions=predictions, references=labels, **kwargs) def get_acc(self, eval_pred): return self.get_func(self.accuracy, eval_pred) def get_f1(self, eval_pred): return self.get_func(self.f1, eval_pred, {'average':'macro'}) def get_cm(self, eval_pred): return self.get_func(self.cm, eval_pred) def set_callbacks(self, val_set): \"\"\"Returns the callbacks to be used in the model Args: val_set (tf.data.Dataset): The validation dataset Returns: list: The callbacks to be used in the model \"\"\" acc_callback = KerasMetricCallback(metric_fn=self.get_acc, eval_dataset=val_set) f1_callback = KerasMetricCallback(metric_fn=self.get_f1, eval_dataset=val_set) cm_callback = KerasMetricCallback(metric_fn=self.get_cm, eval_dataset=val_set) return [acc_callback, f1_callback, cm_callback] def compile_model(self): self.model.compile(optimizer=self.optimizer) def train_model(self, train_set, val_set, epochs=5): \"\"\"Trains the model using the train and validation set Args: train_set (tf.data.Dataset): The training dataset val_set (tf.data.Dataset): The validation dataset epochs (int, optional): The number of epochs. Defaults to 5. \"\"\" self.model.fit(x=train_set, validation_data=val_set, epochs=epochs, callbacks=self.set_callbacks(val_set)) def test_model(self, test_set): \"\"\"Tests the model using", "source": "hf_tools.py"}, {"content": "the test set Args: test_set (tf.data.Dataset): The test dataset \"\"\" self.model.evaluate(test_set, callbacks=self.set_callbacks(test_set)) def visualize_data(self): pass", "source": "hf_tools.py"}, {"content": "import pandas as pd import os import pyodbc import yaml def load_config(yml_path, verbose=False): \"\"\"Loads the config from a yaml file Args: yml_path (string): Path of the yaml file. Should contain driver, server, database, username and password fields. verbose (bool): Whether to print the info_dict Returns: Dict(string, string): A dict containing driver, server, database, username and password fields. \"\"\" with open(yml_path) as info: info_dict = yaml.safe_load(info) if verbose: print(info_dict) return info_dict def get_dataset(table, csv_path, config_dict): if(os.path.exists(csv_path)): print(\"File already exists! Continuing...\") raw_df = pd.read_csv(csv_path) else: print(\"File not found! Downloading...\") raw_df = download_table(table, csv_path, config_dict) return raw_df def download_table(table, csv_path, config_dict): \"\"\"Gets the raw data from an ODBC server Args: csv_path (string): the path to save the csv config_dict (dict(string,string)): A dictionary containing driver, server, database, username and password fields. Returns: Dataframe: A dataframe of the table at the given database. \"\"\" driver = config_dict['driver'] server = config_dict['server'] database = config_dict['database'] username = config_dict['username'] password = config_dict['password'] conn = pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) database = pd.read_sql_query(f\"SELECT * FROM {table}\", conn) final = database os.makedirs(os.path.dirname(csv_path), exist_ok=True) final.to_csv(csv_path, header=True) print(\"Downloaded and written to \" + csv_path + \" successfully.\") return final", "source": "load_data.py"}, {"content": "from sklearn.pipeline import Pipeline from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import GridSearchCV from sklearn.preprocessing import StandardScaler from sklearn.metrics import f1_score, accuracy_score, confusion_matrix from .data_engin import DataEngin from sklearn.model_selection import train_test_split from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer import joblib #import h5py class sent_class(): def __init__(self): # The pipeline should: # 1) Clean the text # 2) Tokenize # 3) Stem # 4) Apply the TfidfTransformer # (Tokenizer cleans, tokenizes then stems the output) # (TfidfVectorizer does CountVectorizer + TfidfTransformer) self.de = DataEngin() self.tokenizer = self.de.tokenize self.vct = TfidfVectorizer( tokenizer=self.tokenizer, max_features=5000, min_df=0.01, max_df=0.8 ) self.clf = RandomForestClassifier( criterion='gini', n_estimators=20, max_depth=12, min_samples_split=10, random_state=42 ) self.pipeline = Pipeline(steps=[ ('tfidf', self.vct), ('clf', self.clf) ]) def set_params(self, params): self.clf.set_params(**params) def train(self, X_train, y_train): \"\"\"Applies the pipeline to the train set Args: X_train (): features of the train set y_train (): the label of the train set Returns: float: the f1 score of the model \"\"\" self.pipeline.fit(X_train, y_train) y_pred = self.pipeline.predict(X_train) print(f\"Accuracy: {accuracy_score(y_train, y_pred)}\") print(f\"F1 Score: {f1_score(y_train, y_pred)}\") print(f\"Confusion Matrix:\\n{confusion_matrix(y_train, y_pred)}\") return f1_score(y_train, y_pred) def evaluate(self, X_test, y_test): \"\"\"Evaluates the result of the model using test Args: X_test (_type_): features of the test set y_test (_type_): the labels of the test set Returns: f1_score: the f1 score of the model \"\"\" y_pred = self.pipeline.predict(X_test) print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\") print(f\"F1 Score: {f1_score(y_test, y_pred)}\") print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\") def process_dataset(self, df, feature): \"\"\"Processes the dataset. Args: df (_type_): _description_ feature (string): The feature to use Returns: pd.DataFrame: The processed dataset \"\"\" print(df.columns) ml_df = df[[feature] + ['label']].copy() # Convert label values to True or False ml_df['label'] = ml_df['label'].map({'pos': True, 'neg': False}) ml_df[feature] = ml_df[feature].str.lower() return ml_df def split_train_test(self, df, feature, test_size=0.2): \"\"\"Splits the dataset into a train and test set Args: df (pd.DataFrame): The dataset feature (string): The feature to use test_size (float, optional): The proportion of the dataset to include in the test set. Defaults to 0.2. \"\"\" X = df[feature].values y = df['label'].values X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=df['label'], test_size=test_size, shuffle=True, random_state=42) return X_train, X_test, y_train, y_test def save_model(self, path): joblib.dump(self.pipeline, path) def load_model(self, path): self.pipeline = joblib.load(path)", "source": "senti_analysis.py"}, {"content": "import shap import numpy as np def get_explainer(classifier): return shap.Explainer(classifier)", "source": "shap.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder from sklearn.feature_extraction import FeatureHasher from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def transform(data_path): \"\"\" Description of the function. :param data_path: ...... :return: ...... \"\"\" # Load the data raw_df = pd.read_csv(data_path) # Data Cleaning def replace_placeholder_values(df, placeholder_list): df.replace(placeholder_list, np.nan, inplace=True) return df placeholder_list = ['?', 'N/A', 'NA', '--'] df = replace_placeholder_values(raw_df, placeholder_list) # Drop ID as it's not necessary df = df.drop('id', axis=1) # a small subset of missing hispanic subset replaced with missing values with the mode of each column df = df.apply(lambda x: x.fillna(x.mode()[0])) # 0 if income below 50K else True if income above 50K df['income_group'] = df['income_group'].apply(lambda x: 0 if x == '- 50000.' else 1) df['financial_gains'] = ((df['capital_gains'] > 0).astype(int) - (df['capital_losses'] > 0).astype(int) + (df['dividends_from_stocks'] > 0).astype(int)) df['income_work'] = ((df['weeks_worked_in_year'] > 0).astype(int) * (df['wage_per_hour'] > 0).astype(int) * 40) # Define X as input features and y as the outcome variable X = df.drop(['income_group'], axis=1) y = df['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) numeric_features = [ 'age', 'wage_per_hour', 'num_persons_worked_for_employer', 'weeks_worked_in_year', 'year', 'capital_gains', 'capital_losses', 'dividends_from_stocks', \"financial_gains\", \"income_work\"] onehot_features = [ 'enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'region_of_previous_residence', 'hispanic_origin', 'member_of_a_labor_union', \"reason_for_unemployment\", 'full_or_part_time_employment_stat', 'class_of_worker', 'citizenship', 'detailed_household_summary_in_household', 'family_members_under_18', 'tax_filer_stat', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'veterans_benefits', 'detailed_industry_recode', 'detailed_occupation_recode', 'own_business_or_self_employed'] hashing_features = [ 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'detailed_household_and_family_stat', 'major_industry_code', 'major_occupation_code', 'state_of_previous_residence'] ordinal_features = ['education'] # Build a preprocessing step for numeric features numeric_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()) ]) # Build a preprocessing step for nominal features nominal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) # Build a preprocessing step for hashing features features # High-cardinality categorical features: When your categorical features have a large number of unique values. hashing_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('hasher', FeatureHasher(n_features=10, input_type='string')) ]) # Build a preprocessing step for ordinal features ordinal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OrdinalEncoder()) ]) # Combine the preprocessing steps using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('onehot', nominal_transformer, onehot_features), ('hash', hashing_transformer, hashing_features), ('ordinal', ordinal_transformer, ordinal_features) ]) # Fit the preprocessor to the training and test data X_train_preprocessed = preprocessor.fit_transform(X_train) X_test_preprocessed = preprocessor.transform(X_test) return X_train_preprocessed, X_test_preprocessed, y_train, y_test", "source": "datapipeline.py"}, {"content": "import numpy as np class DecisionTree: def __init__(self, max_depth=None, min_samples_split=2): \"\"\" Decision Tree Model Args: max_depth (int, optional): Maximum depth of the tree. Defaults to None. min_samples_split (int, optional): Minimum number of samples required to split a node. Defaults to 2. \"\"\" self.tree = None self.max_depth = max_depth self.min_samples_split = min_samples_split @staticmethod def gini(left, right): \"\"\" Args: left (list): List of labels for the left split right (list): List of labels for the right split \"\"\" def calculate_gini(labels): # counts occurence of each unique label label_counts = {} for label in labels: if label not in label_counts: label_counts[label] = 0 label_counts[label] += 1 total_instances = len(labels) sum_of_squared_proportions = 0 for count in label_counts.values(): # occurence of each unique label / total proportion = count / total_instances # square the proportion sum_of_squared_proportions += proportion ** 2 # subtract p_i^2 from 1 gini_impurity = 1 - sum_of_squared_proportions return gini_impurity gini_left = calculate_gini(left) gini_right = calculate_gini(right) total_samples = len(left) + len(right) weighted_gini = ((len(left) / total_samples) * gini_left) + ((len(right) / total_samples) * gini_right) return weighted_gini def fit(self, X, y): \"\"\" Build the decision tree using the training data Args: X (array-like): Training input samples y (array-like): Target values. \"\"\" # Convert X and y to numpy arrays if they're not already X = np.array(X) y = np.array(y).flatten() self.tree = self._build_tree(X, y) def _build_tree(self, X, y, depth=0): \"\"\" Recursively build the decision tree Args: X (numpy.ndarray): The feature matrix y (numpy.ndarray): The target labels depth (int): The current depth of the tree Returns: A leaf node or a dictionary representing the internal node \"\"\" num_samples, num_features = X.shape # Stopping criteria to prevent overfitting and infinite recursion if (self.max_depth is not None and depth >= self.max_depth) or num_samples < self.min_samples_split: return self._create_leaf(y) best_split = self._find_best_split(X, y, num_features) if best_split is None: # return the most common label if no split is possible return self._create_leaf(y) left_indices, right_indices = self._split(X[:, best_split['feature']], best_split['threshold']) # recursive build the tree till it reaches the stopping criteria self._build_tree(X[left_indices], y[left_indices], depth + 1) self._build_tree(X[right_indices], y[right_indices], depth + 1) def _find_best_split(self, X, y, num_features): \"\"\" Find the best split for a node. Args: X (numpy.ndarray): The feature matrix y (numpy.ndarray): The target labels num_features (int): The number of features Returns: A dictionary containing the best split information or None if no split is found \"\"\" # start at infinity so the first gini is always better than the default best_gini = float('inf') # start with no split best_split = None for feature_index in range(num_features): # finds all unique values in each feature thresholds = np.unique(X[:, feature_index]) for threshold in thresholds: # split based on boolean mask left_indices, right_indices = self._split(X[:, feature_index], threshold) # nothing left to split further, continue with the next feature column if len(left_indices) == 0 or len(right_indices) == 0: continue # calculate gini impurity for y based on the indices gini = DecisionTree.gini(y[left_indices], y[right_indices]) if gini < best_gini: best_gini = gini best_split = {'feature': feature_index, 'threshold': threshold} return best_split def _split(self, feature_column, threshold): \"\"\"\" Args: feature_column (numpy.ndarray): The feature column to split threshold (float): The threshold for splitting Returns: A", "source": "decision_tree.py"}, {"content": "tuple containing the indices for the split \"\"\" # boolean mask for left left_indices = np.where(feature_column <= threshold)[0] # boolean mask for right right_indices = np.where(feature_column > threshold)[0] return left_indices, right_indices def _create_leaf(self, y): most_common_label = np.bincount(y).argmax() return {'label': most_common_label} def _traverse_tree(self, x, node): \"\"\"\" Args: x (numpy.ndarray): The input sample node (dict): The current node Returns: The predicted label for the input sample \"\"\" while 'label' not in node: if x[node['feature']] <= node['threshold']: node = node['left'] else: node = node['right'] return node['label'] def predict(self, X): \"\"\"\" Predict the labels for the input samples Args: X (array-like): The input samples Returns: numpy.ndarray: The predicted labels \"\"\" X = np.asarray(X) return np.array([self._traverse_tree(sample, self.tree) for sample in X])", "source": "decision_tree.py"}, {"content": "import pandas as pd import pyodbc import os server = 'aiap-training.database.windows.net' database = 'aiap' username = 'apprentice' password = 'Pa55w.rd' driver = '{ODBC Driver 18 for SQL Server}' class ImportData: \"\"\" Class to encapsulate sql query responses, parse and return dataframes \"\"\" def __init__(self, data_location): \"\"\" instantiate object to connect to sql engine Args: data_location (string): location of the csv file \"\"\" self.data_location = data_location self.conn = pyodbc.connect( f'DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}') def return_table(self, table): \"\"\" Returns the datasets found in the SQL connection Args: table (string): name of table to query Returns: articles (dataframe) : Returns dataframe with from articles table \"\"\" if(os.path.exists(self.data_location)): print(\"File already exists! Continuing...\") raw_df = pd.read_csv(self.data_location) return raw_df else: print(\"File not found! Downloading...\") query = \"SELECT * FROM \" + table query_table = pd.read_sql(query, self.conn) os.makedirs(os.path.dirname(self.data_location), exist_ok=True) query_table.to_csv(self.data_location, index=False) self.conn.close() return query_table", "source": "extract.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = None def train(self, params, X_train, y_train): \"\"\" Train the Random Forest model. :param params: Dictionary of parameters for the Random Forest model :param X_train: Training features :param y_train: Training labels :return: F1 score on the training data \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.model = RandomForestClassifier(**params) self.model.fit(X_train, y_train) y_train_pred = self.model.predict(X_train) train_f1 = f1_score(y_train, y_train_pred) print(f\"Training F1 Score: {train_f1}\") return train_f1 def evaluate(self, X_test, y_test): \"\"\" Evaluate the trained Random Forest model. :param X_test: Test features :param y_test: Test labels :return: F1 score on the test data \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score if self.model is None: raise ValueError(\"Model has not been trained yet. Call train() first.\") y_pred = self.model.predict(X_test) test_f1 = f1_score(y_test, y_pred) print(f\"Test F1 Score: {test_f1}\") return test_f1 def get_default_params(self): \"\"\" Get default parameters for the Random Forest model. :return: Dictionary of default parameters \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return { 'n_estimators': 450, 'max_depth': 30, 'min_samples_split': 6, 'max_features': 'sqrt', 'random_state': 42 }", "source": "model.py"}, {"content": "import numpy as np from src.decision_tree import DecisionTree class RandomForest: def __init__(self, n_trees=5, subsample_size=1.0, feature_proportion=1.0): \"\"\" Simple Random Foreset Classifier Initialize the random forest with the following attributes: Args: n_trees (int): Number of trees in the forest subsample_size (float): Proportion of samples to use for each tree feature_proportion (float): Proportion of features to use for each tree \"\"\" self.n_trees = n_trees self.subsample_size = subsample_size self.feature_proportion = feature_proportion self.forest = [] def fit(self, X, y): \"\"\" Fit the random forest to the training data Args: X (numpy.ndarray): The feature matrix y (numpy.ndarray): The target labels \"\"\" self.forest = [] n_samples, n_features = X.shape subsample_size = int(n_samples * self.subsample_size) feature_subset_size = max(1, int(n_features * self.feature_proportion)) for _ in range(self.n_trees): sample_indices = np.random.choice(n_samples, size=subsample_size, replace=True) feature_indices = np.random.choice(n_features, size=feature_subset_size, replace=False) X_sample = X[sample_indices][:, feature_indices] y_sample = y[sample_indices] tree = DecisionTree() tree.fit(X_sample, y_sample) self.forest.append((tree, feature_indices)) def predict(self, X): \"\"\" Make predictions using the trained random forest Args: X (numpy.ndarray): The feature matrix Returns: numpy.ndarray: The predicted labels \"\"\" predictions = np.zeros((X.shape[0], self.n_trees)) for i, (tree, feature_indices) in enumerate(self.forest): X_subset = X[:, feature_indices] predictions[:, i] = tree.predict(X_subset) final_predictions = [np.bincount(pred.astype(int)).argmax() for pred in predictions] return np.array(final_predictions)", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import requests import os def main(): st.title(\"Image Upload and Inference\") # Get the API URL from an environment variable api_url = os.getenv('API_URL') if not api_url: st.error(\"API_URL environment variable is not set. Please set it in your RunAI job submission.\") return uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"]) if uploaded_file is not None: # Save the uploaded file temporarily with open(\"temp_image.png\", \"wb\") as f: f.write(uploaded_file.getbuffer()) if st.button(\"Run Inference\"): # Prepare the file for the request files = { 'image_file': ('image.png', open('temp_image.png', 'rb'), 'image/png') } # Make the API request try: response = requests.post(api_url, files=files) if response.status_code == 200: st.success(\"Inference completed successfully!\") st.json(response.json()) else: st.error(f\"Error occurred: {response.status_code}\") st.text(response.text) except requests.exceptions.RequestException as e: st.error(f\"Failed to connect to the API: {e}\") # Clean up the temporary file os.remove(\"temp_image.png\") if __name__ == \"__main__\": main()", "source": "app.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import RobustScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from imblearn.over_sampling import SMOTE class DataPipeline: def __init__(self): self.preprocessor = None self.float_columns = None def log_transform(self, X: np.ndarray, epsilon=1e-8) -> np.ndarray: return np.log(X + 1 + epsilon) def create_preprocessor(self, float_columns: list) -> ColumnTransformer: return ColumnTransformer( transformers=[ ('num', Pipeline([('scaler', RobustScaler())]), float_columns) ]) def resample_data(self, X_train: pd.DataFrame, y_train: pd.DataFrame) -> tuple: smote = SMOTE(random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) return X_train_resampled, y_train_resampled def transform_train_data(self, train_data_path: str) -> tuple: \"\"\" Loads and preprocesses the training data. :param train_data_path: str, path to the training data CSV file :return: tuple, (X_train, y_train) preprocessed features and labels as numpy arrays \"\"\" # Load data df = pd.read_csv(train_data_path) # Separate features and target X = df.drop('Class', axis=1) y = df['Class'] # Identify float columns self.float_columns = X.select_dtypes(include=['float64']).columns.tolist() # Apply SMOTE to training data X_train_resampled, y_train_resampled = self.resample_data(X, y) # Create and fit preprocessor self.preprocessor = self.create_preprocessor(self.float_columns) X_preprocessed = self.preprocessor.fit_transform(X_train_resampled) print(\"Training dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_train_resampled, return_counts=True)) return X_preprocessed, y_train_resampled def transform_test_data(self, test_data_path) -> tuple: \"\"\" Loads and preprocesses the test data using the preprocessor fit on the training data. :param test_data_path: str, path to the test data CSV file :return: tuple, (X_test, y_test) preprocessed features and labels as numpy arrays \"\"\" if self.preprocessor is None: raise ValueError(\"Preprocessor not fitted. Run transform_train_data first.\") # Load data df = pd.read_csv(test_data_path) # Separate features and target X = df.drop('Class', axis=1) y = df['Class'] # Apply preprocessing X_preprocessed = self.preprocessor.transform(X) # Convert y to numpy array y_array = y.to_numpy() print(\"Test dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_array, return_counts=True)) return X_preprocessed, y_array", "source": "datapipeline.py"}, {"content": "import pandas as pd import pyodbc import os from dotenv import load_dotenv load_dotenv() server = os.getenv('SERVER') database = os.getenv('DATABASE') username = os.getenv('USER') password = os.getenv('PASSWORD') driver = os.getenv('DRIVER') class ImportData: \"\"\" Class to encapsulate sql query responses, parse and return dataframes \"\"\" def __init__(self, data_location: str): \"\"\" instantiate object to connect to sql engine Args: data_location (string): location of the csv file \"\"\" self.data_location = data_location self.conn = pyodbc.connect( f'DRIVER={driver};SERVER={server};PORT=1433;DATABASE={database};UID={username};PWD={password}') def return_table(self, table: str) -> pd.DataFrame: \"\"\" Returns the datasets found in the SQL connection Args: table (string): name of table to query Returns: articles (dataframe) : Returns dataframe with from articles table \"\"\" if(os.path.exists(self.data_location)): print(\"File already exists! Continuing...\") raw_df = pd.read_csv(self.data_location) return raw_df else: print(\"File not found! Downloading...\") query = \"SELECT * FROM \" + table query_table = pd.read_sql(query, self.conn) os.makedirs(os.path.dirname(self.data_location), exist_ok=True) query_table.to_csv(self.data_location, index=False) self.conn.close() return query_table", "source": "extract.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): # Initialize weights with Xavier initialization self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size) self.b1 = np.zeros((1, hidden_size)) self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size) self.b2 = np.zeros((1, output_size)) # Learning rate self.learning_rate = 1e-3 def forward(self, features: np.ndarray) -> np.ndarray: \"\"\" Takes in the features returns the prediction \"\"\" self.features = features # Ensure features is a 2D array when passing in one sample for test case. if self.features.ndim == 1: self.features = self.features.reshape(1, -1) # hidden layer # y = wx + b ,transpose 'features' to be able to dot product with W1 which is n_h by n_x dimensions () self.Z1 = self.features.dot(self.W1) + self.b1 # activation function (Tanh) self.A1 = np.tanh(self.Z1) # self.A1 = np.maximum(0, self.Z1) # activation function (ReLU) # output layer self.Z2 = self.A1.dot(self.W2) + self.b2 # Softmax activation since its multiclass (3 class) # axis=1 means sum over rows exp_scores = np.exp(self.Z2 - np.max(self.Z2, axis=1, keepdims=True)) self.A2 = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) return self.A2 def loss(self, predictions: np.ndarray, label: int) -> float: \"\"\" Takes in the predictions and label returns the training loss \"\"\" self.label = label # Use cross-entropy loss return -np.log(predictions[0][self.label]).item() def backward(self): \"\"\" Adjusts the internal weights/biases Assumes that forward() has been called before backward() \"\"\" # Backpropagation for output layer delta2 = self.A2.copy() # Subtract 1 to the actual class's predicted probability. # (gradient of loss with respect to pre-activation output z2 before softmax (loss/z2)) delta2[0][self.label] -= 1 # Gradients for weights and biases of the output layer # Output: 16x3 matrix (gradients of the loss with respect to the weights connecting # the hidden layer to the output layer (loss/W2)) dw2 = np.dot(self.A1.T, delta2) # 1x3 matrix (Bias) db2 = np.sum(delta2, axis=0, keepdims=True) # Backpropagation for hidden layer delta1 = np.dot(delta2, self.W2.T) * (1 - self.A1 ** 2) # Gradients for weights and biases of the first layer dw1 = np.dot(self.features.T, delta1) db1 = np.sum(delta1, axis=0, keepdims=True) # Update weights and biases self.W1 -= self.learning_rate * dw1 self.b1 -= self.learning_rate * db1 self.W2 -= self.learning_rate * dw2 self.b2 -= self.learning_rate * db2", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer class Datapipeline: def __init__(self): self.preprocessor = None self.columns = None def create_preprocessor(self, columns: list) -> ColumnTransformer: return ColumnTransformer( transformers=[ ('num', Pipeline([('scaler', StandardScaler())]), columns) ]) def transform(self, data_path: str) -> tuple: \"\"\" Loads and preprocesses the training data. :param train_data_path: str, path to the training data CSV file :return: tuple, (X_train, y_train) preprocessed features and labels as numpy arrays \"\"\" df = pd.read_csv(data_path) # Separate features and target X = df.drop(['y', 'id'], axis=1) y = df['y'] # Identify columns self.columns = X.columns.tolist() # Create and fit preprocessor self.preprocessor = self.create_preprocessor(self.columns) X_preprocessed = self.preprocessor.fit_transform(X) # Convert y to numpy array y_array = y.to_numpy() print(\"Dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_array, return_counts=True)) return X_preprocessed, y_array", "source": "mlp_datapipeline.py"}, {"content": "import abc from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, BatchNormalization from tensorflow.keras.callbacks import EarlyStopping from sklearn.metrics import classification_report, confusion_matrix import seaborn as sns import matplotlib.pyplot as plt import mlflow class BaseModel(abc.ABC): def __init__(self, input_dim: int): self.model = Sequential() self.input_dim = input_dim @abc.abstractmethod def build_model(self): pass def compile_model(self): self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) def log_params(self, params): mlflow.log_params(params) def log_metrics(self, metrics): mlflow.log_metrics(metrics) def train(self, X_train: np.ndarray, y_train: np.ndarray, epochs: int=20, batch_size: int=32, validation_split: float=0.2): history = self.model.fit( X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split) return history def evaluate(self, X_test: np.ndarray, y_test: np.ndarray) -> tuple: test_loss, test_accuracy = self.model.evaluate(X_test, y_test) print(f\"Test Loss: {test_loss}\") print(f\"Test Accuracy: {test_accuracy}\") return test_loss, test_accuracy def predict(self, X: np.ndarray) -> np.ndarray: return (self.model.predict(X) > 0.5).astype(\"int32\") def classification_report(self, y_true: np.ndarray, y_pred: np.ndarray): print(\"Classification Report:\") print(classification_report(y_true, y_pred)) def plot_confusion_matrix(self, y_true: np.ndarray, y_pred: np.ndarray): conf_matrix = confusion_matrix(y_true, y_pred) plt.figure(figsize=(10, 7)) sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Oranges') plt.xlabel('Predicted') plt.ylabel('Actual') plt.title('Confusion Matrix') plt.show() class InitialModel(BaseModel): def __init__(self, input_dim: int=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, input_dim=self.input_dim, activation='relu')) # Input layer self.model.add(Dense(units=64, activation='relu')) # Hidden layer 1 self.model.add(Dense(units=32, activation='relu')) # Hidden layer 2 self.model.add(Dense(units=16, activation='relu')) # Hidden layer 3 self.model.add(Dense(units=1, activation='sigmoid')) # Output layer class BatchNormalizatioModel(BaseModel): def __init__(self, input_dim: int=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, activation='relu', input_dim=self.input_dim)) self.model.add(BatchNormalization()) self.model.add(Dense(units=64, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dense(units=32, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dense(units=16, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dense(units=1, activation='sigmoid')) class EluModel(BaseModel): def __init__(self, input_dim: int=29): super().__init__(input_dim) self.build_model() self.compile_model() def build_model(self): self.model.add(Dense(units=29, activation='elu', input_dim=self.input_dim)) self.model.add(Dense(units=64, activation='elu')) self.model.add(Dense(units=32, activation='elu')) self.model.add(Dense(units=16, activation='elu')) self.model.add(Dense(units=1, activation='sigmoid')) class FinalModel(BaseModel): def __init__(self, input_dim: int=29): super().__init__(input_dim) self.build_model() self.compile_model() self.callbacks = None self.add_callback() def build_model(self): self.model.add(Dense(units=29, activation='relu', input_dim=self.input_dim)) self.model.add(BatchNormalization()) self.model.add(Dropout(0.2)) self.model.add(Dense(units=64, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dropout(0.5)) self.model.add(Dense(units=32, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dropout(0.5)) self.model.add(Dense(units=16, activation='relu')) self.model.add(BatchNormalization()) self.model.add(Dropout(0.5)) self.model.add(Dense(units=1, activation='sigmoid')) def train(self, x_train, y_train, epochs=20, batch_size=32, validation_split=0.2, use_early_stopping=True, patience=5): \"\"\" Train the model with optional early stopping. Args: - x_train: Training data. - y_train: Training labels. - epochs: Number of epochs to train the model. - batch_size: Number of samples per gradient update. - validation_split: Fraction of the training data to be used as validation data. - use_early_stopping: Boolean flag to use early stopping. - patience: Number of epochs with no improvement after which training will be stopped. Returns: - history: A History object. Its History.history attribute is a record of training loss values and metrics values at successive epochs, as well as validation loss values and validation metrics values (if applicable). \"\"\" callbacks = [] if use_early_stopping: early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True) callbacks.append(early_stopping) history = self.model.fit( x_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=callbacks ) return history", "source": "models.py"}, {"content": "import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from sklearn.model_selection import train_test_split from sklearn.metrics import roc_auc_score import random import numpy as np import os def set_seed(seed_value): \"\"\"Set seed for reproducibility.\"\"\" os.environ['PYTHONHASHSEED'] = str(seed_value) random.seed(seed_value) np.random.seed(seed_value) torch.manual_seed(seed_value) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False if torch.cuda.is_available(): torch.cuda.manual_seed(seed_value) torch.cuda.manual_seed_all(seed_value) class NeuralNetwork(nn.Module): def __init__(self, input_dim: int=29): \"\"\" Initializes a NeuralNetwork instance with a specified input dimension. Parameters: input_dim (int): The number of input features. Defaults to 29. Returns: None \"\"\" super(NeuralNetwork, self).__init__() self.layers = nn.Sequential( nn.Linear(input_dim, 256), nn.ReLU(), nn.BatchNorm1d(256), nn.Dropout(0.2), nn.Linear(256, 128), nn.ReLU(), nn.BatchNorm1d(128), nn.Dropout(0.5), nn.Linear(128, 64), nn.ReLU(), nn.BatchNorm1d(64), nn.Dropout(0.5), nn.Linear(64, 32), nn.ReLU(), nn.BatchNorm1d(32), nn.Dropout(0.5), nn.Linear(32, 16), nn.ReLU(), nn.BatchNorm1d(16), nn.Dropout(0.5), nn.Linear(16, 1), nn.Sigmoid() ) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Defines the forward pass of the neural network. Parameters: x: The input tensor to be processed by the network. Returns: The output tensor of the network. \"\"\" return self.layers(x) def predict(self, X: np.ndarray) -> np.ndarray: \"\"\" Predicts the output for a given input. Parameters: X (numpy array): The input features to predict. Returns: numpy array: The predicted output for the given input. \"\"\" self.eval() # Set the model to evaluation mode with torch.no_grad(): X_tensor = torch.FloatTensor(X) outputs = self(X_tensor) return outputs.numpy() def evaluate(self, X: np.ndarray, y: np.ndarray) -> tuple: \"\"\" Evaluates the performance of the neural network model on a given dataset. Parameters: X (numpy array): The input features to evaluate. y (numpy array): The corresponding labels for the input features. Returns: tuple: A tuple containing the loss value and the area under the ROC curve (AUC) of the model on the given dataset. \"\"\" self.eval() with torch.no_grad(): X_tensor = torch.FloatTensor(X) y_tensor = torch.FloatTensor(y) outputs = self(X_tensor) loss = nn.BCELoss()(outputs, y_tensor.unsqueeze(1)) auc = roc_auc_score(y, outputs.numpy()) return loss.item(), auc def train_model(model: nn.Module, X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray, epochs: int=20, batch_size: int=32, seed: int=None) -> tuple: \"\"\" Trains a given PyTorch model using the provided training data and hyperparameters. Parameters: model: The PyTorch model to be trained. X_train (numpy array): The input features for training. y_train (numpy array): The corresponding labels for the training input features. X_val (numpy array): The input features for validation. y_val (numpy array): The corresponding labels for the validation input features. epochs (int, optional): The number of epochs to train the model. Defaults to 20. batch_size (int, optional): The batch size for training. Defaults to 32. seed (int, optional): The random seed for reproducibility. Defaults to None. Returns: tuple: A tuple containing the trained model and a dictionary with training history. \"\"\" if seed is not None: set_seed(seed) # Convert data to PyTorch tensors X_train_tensor = torch.FloatTensor(X_train) y_train_tensor = torch.FloatTensor(y_train) # Create DataLoader train_dataset = TensorDataset(X_train_tensor, y_train_tensor) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed) if seed is not None else None) # Define loss function and optimizer criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001) # weight_decay applies L2 regularization # Early stopping setup best_val_loss = float('inf') # set to infinity first so any computed val_loss will become best for 1st round patience = 5 counter = 0 # History", "source": "pytorch_A4P2.py"}, {"content": "dictionary history = { 'loss': [], 'val_loss': [], 'val_auc': [] } # Training loop for epoch in range(epochs): model.train() # Switches to training mode epoch_loss = 0 for batch_X, batch_y in train_loader: # For each batch of input , target optimizer.zero_grad() # gradients in pytorch accumulate by default, so clear gradient from each step before new backward pass outputs = model(batch_X) # forward pass with output shape (batch_size, 1) loss = criterion(outputs, batch_y.unsqueeze(1)) # predicted outputs compared to label for loss calculation. unsqueeze batch_y to match output. https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch loss.backward() # backpropagation (calculates the gradients of the loss with respect to weights) optimizer.step() # updates model parameters using computed gradients and move towrads minimising loss function epoch_loss += loss.item() # accumulates total loss for epoch # Calculate average epoch loss avg_epoch_loss = epoch_loss / len(train_loader) # average the loss per batch for current epoch history['loss'].append(avg_epoch_loss) # Validation val_loss, val_auc = model.evaluate(X_val, y_val) # compute evaluation metric on validation_data history['val_loss'].append(val_loss) history['val_auc'].append(val_auc) print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}') # Early stopping check if val_loss < best_val_loss: best_val_loss = val_loss counter = 0 # rest counter to 0 when val_loss is < best_val loss torch.save(model.state_dict(), 'best_model.pt') else: counter += 1 if counter >= patience: # when counter > = patience then early stop and load the last model when counter = 0 print(\"Early stopping\") model.load_state_dict(torch.load('best_model.pt')) break return model, history def print_final_metrics(history): print(\"\\nFinal training metrics:\") print(f\"Loss: {history['loss'][-1]:.4f}\") print(f\"Validation Loss: {history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history['val_auc'][-1]:.4f}\") def evaluate_model(model, X_test, y_test): test_loss, test_auc = model.evaluate(X_test, y_test) print(f\"Test Loss: {test_loss:.4f}\") print(f\"Test AUC: {test_auc:.4f}\") return test_loss, test_auc", "source": "pytorch_A4P2.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A4.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100] ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import time import logging import logging.config import yaml import mlflow logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) if \"MLFLOW_HPTUNING_TAG\" in os.environ: mlflow.set_tag(\"hptuning_tag\", os.environ.get(\"MLFLOW_HPTUNING_TAG\")) if \"JOB_UUID\" in os.environ: mlflow.set_tag(\"job_uuid\", os.environ.get(\"JOB_UUID\")) mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import os import ast import random import logging import numpy as np import tensorflow as tf from tensorflow.keras import metrics from tensorflow.keras import layers, models from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from omegaconf import DictConfig import hydra import mlflow import wandb from wandb.integration.keras import WandbMetricsLogger import general_utils # type: ignore from a5p2.constants import PREPROCESS_INPUT_MAP from a5p2.metrics import CustomF1Score, CustomPrecision, CustomRecall # Define a class for the data pipeline class DataPipeline: \"\"\"Data pipeline class for loading\"\"\" def __init__(self, data_dir: str, img_size: tuple, batch_size: int, validation_split: float, seed: int, preprocess_func: callable = None, augmentations: callable = None): self.data_dir = data_dir self.img_size = img_size self.batch_size = batch_size self.validation_split = validation_split self.seed = seed self.preprocess_func = preprocess_func self.augmentations = augmentations def load_data(self): \"\"\"Loads the data from the directory provided and returns the train and validation datasets.\"\"\" train_ds = tf.keras.preprocessing.image_dataset_from_directory( self.data_dir, validation_split=self.validation_split, subset=\"training\", seed=self.seed, image_size=self.img_size, batch_size=self.batch_size ) val_ds = tf.keras.preprocessing.image_dataset_from_directory( self.data_dir, validation_split=self.validation_split, subset=\"validation\", seed=self.seed, image_size=self.img_size, batch_size=self.batch_size ) if self.preprocess_func: train_ds = train_ds.map(self.preprocess_func, num_parallel_calls=tf.data.AUTOTUNE) val_ds = val_ds.map(self.preprocess_func, num_parallel_calls=tf.data.AUTOTUNE) if self.augmentations: def augment(image, label): image = self.augmentations(image, training=True) return image, label train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE) # Optimize the pipeline performance train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE) return train_ds, val_ds # Define a class for building different models class ModelBuilder: \"\"\"Builds a model based on the model name and configuration provided.\"\"\" def __init__(self, model_name: str, input_shape: tuple, num_classes: int, hidden_layers: list = None, activation: str = 'relu', batch_norm: bool = True, dropout: bool = True, dropout_rate: float = 0.5, base_model_trainable: bool = False, optimizer_name: str = 'adamw', learning_rate: float = 0.001, top_k: int = 5): if hidden_layers is None: hidden_layers = [512, 256] self.model_name = model_name self.input_shape = input_shape self.num_classes = num_classes self.base_model_trainable = base_model_trainable self.optimizer_name = optimizer_name self.learning_rate = learning_rate self.top_k = top_k self.hidden_layers = hidden_layers self.activation = activation self.batch_norm = batch_norm self.dropout = dropout self.dropout_rate = dropout_rate def build(self): \"\"\"Builds a model based on the model name and configuration provided.\"\"\" base_model = self._load_base_model(input_shape=self.input_shape) inputs = layers.Input(shape=self.input_shape) # Base model extraction x = base_model(inputs, training=False) x = layers.GlobalAveragePooling2D()(x) # Fully-connected layers for units in self.hidden_layers: x = layers.Dense(units)(x) if self.batch_norm: x = layers.BatchNormalization()(x) x = layers.Activation(self.activation)(x) if self.dropout: x = layers.Dropout(self.dropout_rate)(x) # Output layer outputs = layers.Dense(self.num_classes, activation='softmax')(x) # Build and compile model model = models.Model(inputs=inputs, outputs=outputs) model.compile( optimizer=self._get_optimizer(), loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=[ metrics.SparseCategoricalAccuracy(name='sparse_categorical_accuracy'), metrics.SparseTopKCategoricalAccuracy(k=self.top_k, name='sparse_top_k_categorical_accuracy'), CustomPrecision(), CustomRecall(), CustomF1Score() ] ) return model def _get_optimizer(self): \"\"\"Returns the optimizer instance based on the optimizer name.\"\"\" if self.optimizer_name.lower() == 'adam': return tf.keras.optimizers.Adam(self.learning_rate) elif self.optimizer_name.lower() == 'adamw': return tf.keras.optimizers.AdamW(self.learning_rate) elif self.optimizer_name.lower() == 'sgd': return tf.keras.optimizers.SGD(self.learning_rate) elif self.optimizer_name.lower() == 'rmsprop': return tf.keras.optimizers.RMSprop(self.learning_rate) else: raise ValueError(f\"Unsupported optimizer: {self.optimizer_name}\") def _load_base_model(self, weights: str = 'imagenet', include_top: bool = False, input_shape: tuple = None) -> tf.keras.Model: \"\"\"Loads the specified base model without the top layers.\"\"\" try: if self.model_name == 'EfficientNetB7': base_model = tf.keras.applications.EfficientNetB7( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'EfficientNetB0': base_model = tf.keras.applications.EfficientNetB0( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'EfficientNetV2B0': base_model = tf.keras.applications.EfficientNetV2B0( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'EfficientNetV2S': base_model = tf.keras.applications.EfficientNetV2S( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'EfficientNetV2L': base_model = tf.keras.applications.EfficientNetV2L( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'ConvNeXtXLarge': base_model = tf.keras.applications.ConvNeXtXLarge( weights=weights,", "source": "train_model.py"}, {"content": "include_top=include_top, input_shape=input_shape ) elif self.model_name == 'ConvNeXtTiny': base_model = tf.keras.applications.ConvNeXtTiny( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'MobileNetV2': base_model = tf.keras.applications.MobileNetV2( weights=weights, include_top=include_top, input_shape=input_shape ) elif self.model_name == 'ResNet50V2': base_model = tf.keras.applications.ResNet50V2( weights=weights, include_top=include_top, input_shape=input_shape ) except Exception: raise ValueError(f\"Model {self.model_name} is not supported yet.\") # Set base model layers to be trainable or not base_model.trainable = self.base_model_trainable return base_model def get_augmentation_layers(cfg): \"\"\"Create a Keras Sequential model containing augmentation layers based on the configuration.\"\"\" augmentation_layers = [] # Define a mapping of augmentation configurations to Keras layers augmentation_mapping = { \"random_flip_left_right\": lambda: tf.keras.layers.RandomFlip(\"horizontal\"), \"random_flip_up_down\": lambda: tf.keras.layers.RandomFlip(\"vertical\"), \"random_rotation\": lambda: tf.keras.layers.RandomRotation( factor=cfg.augmentations.rotation_factor, fill_mode='nearest' ), \"random_zoom\": lambda: tf.keras.layers.RandomZoom( height_factor=cfg.augmentations.zoom_factor, width_factor=cfg.augmentations.zoom_factor, fill_mode='nearest' ), \"random_brightness\": lambda: tf.keras.layers.RandomBrightness( factor=cfg.augmentations.brightness_factor ), \"random_contrast\": lambda: tf.keras.layers.RandomContrast( factor=cfg.augmentations.contrast_factor ), \"random_crop\": lambda: tf.keras.layers.RandomCrop( height=cfg.augmentations.crop_size[0], width=cfg.augmentations.crop_size[1] ) # Add more augmentation layers as needed based on configuration } # Apply both horizontal and vertical flips if both are enabled if cfg.augmentations.random_flip_left_right and cfg.augmentations.random_flip_up_down: augmentation_layers.append(tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")) else: # Apply individual flips based on configuration if cfg.augmentations.random_flip_left_right: augmentation_layers.append(augmentation_mapping[\"random_flip_left_right\"]()) if cfg.augmentations.random_flip_up_down: augmentation_layers.append(augmentation_mapping[\"random_flip_up_down\"]()) # Apply other augmentations based on configuration for aug_name, aug_layer in augmentation_mapping.items(): if getattr(cfg.augmentations, aug_name, False) and aug_name not in [\"random_flip_left_right\", \"random_flip_up_down\"]: augmentation_layers.append(aug_layer()) assert augmentation_layers, \"No augmentation layers found. Please enable at least one augmentation.\" return tf.keras.Sequential(augmentation_layers) def augmentations(image: tf.Tensor, label: tf.Tensor, cfg: DictConfig) -> tuple[tf.Tensor, tf.Tensor]: \"\"\"Applies data augmentations to the image.\"\"\" augmentations_map = { \"random_flip_left_right\": lambda img: tf.image.random_flip_left_right(img), \"random_flip_up_down\": lambda img: tf.image.random_flip_up_down(img), \"random_brightness\": lambda img: tf.image.random_brightness(img, max_delta=float(cfg.augmentations.brightness_max_delta)), \"random_contrast\": lambda img: tf.image.random_contrast(img, lower=float(cfg.augmentations.contrast_lower), upper=float(cfg.augmentations.contrast_upper)), \"random_rotation\": lambda img: tf.image.rot90(img, k=int(cfg.augmentations.rotation_k)), \"random_shear\": lambda img: tf.keras.preprocessing.image.random_shear(img, intensity=cfg.augmentations.shear_intensity), \"random_shift\": lambda img: tf.keras.preprocessing.image.random_shift(img, wrg=cfg.augmentations.shift_wrg, hrg=cfg.augmentations.shift_hrg), \"random_crop\": lambda img: tf.image.random_crop(img, size=cfg.augmentations.crop_size), \"central_crop\": lambda img: tf.image.central_crop(img, central_fraction=cfg.augmentations.central_fraction) } for aug_name, aug_func in augmentations_map.items(): if getattr(cfg.augmentations, aug_name, False): image = aug_func(image) if not image: raise ValueError(\"Cannot apply augmentations. Can't find valid augmentations.\") # Resize the image back to the target size target_size = ast.literal_eval(cfg.model.image_size) image = tf.image.resize(image, target_size, method=tf.image.ResizeMethod.BILINEAR) # Ensure the image has 3 channels if image.shape[-1] != 3: image = tf.image.grayscale_to_rgb(image) return image, label def get_callbacks(checkpoint_dir_path: str = './models', model_name: str = 'best.keras', use_early_stopping: bool = True, early_stopping_patience: int = 15) -> list: \"\"\"Creates callbacks for training.\"\"\" callbacks = [] if use_early_stopping: early_stopping = EarlyStopping( monitor='val_loss', patience=early_stopping_patience, restore_best_weights=True ) callbacks.append(early_stopping) checkpoint_filepath = os.path.join(checkpoint_dir_path, f\"{model_name}.keras\") checkpoint = ModelCheckpoint(checkpoint_filepath, monitor='val_accuracy', save_best_only=True) callbacks.append(checkpoint) # Add Wandb callback if logged in if wandb.api.api_key: callbacks.append(WandbMetricsLogger()) return callbacks def train(model: tf.keras.Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, epochs: int = 20, callbacks: list = None) -> tf.keras.callbacks.History: \"\"\"Trains the model and returns training history.\"\"\" history = model.fit( train_ds, validation_data=val_ds, epochs=epochs, verbose=1, callbacks=callbacks ) return history @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def train_model(cfg: DictConfig): \"\"\" Train a model with Hydra configuration and log the experiment using MLFlow. Parameters: cfg (DictConfig): Configuration object provided by Hydra. \"\"\" # Initialize logger logger = logging.getLogger(__name__) logger.info(\"Initializing experiment...\") # Setup logging using `general_utils` general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) # Initialize MLflow mlflow_init_status, mlflow_run = general_utils.mlflow_init( cfg, setup_mlflow=cfg.setup_mlflow, autolog=cfg.mlflow_autolog ) # Log initial parameters using MLflow initial_params = { \"epochs\": cfg.training.epochs, \"batch_size\": cfg.training.batch_size, \"input_shape\": cfg.model.input_shape, \"num_classes\": cfg.model.num_classes, \"model_name\": cfg.model.model_name, \"loss\": cfg.training.loss, \"random_seed\": cfg.training.random_seed, \"hidden_layers\": cfg.model.hidden_layers, \"activation\": cfg.model.activation, \"dropout\": cfg.model.use_dropout, \"dropout_rate\": cfg.model.dropout_rate, \"batch_norm\": cfg.model.use_batch_norm, \"base_model_trainable\": cfg.model.base_model_trainable,", "source": "train_model.py"}, {"content": "\"optimizer\": cfg.training.optimizer, \"learning_rate\": cfg.training.learning_rate, \"early_stopping\": cfg.training.use_early_stopping, \"early_stopping_patience\": cfg.training.early_stopping_patience, } general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=initial_params, ) # Log augmentation parameters using MLflow augmentations_params = { \"random_flip_left_right\": cfg.augmentations.random_flip_left_right, \"random_flip_up_down\": cfg.augmentations.random_flip_up_down, \"random_brightness\": cfg.augmentations.random_brightness, \"brightness_max_delta\": cfg.augmentations.brightness_max_delta, \"random_contrast\": cfg.augmentations.random_contrast, \"contrast_lower\": cfg.augmentations.contrast_lower, \"contrast_upper\": cfg.augmentations.contrast_upper, \"random_rotation\": cfg.augmentations.random_rotation, \"rotation_k\": cfg.augmentations.rotation_k, \"random_shear\": cfg.augmentations.random_shear, \"shear_intensity\": cfg.augmentations.shear_intensity, \"random_shift\": cfg.augmentations.random_shift, \"shift_wrg\": cfg.augmentations.shift_wrg, \"shift_hrg\": cfg.augmentations.shift_hrg, \"random_crop\": cfg.augmentations.random_crop, \"crop_size\": cfg.augmentations.crop_size, \"central_crop\": cfg.augmentations.central_crop, \"central_fraction\": cfg.augmentations.central_fraction, } general_utils.mlflow_log( mlflow_init_status, \"log_params\", params=augmentations_params, ) # Log parameters to wandb if wandb.api.api_key: wandb.init(project='Singapore Food Classification', name='train-model') wandb.config.update(initial_params) # Set random seed for reproducibility os.environ[\"PYTHONHASHSEED\"] = str(cfg.training.random_seed) tf.random.set_seed(cfg.training.random_seed) np.random.seed(cfg.training.random_seed) random.seed(cfg.training.random_seed) # Initialize data pipeline preprocess_input_fn = PREPROCESS_INPUT_MAP.get(cfg.model.model_name) if preprocess_input_fn is None: raise ValueError(f\"Preprocess function for model {cfg.model.model_name} not found.\") augmentation_layers = get_augmentation_layers(cfg) # Augmentation layers pipeline = DataPipeline( data_dir=cfg.data_dir_path, img_size=ast.literal_eval(cfg.model.image_size), batch_size=cfg.training.batch_size, validation_split=cfg.data.validation_split, seed=cfg.training.random_seed, preprocess_func=lambda img, lbl: (preprocess_input_fn(img), lbl), # Preprocessingg augmentations=augmentation_layers ) train_ds, val_ds = pipeline.load_data() # Build model model_builder = ModelBuilder( model_name=cfg.model.model_name, input_shape=tuple(ast.literal_eval(cfg.model.input_shape)), num_classes=cfg.model.num_classes, hidden_layers=cfg.model.hidden_layers, activation=cfg.model.activation, batch_norm=cfg.model.use_batch_norm, dropout=cfg.model.use_dropout, dropout_rate=cfg.model.dropout_rate, base_model_trainable=cfg.model.base_model_trainable, optimizer_name=cfg.training.optimizer, learning_rate=cfg.training.learning_rate, top_k=cfg.metrics.top_k, ) model = model_builder.build() model.summary() # Calculate and log the total number of parameters, trainable parameters, and non-trainable parameters total_params = model.count_params() trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights]) non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights]) mlflow.log_param(\"total_params\", total_params) mlflow.log_param(\"trainable_params\", trainable_params) mlflow.log_param(\"non_trainable_params\", non_trainable_params) # Log parameters to wandb if wandb.api.api_key: wandb.config.update({ \"total_params\": total_params, \"trainable_params\": trainable_params, \"non_trainable_params\": non_trainable_params }) # Get callbacks for early stopping and checkpointing callbacks = get_callbacks( checkpoint_dir_path=cfg.model_checkpoint_dir_path, model_name=cfg.model.model_name, use_early_stopping=cfg.training.use_early_stopping, early_stopping_patience=cfg.training.early_stopping_patience) # Train the model and track history history = train( model=model, train_ds=train_ds, val_ds=val_ds, epochs=cfg.training.epochs, callbacks=callbacks ) # Log the stopped_epoch from EarlyStopping callback to MLflow for callback in callbacks: if isinstance(callback, EarlyStopping): mlflow.log_metric(\"stopped_epoch\", callback.stopped_epoch) # log to wandb also if wandb.api.api_key: wandb.config.update({\"stopped_epoch\": callback.stopped_epoch}) break # Evaluate the model on the validation set evaluation_results = model.evaluate(val_ds) # Log only the final epoch scores of validation metrics to MLflow metric_names = [\"loss\", \"sparse_categorical_accuracy\", \"sparse_top_k_categorical_accuracy\", \"precision\", \"recall\", \"f1_score\"] metrics_dict = dict(zip(metric_names, evaluation_results)) for metric_name, metric_value in zip(metric_names, evaluation_results): mlflow.log_metric(f\"{metric_name}\", metric_value) if wandb.api.api_key: wandb.log({f\"{metric_name}\": metric_value}) # Log the model to MLflow mlflow.keras.log_model(model, \"model\") # Finalize MLflow logging if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") # End the wandb run if wandb.api.api_key: wandb.finish() return metrics_dict['loss'], metrics_dict['f1_score'] if __name__ == \"__main__\": train_model()", "source": "train_model.py"}, {"content": "from tensorflow.keras import applications PREPROCESS_INPUT_MAP = { \"ConvNeXtBase\": applications.convnext.preprocess_input, \"ConvNeXtLarge\": applications.convnext.preprocess_input, \"ConvNeXtSmall\": applications.convnext.preprocess_input, \"ConvNeXtTiny\": applications.convnext.preprocess_input, \"ConvNeXtXLarge\": applications.convnext.preprocess_input, \"DenseNet121\": applications.densenet.preprocess_input, \"DenseNet169\": applications.densenet.preprocess_input, \"DenseNet201\": applications.densenet.preprocess_input, \"EfficientNetB0\": applications.efficientnet.preprocess_input, \"EfficientNetB1\": applications.efficientnet.preprocess_input, \"EfficientNetB2\": applications.efficientnet.preprocess_input, \"EfficientNetB3\": applications.efficientnet.preprocess_input, \"EfficientNetB4\": applications.efficientnet.preprocess_input, \"EfficientNetB5\": applications.efficientnet.preprocess_input, \"EfficientNetB6\": applications.efficientnet.preprocess_input, \"EfficientNetB7\": applications.efficientnet.preprocess_input, \"EfficientNetV2B0\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B1\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B2\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2B3\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2L\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2M\": applications.efficientnet_v2.preprocess_input, \"EfficientNetV2S\": applications.efficientnet_v2.preprocess_input, \"InceptionResNetV2\": applications.inception_resnet_v2.preprocess_input, \"InceptionV3\": applications.inception_v3.preprocess_input, \"MobileNet\": applications.mobilenet.preprocess_input, \"MobileNetV2\": applications.mobilenet_v2.preprocess_input, \"MobileNetV3Large\": applications.mobilenet_v3.preprocess_input, \"MobileNetV3Small\": applications.mobilenet_v3.preprocess_input, \"NASNetLarge\": applications.nasnet.preprocess_input, \"NASNetMobile\": applications.nasnet.preprocess_input, \"ResNet50\": applications.resnet.preprocess_input, \"ResNet101\": applications.resnet.preprocess_input, \"ResNet152\": applications.resnet.preprocess_input, \"ResNet50V2\": applications.resnet_v2.preprocess_input, \"ResNet101V2\": applications.resnet_v2.preprocess_input, \"ResNet152V2\": applications.resnet_v2.preprocess_input, \"VGG16\": applications.vgg16.preprocess_input, \"VGG19\": applications.vgg19.preprocess_input, \"Xception\": applications.xception.preprocess_input, }", "source": "constants.py"}, {"content": "import tensorflow as tf from tensorflow.keras import backend as K class CustomRecall(tf.keras.metrics.Metric): def __init__(self, name='recall', num_classes=12, **kwargs): super(CustomRecall, self).__init__(name=name, **kwargs) self.num_classes = num_classes self.true_positives = self.add_weight(name='true_positives', initializer='zeros') self.all_positives = self.add_weight(name='all_positives', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes) # Convert y_true to one-hot y_pred = tf.cast(y_pred, tf.float32) # Ensure y_pred is float32 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) all_positives = K.sum(K.round(K.clip(y_true, 0, 1))) self.true_positives.assign_add(true_positives) self.all_positives.assign_add(all_positives) def result(self): return self.true_positives / (self.all_positives + K.epsilon()) def reset_states(self): self.true_positives.assign(0) self.all_positives.assign(0) class CustomPrecision(tf.keras.metrics.Metric): def __init__(self, name='precision', num_classes=12, **kwargs): super(CustomPrecision, self).__init__(name=name, **kwargs) self.num_classes = num_classes self.true_positives = self.add_weight(name='true_positives', initializer='zeros') self.predicted_positives = self.add_weight(name='predicted_positives', initializer='zeros') def update_state(self, y_true, y_pred, sample_weight=None): y_true = tf.one_hot(tf.cast(y_true, tf.int32), depth=self.num_classes) # Convert y_true to one-hot y_pred = tf.cast(y_pred, tf.float32) # Ensure y_pred is float32 true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1))) self.true_positives.assign_add(true_positives) self.predicted_positives.assign_add(predicted_positives) def result(self): return self.true_positives / (self.predicted_positives + K.epsilon()) def reset_states(self): self.true_positives.assign(0) self.predicted_positives.assign(0) class CustomF1Score(tf.keras.metrics.Metric): def __init__(self, name='f1_score', num_classes=12, **kwargs): super(CustomF1Score, self).__init__(name=name, **kwargs) self.precision = CustomPrecision(num_classes=num_classes) self.recall = CustomRecall(num_classes=num_classes) def update_state(self, y_true, y_pred, sample_weight=None): self.precision.update_state(y_true, y_pred, sample_weight) self.recall.update_state(y_true, y_pred, sample_weight) def result(self): precision = self.precision.result() recall = self.recall.result() return 2 * ((precision * recall) / (precision + recall + K.epsilon())) def reset_states(self): self.precision.reset_states() self.recall.reset_states()", "source": "metrics.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "# \"\"\" Convolutional Neural Network model. \"\"\" import torch import torch.nn as nn import torch.optim as optim import numpy as np class BaseModel(nn.Module): \"\"\" Base model for neural networks. \"\"\" def __init__(self): \"\"\" Initialize the base model. \"\"\" super(BaseModel, self).__init__() def forward(self, x): \"\"\" Forward pass of the model. \"\"\" raise NotImplementedError(\"Forward method not implemented\") def fit(self, dataloader: torch.utils.data.DataLoader, num_epochs: int=20): \"\"\" Train the model using the provided dataloader. \"\"\" self.train() for epoch in range(num_epochs): total_loss = 0 for batch_features, batch_labels in dataloader: outputs = self(batch_features) loss = self.criterion(outputs, batch_labels[:, -1, :]) self.optimizer.zero_grad() loss.backward() self.optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\") def predict(self, dataloader: torch.utils.data.DataLoader) -> np.ndarray: \"\"\" Generate predictions for the input data using the trained model. \"\"\" self.eval() predictions = [] with torch.no_grad(): for batch_features, _ in dataloader: outputs = self(batch_features) predictions.append(outputs.cpu().numpy()) return np.concatenate(predictions, axis=0) def evaluate(self, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader) -> dict: \"\"\" Evaluate the model using the provided dataloaders. \"\"\" self.eval() def compute_metrics(dataloader): total_mse = 0 total_samples = 0 with torch.no_grad(): for batch_features, batch_labels in dataloader: outputs = self(batch_features) mse = self.criterion(outputs, batch_labels[:, -1, :]) total_mse += mse.item() * batch_features.size(0) total_samples += batch_features.size(0) avg_mse = total_mse / (total_samples if total_samples > 0 else float('inf')) avg_rmse = np.sqrt(avg_mse) return avg_mse, avg_rmse train_mse, train_rmse = compute_metrics(train_dataloader) test_mse, test_rmse = compute_metrics(test_dataloader) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse } class CNNModel(BaseModel): \"\"\" Convolutional Neural Network model. \"\"\" def __init__(self, input_channels: int, sequence_length: int, output_size: int, num_filters: int = 64, kernel_size: int = 3, dropout_rate: float = 0.2): \"\"\" Initialize the CNN model. \"\"\" super(CNNModel, self).__init__() self.conv1 = nn.Conv1d(input_channels, num_filters, kernel_size, padding=kernel_size//2) self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2) self.pool = nn.MaxPool1d(2) self.dropout = nn.Dropout(dropout_rate) # Calculate the size of the flattened features after convolutions and pooling self.flatten_size = self._get_flatten_size(input_channels, sequence_length) self.fc1 = nn.Linear(self.flatten_size, 50) self.fc2 = nn.Linear(50, output_size) self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters(), lr=0.001) def _get_flatten_size(self, input_channels: int, sequence_length: int) -> int: \"\"\" Calculate the size of the flattened features after convolutions and pooling. \"\"\" x = torch.randn(1, input_channels, sequence_length) x = self.conv1(x) if x.size(2) > 1: x = self.pool(x) x = self.conv2(x) if x.size(2) > 1: x = self.pool(x) return x.view(1, -1).size(1) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass of the model. \"\"\" # x shape: (batch_size, sequence_length, input_channels) # Change to (batch_size, input_channels, sequence_length) x = x.permute(0, 2, 1) x = torch.relu(self.conv1(x)) if x.size(2) > 1: x = self.pool(x) x = self.dropout(x) x = torch.relu(self.conv2(x)) if x.size(2) > 1: x = self.pool(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = torch.relu(self.fc1(x)) x = self.fc2(x) return x class CNN2DModel(BaseModel): \"\"\" Convolutional Neural Network (CNN) 2D model for time series forecasting. \"\"\" def __init__(self, input_channels: int, sequence_length: int, output_size: int, num_filters: int = 64, kernel_size: int = 3, dropout_rate: float = 0.2): \"\"\" Initialize the CNN 2D model. \"\"\" super(CNN2DModel, self).__init__() # Override the parent class variables with the child class variables self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding=kernel_size//2) self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2) self.pool = nn.MaxPool2d(2) #padding=kernel_size//2) self.dropout = nn.Dropout(dropout_rate) # Calculate the", "source": "cnn_model.py"}, {"content": "size of the flattened features after convolutions and pooling self.flatten_size = self._get_flatten_size(input_channels, sequence_length) self.fc1 = nn.Linear(self.flatten_size, 50) self.fc2 = nn.Linear(50, output_size) self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters(), lr=0.001) def _get_flatten_size(self, input_channels: int, sequence_length: int) -> int: \"\"\" Calculate the size of the flattened features after convolutions and pooling. \"\"\" x = torch.randn(1, 1, sequence_length, input_channels) x = self.conv1(x) x = self.pool(x) x = self.conv2(x) x = self.pool(x) return x.view(1, -1).size(1) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass of the model. \"\"\" # x shape: (batch_size, sequence_length, input_channels) # Add a channel dimension # x shape: (batch_size, 1, sequence_length, input_channels) x = x.unsqueeze(1) x = torch.relu(self.conv1(x)) x = self.pool(x) x = self.dropout(x) x = torch.relu(self.conv2(x)) x = self.pool(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = torch.relu(self.fc1(x)) x = self.fc2(x) return x", "source": "cnn_model.py"}, {"content": "\"\"\" A module for data preprocessing and feature engineering \"\"\" import pandas as pd class DataPipeline: \"\"\" A class for data preprocessing and feature engineering \"\"\" def __init__(self, fill_method: str = 'bfill', use_rnn: bool = False): \"\"\"\" Initialize the DataPipeline class with the specified fill method \"\"\" self.features = ['PRES', 'TEMP', 'Iws', 'Is', 'Ir', 'DEWP', 'cbwd', 'pm2.5'] self.lag_features = { 'pm2.5': 24, 'Iws': 5, 'cbwd': 5, 'DEWP': 1, 'PRES': 1, 'TEMP': 1, 'Is': 1, 'Ir': 1 } self.fill_method = fill_method self.use_rnn = use_rnn def convert_wind_direction(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Convert the 'cbwd' column to one-hot encoded columns \"\"\" df['cbwd'] = df['cbwd'].astype('category').cat.codes return df def create_target_variable(self, df: pd.DataFrame, horizon: int ) -> pd.DataFrame: \"\"\" Create the target variable for the specified horizon \"\"\" df[f'target_{horizon}'] = df['pm2.5'].shift(-horizon) return df def create_lag_features(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Create lag features for the specified columns in the DataFrame \"\"\" lagged_data = {} for feature, max_lag in self.lag_features.items(): for lag in range(1, max_lag + 1): lagged_data[f'{feature}_lag_{lag}'] = df[feature].shift(lag) lagged_df = pd.DataFrame(lagged_data) new_df = pd.concat([df, lagged_df], axis=1) return new_df def handle_missing_data(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Handle missing data using the specified method \"\"\" if self.fill_method == 'ffill': df.ffill(inplace=True) elif self.fill_method == 'bfill': df.bfill(inplace=True) elif self.fill_method == 'moving_avg': df['pm2.5'] = df['pm2.5'].fillna(df['pm2.5'].rolling(window=3, min_periods=1).mean()) return df def run_data_pipeline(self, csv_path: str, horizon: int=1, downsample: bool=False, upsample: bool=False) -> pd.DataFrame: \"\"\" Execute the complete data pipeline \"\"\" # Step 1: Ingest the CSV file df = pd.read_csv(csv_path) # Step 2: Combine year, month, day, and hour into a single datetime column df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) # Step 3: Drop the original date and time columns df = df.drop(['year', 'month', 'day', 'hour'], axis=1) # Step 4: Sort the DataFrame by datetime df = df.sort_values('datetime') # Step 5: Handle missing values df = self.handle_missing_data(df) # Step 6: Downsample if required if downsample: df = df.resample('10D', on='datetime').mean() # Step 7: Convert wind direction to one-hot encoded columns df = self.convert_wind_direction(df) # Step 8: Create lag features (only for non-RNN) if not self.use_rnn: df = self.create_lag_features(df) # Step 9: Create target variable for the specified horizon df = self.create_target_variable(df, horizon) # Step 10: Drop rows with NaN values (due to lagging and target creation) df = df.dropna() # Step 11: Upsample if required if upsample: df = df.resample('h').interpolate(method='linear') # Step 12: Select final feature set based on the model type (RNN or non-RNN) if self.use_rnn: final_features = ['datetime'] + self.features else: final_features = ['datetime'] for feature, _ in self.lag_features.items(): if feature == 'pm2.5': final_features.extend([f'pm2.5_lag_{i}' for i in range(1, 25)]) elif feature in ['Iws', 'cbwd']: final_features.extend([f'{feature}_lag_{i}' for i in range(1, 6)]) else: final_features.append(f'{feature}_lag_1') final_features.append(f'target_{horizon}') df = df[final_features] # Return the processed DataFrame return df", "source": "datapipeline.py"}, {"content": "\"\"\" This module contains the function to run a machine learning experiment. \"\"\" import pandas as pd from src.datapipeline import DataPipeline from src.ml_model import ForecastModel def run_experiment(data_path: str, horizons: list = None): \"\"\" Run a machine learning experiment using the specified data and horizons. \"\"\" if horizons is None: horizons = [1, 3, 6, 12, 24] pipeline = DataPipeline() metrics_list = [] for horizon in horizons: # Prepare data for the current horizon df = pipeline.run_data_pipeline(data_path, horizon) # Determine the split point (e.g., use the last 20% of the data for testing) split_point = int(len(df) * 0.8) # Split the data train_df = df.iloc[:split_point] test_df = df.iloc[split_point:] # Separate features and target target_col = f'target_{horizon}' feature_cols = [col for col in df.columns if col not in ['datetime', target_col]] x_train, y_train = train_df[feature_cols], train_df[target_col] x_test, y_test = test_df[feature_cols], test_df[target_col] # Initialize and train the model model = ForecastModel() model.fit(x_train, y_train) # Evaluate the model evaluation_metrics = model.evaluate(x_train, y_train, x_test, y_test) metrics_list.append({ 'Horizon': horizon, 'Train RMSE': evaluation_metrics['train_rmse'], 'Test RMSE': evaluation_metrics['test_rmse'], 'Train MSE': evaluation_metrics['train_mse'], 'Test MSE': evaluation_metrics['test_mse'] }) if horizons: # Create a DataFrame from the metrics metrics_df = pd.DataFrame(metrics_list) return model, metrics_df", "source": "ml_experiment.py"}, {"content": "\"\"\" Machine learning model for forecasting \"\"\" import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel: \"\"\" Machine learning model for forecasting \"\"\" def __init__(self, n_estimators: int = 100, random_state: int = 42): \"\"\" Initialize the model with the specified hyperparameters \"\"\" self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state) def fit(self, x: np.ndarray, y: np.ndarray): \"\"\" Fit the model to the training data \"\"\" self.model.fit(x, y) def evaluate(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray) -> dict: \"\"\" Evaluate the model on the train and test data \"\"\" # Predict on train and test data y_train_pred = self.model.predict(x_train) y_test_pred = self.model.predict(x_test) # Calculate MSE for train and test train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) # Calculate RMSE for train and test (often easier to interpret as it's # in the same units as the target variable) train_rmse = np.sqrt(train_mse) test_rmse = np.sqrt(test_mse) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse } def predict(self, x: np.ndarray) -> np.ndarray: \"\"\" Make predictions using the trained model \"\"\" return self.model.predict(x) def feature_importance(self): \"\"\" Return feature importances of the model \"\"\" # Return feature importances if available if hasattr(self.model, 'feature_importances_'): return self.model.feature_importances_ return None", "source": "ml_model.py"}, {"content": "\"\"\" RNN model for time series forecasting \"\"\" import torch import torch.nn as nn import torch.optim as optim import numpy as np class RNNModel(nn.Module): \"\"\" Recurrent Neural Network (RNN) model for time series forecasting \"\"\" def __init__(self, input_size: int, num_rnn: int, num_layers: int, output_size: int): \"\"\" Initialize the RNN model \"\"\" super(RNNModel, self).__init__() self.hidden_size = num_rnn self.num_layers = num_layers self.rnn = nn.RNN(input_size, num_rnn, num_layers, batch_first=True) self.fc = nn.Linear(num_rnn, output_size) self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters(), lr=0.001) def forward(self, x: torch.Tensor) -> torch.Tensor: \"\"\" Forward pass of the model \"\"\" h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device) out, _ = self.rnn(x, h0) out = self.fc(out[:, -1, :]) return out def fit(self, dataloader: torch.utils.data.DataLoader, num_epochs: int=20): \"\"\" Train the model \"\"\" self.train() for epoch in range(num_epochs): total_loss = 0 for batch_features, batch_labels in dataloader: outputs = self(batch_features) loss = self.criterion(outputs, batch_labels[:, -1, :]) self.optimizer.zero_grad() loss.backward() self.optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}') def predict(self, dataloader: torch.utils.data.DataLoader) -> np.ndarray: \"\"\" Generate predictions for the input data using the trained model \"\"\" self.eval() predictions = [] with torch.no_grad(): for batch_features, _ in dataloader: outputs = self(batch_features) predictions.append(outputs.cpu().numpy()) return np.concatenate(predictions, axis=0) def evaluate(self, train_dataloader: torch.utils.data.DataLoader, test_dataloader: torch.utils.data.DataLoader) -> dict: \"\"\" Evaluate the model using the provided dataloaders \"\"\" self.eval() def compute_metrics(dataloader): total_mse = 0 total_samples = 0 with torch.no_grad(): for batch_features, batch_labels in dataloader: outputs = self(batch_features) mse = self.criterion(outputs, batch_labels[:, -1, :]) total_mse += mse.item() * batch_features.size(0) total_samples += batch_features.size(0) avg_mse = total_mse / total_samples avg_rmse = np.sqrt(avg_mse) return avg_mse, avg_rmse train_mse, train_rmse = compute_metrics(train_dataloader) test_mse, test_rmse = compute_metrics(test_dataloader) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse }", "source": "rnn_model.py"}, {"content": "\"\"\" Module for creating PyTorch datasets for generating windows of data. \"\"\" import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): \"\"\" PyTorch Dataset for generating windows of data. \"\"\" def __init__(self, data: pd.DataFrame, lookback: int, lookahead: int, label_columns=None): self.data = data self.lookback = lookback self.lookahead = lookahead self.label_columns = label_columns self.total_window_size = lookback + lookahead self.input_slice = slice(0, lookback) self.label_start = self.total_window_size - lookahead self.labels_slice = slice(self.label_start, None) self.length = len(data) - self.total_window_size + 1 if label_columns is not None: self.label_columns_indices = {name: i for i, name in enumerate(label_columns)} self.column_indices = {name: i for i, name in enumerate(data.columns)} def __len__(self): \"\"\" Return the number of windows in the dataset. \"\"\" return self.length def __getitem__(self, idx: int) -> tuple: \"\"\" Generate a window of data. \"\"\" features = self.data.iloc[idx:idx + self.total_window_size].values inputs = features[self.input_slice, :] labels = features[self.labels_slice, :] if self.label_columns is not None: labels = np.stack( [labels[:, self.column_indices[name]] for name in self.label_columns], axis=-1 ) inputs = torch.tensor(inputs, dtype=torch.float32) labels = torch.tensor(labels, dtype=torch.float32) return inputs, labels # Example usage: # Assuming train_df, val_df, and test_df are your dataframes # lookback = 6 # lookahead = 1 # label_columns = ['PM2.5'] # train_dataset = WindowGenerator(train_df, lookback, lookahead, label_columns) # val_dataset = WindowGenerator(val_df, lookback, lookahead, label_columns) # test_dataset = WindowGenerator(test_df, lookback, lookahead, label_columns) # train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)", "source": "windowing.py"}, {"content": "# Description: Test cases for the function some_function_to_test import unittest # from ..test import some_function_to_test class TestSomeFunction(unittest.TestCase): \"\"\" Test the function some_function_to_test \"\"\" def test_case_1(self): \"\"\" Test case 1 \"\"\" # Placeholder for the first test case pass if __name__ == '__main__': unittest.main()", "source": "test_pass.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "# Description: Test cases for the function some_function_to_test # import unittest # # from ..test import some_function_to_test # class TestSomeFunction(unittest.TestCase): # \"\"\" Test the function some_function_to_test \"\"\" # def test_case_1(self): # \"\"\" Test case 1 \"\"\" # # Placeholder for the first test case # pass # if __name__ == '__main__': # unittest.main() import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import fitz # PyMuPDF from sentence_transformers import SentenceTransformer from langchain.text_splitter import RecursiveCharacterTextSplitter import streamlit as st from sklearn.metrics.pairwise import cosine_similarity import numpy as np import openai from dotenv import load_dotenv import os # Load environment variables from .env file load_dotenv() # Set up Azure OpenAI credentials openai.api_type = \"azure\" openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\") openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\") model_name = os.getenv(\"AZURE_OPENAI_MODEL\") # Load Sentence Transformers model for embeddings embedding_model = SentenceTransformer('all-MiniLM-L6-v2') # Function to extract text from PDF def extract_text_from_pdf(pdf_path): doc = fitz.open(pdf_path) text = \"\" for page in doc: text += page.get_text() return text # Define a simple Document class with metadata class Document: def __init__(self, id, text, metadata=None): self.id = id self.page_content = text self.metadata = metadata if metadata is not None else {} # Load data batch_13_csv = pd.read_csv('data/AIAP_Batch_13_Technical_Assessment_Results.csv') batch_14_csv = pd.read_csv('data/AIAP_Batch_14_Technical_Assessment_Results.csv') batch_13_pdf_text = extract_text_from_pdf('data/AIAP_Batch 13_Technical_Assessment.pdf') batch_14_pdf_text = extract_text_from_pdf('data/AIAP_Batch 14_Technical_Assessment.pdf') # Combine CSV and PDF data into a single list of documents docs = [] for _, row in batch_13_csv.iterrows(): metadata = {'batch': '13', 'candidate': row['Candidate'], 'assessor': row['Assessor'], 'result': row['Result']} docs.append(Document(f'batch_13_{row[\"Candidate\"]}', row['Candidate'] + ' ' + row['Assessor'] + ' ' + row['Result'], metadata)) for _, row in batch_14_csv.iterrows(): metadata = {'batch': '14', 'candidate': row['Candidate'], 'assessor': row['Assessor'], 'result': row['Result']} docs.append(Document(f'batch_14_{row[\"Candidate\"]}', row['Candidate'] + ' ' + row['Assessor'] + ' ' + row['Result'], metadata)) docs.append(Document('batch_13_pdf', batch_13_pdf_text, {'batch': '13', 'type': 'pdf'})) docs.append(Document('batch_14_pdf', batch_14_pdf_text, {'batch': '14', 'type': 'pdf'})) # Load and process documents text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) split_docs = text_splitter.split_documents(docs) # Generate embeddings embeddings = [] for doc in split_docs: embedding = embedding_model.encode(doc.page_content) embeddings.append({'id': doc.id, 'text': doc.page_content, 'embedding': embedding, 'metadata': doc.metadata}) # Function to handle question (RAG model) def handle_question(question, embeddings): # Generate embedding for the question question_embedding = embedding_model.encode(question) # Calculate similarities between the question embedding and document embeddings similarities = [] for doc in embeddings: similarity = cosine_similarity([question_embedding], [doc['embedding']])[0][0] similarities.append((similarity, doc)) # Sort documents by similarity in descending order similarities.sort(reverse=True, key=lambda x: x[0]) # Retrieve top 5 most similar documents top_docs = similarities[:5] # Concatenate the text of the top documents to form the context context = \" \".join([doc[1]['text'] for doc in top_docs]) # Ensure the input text does not exceed the model's maximum sequence length max_length = 2048 - len(question) # Adjust for the length of the question context = context[:max_length] # Truncate to the maximum length # Generate an answer using the context and the question response = openai.Completion.create( model=model_name, # Use the appropriate model name messages=[ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": question + \"\\n\\n\" + context} ], max_tokens=150, n=1, stop=None, temperature=0.7, ) generated_answer = response.choices[0].message['content'].strip() # Prepare reasoning and retrieved documents for display reasoning = \"Top 5 similar documents retrieved based on cosine similarity.\" retrieved_docs = [doc[1] for doc in top_docs] return generated_answer, retrieved_docs, reasoning # Streamlit interface st.title(\"Question Answering over Documents Platform\") st.header(\"Questions and Answers\") # Display chat history if 'chat_history' not in st.session_state: st.session_state.chat_history = [] for message in st.session_state.chat_history: with st.chat_message(message[\"role\"]): st.write(message[\"content\"]) if \"reasoning\" in message: with st.expander(\"View Reasoning and Code\"): st.write(message[\"reasoning\"]) # Get the pre-filled question from the session state pre_filled_question = st.session_state.get(\"common_question\", \"\") #", "source": "my_app.py"}, {"content": "User input user_question = st.chat_input(\"Ask your question here:\") # If there's a pre-filled question from common questions, set the input text if pre_filled_question and not user_question: user_question = pre_filled_question st.session_state.common_question = \"\" # Clear after use if user_question: st.session_state.chat_input = \"\" # Clear input box after use st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question}) with st.chat_message(\"user\"): st.write(user_question) with st.chat_message(\"assistant\"): with st.spinner(\"Retrieving answer...\"): answer, retrieved_docs, reasoning = handle_question(user_question, embeddings) st.write(answer) with st.expander(\"View Reasoning\"): st.write(reasoning) if retrieved_docs: with st.expander(\"Retrieved Documents\"): for doc in retrieved_docs: st.write(f\"**Document Source:** {doc['id']}\") st.write(f\"{doc['text'][:500]}\") st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"reasoning\": reasoning})", "source": "my_app.py"}, {"content": "# Import the libraries needed import numpy as np import pandas as pd from sklearn.preprocessing import OneHotEncoder,StandardScaler, OrdinalEncoder, PowerTransformer, QuantileTransformer from sklearn.compose import ColumnTransformer from sklearn.model_selection import train_test_split nan_code = 'Not Applicable' def handle_missing_values(df): # Replace ? and Not in universe as NaN df = df.replace('?',np.nan) df = df.replace('Not in universe',np.nan) df = df.replace('Not in universe or children',np.nan) df = df.replace('Not in universe under 1 year old',np.nan) return df def transform(data_path,features_out=False): \"\"\" Description of the function. :param data_path: ...... :return: ...... \"\"\" # Load in the data df = pd.read_csv(data_path) # Handle missing values df = handle_missing_values(df) # Make the target group (income_group) into a binary column income_group_dict = {'- 50000.': 0,'50000+.': 1} df['income_group'] = [income_group_dict[item] for item in df['income_group']] target_col = 'income_group' # Assign appropiate int dtype of numerical columns numerical_cols = ['age' , 'wage_per_hour', 'num_persons_worked_for_employer', 'weeks_worked_in_year', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] for col in numerical_cols: df[col] = df[col].astype('int') # Set up the order and ordinal for education and veterans benefits education_order = ['Children','Less than 1st grade','1st 2nd 3rd or 4th grade','5th or 6th grade', '7th and 8th grade','9th grade','10th grade','11th grade','12th grade no diploma','High school graduate', 'Some college but no degree','Associates degree-occup /vocational','Associates degree-academic program', 'Bachelors degree(BA AB BS)','Masters degree(MA MS MEng MEd MSW MBA)','Doctorate degree(PhD EdD)', 'Prof school degree (MD DDS DVM LLB JD)'] ordinal_cols = ['education'] ordinal_orders=[education_order] # The remaining columns which are not numerical, ordinal, target are categorical categorical_cols = ['marital_stat', 'race', 'sex', 'citizenship', 'class_of_worker', 'major_industry_code', 'major_occupation_code', 'full_or_part_time_employment_stat', 'detailed_household_summary_in_household', 'own_business_or_self_employed', 'tax_filer_stat', 'veterans_benefits'] print(f'{len(numerical_cols)} numeric:',numerical_cols) print(f'{len(ordinal_cols)} ordinal:',ordinal_cols) print(f'{len(categorical_cols)} category:',categorical_cols) print(f'TARGET:',target_col) #skewed_cols = ['wage_per_hour','capital_gains','capital_losses','dividends_from_stocks'] #weeks_worked_in_year pt = PowerTransformer() qt = QuantileTransformer() #df[skewed_cols] = qt.fit_transform(df[skewed_cols]) # Define X as input features and y as the outcome variable X = df[numerical_cols + ordinal_cols + categorical_cols] y = df[target_col] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Build a preprocessing step for numeric features scaler = StandardScaler() # MinMax (0-1), Robust-Scaling: more attention to median # Build a preprocessing step for nominal features one_hot_encoder = OneHotEncoder(drop='if_binary',handle_unknown='infrequent_if_exist') #{'ignore', 'error', 'infrequent_if_exist'} # Build a preprocessing step for ordinal features ordinal_encoder = OrdinalEncoder(categories=ordinal_orders) preprocessor = ColumnTransformer( [ (\"OneHotEncoder\",one_hot_encoder,categorical_cols), (\"StandardScaler\",scaler,numerical_cols), (\"OrdinalEncoder\",ordinal_encoder,ordinal_cols) ],sparse_threshold=0 # force numpy ) X_train_arr=preprocessor.fit_transform(X_train,y=y_train) X_test_arr=preprocessor.transform(X_test) y_train_arr = np.array(y_train) y_test_arr = np.array(y_test) feature_names = preprocessor.get_feature_names_out() if features_out: return X_train_arr, X_test_arr, y_train_arr, y_test_arr, feature_names else: return X_train_arr, X_test_arr, y_train_arr, y_test_arr", "source": "datapipeline.py"}, {"content": "import numpy as np from collections import Counter class Node: def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None): #WHAT IS THIS * self.feature = feature self.threshold = threshold self.left = left self.right = right self.value = value def is_leaf_node(self): return self.value is not None class DecisionTree: def __init__(self,min_samples_split=2, max_depth=100, n_features=None): self.min_samples_split = min_samples_split self.max_depth = max_depth = max_depth self.n_features = n_features self.root = None def predict(self,X): return np.array([self._traverse_tree(x,self.root) for x in X]) def _traverse_tree(self, x, node): if node.is_leaf_node(): return node.value if x[node.feature] <= node.threshold: return self._traverse_tree(x,node.left) else: return self._traverse_tree(x,node.right) def fit(self, X, y): self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features) self.root = self._grow_tree(X,y) #self.feature_importances = dict.fromkeys(range(X.shape[1]),0) #self._calculate_feature_importances(self.tree) def _grow_tree(self, X, y, depth=0): n_samples, n_feats = X.shape n_labels = len(np.unique(y)) # checking stopping criteria if (depth>=self.max_depth or n_labels==1 or n_samples<self.min_samples_split): leaf_value = self._most_common_label(y) return Node(value=leaf_value) feat_idxs = np.random.choice(n_feats, self.n_features, replace=False) # find the best split best_feature, best_thresh = self._best_split(X,y,feat_idxs) # create child nodes left_idxs, right_idxs = self._split(X_col=X[:,best_feature],split_threshold=best_thresh) # recursive calling left = self._grow_tree(X[left_idxs,:],y[left_idxs],depth=depth+1) right = self._grow_tree(X[right_idxs,:],y[right_idxs],depth=depth+1) return Node(best_feature,best_thresh,left,right) def _best_split(self,X,y,feat_idxs): # find best from all the splits best_gain = -1 split_idx, split_thresh = None, None for feat_idx in feat_idxs: X_col = X[:,feat_idx] threshs = np.unique(X_col) for thresh in threshs: gain = self._info_gain(y,X_col,thresh) if gain > best_gain: best_gain = gain split_idx = feat_idx split_thresh = thresh if(best_gain>-1): print(f'BEST split at {split_idx} with {split_thresh} to get gain of {best_gain}') return split_idx, split_thresh def _entropy(self, y): hist = np.bincount(y) ps = hist / len(y) return - np.sum([p * np.log(p) for p in ps if p>0]) def _info_gain(self,y,X_col, thresh): # IG = Entropy(Parent) - weighted_avg * Entropy(Child) parent_entropy = self._entropy(y) # create children left_idxs, right_idxs = self._split(X_col,thresh) if(len(left_idxs)==0 or len(right_idxs)==0): return 0 # weighted entropy n = len(y) n_l, n_r = len(left_idxs), len(right_idxs) e_l, e_r = self._entropy(y[left_idxs]), self._entropy(y[right_idxs]) child_entropy = (n_l/n) * e_l + (n_r/n) * e_r # Infomation Gain info_gain = parent_entropy - child_entropy return info_gain def _split(self, X_col, split_threshold): left_idxs = np.argwhere(X_col <= split_threshold).flatten() right_idxs = np.argwhere(X_col > split_threshold).flatten() return left_idxs, right_idxs def _most_common_label(self,y): counter = Counter(y) return counter.most_common(1)[0][0] # define private class using __ def __gini(self,label_0,label_1): # gini impurity formula: 1 - p0^2 - p1^2 # p0: porportion of 0s in the node node_size = len(label_0) p0 = (np.array(label_0) == 1).sum()/node_size p1 = (np.array(label_1) == 1).sum()/node_size return 1 - p0**2 - p1**2 #def split(self) #def fit(self,X,y): # X: m*n array with m training examples of n features # y: m*1 array of labels #for", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,ConfusionMatrixDisplay class Model: def __init__(self): self.model = RandomForestClassifier(random_state=42) def train(self, params, X_train, y_train): \"\"\" This function trains the initialised model and returns the train f1 score. :param params: ...... :param X_train: ...... :param y_train: ...... :return: f1-score as evaluation metric \"\"\" self.model.set_params(**params) self.model.fit(X_train, y_train) y_pred = self.model.predict(X_train) return f1_score(y_train, y_pred) def evaluate(self, X_test, y_test): \"\"\" This function should use the trained model to predict the target for the test data and return the test f1 score. :param X_test: ...... :param y_test: ...... :return: ...... \"\"\" y_pred = self.model.predict(X_test) print('F1-score:',f1_score(y_test, y_pred)) return f1_score(y_test, y_pred,average='weighted') def get_default_params(self,default=False): \"\"\" This function should return the parameters to be used for training the model from scratch. ::params: default :return: ...... \"\"\" default_params = { 'n_estimators': 100, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'max_depth': None, } optimal_params = { 'n_estimators': 100, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'max_depth': None, 'class_weight': \"balanced\" } if default: return default_params else: return optimal_params # # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model", "source": "model.py"}, {"content": "import numpy as np from collections import Counter from decision_tree import DecisionTree class RandomForest: def __init__(self,n_trees=10): self.n_trees = n_trees self.trees = [] def fit(self,X,y): n_samples, n_features = X.shape self.trees = [] for _ in range(self.n_trees): indices = np.random.choice(len(X), size=int(len(X)*0.2), replace=False) X_sample, y_sample = X[indices], y[indices] tree = DecisionTree(max_depth=10) tree.fit(X_sample,y_sample) self.trees.append(tree) def predict(self,X): for tree in self.trees: tree.predict(X)", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "# Import necessary libraries import numpy as np import os import time import pyspark from pyspark.ml import Pipeline from pyspark.ml.evaluation import RegressionEvaluator from pyspark.ml.feature import Normalizer, OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler, from pyspark.ml.functions import vector_to_array from pyspark.ml.regression import RandomForestRegressor from pyspark.ml.tuning import CrossValidator, ParamGridBuilder from pyspark.sql.functions import col, count, countDistinct, isnan, mean, monotonically_increasing_id, regexp_extract, pandas_udf, PandasUDFType, stddev, substring, udf, when import pyspark.sql.types as T def transform(df): ### NUMERICAL ### # Extract integer columns for normalization int_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.IntegerType)] int_cols.remove(\"resale_price\") # Initialize VectorAssembler and Normalizer va_norm = VectorAssembler(inputCols=int_cols, outputCol=\"va_norm_features\") normalize = Normalizer(inputCol=\"va_norm_features\", outputCol=\"norm_features\") ### CATEGORICAL ### # Extract string columns for one-hot encoding str_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, T.StringType)] # Initialize StringIndexer and OneHotEncoder si_cols = [colname + \"_si\" for colname in str_cols] si = StringIndexer(inputCols=str_cols, outputCols=si_cols) ohe_cols = [colname + \"_ohe\" for colname in str_cols] ohe = OneHotEncoder(dropLast=False, inputCols=si_cols, outputCols=ohe_cols) ### PIPELINE ### # Create a pipeline pipe = Pipeline(stages=[va_norm, normalize, si, ohe, va]) # split data into training and testing sets (trainingData, testData) = df.randomSplit([0.8, 0.2], seed=123) # Fit and transform the training data using the pipeline pipeline_model = pipe.fit(trainingData) df_train_transformed = pipeline_model.transform(trainingData) df_train_dense = df_train_transformed.select(\"features\", \"resale_price\") return df_train_dense", "source": "datapipeline.py"}, {"content": "import datapeline", "source": "__init__.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, FunctionTransformer from sklearn.compose import ColumnTransformer from imblearn.over_sampling import SMOTE, BorderlineSMOTE, SVMSMOTE, ADASYN def apply_SMOTE(X,y,smote_type='SMOTE'): \"\"\" Apply SMOTE to oversample minority class to balance the target variable. Args X : numpy array y : numpy array smote_type: str, default='SMOTE' Type of SMOTE to apply. Options are 'SMOTE', 'BorderlineSMOTE', 'SVMSMOTE', 'ADASYN'. Returns X_resampled : numpy array y_resampled : numpy array \"\"\" # default SMOTE smote = SMOTE(sampling_strategy='minority', random_state=42) if smote_type == 'BorderlineSMOTE': smote = BorderlineSMOTE(sampling_strategy='minority', random_state=42) elif smote_type == 'SVMSMOTE': smote = SVMSMOTE(sampling_strategy='minority', random_state=42) elif smote_type == 'ADASYN': smote = ADASYN(sampling_strategy='minority', random_state=42) elif smote_type == 'None': return X,y X_resampled, y_resampled = smote.fit_resample(X, y) return X_resampled, y_resampled class DataPipeline : def __init__(self): self.preprocessor = None self.target_col = None self.scaler_type = None self.smote_type = None def transform_train_data(self, data_path, target_col='Class',scaler_type='StandardScaler',smote_type='SMOTE',verbose=False): \"\"\" Description of the function. :param train_data_path: ...... :return: ...... \"\"\" self.target_col = target_col self.scaler_type = scaler_type self.smote_type = smote_type # Load the data df = pd.read_csv(data_path) # Drop Duplicates df = df.drop_duplicates() # Define X as input features and y as the outcome variable X = df.drop(columns=[target_col]) y = df[target_col] scaler = StandardScaler() if(scaler_type=='LogScaler'): scaler = FunctionTransformer(np.log1p, validate=True) elif(scaler_type=='MinMaxScaler'): scaler = MinMaxScaler() elif(scaler_type=='RobustScaler'): scaler = RobustScaler() if (scaler_type != 'None'): self.preprocessor = ColumnTransformer( [ (\"AmountScaler\",scaler,['Amount']), ],sparse_threshold=0, remainder='passthrough' # force numpy ) X_train = np.array(X,dtype='float') y_train = np.array(y,dtype='int') if(self.preprocessor is not None): X_train = self.preprocessor.fit_transform(X,y=y) y_train = np.array(y,dtype='int') # Apply SMOTE to balance the target variable X_train, y_train = apply_SMOTE(X_train,y_train,smote_type=smote_type) if(verbose): print(f'Scaler: {scaler_type}, SMOTE: {smote_type}') print(f'Amount Description:\\nMin: {X_train[:,-1].min()}, Max: {X_train[:,-1].max()}, Median: {np.median(X_train[:,-1])}') print(f'Class Distribution:\\n{np.unique(y_train,return_counts=True)}') y_train = y_train.reshape(-1,1) return X_train, y_train def transform_test_data(self, data_path,verbose=False): \"\"\" Description of the function. :param test_data_path: ...... :return: ...... \"\"\" # Load the data df = pd.read_csv(data_path) # Define X as input features and y as the outcome variable X = df.drop(columns=[self.target_col]) y = df[self.target_col] X_test = np.array(X,dtype='float') y_test = np.array(y,dtype='int') if(self.preprocessor is not None): X_test = self.preprocessor.transform(X) y_test = np.array(y,dtype='int') if(verbose): print(f'Scaler: {self.scaler_type}, SMOTE: {self.smote_type}') print(f'Amount Description:\\nMin: {X_test[:,-1].min()}, Max: {X_test[:,-1].max()}, Median: {np.median(X_test[:,-1])}') print(f'Class Distribution:\\n{np.unique(y_test,return_counts=True)}') y_test = y_test.reshape(-1,1) return X_test, y_test", "source": "datapipeline.py"}, {"content": "import sys import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' import warnings import time import mlflow from tensorflow.keras import metrics from datapipeline import DataPipeline from sklearn.metrics import f1_score from aiap_dsp_mlops.modeling import models def main(): warnings.filterwarnings('ignore') if len(sys.argv) < 1: return \"Please provide a description for the run\" mlflow.set_tracking_uri(\"http://127.0.0.1:8080/\") mlflow.set_experiment(\"A4P1-creditcard-fraud\") mlflow.tensorflow.autolog(silent=False,log_models=False,log_datasets=False,checkpoint=False) tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(\"A4P1-creditcard-fraud\") print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) # Arguments train_data_path = 'data/processed/creditcard_train.csv' test_data_path = 'data/processed/creditcard_test.csv' scaler_type = 'MinMaxScaler' smote_type = 'ADASYN' # Model parameters optimizer = 'adam' loss = 'binary_crossentropy' epochs = 5 batch_size = 32 validation_split = 0.2 # Load the dataset dpl = DataPipeline() X_train, y_train = dpl.transform_train_data(train_data_path,scaler_type=scaler_type, smote_type=smote_type,verbose=False) X_test, y_test = dpl.transform_test_data(test_data_path,verbose=False) print(\"Loaded and transformed train and test data\") run_name = 'baseline_' + str(time.time_ns()) with mlflow.start_run(description=sys.argv[1], run_name=run_name) as run: mlflow.log_param(\"scaler_type\", scaler_type) mlflow.log_param(\"smote_type\", smote_type) mlflow.log_param(\"optimizer\", optimizer) mlflow.log_param(\"loss_fn\", loss) print(\"Training the model...\") # Load the model model = models.make_baseline_model(input_shape=X_train.shape[1],activation='relu', output_activation='sigmoid',loss='binary_crossentropy') # Compile the model model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(),metrics.Recall()]) # Training the model model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2,verbose=0) # Evaluate the model results = model.evaluate(X_test, y_test, verbose=0) print(\"Test Loss\", results[0]) print(\"Test Precision\", results[1]) print(\"Test Recall\", results[2]) y_probs = model.predict(X_test) y_pred = (y_probs > 0.5).astype(int) f1 = f1_score(y_test, y_pred) mlflow.log_metric(key=\"test_precision\", value=results[1]) mlflow.log_metric(key=\"test_recall\", value=results[2]) mlflow.log_metric(key=\"test_f1\", value=f1) if __name__ == \"__main__\": main()", "source": "mlflow_train_model.py"}, {"content": "import numpy as np def ReLU(x): return np.maximum(0, x) def softmax(x): return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum()) class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): \"\"\" Initializes the weights and biases \"\"\" self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # w1 is the weight matrix for the first layer w1 = np.random.randn(hidden_size, input_size) * 0.01 b1 = np.zeros((hidden_size, 1)) w2 = np.random.randn(output_size, hidden_size) * 0.01 b2 = np.zeros((output_size, 1)) self.parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2} self.cache = None self.X = None self.y = None print(w1.shape, b1.shape, w2.shape, b2.shape) def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" w1 = self.parameters['w1'] b1 = self.parameters['b1'] w2 = self.parameters['w2'] b2 = self.parameters['b2'] # z1 is the output of the first layer w1*x + b1 z1 = np.dot(w1, features) + b1 # a1 is the output of the first layer after applying the activation function a1 = ReLU(z1) # z2 is the output of the second layer x*w2 + b2 z2 = np.dot(w2, a1) + b2 # softmax function to the last layer a2 = softmax(z2) #update self self.cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2} self.X = features return a2 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" # predictions = 0.2 , 0.3, 0.5 # label = 0 ==> [1, 0, 0] or 1 ==> [0, 1, 0] or 2 ==> [0, 0, 1] ## Cross entropy loss loss = -np.sum(np.log(predictions[label])) y = np.zeros_like(predictions) y[label] = 1 #convert to [0, 1, 0] format self.cache['y'] = y return loss def backward(self, learning_rate=0.003): \"\"\" Adjusts the internal weights/biases \"\"\" num_samples = self.X.shape[1] #print(num_samples) w1 = self.parameters['w1'] b1 = self.parameters['b1'] w2 = self.parameters['w2'] b2 = self.parameters['b2'] a1 = self.cache['a1'] a2 = self.cache['a2'] y = self.cache['y'] # backpropagation: calculate dW1, db1, dW2, db2 # Calculate the gradient of the loss with respect to z2 dz2 = a2 - y # Calculate the gradient of the loss with respect to w2 (output layer weights) dW2 = np.dot(dz2, a1.T) / num_samples # Calculate the gradient of the loss with respect to b2 (output layer biases) db2 = np.sum(dz2, axis=1, keepdims=True) / num_samples # Calculate the gradient of the loss with respect to z1 (hidden layer pre-activation) dz1 = np.multiply(np.dot(w2.T, dz2), (1 - np.square(a1))) # Calculate the gradient of the loss with respect to w1 (hidden layer weights) dw1 = np.dot(dz1, self.X.T) / num_samples # Calculate the gradient of the loss with respect to b1 (hidden layer biases) db1 = np.sum(dz1, axis=1, keepdims=True) / num_samples # Update rule for each parameter w1 -= learning_rate * dw1 b1 -= learning_rate * db1 w2 -= learning_rate * dW2 b2 -= learning_rate * db2 self.parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2} def predict(self, features): \"\"\" Takes in the features returns the prediction \"\"\" return np.argmax(self.forward(features))", "source": "mlp.py"}, {"content": "import numpy as np def ReLU(x): return np.maximum(0, x) def softmax(x): return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum()) def one_hot(Y): one_hot_Y = np.zeros((Y.size, Y.max()+1)) one_hot_Y[np.arange(Y.size), Y] = 1 one_hot_Y = one_hot_Y.T return one_hot_Y def deriv_relu(x): return x > 0 class MLPCustom: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10,batch_size=2): \"\"\" Initializes the weights and biases \"\"\" self.input_size = input_size self.hidden_size = hidden_size self.output_size = output_size # w1 is the weight matrix for the first layer # w[a,b] is the weight from input 1 to hidden 1 w1 = np.random.randn(hidden_size, input_size) * 0.01 b1 = np.zeros((hidden_size, 1)) w2 = np.random.randn(output_size, hidden_size) * 0.01 b2 = np.zeros(( output_size, 1)) self.parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2} self.cache = None self.X = None self.y = None #print(w1.shape, b1.shape, w2.shape, b2.shape) def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" w1 = self.parameters['w1'] b1 = self.parameters['b1'] w2 = self.parameters['w2'] b2 = self.parameters['b2'] print(features.shape) # z1 is the output of the first layer x*w1 + b1 z1 = np.matmul(w1, features) + b1 # a1 is the output of the first layer after applying the activation function a1 = ReLU(z1) # z2 is the output of the second layer x*w2 + b2 z2 = np.matmul(w2, a1) + b2 # softmax function to the last layer a2 = softmax(z2) #update self self.cache = {'z1': z1, 'a1': a1, 'z2': z2, 'a2': a2} self.X = features return a2 def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" # predictions = 0.2 , 0.3, 0.5 # label = 0 ==> [1, 0, 0] or 1 ==> [0, 1, 0] or 2 ==> [0, 0, 1] ## Cross entropy loss loss = -np.sum(np.log(predictions[label])) y = np.zeros_like(predictions) y[label] = 1 #convert to [0, 1, 0] format self.cache['y'] = y return loss def backward(self, learning_rate=0.003): \"\"\" Adjusts the internal weights/biases \"\"\" num_samples = self.X.shape[1] print(num_samples) w1 = self.parameters['w1'] b1 = self.parameters['b1'] w2 = self.parameters['w2'] b2 = self.parameters['b2'] a1 = self.cache['a1'] a2 = self.cache['a2'] y = self.cache['y'] # backpropagation: calculate dW1, db1, dW2, db2 dz2 = a2 - y dW2 = np.matmul(dz2, a1.T) / num_samples db2 = np.sum(dz2, axis=1, keepdims=True) / num_samples dz1 = np.matmul(w2.T, dz2) * deriv_relu(a1) dw1 = np.matmul(dz1, self.X.T) / num_samples db1 = np.sum(dz1, axis=1, keepdims=True) / num_samples # Update rule for each parameter w1 -= learning_rate * dw1 b1 -= learning_rate * db1 w2 -= learning_rate * dW2 b2 -= learning_rate * db2 self.parameters = {'w1': w1, 'b1': b1, 'w2': w2, 'b2': b2} def predict(self, features): \"\"\" Takes in the features returns the prediction \"\"\" return np.argmax(self.forward(features))", "source": "mlpv2.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler class Datapipeline(): def __init__(self): self.preprocessor = None def transform(self,data_path): df = pd.read_csv(data_path) df.drop(columns=['id'], inplace=True) X = df.drop(columns=['y']) y = df['y'] X = np.array(X,dtype='float') y = np.array(y,dtype='int') self.preprocessor = StandardScaler() X = self.preprocessor.fit_transform(X) y = y.reshape(-1,1) X = np.expand_dims(X, axis=2) return X, y def transform_train_data(self,data_path): df = pd.read_csv(data_path) X = df.drop(columns=['y']) y = df['y'] X = np.array(X,dtype='float') y = np.array(y,dtype='int') self.preprocessor = StandardScaler() X = self.preprocessor.fit_transform(X) y = y.reshape(-1,1) X = np.expand_dims(X, axis=2) return X, y def transform_test_data(self,data_path): df = pd.read_csv(data_path) X = df.drop(columns=['y']) y = df['y'] X = np.array(X,dtype='float') y = np.array(y,dtype='int') if(self.preprocessor is not None): X = self.preprocessor.transform(X) else: print('No Preprocessor found. No Scaling Done') y = y.reshape(-1,1) X = np.expand_dims(X, axis=2) return X, y", "source": "mlp_datapipeline.py"}, {"content": "#pytorch libraries import torch import torch.nn as nn import torch.nn.functional as F from torch.utils.data import DataLoader,Dataset,WeightedRandomSampler from torch.optim import Adam from torch.autograd import Variable import numpy as np import random import os def set_seed(seed = 55): '''Sets the seed of the entire notebook so results are the same every time we run. This is for REPRODUCIBILITY.''' np.random.seed(seed) random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) # When running on the CuDNN backend, two further options must be set torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False # Set a fixed value for the hash seed os.environ['PYTHONHASHSEED'] = str(seed) print('> SEEDING DONE') set_seed(RANDOM_SEED) class Pytorch_MLP(nn.Module): def __init__(self, n_inputs): super(Pytorch_MLP, self).__init__() self.mlp = nn.Sequential( nn.Linear(n_inputs, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 1), nn.Sigmoid() ) def forward(self,X): return self.mlp(X)", "source": "pytorch_model.py"}, {"content": "import sys import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' import warnings warnings.filterwarnings('ignore') import time import mlflow import tensorflow as tf from tensorflow.keras import metrics from tensorflow.keras.callbacks import EarlyStopping from datapipeline import DataPipeline from sklearn.metrics import classification_report,confusion_matrix,f1_score from aiap_dsp_mlops.modeling import models import omegaconf import hydra import logging # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(args): mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) mlflow.tensorflow.autolog(silent=False,log_models=False,log_datasets=False,checkpoint=False) tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(args[\"mlflow_exp_name\"]) print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) # Arguments train_data_path = args[\"train_data_path\"] test_data_path = args[\"test_data_path\"] scaler_type = args['scaling'] smote_type = args[\"smote_type\"] # Model parameters hidden_layers = args[\"hidden_layers\"] optimizer = args[\"optimizer\"] loss = args[\"loss_fn\"] epochs = args[\"epochs\"] batch_size = args[\"batch_size\"] validation_split = args[\"validation_split\"] early_stopping_patience = args[\"early_stopping_patience\"] # Load the dataset dpl = DataPipeline() X_train, y_train = dpl.transform_train_data(train_data_path,scaler_type=scaler_type, smote_type=smote_type,verbose=False) X_test, y_test = dpl.transform_test_data(test_data_path,verbose=False) print(\"Loaded and transformed train and test data\") run_name = args['mlflow_run_name'] + str(time.time_ns()) with mlflow.start_run(description='hydra train', run_name=run_name) as run: mlflow.log_param(\"scaler_type\", scaler_type) mlflow.log_param(\"smote_type\", smote_type) mlflow.log_param(\"optimizer\", optimizer) mlflow.log_param(\"loss_fn\", loss) mlflow.log_param(\"hidden_layers\", hidden_layers) mlflow.log_param(\"early_stopping_patience\", early_stopping_patience) print(\"Training the model...\") # Load the model model = models.make_model(input_shape=X_train.shape[1],hidden_layers=hidden_layers,activation='relu', output_activation='sigmoid',loss='binary_crossentropy') # Compile the model model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[metrics.Precision(),metrics.Recall()]) callback = EarlyStopping(monitor='val_loss', patience=early_stopping_patience) # Training the model model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=validation_split,verbose=0,callbacks=[callback]) # Evaluate the model results = model.evaluate(X_test, y_test, verbose=0) print(\"Test Loss\", results[0]) print(\"Test Precision\", results[1]) print(\"Test Recall\", results[2]) y_probs = model.predict(X_test) y_pred = (y_probs > 0.5).astype(int) f1 = f1_score(y_test, y_pred) mlflow.log_metric(key=\"test_loss\", value=results[0]) mlflow.log_metric(key=\"test_precision\", value=results[1]) mlflow.log_metric(key=\"test_recall\", value=results[2]) mlflow.log_metric(key=\"test_f1\", value=f1) # returns loss return results[0], f1 if __name__ == \"__main__\": main()", "source": "train_model.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "\"\"\"This package contains modules pertaining to differing parts of the end-to-end workflow, excluding the source for serving the model through a REST API.\"\"\" from . import modeling from . import general_utils", "source": "__init__.py"}, {"content": "\"\"\"Module for containing architectures/definition of models.\"\"\" import tensorflow as tf from tensorflow.keras.layers import Dense, Flatten,Input, Dropout from tensorflow.keras import Model,Sequential from tensorflow.keras.metrics import F1Score def make_model(input_shape,hidden_layers=[64,32,8],activation='relu', output_activation='sigmoid',loss='binary_crossentropy'): \"\"\" Create a simple feedforward neural network model. Args: input_shape: The shape of the input data (number of features). activation (str, optional): Activation function to use for the hidden layers. Defaults to 'relu'. output_activation (str, optional): Activation function to use for the output layer. Defaults to 'sigmoid'. loss (str, optional): Loss function to use for model compilation. Defaults to 'binary_crossentropy'. optimizer (str, optional): Optimizer to use for model compilation. Defaults to 'adam'. metrics (list, optional): List of metrics to be evaluated by the model during training and testing. Defaults to ['accuracy']. Returns: keras.models.Sequential: A compiled Keras Sequential model. \"\"\" model = Sequential() model.add(Input(shape=(input_shape,))) for param in hidden_layers: if( isinstance(param, int) ): model.add(Dense(param, activation=activation)) elif (isinstance(param, float)): model.add(Dropout(param)) model.add(Dense(1, activation=output_activation)) return model", "source": "models.py"}, {"content": "\"\"\"Utilities for model training and experimentation workflows. \"\"\"", "source": "utils.py"}, {"content": "\"\"\"This `modeling` module includes module(s) relevant for the model training pipeline.\"\"\" from . import models from . import utils", "source": "__init__.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A4.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' import warnings warnings.filterwarnings('ignore') import mlflow import tensorflow as tf import time import mlops as amlo from mlops.pre_process import PreprocessPipeline import omegaconf import hydra import logging # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train.yaml\") def main(args): logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") amlo.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) mlflow.set_tracking_uri(args.mlflow.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow.mlflow_exp_name) mlflow.tensorflow.autolog(silent=False,log_models=False,log_datasets=False,checkpoint=False) tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(args.mlflow.mlflow_exp_name) print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) random_seed = args.general.random_seed image_main_dir = args.preprocess.image_data_dir if not os.path.exists(image_main_dir): logger.error(f\"Path {image_main_dir} does not exist\") pretrained_model = args.model.pretrained_model epochs = args.model.epochs learning_rate = args.model.learning_rate batch_size = args.preprocess.batch_size optimizer = args.model.optimizer loss_fn = args.model.loss_fn logger.info(\"Loading image data\") pipeline = PreprocessPipeline(args) train_ds, val_ds = pipeline.transform_data() logger.info(\"Train Val Data Loaded\") run_name = args.mlflow['mlflow_run_name'] + str(time.time_ns()) model_checkpoint_dir_path = os.path.join(args.mlflow.model_checkpoint_dir_path,run_name) os.makedirs(model_checkpoint_dir_path, exist_ok=True) with mlflow.start_run(description='hydra train', run_name=run_name) as run: mlflow.log_param(\"epochs\", epochs) mlflow.log_param(\"learning_rate\", learning_rate) mlflow.log_param(\"optimizer\", optimizer) mlflow.log_param(\"loss_fn\", loss_fn) mlflow.log_param(\"batch_size\", batch_size) logger.info(\"Training the model...\") base_model = tf.keras.applications.MobileNetV2(input_shape= args.preprocess.input_shape, weights=\"imagenet\", include_top=False) for layer in base_model.layers: layer.trainable = False preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input global_average_layer = tf.keras.layers.GlobalAveragePooling2D() prediction_layer = tf.keras.layers.Dense(12,activation='softmax') inputs = tf.keras.Input(shape=(args.preprocess.input_shape)) x = preprocess_input(inputs) x = base_model(x, training=False) x = global_average_layer(x) outputs = prediction_layer(x) model = tf.keras.Model(inputs, outputs) # since the labels are integer we use sparse_categorical_crossentropy model.compile(optimizer=args.model.optimizer, loss=args.model.loss_fn, metrics=[args.model.metrics]) model.fit(train_ds, validation_data=val_ds, epochs=args.model.epochs) # Save the trained model trained_model_path = os.path.join( model_checkpoint_dir_path, \"trained_model.keras\" ) model.save(trained_model_path) logger.info(f\"Trained model saved to {trained_model_path}.\") eval_results = model.evaluate(val_ds, verbose=1) test_loss, accuracy = eval_results #logger.info(\"Test Loss\", test_loss) #logger.info(\"Test Accuracy\", accuracy) mlflow.log_metric(key=\"val_loss\", value=test_loss) mlflow.log_metric(key=\"val_accuracy\", value=accuracy) #callback = EarlyStopping(monitor='val_loss', patience=early_stopping_patience) # Training the model # model.fit(X_train, y_train, epochs=epochs, # batch_size=batch_size, validation_split=validation_split,verbose=0,callbacks=[callback]) # returns loss return test_loss, accuracy if __name__ == \"__main__\": main()", "source": "tf_train.py"}, {"content": "import os os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0' os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' import warnings warnings.filterwarnings('ignore') import mlflow import tensorflow as tf import datetime import mlops as amlo from mlops.pre_process import PreprocessPipeline import omegaconf import hydra import logging # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train.yaml\") def main(args): logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") amlo.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) mlflow_init_status, mlflow_run = amlo.general_utils.mlflow_init( args.mlflow, setup_mlflow=args.mlflow.setup_mlflow, autolog=args.mlflow.mlflow_autolog ) if not mlflow_init_status: raise Exception(\"Failed to initialise mlflow\") # Reduce model memory footprint model_weights_float16 = args.general.model_weights_float16 if model_weights_float16: tf.keras.backend.set_floatx('float16') tf.config.set_soft_device_placement(True) logger.info(tf.test.is_gpu_available()) # Unpack general and model_training args random_seed = args.general.random_seed image_main_dir = args.preprocess.image_data_dir if not os.path.exists(image_main_dir): logger.error(f\"Path {image_main_dir} does not exist\") pretrained_model = args.model.pretrained_model epochs = args.model.epochs learning_rate = args.model.learning_rate batch_size = args.preprocess.batch_size tf.keras.backend.clear_session() #amlo.general_utils.reset_random_seeds(seed = random_seed, random_level = 2) logger.info(\"Loading image data\") pipeline = PreprocessPipeline(args) train_ds, val_ds = pipeline.transform_data() logger.info(\"Train Val Data Loaded\") amlo.general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"batch_size\": batch_size, \"learning_rate\": learning_rate, \"epochs\": epochs, \"pretrained_model\": pretrained_model, }, ) model_checkpoint_dir_path = os.path.join(args.mlflow.model_checkpoint_dir_path, args.mlflow.mlflow_run_name, datetime.datetime.now().strftime('%d%m%y_%H%M%S')) os.makedirs(model_checkpoint_dir_path, exist_ok=True) base_model = tf.keras.applications.MobileNetV2(input_shape= args.preprocess.input_shape, weights=\"imagenet\", include_top=False) for layer in base_model.layers: layer.trainable = False preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input global_average_layer = tf.keras.layers.GlobalAveragePooling2D() prediction_layer = tf.keras.layers.Dense(12,activation='softmax') inputs = tf.keras.Input(shape=(args.preprocess.input_shape)) x = preprocess_input(inputs) x = base_model(x, training=False) x = global_average_layer(x) outputs = prediction_layer(x) model = tf.keras.Model(inputs, outputs) # since the labels are integer we use sparse_categorical_crossentropy model.compile(optimizer=args.model.optimizer, loss=args.model.loss_fn, metrics=[args.model.metrics]) model.fit(train_ds, validation_data=val_ds, epochs=args.model.epochs) # Save the trained model trained_model_path = os.path.join( model_checkpoint_dir_path, \"trained_model.keras\" ) model.save(trained_model_path) logger.info(f\"Trained model saved to {trained_model_path}.\") # Log the trained model to mlflow amlo.general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=trained_model_path, artifact_path=\"model\", ) eval_results = model.evaluate(val_ds, verbose=1) test_loss, accuracy = eval_results mlflow.log_metric(key=\"val_loss\", value=test_loss) mlflow.log_metric(key=\"val_accuracy\", value=accuracy) # Log model configuration to mlflow amlo.general_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) amlo.general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") return test_loss, accuracy if __name__ == \"__main__\": main()", "source": "train.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception: logger.error(\"MLflow initialisation has failed.\") return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "import tensorflow as tf class PreprocessPipeline: def __init__(self, args): self.random_seed = args.general.random_seed self.args = args.preprocess def transform_data(self): img_width, img_height, n_channels = self.args.input_shape batch_size = self.args.batch_size data_dir = self.args.image_data_dir RANDOM_SEED = self.random_seed train_ds , val_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=0.2, subset=\"both\", seed=RANDOM_SEED, image_size=(img_height, img_width), batch_size=batch_size) return train_ds, val_ds", "source": "pre_process.py"}, {"content": "from . import general_utils from . import pre_process", "source": "__init__.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd import numpy as np class DataPipeline: def __init__(self): self.scaler = None def load_data(self,data_path): df = pd.read_csv(data_path) # Create a datetime column df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) df.set_index('datetime', inplace=True) return df def handle_missing_values(self,df, method='drop'): if method == 'drop': df = df.dropna() elif method == 'ffill': df = df.ffill() elif method == 'bfill': df = df.bfill() elif method == 'interpolate': df = df.interpolate(method='polynomial', order=2) elif method == 'moving_average': df = df.fillna(df.rolling(window=24*7, min_periods=1).mean()) else: raise ValueError(f'Invalid method: {method}') df.reset_index(drop=True, inplace=True) return df def run_data_pipeline(self, csv_path, missing_values='ignore'): # Your code here df = self.load_data(csv_path) if missing_values != 'ignore': df = self.handle_missing_values(df, missing_values) return df def create_time_lagged_features(self, df, time_steps=[1]): df_copy = df.copy() lagged_cols = [] for time_step in time_steps: for col in df.columns: col_name = col+'_lag'+str(time_step) lagged_cols.append(col_name) df_copy[col_name] = df_copy[col].shift(time_step) return df_copy , lagged_cols def create_future_targets(self, df,target_col, future_time_steps=[0]): df_copy = df.copy() lead_cols = [] for time_step in future_time_steps: col_name = target_col+'_lead'+str(time_step) df_copy[col_name] = df_copy[target_col].shift(-time_step) lead_cols.append(col_name) return df_copy, lead_cols def scale_data(self,df,scaler_type='standard'): scaler = None if scaler_type == 'standard': from sklearn.preprocessing import StandardScaler scaler = StandardScaler() elif scaler_type == 'minmax': from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() elif scaler_type == 'robust': from sklearn.preprocessing import RobustScaler scaler = RobustScaler() else: raise ValueError(f'Invalid scaler type: {scaler_type}') scaled_data = scaler.fit_transform(df) return scaled_data, scaler def run_processing_pipeline(self,df, feature_cols, target_col, tvs=0.2, time_steps=[1], future_time_steps=[1], scaler_type='standard'): train_df = df.iloc[0:int(len(df)*(1-tvs))] val_df = df.iloc[int(len(df)*(1-tvs)):] train_df, lagged_cols = self.create_time_lagged_features(train_df[feature_cols], time_steps) train_df, lead_cols = self.create_future_targets(train_df, target_col, future_time_steps) train_df = train_df.dropna() val_df, lagged_cols = self.create_time_lagged_features(val_df[feature_cols], time_steps) val_df, lead_cols = self.create_future_targets(val_df, target_col, future_time_steps) val_df = val_df.dropna() X_train, y_train = train_df[lagged_cols], train_df[lead_cols] X_val, y_val = val_df[lagged_cols], val_df[lead_cols] X_train_scaled, scaler = self.scale_data(X_train,scaler_type=scaler_type) self.scaler = scaler X_val_scaled = scaler.transform(X_val) y_train = np.array(y_train) y_val = np.array(y_val) return X_train_scaled, y_train, X_val_scaled, y_val def run_cv_processing_pipeline(self,df,feature_cols,target_col,train_index,val_index,time_steps=1,future_time_steps=1,scaler_type='standard'): train_df = df.iloc[train_index] val_df = df.iloc[val_index] train_df, lagged_cols = self.create_time_lagged_features(train_df[feature_cols], time_steps) train_df, lead_cols = self.create_future_targets(train_df, target_col, future_time_steps) train_df = train_df.dropna() val_df, lagged_cols = self.create_time_lagged_features(val_df[feature_cols], time_steps) val_df, lead_cols = self.create_future_targets(val_df, target_col, future_time_steps) val_df = val_df.dropna() X_train, y_train = train_df[lagged_cols], train_df[lead_cols] X_val, y_val = val_df[lagged_cols], val_df[lead_cols] X_train_scaled, scaler = self.scale_data(X_train,scaler_type=scaler_type) self.scaler = scaler X_val_scaled = scaler.transform(X_val) y_train = np.array(y_train) y_val = np.array(y_val) return X_train_scaled, y_train, X_val_scaled, y_val", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split dpl = DataPipeline() df = dpl.run_data_pipeline(data_path,missing_values='ignore') tvs = 0.2 train_df = df.iloc[0:int(len(df)*(1-tvs))] val_df = df.iloc[int(len(df)*(1-tvs)):] for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import root_mean_squared_error class ForecastModel: def __init__(self,verbose=0): self.model = RandomForestRegressor(random_state=55,verbose=verbose,n_jobs=-1) def fit(self, X, y): self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): y_train_pred = self.model.predict(X_train) train_error = root_mean_squared_error(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = root_mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch import torch.nn as nn class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): super(RNNModel, self).__init__() self.hidden_dim = num_rnn self.num_layers = num_layers self.output_size = output_size # batch first means input and output will have shape (batch, seq, feature) self.rnn = nn.RNN(input_size, num_rnn, num_layers, batch_first=True) self.fc = nn.Linear(num_rnn, output_size) def forward(self, x): # X has shape (batch, seq_length, input_size) batch_size = x.size(0) # initialize hidden state with zeros h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim) #print(batch_size) # get rnn outputs r_out, _ = self.rnn(x, h0) #print('r_out',r_out.shape) # shape output to be (batch*seq, num_rnn) #r_out = r_out.view(-1, self.hidden_dim) out = self.fc(r_out[:, -1, :]) #print('out',out.shape) out = out.view(batch_size, 1, self.output_size) #print('fc',out.shape) return out def fit(self, dataloader): features, labels = next(iter(dataloader)) #print(self.forward(features)) # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset, IterableDataset class WindowGenerator(IterableDataset): def __init__(self, df, lookback, lookahead, feature_cols, target_col): self.lookback = lookback self.lookahead = lookahead self.feature_cols = feature_cols self.target_col = target_col self.n_features = len(feature_cols) self.data = torch.FloatTensor(df[feature_cols + [target_col]].values) self.length = len(df) - lookback - lookahead self._i = -1 def __len__(self): return self.length def __iter__(self): return self def __getitem__(self, idx): features = self.data[idx:idx+self.lookback,:self.n_features] labels = self.data[idx+self.lookback:idx+self.lookback+self.lookahead,self.n_features:] if(torch.isnan(features).any() or torch.isnan(labels).any()): #print(\"Nan values found\", idx, features, labels) return next(self) # raise ValueError return features, labels def __next__(self): self._i += 1 if self._i == self.length: # out of instances self._i = -1 # reset the iterable raise StopIteration # stop the iteration try: return self.__getitem__(self._i) except ValueError: return next(self)", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53,0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin from sklearn.feature_selection import SelectPercentile, mutual_info_classif class LogTransformerWithFeatureNames(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that applies a log1p transformation to specified features and preserves their names in the output. Attributes: feature_names (list of str): List of feature names to be transformed. transformer (FunctionTransformer): The internal transformer that applies the log1p transformation. \"\"\" def __init__(self, feature_names): \"\"\" Initializes the transformer with the specified feature names. :param feature_names: List of feature names to be transformed. \"\"\" self.feature_names = feature_names self.transformer = FunctionTransformer(np.log1p, validate=False) def fit(self, X, y=None): \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" self.transformer.fit(X, y) return self def transform(self, X): \"\"\" Apply the log1p transformation to the input data. :param X: Input data to be transformed. :return: Transformed data with log1p applied to specified features. \"\"\" transformed_X = self.transformer.transform(X.values) # Convert back to DataFrame after transformation transformed_X_df = pd.DataFrame(transformed_X, columns=self.get_feature_names_out(X.columns)) return transformed_X_df def get_feature_names_out(self, input_features=None): \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, with \"log_\" prefixed to the original names. \"\"\" return [f\"log_{feature}\" for feature in self.feature_names] class FinancialPresenceTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that creates binary features indicating the presence of financial variables. Attributes: features (list of str): List of feature names to be transformed into binary indicators. \"\"\" def __init__(self, features): \"\"\" Initializes the transformer with the specified features. :param features: List of feature names to be transformed into binary indicators. \"\"\" self.features = features def fit(self, X, y=None): \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X): \"\"\" Apply the binary transformation to the input data, creating 'has_{feature}' columns. :param X: Input data to be transformed. :return: Transformed data with binary presence indicators for specified features. \"\"\" X_transformed = X.copy() for feature in self.features: X_transformed[f'has_{feature}'] = (X_transformed[feature] > 0).astype(int) return X_transformed.drop(columns=self.features) def get_feature_names_out(self, input_features=None): \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, where each original feature is prefixed with \"has_\". \"\"\" return [f'has_{feature}' for feature in self.features] class InteractionFeatureTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that creates a new feature as the product of two specified features. Attributes: feature_1 (str): Name of the first feature. feature_2 (str): Name of the second feature. new_feature_name (str): Name of the new interaction feature created by multiplying feature_1 and feature_2. \"\"\" def __init__(self, feature_1, feature_2, new_feature_name): \"\"\" Initializes the transformer with the specified features and the name for the new interaction feature. :param feature_1: Name of the first feature. :param feature_2: Name of the second feature. :param new_feature_name: Name of the new interaction feature. \"\"\" self.feature_1 = feature_1 self.feature_2 =", "source": "datapipeline.py"}, {"content": "feature_2 self.new_feature_name = new_feature_name def fit(self, X, y=None): \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X): \"\"\" Apply the interaction transformation to the input data, creating a new feature. :param X: Input data to be transformed. :return: Transformed data with the new interaction feature added. \"\"\" X_transformed = X.copy() X_transformed[self.new_feature_name] = X_transformed[self.feature_1] * X_transformed[self.feature_2] return X_transformed def get_feature_names_out(self, input_features=None): \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, including the new interaction feature. \"\"\" return list(input_features) + [self.new_feature_name] def transform(data_path, test_size=0.1, sample_size=None, return_as_df=False, percentile=None, drop_duplicates=False): \"\"\" Transform the input dataset by applying a series of preprocessing steps. :param data_path: Path to the dataset (CSV file). :param test_size: Proportion of the dataset to include in the test split. :param sample_size: Number of rows to sample from the dataset. If None, use the entire dataset. :param return_as_df: Boolean to return DataFrame or NumPy arrays. Defaults to False (NumPy arrays). :param percentile: Percentile of features to keep. If None, use all features. :param drop_duplicates: Boolean flag to indicate whether to drop duplicate rows. Default is False. :return: X_train, X_test, y_train, y_test as numpy arrays or DataFrames based on return_as_df. \"\"\" # Load the dataset df = pd.read_csv(data_path) # Clean the data df = data_cleaning(df, drop_duplicates) # Sample a variable number of rows, if specified if sample_size is not None: df = df.sample(n=sample_size, random_state=42) # Separate the target variable 'income_group' from the features y = df['income_group'].values df.drop('income_group', axis=1, inplace=True) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=test_size, random_state=42) # Create the preprocessing pipeline preprocessor = create_preprocessor(percentile=percentile) # Fit the preprocessor on the training data X_train_transformed = preprocessor.fit_transform(X_train, y_train) # Transform the test data using the same preprocessor X_test_transformed = preprocessor.transform(X_test) # Convert the transformed data into DataFrames if needed if return_as_df: X_train_transformed = pd.DataFrame(X_train_transformed, columns=preprocessor.get_feature_names_out()) X_test_transformed = pd.DataFrame(X_test_transformed, columns=preprocessor.get_feature_names_out()) return X_train_transformed, X_test_transformed, y_train, y_test def data_cleaning(df, drop_duplicates): \"\"\" Perform data cleaning steps on the dataframe. :param df: The input pandas DataFrame to be cleaned. :param drop_duplicates: Boolean flag to drop duplicates or not. :return: The cleaned DataFrame. \"\"\" # Convert the target variable 'income_group' to a binary format df['income_group'] = df['income_group'].replace({'- 50000.': 0, '50000+.': 1}) # Drop the 'id' column as it is unnecessary df.drop('id', axis=1, inplace=True) # Replace \"Not in universe\" with \"Not applicable\" in the column df['enroll_in_edu_inst_last_wk'] = df['enroll_in_edu_inst_last_wk'].replace('Not in universe', 'Not applicable') # Replace \"?\" with \"Not Known\" in the country_of_birth_father column df['country_of_birth_father'] = df['country_of_birth_father'].replace('?', 'Not Known') # Replace \"?\" with \"Not Known\" in the country_of_birth_mother column df['country_of_birth_mother'] = df['country_of_birth_mother'].replace('?', 'Not Known') # Replace \"?\" with \"Not Known\" in the country_of_birth_self column df['country_of_birth_self'] = df['country_of_birth_self'].replace('?', 'Not Known') # Replace \"Not in universe or children\" with \"Not applicable for this or child questions\" df['major_industry_code'] = df['major_industry_code'].replace('Not in universe or children', 'Not applicable for this or child questions') # Replace \"Not in universe\" with \"Not applicable\"", "source": "datapipeline.py"}, {"content": "in the column df['major_occupation_code'] = df['major_occupation_code'].replace('Not in universe', 'Not applicable') # Find the mode of the 'hispanic_origin' column mode_value_hispanic_origin = df['hispanic_origin'].mode()[0] # Replace None with the mode df['hispanic_origin'] = df['hispanic_origin'].fillna(mode_value_hispanic_origin) # Replace \"Not in universe\" with \"Not applicable\" in the column df['member_of_a_labor_union'] = df['member_of_a_labor_union'].replace('Not in universe', 'Not applicable') # Replace \"Not in universe\" with \"Not applicable\" in the column df['reason_for_unemployment'] = df['reason_for_unemployment'].replace('Not in universe', 'Not applicable') # Replace \"Not in universe\" with \"Not applicable\" in the column df['family_members_under_18'] = df['family_members_under_18'].replace('Not in universe', 'Not applicable') # Replace \"Not in universe\" with \"Not applicable\" in the column df['region_of_previous_residence'] = df['region_of_previous_residence'].replace('Not in universe', 'Not applicable') # Find the mode of the 'state_of_previous_residence' column mode_value_state_of_previous_residence = df['state_of_previous_residence'].mode()[0] # Replace None with the mode df['state_of_previous_residence'] = df['state_of_previous_residence'].fillna(mode_value_state_of_previous_residence) # Replace \"Not in universe\" with \"Not applicable\" in the column df['state_of_previous_residence'] = df['state_of_previous_residence'].replace('Not in universe', 'Not applicable') # Replace \"?\" with \"Not Known\" in the migration_code_change_in_msa column df['migration_code_change_in_msa'] = df['migration_code_change_in_msa'].replace({'?': 'Not Known', 'Not in universe': 'Not applicable' }) # Replace \"?\" with \"Not Known\" in the migration_code_change_in_reg column df['migration_code_change_in_reg'] = df['migration_code_change_in_reg'].replace({'?': 'Not Known', 'Not in universe': 'Not applicable' }) # Replace \"Not in universe\" with \"Not applicable\" in the column df['fill_inc_questionnaire_for_veteran_s_admin'] = df['fill_inc_questionnaire_for_veteran_s_admin'].replace('Not in universe', 'Not applicable') # Replace values in the 'veterans_benefits' column df['veterans_benefits'] = df['veterans_benefits'].replace({ '2': 'Not Applicable', '1': 'No', '0': 'Yes' }) # Replace \"?\" with \"Not Known\" in the migration_code_move_within_reg column df['migration_code_move_within_reg'] = df['migration_code_move_within_reg'].replace({'?': 'Not Known', 'Not in universe': 'Not applicable' }) # Replace \"Not in universe under 1 year old\" with \"Not applicable\" in the column df['live_in_this_house_1_year_ago'] = df['live_in_this_house_1_year_ago'].replace('Not in universe under 1 year old', 'Not applicable') # Replace \"?\" with \"Not Known\" in the migration_prev_res_in_sunbelt column df['migration_prev_res_in_sunbelt'] = df['migration_prev_res_in_sunbelt'].replace({'?': 'Not Known', 'Not in universe': 'Not applicable' }) # Drop duplicate rows if drop_duplicates is True if drop_duplicates: df = df.drop_duplicates() return df def create_preprocessor(percentile=None): \"\"\" Creates the preprocessing pipeline by combining various feature transformation pipelines. :param percentile: Percentile of features to select. If None, keep all features. :return: A ColumnTransformer object containing the complete preprocessing pipeline. \"\"\" # ASSIGNING VARIABLES # Numeric columns numeric_features = ['age', 'num_persons_worked_for_employer', 'weeks_worked_in_year'] # Columns for one hot encoder nominal_features = ['enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'class_of_worker', 'major_industry_code', 'major_occupation_code', 'hispanic_origin', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'detailed_household_summary_in_household', 'family_members_under_18', 'tax_filer_stat', 'region_of_previous_residence', 'state_of_previous_residence', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt', 'detailed_industry_recode', 'detailed_occupation_recode', 'own_business_or_self_employed', 'veterans_benefits', 'year'] # Columns for ordinal encoder ordinal_features = ['education', 'full_or_part_time_employment_stat'] # Columns for interaction pipeline interaction_features = ['weeks_worked_in_year', 'num_persons_worked_for_employer'] # Columns for log transformation log_features = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] # Columns for binary feature creation financial_features = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] # Columns for polynomial features age_feature = ['age'] # The feature for polynomial expansion # Ordinal ranks for education education_order = [ 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate", "source": "datapipeline.py"}, {"content": "degree(PhD EdD)', 'Children' ] # Ordinal ranks for employment employment_order = [ 'Children or Armed Forces', 'Not in labor force', 'Unemployed part- time', 'Unemployed full-time', 'PT for econ reasons usually PT', 'PT for econ reasons usually FT', 'PT for non-econ reasons usually FT', 'Full-time schedules' ] # 1. Numeric Pipeline numeric_pipeline = Pipeline(steps=[ ('scaler', StandardScaler()) # Scale numeric features ]) # 2. Log Transformation Pipeline (with Scaling) log_pipeline = Pipeline(steps=[ ('log_transform', LogTransformerWithFeatureNames(log_features)), ('scaler', StandardScaler()) ]) # 3. Nominal Pipeline (One-Hot Encoding) nominal_pipeline = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-hot encode nominal features ]) # 4. Ordinal Pipeline ordinal_pipeline = Pipeline(steps=[ ('ordinal', OrdinalEncoder(categories=[education_order, employment_order])) # Ordinal encoding for ordered categories ]) # 5. Financial Presence Pipeline financial_pipeline = Pipeline(steps=[ ('financial_presence', FinancialPresenceTransformer(features=financial_features)) # Create binary features for financial presence ]) # 6. Interaction Feature Pipeline (with Scaling) interaction_pipeline = Pipeline(steps=[ ('interaction', InteractionFeatureTransformer(interaction_features[0], interaction_features[1], 'employment_intensity')), ('scaler', StandardScaler()) # Scale the interaction feature ]) # 7. Pipeline for Age (with Polynomial Features) age_pipeline = Pipeline(steps=[ ('poly', PolynomialFeatures(degree=3, include_bias=False)), # Generate polynomial features for 'age' ('scaler', StandardScaler()) # Scale the polynomial features ]) # Combine all pipelines into a ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('age', age_pipeline, age_feature), # Apply age pipeline to 'age' ('num', numeric_pipeline, numeric_features), # Scale numeric features ('log', log_pipeline, log_features), # Apply the log pipeline to specified features ('nom', nominal_pipeline, [col for col in nominal_features if col not in ordinal_features]), # One-hot encode nominal features ('ord', ordinal_pipeline, ordinal_features), # Ordinal encoding for ordered categories ('fin', financial_pipeline, financial_features), # Create binary financial presence features ('int', interaction_pipeline, interaction_features) # Create and scale interaction features ] ) # If a percentile is provided, add SelectPercentile step if percentile is not None: preprocessor = Pipeline(steps=[ ('preprocessor', preprocessor), ('select_percentile', SelectPercentile(score_func=mutual_info_classif, percentile=percentile)) ]) return preprocessor", "source": "datapipeline.py"}, {"content": "import numpy as np class DecisionTree: def __init__(self, max_depth=None, min_samples_split=2): \"\"\" Initializes the decision tree with optional max_depth and min_samples_split. :param max_depth: The maximum depth of the tree. If None, there is no limit. :param min_samples_split: The minimum number of samples required to split a node. \"\"\" self.max_depth = max_depth self.min_samples_split = min_samples_split self.tree = None def fit(self, X, y, depth=0): \"\"\" Fit the decision tree to the training data. :param X: The training data, a NumPy array of shape (m, n) where 'm' is the number of samples and 'n' is the number of features. :param y: The labels, a NumPy array of shape (m,). :param depth: The current depth of the tree (used for max_depth check). \"\"\" # Check for stopping conditions if len(set(y)) == 1: return {'label': y[0]} # Pure node (all labels are the same) if self.max_depth is not None and depth >= self.max_depth: return {'label': self._most_common_label(y)} # Max depth reached if len(X) < self.min_samples_split: return {'label': self._most_common_label(y)} # Not enough samples to split # Find the best feature and value to split the data best_split = self._find_best_split(X, y) if best_split is None: return {'label': self._most_common_label(y)} # No good split found # Split the data based on the best split left_indices, right_indices = self._split(X, best_split['feature'], best_split['value']) left_subtree = self.fit(X[left_indices], y[left_indices], depth + 1) right_subtree = self.fit(X[right_indices], y[right_indices], depth + 1) self.tree = {'feature': best_split['feature'], 'value': best_split['value'], 'left': left_subtree, 'right': right_subtree} return self.tree def _find_best_split(self, X, y): \"\"\" Find the best feature and value to split the data. :param X: The training data. :param y: The labels. :return: A dictionary with the best feature and value, or None if no good split is found. \"\"\" best_gini = float('inf') best_split = None for feature in range(X.shape[1]): # Iterate through each feature unique_values = np.unique(X[:, feature]) # Get unique values of the feature for value in unique_values: # Iterate through each value left_indices, right_indices = self._split(X, feature, value) if len(left_indices) == 0 or len(right_indices) == 0: continue # Skip invalid splits # Calculate Gini impurity for the split gini = self.gini(list(y[left_indices]), list(y[right_indices])) if gini < best_gini: best_gini = gini best_split = {'feature': feature, 'value': value} return best_split def _split(self, X, feature, value): \"\"\" Split the dataset into left and right subsets based on the given feature and value. :param X: The dataset. :param feature: The feature index to split on. :param value: The value of the feature to split on. :return: Two lists of indices: left and right, representing the two subsets. \"\"\" left_indices = np.where(X[:, feature] <= value)[0] right_indices = np.where(X[:, feature] > value)[0] return left_indices, right_indices def gini(self, left_labels, right_labels): \"\"\" Calculate the Gini impurity for two nodes (left and right). :param left_labels: List of labels in the left node :param right_labels: List of labels in the right node :return: Gini impurity score for the split \"\"\" # Combine both left and right node labels total_labels = left_labels + right_labels total = len(total_labels) # If there are no labels, return 0 if total == 0: return 0 # Gini impurity for the left node def gini_impurity(labels): \"\"\" Calculate the", "source": "decision_tree.py"}, {"content": "Gini impurity for a given set of labels. Gini impurity measures the likelihood of an incorrect classification of a randomly chosen element, assuming that the element is classified according to the distribution of labels in the dataset. :param labels: List of labels (e.g., [0, 1, 0, 1, 1]) for which Gini impurity is calculated. :return: Gini impurity score (float), ranging between 0 (perfectly pure) and 0.5 (maximum impurity). \"\"\" size = len(labels) if size == 0: return 0 p_0 = labels.count(0) / size p_1 = labels.count(1) / size return 1 - (p_0 ** 2) - (p_1 ** 2) # Calculate the Gini impurity for the left and right nodes left_size = len(left_labels) right_size = len(right_labels) gini_left = gini_impurity(left_labels) gini_right = gini_impurity(right_labels) # Calculate the weighted Gini impurity for the split gini_split = (left_size / total) * gini_left + (right_size / total) * gini_right return gini_split def _most_common_label(self, y): \"\"\" Return the most common label from the list. :param y: The list of labels. :return: The most common label. \"\"\" return np.bincount(y).argmax() def predict(self, X): \"\"\" Predict the label for each sample in X. :param X: NumPy array of shape (m, n) where 'm' is the number of samples and 'n' is the number of features. :return: NumPy array of predicted labels. \"\"\" return np.array([self._predict_single(input_data, self.tree) for input_data in X]) def _predict_single(self, x, tree): \"\"\" Predict a single sample using the decision tree. :param x: A single sample (array) of shape (n,). :param tree: The current node in the decision tree. :return: The predicted label for the sample. \"\"\" if 'label' in tree: return tree['label'] # If we reached a leaf, return the label feature_val = x[tree['feature']] if feature_val <= tree['value']: return self._predict_single(x, tree['left']) else: return self._predict_single(x, tree['right'])", "source": "decision_tree.py"}, {"content": "import logging from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import f1_score, precision_score, recall_score from imblearn.pipeline import Pipeline as ImbPipeline from imblearn.combine import SMOTETomek from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.tree import DecisionTreeClassifier from yellowbrick.classifier import ConfusionMatrix, ClassificationReport, ROCAUC, PrecisionRecallCurve, ClassPredictionError, DiscriminationThreshold import matplotlib.pyplot as plt from sklearn.pipeline import Pipeline logger = logging.getLogger(__name__) class Model: def __init__(self, model_type='random_forest'): \"\"\" Initialize the model based on the model type. :param model_type: str, either 'random_forest', 'logistic_regression', 'adaboost', or 'gradient_boost'. \"\"\" self.model_type = model_type self.pipeline = None self.params = self.get_default_params() # Load default params on initialization def set_params(self, custom_params): \"\"\" Customize the parameters for the model. :param custom_params: Dictionary of custom parameters. \"\"\" # Update the default parameters with any custom parameters provided self.params.update(custom_params) return self.params def train(self, params, X_train, y_train): \"\"\" Train the specified model (Random Forest) with SMOTETomek for handling imbalanced data. :param params: Dictionary of hyperparameters to be used for the model. :param X_train: Training feature matrix. :param y_train: Training target vector. :return: Train F1-score as a float. \"\"\" self.params = params if self.model_type == 'random_forest': # Create a pipeline with SMOTETomek and RandomForestClassifier self.pipeline = ImbPipeline(steps=[ ('smote_tomek', SMOTETomek(random_state=42)), ('classifier', RandomForestClassifier(**self.params)) ]) elif self.model_type == 'logistic_regression': # Create a pipeline with SMOTETomek and LogisticRegression self.pipeline = ImbPipeline(steps=[ ('smote_tomek', SMOTETomek(random_state=42)), ('classifier', LogisticRegression(**self.params)) ]) elif self.model_type == 'adaboost': # # Create a pipeline with SMOTETomek and AdaBoostClassifier # self.pipeline = ImbPipeline(steps=[ # ('smote_tomek', SMOTETomek(random_state=42)), # ('classifier', AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), **self.params)) # ]) # Pass decision tree parameters directly into the DecisionTreeClassifier decision_tree_params = { 'max_depth': self.params.pop('max_depth', 1), 'min_samples_leaf': self.params.pop('min_samples_leaf', 1), 'min_samples_split': self.params.pop('min_samples_split', 2), 'random_state': self.params.get('random_state', 42) } self.pipeline = ImbPipeline(steps=[ ('smote_tomek', SMOTETomek(random_state=42)), ('classifier', AdaBoostClassifier( estimator=DecisionTreeClassifier(**decision_tree_params), **self.params )) ]) elif self.model_type == 'gradient_boost': # Create a pipeline with SMOTETomek and GradientBoostingClassifier self.pipeline = ImbPipeline(steps=[ ('smote_tomek', SMOTETomek(random_state=42)), ('classifier', GradientBoostingClassifier(**self.params)) ]) else: raise ValueError(f\"Unsupported model type: {self.model_type}\") # Fit the pipeline on the training data self.pipeline.fit(X_train, y_train) # Predict on the training data to calculate the training F1 score y_train_pred = self.pipeline.predict(X_train) train_f1 = f1_score(y_train, y_train_pred) return train_f1 def evaluate(self, X_test, y_test): \"\"\" Evaluate the trained model on the test data. :param X_test: Test feature matrix. :param y_test: Test target vector. :return: Test F1-score as a float. \"\"\" if self.pipeline is None: logger.error(\"Pipeline is not trained yet. Call the train method first.\") return None # Predict on the test data y_test_pred = self.pipeline.predict(X_test) test_f1 = f1_score(y_test, y_test_pred) return test_f1 def get_default_params(self): \"\"\" Get the default parameters for the selected model. :return: Dictionary of default parameters. \"\"\" if self.model_type == 'random_forest': return { 'criterion': 'log_loss', # The function to measure the quality of a split 'max_features': 'sqrt', # The number of features to consider when looking for the best split 'n_estimators': 288, # Number of trees in the forest 'max_depth': 20, # Maximum depth of the tree 'min_samples_split': 9, # Minimum number of samples required to split an internal node 'random_state': 42, # Ensuring reproducibility 'n_jobs': -1 # Use all available cores for parallel processing } elif self.model_type == 'logistic_regression': return { 'penalty': 'l1', 'C': 0.59874749104614, 'random_state': 42, 'solver': 'liblinear', 'max_iter': 1000 } elif self.model_type == 'adaboost':", "source": "model.py"}, {"content": "return { 'n_estimators': 186, 'learning_rate': 0.5037955963643908, 'max_depth': 1, # No estimator__ prefix 'min_samples_leaf': 5, # No estimator__ prefix 'min_samples_split': 15, # No estimator__ prefix 'random_state': 42 } elif self.model_type == 'gradient_boost': return { 'n_estimators': 242, 'learning_rate': 0.07955160864261275, 'max_depth': 7, 'max_features': 'sqrt', 'min_samples_leaf': 7, 'min_samples_split': 14, 'subsample': 0.9266653415629146, 'random_state': 42 } # { # 'classifier__learning_rate': 0.047454011884736254, # 'classifier__max_depth': 7, # 'classifier__max_features': None, # 'classifier__min_samples_leaf': 8, # 'classifier__min_samples_split': 8, # 'classifier__n_estimators': 221, # 'classifier__subsample': 0.7467983561008608 # } else: raise ValueError(f\"Unsupported model type: {self.model_type}\") def fine_tuning(self, param_dist, algo_type='random', n_iter=20, cv=5, scoring='f1', X_train=None, y_train=None): \"\"\" Perform hyperparameter fine-tuning using RandomizedSearchCV or GridSearchCV. :param param_dist: Dictionary of hyperparameters to test. :param algo_type: Type of hyperparameter search algorithm ('random' for RandomizedSearchCV, 'grid' for GridSearchCV). :param n_iter: Number of iterations for RandomizedSearchCV (only applicable if algo_type='random'). :param cv: Number of cross-validation folds. :param scoring: Scoring metric. :param X_train: Training feature matrix. :param y_train: Training target vector. :return: Best model and best parameters. \"\"\" # Check if the pipeline is trained if self.pipeline is None: logger.error(\"Pipeline is not trained yet. Call the train method first.\") return None # Choose between RandomizedSearchCV and GridSearchCV if algo_type == 'random': search = RandomizedSearchCV( estimator=self.pipeline, param_distributions=param_dist, n_iter=n_iter, cv=cv, scoring=scoring, random_state=42, n_jobs=-1 ) elif algo_type == 'grid': search = GridSearchCV( estimator=self.pipeline, param_grid=param_dist, cv=cv, scoring=scoring, n_jobs=-1 ) else: raise ValueError(\"Unsupported search algorithm. Choose 'random' or 'grid'.\") # Fit the search algorithm to the data search.fit(X_train, y_train) # Get the best model and best parameters best_model = search.best_estimator_ best_params = search.best_params_ # Update self.pipeline with the best model self.pipeline = best_model # Update self.params with the best parameters self.params = best_params return best_model, best_params def print_scores(self, X_train, X_test, y_train, y_test): \"\"\" Print F1-score, precision, and recall for both training and test sets. :param X_train: Training feature matrix. :param X_test: Test feature matrix. :param y_train: Training target vector. :param y_test: Test target vector. \"\"\" if self.pipeline is None: logger.error(\"Pipeline is not trained yet. Call the train method first.\") return None # Predict on training and test data y_train_pred = self.pipeline.predict(X_train) y_test_pred = self.pipeline.predict(X_test) # Calculate and print scores for the training set train_f1 = f1_score(y_train, y_train_pred) train_precision = precision_score(y_train, y_train_pred) train_recall = recall_score(y_train, y_train_pred) print(f\"Train F1 Score: {train_f1:.4f}\") print(f\"Train Precision: {train_precision:.4f}\") print(f\"Train Recall: {train_recall:.4f}\") # Calculate and print scores for the test set test_f1 = f1_score(y_test, y_test_pred) test_precision = precision_score(y_test, y_test_pred) test_recall = recall_score(y_test, y_test_pred) print(f\"Test F1 Score: {test_f1:.4f}\") print(f\"Test Precision: {test_precision:.4f}\") print(f\"Test Recall: {test_recall:.4f}\") def create_fit_and_visualize_pipeline(self, X_train, X_test, y_train, y_test, label_mapping): \"\"\" Visualize a fitted scikit-learn pipeline with the specified preprocessor and model. :param X_train: Training features :param X_test: Test features :param y_train: Training labels :param y_test: Test labels :param label_mapping: Mapping for the labels :return: Pipeline: Fitted scikit-learn pipeline \"\"\" if self.pipeline is None: logger.error(\"Pipeline is not trained yet. Call the train method first.\") return None # Use the fitted pipeline (self.pipeline) for visualizations pipeline = self.pipeline # Create confusion matrix fig, ax = plt.subplots(figsize=(6, 6)) cm_viz = ConfusionMatrix(pipeline, classes=[\"Income < 50K\", \"Income >= 50K\"], label_encoder=label_mapping, ax=ax ) cm_viz.score(X_test, y_test) cm_viz.show() # Create classification report fig, ax = plt.subplots(figsize=(6, 6)) cr_viz = ClassificationReport(pipeline, classes=[\"Income < 50K\",", "source": "model.py"}, {"content": "\"Income >= 50K\"], label_encoder=label_mapping, support=True, ax=ax ) cr_viz.score(X_test, y_test) cr_viz.show() # Create ROC AUC graph fig, ax = plt.subplots(figsize=(6, 6)) roc_viz = ROCAUC(pipeline, classes=[\"Income < 50K\", \"Income >= 50K\"], ax=ax ) roc_viz.fit(X_train, y_train) roc_viz.score(X_test, y_test) roc_viz.show() # Create precision recall curve fig, ax = plt.subplots(figsize=(6, 6)) prc_viz = PrecisionRecallCurve(pipeline) prc_viz.fit(X_train, y_train) prc_viz.score(X_test, y_test) prc_viz.show() # Create class prediction error plot fig, ax = plt.subplots(figsize=(6, 6)) cpe_viz = ClassPredictionError(pipeline, classes=[\"Income < 50K\", \"Income >= 50K\"], ax=ax ) cpe_viz.score(X_test, y_test) cpe_viz.show() # Create discrimination threshold plot fig, ax = plt.subplots(figsize=(6, 6)) dt_viz = DiscriminationThreshold(pipeline) dt_viz.fit(X_train, y_train) dt_viz.show() return pipeline", "source": "model.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree # Import the DecisionTree class you previously implemented class RandomForest: def __init__(self, n_trees=5, max_depth=None, min_samples_split=2, subsample_size=1.0, sample_with_replacement=True, feature_proportion=1.0): \"\"\" Initializes the random forest with the specified number of trees and parameters. :param n_trees: The number of decision trees in the forest. :param max_depth: The maximum depth of each tree. :param min_samples_split: The minimum number of samples required to split a node. :param subsample_size: The proportion of the dataset to subsample for each tree (1.0 means 100%). :param sample_with_replacement: Whether to sample with replacement (default is True). :param feature_proportion: The proportion of features to be used in each tree (between 0 and 1). \"\"\" self.n_trees = n_trees self.max_depth = max_depth self.min_samples_split = min_samples_split self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.trees = [] # This will store the list of trained decision trees self.feature_indices = [] # List to store the features selected for each tree def _bootstrap_sample(self, X, y): \"\"\" Create a bootstrap sample (sampled with or without replacement) from the dataset. :param X: The feature matrix. :param y: The labels. :return: A tuple of (X_sample, y_sample), the subsampled dataset. \"\"\" n_samples = X.shape[0] n_subsample = int(self.subsample_size * n_samples) # Calculate subsample size # Choose indices either with or without replacement indices = np.random.choice(n_samples, size=n_subsample, replace=self.sample_with_replacement) return X[indices], y[indices] def fit(self, X, y): \"\"\" Fit the random forest to the training data. :param X: The training data, a NumPy array of shape (m, n). :param y: The labels, a NumPy array of shape (m,). \"\"\" self.trees = [] self.feature_indices = [] n_features = X.shape[1] # Total number of features for _ in range(self.n_trees): # Create a bootstrap sample X_sample, y_sample = self._bootstrap_sample(X, y) # Select a random subset of features based on feature_proportion n_subsample_features = int(self.feature_proportion * n_features) feature_indices = np.random.choice(n_features, size=n_subsample_features, replace=False) self.feature_indices.append(feature_indices) # Store the selected features for this tree # Initialize a decision tree and fit it to the bootstrap sample with the selected features tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split) tree.fit(X_sample[:, feature_indices], y_sample) # Fit using the selected features # Add the trained tree to the list of trees self.trees.append(tree) def predict(self, X): \"\"\" Predict the labels for the input data using the random forest (majority voting). :param X: The input data, a NumPy array of shape (m, n). :return: A NumPy array of shape (m,) with the predicted labels. \"\"\" # Get predictions from all the trees using their respective feature subsets tree_preds = np.array([tree.predict(X[:, feature_indices]) for tree, feature_indices in zip(self.trees, self.feature_indices)]) # Perform majority voting across the trees majority_vote = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_preds) return majority_vote", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "# Configuration file for the Sphinx documentation builder. # # This file only contains a selection of the most common options. For a full # list see the documentation: # https://www.sphinx-doc.org/en/master/usage/configuration.html # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys sys.path.insert(0, os.path.abspath(\"../src\")) # -- Project information ----------------------------------------------------- project = \"assignment 4\" copyright = \"2024, wes_lee\" author = \"wes_lee\" # The full version, including alpha/beta/rc tags release = \"v1.0.0\" # -- General configuration --------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ \"sphinx.ext.napoleon\", \"sphinx.ext.autodoc\" ] # Add any paths that contain templates here, relative to this directory. templates_path = [\"_templates\"] # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path. exclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"] # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = \"alabaster\" # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named \"default.css\" will overwrite the builtin \"default.css\". html_static_path = [\"static\"]", "source": "conf.py"}, {"content": "\"\"\"Script to conduct batch inferencing. \"\"\" import os import datetime import logging import glob import hydra import jsonlines import assignment_4 as ass4 # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"batch_infer.yaml\") def main(args): \"\"\"This main function does the following: - load logging config - gets list of files to be loaded for inferencing - loads trained model - conducts inferencing on data - outputs prediction results to a jsonline file - returns an error if there are no files/data specified to be inferred \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") logger_config_path = os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ass4.general_utils.setup_logging(logger_config_path) logger.info(\"Loading the model...\") loaded_model = ass4.modeling.utils.load_model() glob_expr = os.path.join( args[\"input_data_dir\"], args[\"file_check_glob\"] ) logger.info(\"Conducting inferencing on text files...\") jsonl_path = os.path.join( os.getcwd(), args[\"output_path\"] ) has_text = False # Checking if for loop is running for text_file in glob.glob(glob_expr): has_text = True text = open(text_file).read() pred_str = loaded_model.predict(text) curr_time = datetime.datetime.now(datetime.timezone.utc).strftime( \"%Y-%m-%dT%H:%M:%S%z\" ) curr_res_jsonl = { \"time\": curr_time, \"text_filepath\": text_file, \"prediction\": pred_str, } with jsonlines.open(jsonl_path, mode=\"a\") as writer: writer.write(curr_res_jsonl) writer.close() if not has_text: e = \"Folder {} has no files to infer according to the given glob '{}'.\".format( os.path.abspath(args[\"input_data_dir\"]), args[\"file_check_glob\"] ) logger.error(e) raise FileNotFoundError(e) logger.info(\"Batch inferencing has completed.\") logger.info(\"Output result location: %s\", jsonl_path) if __name__ == \"__main__\": main()", "source": "batch_infer.py"}, {"content": "\"\"\"Script for testing of logging to MLflow server. \"\"\" import sys import time import random import mlflow def main(): \"\"\"Main function for testing of logging to MLflow server.\"\"\" mlflow.set_tracking_uri(sys.argv[1]) mlflow.set_experiment(sys.argv[2]) dummy_param_1 = 64 dummy_param_2 = 1.0 tracking_uri = mlflow.get_tracking_uri() experiment = mlflow.get_experiment_by_name(sys.argv[2]) print(\"Current tracking URI: \", tracking_uri) print(\"Current unique experiment ID: \", experiment.experiment_id) print(\"Current location of artifacts: \", experiment.artifact_location) with mlflow.start_run(): print(\"Logging parameters...\") mlflow.log_param(\"dummy_param_1\", dummy_param_1) mlflow.log_param(\"dummy_param_2\", dummy_param_2) print(\"Logging metrics...\") for step in range(1, 5): mlflow.log_metric(\"dummy_metric_1\", time.time(), step=step) mlflow.log_metric(\"dummy_metric_2\", random.uniform(0.1, 0.5), step=step) time.sleep(2) dummy_text_content = \"This text content should be uploaded to the bucket.\" with open(\"text_artifact.txt\", \"w\", encoding=\"utf-8\") as file: file.write(dummy_text_content) mlflow.log_artifact(\"text_artifact.txt\") artifact_uri = mlflow.get_artifact_uri() print(\"Current artifact URI: \", artifact_uri) if __name__ == \"__main__\": main()", "source": "mlflow_test.py"}, {"content": "\"\"\" This script is a template for processing data. \"\"\" import os import shutil import logging import hydra import assignment_4 as ass4 # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"process_data.yaml\") def main(args) -> None: \"\"\"This function is a template to process data. Parameters ---------- args: omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") ass4.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) raw_data_dir_path = args[\"raw_data_dir_path\"] dataset = ass4.data_prep.datasets.DummyDataset( raw_data_dir_path ) processed_data_dir_path = args[\"processed_data_dir_path\"] os.makedirs(args[\"processed_data_dir_path\"], exist_ok=True) logger.info( \"Copying files from %s to %s.\", raw_data_dir_path, processed_data_dir_path ) dataset.save_data(processed_data_dir_path) logger.info(\"All raw data has been processed.\") if __name__ == \"__main__\": main()", "source": "process_data.py"}, {"content": "\"\"\" This script is for training a dummy model on a dummy dataset. \"\"\" import os import logging import omegaconf import hydra import mlflow import assignment_4 as ass4 # pylint: disable = no-value-for-parameter @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"train_model.yaml\") def main(args): \"\"\"This is the main function for 'training' the model. Parameters ---------- args : omegaconf.DictConfig An omegaconf.DictConfig object containing arguments for the main function. \"\"\" logger = logging.getLogger(__name__) logger.info(\"Setting up logging configuration.\") ass4.general_utils.setup_logging( logging_config_path=os.path.join( hydra.utils.get_original_cwd(), \"conf\", \"logging.yaml\" ) ) mlflow_init_status, mlflow_run = ass4.general_utils.mlflow_init( args, setup_mlflow=args[\"setup_mlflow\"], autolog=args[\"mlflow_autolog\"] ) ass4.general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={ \"dummy_param1\": args[\"dummy_param1\"], \"dummy_param2\": args[\"dummy_param2\"], }, ) model = ass4.modeling.models.DummyModel() ass4.general_utils.mlflow_log( mlflow_init_status, \"log_dict\", dictionary=omegaconf.OmegaConf.to_container(args, resolve=True), artifact_file=\"train_model_config.json\", ) os.makedirs(args[\"artifact_dir_path\"], exist_ok=True) artifact_path = os.path.join( args[\"artifact_dir_path\"], \"output.txt\" ) with open(artifact_path, \"w\") as f: f.write('\\n'.join([f'{x}: {args[x]}' for x in args])) ass4.general_utils.mlflow_log( mlflow_init_status, \"log_artifact\", local_path=artifact_path, artifact_path=\"outputs\" ) if mlflow_init_status: artifact_uri = mlflow.get_artifact_uri() logger.info(\"Artifact URI: %s\", artifact_uri) ass4.general_utils.mlflow_log( mlflow_init_status, \"log_params\", params={\"artifact_uri\": artifact_uri} ) logger.info( \"Model training with MLflow run ID %s has completed.\", mlflow_run.info.run_id, ) mlflow.end_run() else: logger.info(\"Model training has completed.\") # Outputs for conf/train_model.yaml for hydra.sweeper.direction return args[\"dummy_param1\"], args[\"dummy_param2\"] if __name__ == \"__main__\": main()", "source": "train_model.py"}, {"content": "\"\"\"Utilities or functions that are useful across all the different modules in this package can be defined here.\"\"\" import os import logging import logging.config import yaml import mlflow import time logger = logging.getLogger(__name__) def setup_logging( logging_config_path=\"./conf/logging.yaml\", default_level=logging.INFO ): \"\"\"Set up configuration for logging utilities. Parameters ---------- logging_config_path : str, optional Path to YAML file containing configuration for Python logger, by default \"./config/logging_config.yaml\" default_level : logging object, optional, by default logging.INFO \"\"\" try: with open(logging_config_path, \"rt\", encoding=\"utf-8\") as file: log_config = yaml.safe_load(file.read()) logging.config.dictConfig(log_config) except Exception as error: logging.basicConfig( format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\", level=default_level, ) logger.info(error) logger.info(\"Logging config file is not found. Basic config is being used.\") def mlflow_init(args, run_name='train-model', setup_mlflow=False, autolog=False): \"\"\"Initialise MLflow connection. Parameters ---------- args : dict Dictionary containing the pipeline's configuration passed from Hydra. setup_mlflow : bool, optional Choice to set up MLflow connection, by default False autolog : bool, optional Choice to set up MLflow's autolog, by default False Returns ------- init_success : bool Boolean value indicative of success of intialising connection with MLflow server. mlflow_run : Union[None, `mlflow.entities.Run` object] On successful initialisation, the function returns an object containing the data and properties for the MLflow run. On failure, the function returns a null value. \"\"\" init_success = False mlflow_run = None if setup_mlflow: try: mlflow.set_tracking_uri(args[\"mlflow_tracking_uri\"]) mlflow.set_experiment(args[\"mlflow_exp_name\"]) if autolog: mlflow.autolog() run_name = args.get('mlflow_run_name', run_name) # conf files take precedence if \"MLFLOW_HPTUNING_TAG\" in os.environ: run_name += \"-hp\" run_name += \"-{:.0f}\".format(time.time()) mlflow.start_run(run_name=run_name) set_tag = lambda env_var, tag_name='': mlflow.set_tag( tag_name if tag_name != '' else env_var.lower(), os.environ.get(env_var) ) if env_var in os.environ else None set_tag(\"MLFLOW_HP_TUNING_TAG\", \"hptuning_tag\") set_tag(\"JOB_UUID\") set_tag(\"JOB_NAME\") mlflow_run = mlflow.active_run() init_success = True logger.info(\"MLflow initialisation has succeeded.\") logger.info(\"UUID for MLflow run: %s\", mlflow_run.info.run_id) except Exception as e: logger.error(\"MLflow initialisation has failed.\") logger.error(e) return init_success, mlflow_run def mlflow_log(mlflow_init_status, log_function, **kwargs): \"\"\"Custom function for utilising MLflow's logging functions. This function is only relevant when the function `mlflow_init` returns a \"True\" value, translating to a successful initialisation of a connection with an MLflow server. Parameters ---------- mlflow_init_status : bool Boolean value indicative of success of intialising connection with MLflow server. log_function : str Name of MLflow logging function to be used. See https://www.mlflow.org/docs/latest/python_api/mlflow.html **kwargs Keyword arguments passed to `log_function`. \"\"\" if mlflow_init_status: try: method = getattr(mlflow, log_function) method( **{ key: value for key, value in kwargs.items() if key in method.__code__.co_varnames } ) except Exception as error: logger.error(error)", "source": "general_utils.py"}, {"content": "\"\"\"This package contains modules pertaining to differing parts of the end-to-end workflow, excluding the source for serving the model through a REST API.\"\"\" from . import data_prep from . import modeling from . import general_utils", "source": "__init__.py"}, {"content": "\"\"\"Dataset classes for defining how datasets are to be loaded. \"\"\" import os import shutil class DummyDataset(): \"\"\"Dummy dataset class.\"\"\" def __init__(self, data_dir_path) -> None: if not os.path.isdir(data_dir_path): e = \"Path is not a directory, or does not exist: {}\".format(data_dir_path) raise ValueError(e) self.data_dir_path = data_dir_path def save_data(self, processed_data_dir_path) -> None: shutil.copytree( self.data_dir_path, processed_data_dir_path, dirs_exist_ok=True )", "source": "datasets.py"}, {"content": "\"\"\"Definition of transforms sequence for data preparation. \"\"\"", "source": "transforms.py"}, {"content": "\"\"\"This `data_prep` module includes module(s) which contains utilities for processing, cleaning, or loading data.\"\"\" from . import datasets from . import transforms", "source": "__init__.py"}, {"content": "\"\"\"Module for containing architectures/definition of models.\"\"\" class DummyModel(): \"\"\"Dummy model that returns the input.\"\"\" def __init__(self): pass def predict(self, x): \"\"\"'Prediction' of the dummy model. Parameters ---------- x : str Input string. Returns ------- str Output string. \"\"\" return x", "source": "models.py"}, {"content": "\"\"\"Utilities for model training and experimentation workflows. \"\"\" import assignment_4 as ass4 def load_model(): \"\"\"Load dummy model. A sample utility function to be used. Returns ------- loaded_model : Object containing dummy model. \"\"\" loaded_model = ass4.modeling.models.DummyModel() return loaded_model", "source": "utils.py"}, {"content": "\"\"\"This `modeling` module includes module(s) relevant for the model training pipeline.\"\"\" from . import models from . import utils", "source": "__init__.py"}, {"content": "\"\"\"Configuration module for the FastAPI application.\"\"\" import pydantic_settings class Settings(pydantic_settings.BaseSettings): \"\"\"Settings for the FastAPI application.\"\"\" API_NAME: str = \"assignment 4 - Fastapi\" API_V1_STR: str = \"/api/v1\" LOGGER_CONFIG_PATH: str = \"../conf/logging.yaml\" MODEL_UUID: str = \"change-this\" SETTINGS = Settings()", "source": "config.py"}, {"content": "\"\"\"FastAPI dependencies and global variables.\"\"\" import assignment_4 as ass4 import assignment_4_fastapi as ass4_fapi PRED_MODEL = ass4.modeling.utils.load_model()", "source": "deps.py"}, {"content": "\"\"\"Main module for initialising and defining the FastAPI application.\"\"\" import logging import fastapi from fastapi.middleware.cors import CORSMiddleware import assignment_4 as ass4 import assignment_4_fastapi as ass4_fapi LOGGER = logging.getLogger(__name__) LOGGER.info(\"Setting up logging configuration.\") ass4.general_utils.setup_logging( logging_config_path=ass4_fapi.config.SETTINGS.LOGGER_CONFIG_PATH ) API_V1_STR = ass4_fapi.config.SETTINGS.API_V1_STR APP = fastapi.FastAPI( title=ass4_fapi.config.SETTINGS.API_NAME, openapi_url=f\"{API_V1_STR}/openapi.json\" ) API_ROUTER = fastapi.APIRouter() API_ROUTER.include_router( ass4_fapi.v1.routers.model.ROUTER, prefix=\"/model\", tags=[\"model\"] ) APP.include_router(API_ROUTER, prefix=ass4_fapi.config.SETTINGS.API_V1_STR) ORIGINS = [\"*\"] APP.add_middleware( CORSMiddleware, allow_origins=ORIGINS, allow_credentials=True, allow_methods=[\"*\"], allow_headers=[\"*\"], )", "source": "main.py"}, {"content": "\"\"\"Package for FastAPI application.\"\"\" from . import config from . import deps from . import v1", "source": "__init__.py"}, {"content": "\"\"\"This package contains the v1 API submodules.\"\"\" from . import routers", "source": "__init__.py"}, {"content": "\"\"\"Module containing definitions and workflows for FastAPI's application endpoints.\"\"\" import os import logging import fastapi import assignment_4_fastapi as ass4_fapi logger = logging.getLogger(__name__) ROUTER = fastapi.APIRouter() PRED_MODEL = ass4_fapi.deps.PRED_MODEL @ROUTER.post(\"/predict\", status_code=fastapi.status.HTTP_200_OK) def predict(data: str = fastapi.Body()): \"\"\"Endpoint that returns the input as-is. Parameters ---------- data : str Input text. Returns ------- result_dict : dict Dictionary containing the input string. Raises ------ fastapi.HTTPException A 500 status error is returned if the prediction steps encounters any errors. \"\"\" result_dict = {\"data\": []} try: logger.info(\"Copying input...\") pred_str = PRED_MODEL.predict(data) result_dict[\"data\"].append( {\"input\": pred_str} ) logger.info(\"Input: %s\", data) except Exception as error: logger.error(error) raise fastapi.HTTPException(status_code=500, detail=\"Internal server error.\") return result_dict @ROUTER.get(\"/version\", status_code=fastapi.status.HTTP_200_OK) def model_version(): \"\"\"Get sample version used for the API. Returns ------- dict Dictionary containing the sample version. \"\"\" return {\"data\": {\"model_uuid\": ass4_fapi.config.SETTINGS.MODEL_UUID}}", "source": "model.py"}, {"content": "\"\"\"This package contains the v1 endpoints for the FastAPI application.\"\"\" from . import model", "source": "__init__.py"}, {"content": "def test_dummy(): \"\"\"A test dummy such that the test CI/CD doesn't fail.\"\"\" assert True == True", "source": "test_dummy.py"}, {"content": "import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src'))) import pandas as pd from datapipeline import Datapipeline def test_create_preprocessor(): # Step 2: Create Sample DataFrame data = { 'age': [25, 45, 35, 50], 'hours_per_week': [40, 50, 60, 20], 'wage_per_hour': [15, 20, 25, 30], 'capital_gains': [0, 1000, 500, 0], 'capital_losses': [0, 0, 0, 100], 'dividends_from_stocks': [0, 200, 0, 0], 'marital_status': ['Never-married', 'Married', 'Divorced', 'Widowed'], 'occupation': ['Tech-support', 'Craft-repair', 'Other-service', 'Sales'], 'education': ['Bachelors degree(BA AB BS)', 'High school graduate', 'Some college but no degree', 'Masters degree(MA MS MEng MEd MSW MBA)'], 'full_or_part_time_employment_stat': ['Full-time schedules', 'PT for econ reasons usually FT', 'Not in labor force', 'Unemployed full-time'], 'sex': ['Male', 'Female', 'Female', 'Male'], 'weeks_worked_in_year': [52, 40, 30, 20], 'num_persons_worked_for_employer': [1, 2, 3, 4] } df = pd.DataFrame(data) # Step 3: Define Feature Lists numeric_features = ['age', 'hours_per_week'] log_features = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] nominal_features = ['marital_status', 'occupation'] ordinal_features = ['education', 'full_or_part_time_employment_stat'] binary_features = ['capital_gains'] interaction_features = ['weeks_worked_in_year', 'num_persons_worked_for_employer'] poly_features = ['age'] # Step 4: Define Ordinal Categories education_order = [ 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', 'Children' ] employment_order = [ 'Children or Armed Forces', 'Not in labor force', 'Unemployed part- time', 'Unemployed full-time', 'PT for econ reasons usually PT', 'PT for econ reasons usually FT', 'PT for non-econ reasons usually FT', 'Full-time schedules' ] ordinal_categories = [education_order, employment_order] # Instantiate the Datapipeline class pipeline = Datapipeline() # Set feature lists pipeline.set_features( numeric_features=numeric_features, log_features=log_features, nominal_features=nominal_features, ordinal_features=ordinal_features, ordinal_categories=ordinal_categories, binary_features=binary_features, interaction_features=interaction_features, poly_features=poly_features, percentile=None # Set percentile for feature selection ) # Create the preprocessor preprocessor = pipeline.create_preprocessor() # Fit and transform the sample data using the created preprocessor preprocessed_data = preprocessor.fit_transform(df) # Step 6: Get Feature Names feature_names = preprocessor.get_feature_names_out() # Step 7: Create DataFrame with Transformed Data transformed_df = pd.DataFrame(preprocessed_data, columns=feature_names) # Print the transformed data print(transformed_df) if __name__ == \"__main__\": test_create_preprocessor()", "source": "test_create_preprocessor.py"}, {"content": "import sys import os sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src'))) import pandas as pd from datapipeline import Datapipeline def test_data_cleaning(): # Sample data data = { 'feature1': [1, 2, 2, None], 'feature2': ['A', 'B', 'B', 'A'], 'target': ['positive', 'negative', 'positive', 'negative'], 'unnecessary_column': [10, 20, 20, 10] } df = pd.DataFrame(data) # Instantiate the Datapipeline class pipeline = Datapipeline() # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=True, target_column='target', columns_to_drop=['unnecessary_column'], replacements={'feature2': {'A': 'Alpha', 'B': 'Beta'}}, columns_to_impute=['feature1'] ) # Clean the data cleaned_df = pipeline.data_cleaning(df) # Print the cleaned DataFrame print(cleaned_df) if __name__ == \"__main__\": test_data_cleaning()", "source": "test_data_cleaning.py"}, {"content": "import sys import os import pandas as pd import numpy as np sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src'))) from datapipeline import Datapipeline def test_transform_train_data(): # Sample data data = { 'numeric_feature1': [1.0, 2.0, 3.0, 4.0], 'numeric_feature2': [10.0, 20.0, 30.0, 40.0], 'log_feature': [1, 10, 100, 1000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [1, 2, 3, 4], 'interaction_feature2': [5, 6, 7, 8], 'poly_feature': [1, 2, 3, 4], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_train_data.csv', index=False) # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.1, sample_size=None, return_as_df=True ) # Transform the training data X_train, y_train = pipeline.transform_train_data('sample_train_data.csv') # Print the transformed training data print(\"Transformed Training Data:\") print(\"X_train:\") print(X_train) print(\"y_train:\", y_train) # Test 1: Check if the transformed data is a DataFrame assert isinstance(X_train, pd.DataFrame), \"X_train should be a DataFrame\" assert isinstance(y_train, np.ndarray), \"y_train should be a numpy array\" # Test 2: Check if the target variable has been correctly separated assert 'income_group' not in X_train.columns, \"Target column should not be in X_train\" assert len(y_train) == len(df), \"y_train should have the same number of rows as the original data\" def test_transform_test_data(): # Sample data data = { 'numeric_feature1': [5.0, 6.0, 7.0, 8.0], 'numeric_feature2': [50.0, 60.0, 70.0, 80.0], 'log_feature': [5, 50, 500, 5000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [5, 6, 7, 8], 'interaction_feature2': [9, 10, 11, 12], 'poly_feature': [5, 6, 7, 8], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_test_data.csv', index=False) # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.1, sample_size=None, return_as_df=True ) # Transform the training data first to fit the preprocessor pipeline.transform_train_data('sample_train_data.csv') # Transform the testing data X_test, y_test = pipeline.transform_test_data('sample_test_data.csv') # Print the transformed testing data print(\"Transformed Testing Data:\") print(\"X_test:\") print(X_test) print(\"y_test:\", y_test) # Test 1: Check if the transformed data is a DataFrame assert isinstance(X_test, pd.DataFrame), \"X_test should be a DataFrame\" assert isinstance(y_test, np.ndarray), \"y_test should be a numpy array\" # Test 2: Check if the target variable has been correctly separated assert 'income_group' not in X_test.columns, \"Target column should not be in X_test\" assert len(y_test) == len(df), \"y_test should have the same number of rows as the original data\" if __name__ == \"__main__\": test_transform_train_data() test_transform_test_data()", "source": "test_train_and_test_transform.py"}, {"content": "import sys import os import pandas as pd sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src'))) from datapipeline import Datapipeline def test_transform(): # Sample data data = { 'numeric_feature1': [1.0, 2.0, 3.0, 4.0], 'numeric_feature2': [10.0, 20.0, 30.0, 40.0], 'log_feature': [1, 10, 100, 1000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [1, 2, 3, 4], 'interaction_feature2': [5, 6, 7, 8], 'poly_feature': [1, 2, 3, 4], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_data.csv', index=False) # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.1, sample_size=None, return_as_df=True ) # Transform the data X_train, X_test, y_train, y_test = pipeline.transform('sample_data.csv') # Print the transformed data print(\"X_train:\") print(X_train) print(\"X_test:\") print(X_test) print(\"y_train:\", y_train) print(\"y_test:\", y_test) if __name__ == \"__main__\": test_transform()", "source": "test_transform.py"}, {"content": "# Standard library imports from typing import Optional, List, Union, Tuple, Dict # Third-party library imports import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin from sklearn.feature_selection import SelectPercentile, mutual_info_classif from sklearn.impute import SimpleImputer class Datapipeline: def __init__(self): self.preprocessor = None self.numeric_features = None self.log_features = None self.nominal_features = None self.ordinal_features = None self.ordinal_categories = None self.binary_features = None self.interaction_features = None self.poly_features = None self.percentile = None def set_features( self, numeric_features: Optional[List[str]] = None, log_features: Optional[List[str]] = None, nominal_features: Optional[List[str]] = None, ordinal_features: Optional[List[str]] = None, ordinal_categories: Optional[List[List[str]]] = None, binary_features: Optional[List[str]] = None, interaction_features: Optional[List[str]] = None, poly_features: Optional[List[str]] = None, percentile: Optional[int] = None ): self.numeric_features = numeric_features self.log_features = log_features self.nominal_features = nominal_features self.ordinal_features = ordinal_features self.ordinal_categories = ordinal_categories self.binary_features = binary_features self.interaction_features = interaction_features self.poly_features = poly_features self.percentile = percentile # self.test_size = 0.1 # self.sample_size = None # self.return_as_df = False def set_cleaning_params( self, drop_duplicates: bool = False, target_column: str = None, columns_to_drop: List[str] = None, replacements: Dict[str, Dict] = None, columns_to_impute: List[str] = None ): self.drop_duplicates = drop_duplicates self.target_column = target_column self.columns_to_drop = columns_to_drop self.replacements = replacements self.columns_to_impute = columns_to_impute def set_transform_params( self, test_size: float = 0.1, sample_size: Optional[int] = None, return_as_df: bool = False ): self.test_size = test_size self.sample_size = sample_size self.return_as_df = return_as_df @staticmethod def convert_target_variable(df: pd.DataFrame, target_column: str) -> pd.DataFrame: \"\"\" Convert the target variable to a binary format. :param df: The input pandas DataFrame. :param target_column: The name of the target column. :return: The DataFrame with the target variable converted. \"\"\" df[target_column] = df[target_column].apply(lambda x: 1 if bool(x) else 0) return df @staticmethod def drop_unnecessary_columns(df: pd.DataFrame, columns_to_drop: List[str]) -> pd.DataFrame: \"\"\" Drop unnecessary columns from the DataFrame. :param df: The input pandas DataFrame. :param columns_to_drop: List of columns to drop. :return: The DataFrame with unnecessary columns dropped. \"\"\" return df.drop(columns=columns_to_drop) @staticmethod def replace_column_values(df: pd.DataFrame, column_name: str, replacements: Dict) -> pd.DataFrame: \"\"\" Replace column values with more meaningful values. :param df: The input pandas DataFrame. :param column_name: The name of the column to replace values in. :param replacements: A dictionary of replacements {old_value: new_value}. :return: The DataFrame with replaced values. \"\"\" df[column_name] = df[column_name].replace(replacements) return df @staticmethod def impute_missing_values(df: pd.DataFrame, column_name: str) -> pd.DataFrame: \"\"\" Impute missing values in a column with the mode. :param df: The input pandas DataFrame. :param column_name: The name of the column to impute. :return: The DataFrame with imputed values. \"\"\" imputer = SimpleImputer(strategy='most_frequent') df[column_name] = imputer.fit_transform(df[[column_name]]) return df def data_cleaning(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Perform data cleaning steps on the dataframe. This function performs several data cleaning steps on the input DataFrame, including: - Converting the target variable to a binary format. - Dropping unnecessary columns. - Replacing column values with more meaningful values. - Finding the mode of a column and replacing None values with the mode. - Dropping duplicate rows if specified. :param df: The input pandas DataFrame to be cleaned. :return: The cleaned DataFrame. \"\"\" # Convert the", "source": "datapipeline.py"}, {"content": "target variable to a binary format if self.target_column: df = Datapipeline.convert_target_variable(df, self.target_column) # Drop unnecessary columns if self.columns_to_drop: df = Datapipeline.drop_unnecessary_columns(df, self.columns_to_drop) # Replace column values with more meaningful values if self.replacements: for column_name, replacement_dict in self.replacements.items(): df = Datapipeline.replace_column_values(df, column_name, replacement_dict) # Impute missing values if self.columns_to_impute: for column_name in self.columns_to_impute: df = Datapipeline.impute_missing_values(df, column_name) # Drop duplicate rows if drop_duplicates is True if self.drop_duplicates: df = df.drop_duplicates() return df def create_preprocessor(self) -> Union[Pipeline, ColumnTransformer]: \"\"\" Creates the preprocessing pipeline by combining various feature transformation pipelines. :return: A ColumnTransformer object containing the complete preprocessing pipeline, or a Pipeline object if percentile is specified. \"\"\" transformers = [] # 1. Numeric Pipeline if self.numeric_features: numeric_pipeline = Pipeline(steps=[ ('scaler', StandardScaler()) # Scale numeric features ]) transformers.append(('num', numeric_pipeline, self.numeric_features)) # 2. Log Transformation Pipeline (with Scaling) if self.log_features: log_pipeline = Pipeline(steps=[ ('log_transform', LogTransformerWithFeatureNames(self.log_features)), ('scaler', StandardScaler()) ]) transformers.append(('log', log_pipeline, self.log_features)) # 3. Nominal Pipeline (One-Hot Encoding) if self.nominal_features: nominal_pipeline = Pipeline(steps=[ ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # One-hot encode nominal features ]) transformers.append(('nom', nominal_pipeline, self.nominal_features)) # 4. Ordinal Pipeline if self.ordinal_features and self.ordinal_categories: ordinal_pipeline = Pipeline(steps=[ ('ordinal', OrdinalEncoder(categories=self.ordinal_categories)) # Ordinal encoding for ordered categories ]) transformers.append(('ord', ordinal_pipeline, self.ordinal_features)) # 5. Binarization Pipeline if self.binary_features: binarization_pipeline = Pipeline(steps=[ ('binarization', BinarizationFeatureTransformer(features=self.binary_features)) # Create binary features ]) transformers.append(('bin', binarization_pipeline, self.binary_features)) # 6. Interaction Feature Pipeline (with Scaling) if self.interaction_features: interaction_pipeline = Pipeline(steps=[ ('interaction', InteractionFeatureTransformer(self.interaction_features[0], self.interaction_features[1], 'employment_intensity')), ('scaler', StandardScaler()) # Scale the interaction feature ]) transformers.append(('int', interaction_pipeline, self.interaction_features)) # 7. Pipeline Polynomial Features if self.poly_features: poly_pipeline = Pipeline(steps=[ ('poly', PolynomialFeatures(degree=3, include_bias=False)), # Generate polynomial features ('scaler', StandardScaler()) # Scale the polynomial features ]) transformers.append(('poly', poly_pipeline, self.poly_features)) # Check if transformers list is empty if not transformers: raise ValueError(\"No features specified for transformation. Please set at least one feature type.\") # Combine all pipelines into a ColumnTransformer preprocessor = ColumnTransformer(transformers=transformers) # If a percentile is provided, add SelectPercentile step if self.percentile is not None: preprocessor = Pipeline(steps=[ ('preprocessor', preprocessor), ('select_percentile', SelectPercentile(score_func=mutual_info_classif, percentile=self.percentile)) ]) self.preprocessor = preprocessor return preprocessor def transform(self, data_path: str) -> Tuple[Union[np.ndarray, pd.DataFrame], Union[np.ndarray, pd.DataFrame], np.ndarray, np.ndarray]: \"\"\" Transform the input dataset by applying a series of preprocessing steps. :param data_path: Path to the dataset (CSV file). :return: A tuple containing: - X_train_transformed: Transformed training features as numpy arrays or DataFrames based on return_as_df. - X_test_transformed: Transformed testing features as numpy arrays or DataFrames based on return_as_df. - y_train: Training target values as numpy arrays. - y_test: Testing target values as numpy arrays. \"\"\" # Load the dataset df = pd.read_csv(data_path) # Clean the data df = self.data_cleaning(df) # Sample a variable number of rows, if specified if self.sample_size is not None: df = df.sample(n=self.sample_size, random_state=42) # Separate the target variable from the features y = df[self.target_column].values df.drop(self.target_column, axis=1, inplace=True) # Split the dataset into training and testing sets X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=self.test_size, random_state=42, stratify=y) # Create the preprocessing pipeline self.preprocessor = self.create_preprocessor() # Fit the preprocessor on the training data X_train_transformed = self.preprocessor.fit_transform(X_train, y_train) # Transform the test data using the same preprocessor X_test_transformed = self.preprocessor.transform(X_test) # Convert the transformed data into DataFrames if", "source": "datapipeline.py"}, {"content": "needed if self.return_as_df: X_train_transformed = pd.DataFrame(X_train_transformed, columns=self.preprocessor.get_feature_names_out()) X_test_transformed = pd.DataFrame(X_test_transformed, columns=self.preprocessor.get_feature_names_out()) return X_train_transformed, X_test_transformed, y_train, y_test def transform_train_data(self, train_data_path: str) -> Tuple[Union[np.ndarray, pd.DataFrame], np.ndarray]: \"\"\" Transform the training data by applying a series of preprocessing steps. :param train_data_path: Path to the training dataset (CSV file). :return: A tuple containing: - X_train_transformed: Transformed training features as numpy arrays or DataFrames based on return_as_df. - y_train: Training target values as numpy arrays. \"\"\" # Load the dataset df = pd.read_csv(train_data_path) # Clean the data df = self.data_cleaning(df) # Separate the target variable from the features y_train = df[self.target_column].values df.drop(self.target_column, axis=1, inplace=True) # Create the preprocessing pipeline self.preprocessor = self.create_preprocessor() # Fit the preprocessor on the training data X_train_transformed = self.preprocessor.fit_transform(df, y_train) # Convert the transformed data into DataFrames if needed if self.return_as_df: X_train_transformed = pd.DataFrame(X_train_transformed, columns=self.preprocessor.get_feature_names_out()) return X_train_transformed, y_train def transform_test_data(self, test_data_path: str) -> Tuple[Union[np.ndarray, pd.DataFrame], np.ndarray]: \"\"\" Transform the testing data by applying a series of preprocessing steps. :param test_data_path: Path to the testing dataset (CSV file). :return: A tuple containing: - X_test_transformed: Transformed testing features as numpy arrays or DataFrames based on return_as_df. - y_test: Testing target values as numpy arrays. \"\"\" # Load the dataset df = pd.read_csv(test_data_path) # Clean the data df = self.data_cleaning(df) # Separate the target variable from the features y_test = df[self.target_column].values df.drop(self.target_column, axis=1, inplace=True) # Transform the test data using the preprocessor X_test_transformed = self.preprocessor.transform(df) # Convert the transformed data into DataFrames if needed if self.return_as_df: X_test_transformed = pd.DataFrame(X_test_transformed, columns=self.preprocessor.get_feature_names_out()) return X_test_transformed, y_test def load_data(self, data_path: str) -> pd.DataFrame: \"\"\" Load data from a CSV file and convert it to a pandas DataFrame. :param data_path: Path to the dataset (CSV file). :return: A pandas DataFrame containing the loaded data. \"\"\" # Load the dataset df = pd.read_csv(data_path) return df class LogTransformerWithFeatureNames(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that applies a log1p transformation to specified features and preserves their names in the output. Attributes: feature_names (list of str): List of feature names to be transformed. transformer (FunctionTransformer): The internal transformer that applies the log1p transformation. \"\"\" def __init__(self, feature_names: List[str]): \"\"\" Initializes the transformer with the specified feature names. :param feature_names: List of feature names to be transformed. \"\"\" self.feature_names = feature_names self.transformer = FunctionTransformer(np.log1p, validate=False) def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'LogTransformerWithFeatureNames': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" self.transformer.fit(X, y) return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the log1p transformation to the input data. :param X: Input data to be transformed. :return: Transformed data with log1p applied to specified features. \"\"\" transformed_X = self.transformer.transform(X[self.feature_names].values) # Convert back to DataFrame after transformation transformed_X_df = pd.DataFrame(transformed_X, columns=self.get_feature_names_out(X.columns)) return transformed_X_df def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, with \"log_\" prefixed to the original names. \"\"\" return [f\"log_{feature}\" for feature in self.feature_names] class BinarizationFeatureTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that", "source": "datapipeline.py"}, {"content": "creates binary features indicating the presence of some variables. Attributes: features (list of str): List of feature names to be transformed into binary indicators. \"\"\" def __init__(self, features: List[str]): \"\"\" Initializes the transformer with the specified features. :param features: List of feature names to be transformed into binary indicators. \"\"\" self.features = features def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'BinarizationFeatureTransformer': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the binary transformation to the input data, creating 'has_{feature}' columns. :param X: Input data to be transformed. :return: Transformed data with binary presence indicators for specified features. \"\"\" X_transformed = X.copy() for feature in self.features: X_transformed[f'has_{feature}'] = (X_transformed[feature] > 0).astype(int) return X_transformed.drop(columns=self.features) def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, where each original feature is prefixed with \"has_\". \"\"\" return [f'has_{feature}' for feature in self.features] class InteractionFeatureTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that creates a new feature as the product of two specified features. Attributes: feature_1 (str): Name of the first feature. feature_2 (str): Name of the second feature. new_feature_name (str): Name of the new interaction feature created by multiplying feature_1 and feature_2. \"\"\" def __init__(self, feature_1: str, feature_2: str, new_feature_name: str): \"\"\" Initializes the transformer with the specified features and the name for the new interaction feature. :param feature_1: Name of the first feature. :param feature_2: Name of the second feature. :param new_feature_name: Name of the new interaction feature. \"\"\" self.feature_1 = feature_1 self.feature_2 = feature_2 self.new_feature_name = new_feature_name def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'InteractionFeatureTransformer': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the interaction transformation to the input data, creating a new feature. :param X: Input data to be transformed. :return: Transformed data with the new interaction feature added. \"\"\" X_transformed = X.copy() X_transformed[self.new_feature_name] = X_transformed[self.feature_1] * X_transformed[self.feature_2] return X_transformed def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, including the new interaction feature. \"\"\" return list(input_features) + [self.new_feature_name]", "source": "datapipeline.py"}, {"content": "# Standard library imports from typing import Optional # Third-party library imports import numpy as np class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size: int = 3072, hidden_size: int = 100, output_size: int = 10) -> None: \"\"\" Initializes the two-layer MLP with given sizes for input, hidden, and output layers. Args: input_size (int): Number of input features. hidden_size (int): Number of neurons in the hidden layer. output_size (int): Number of output classes. \"\"\" # Initialize weights and biases with Xavier/Glorot initialization self.w1: np.ndarray = np.random.randn(input_size, hidden_size) * np.sqrt(1 / input_size) self.b1: np.ndarray = np.zeros(hidden_size) self.w2: np.ndarray = np.random.randn(hidden_size, output_size) * np.sqrt(1 / hidden_size) self.b2: np.ndarray = np.zeros(output_size) def relu(self, x: np.ndarray) -> np.ndarray: \"\"\" Applies the ReLU activation function element-wise. Args: x (np.ndarray): Input array. Returns: np.ndarray: Output array after applying ReLU. \"\"\" return np.maximum(0, x) def softmax(self, x: np.ndarray) -> np.ndarray: \"\"\" Applies the softmax function to each row of the input x. Ensures numerical stability by subtracting the max from each row. Args: x (np.ndarray): Input array of shape (batch_size, output_size). Returns: np.ndarray: Softmax probabilities of shape (batch_size, output_size). \"\"\" # Subtract the max for numerical stability shiftx = x - np.max(x, axis=1, keepdims=True) exps = np.exp(shiftx) return exps / np.sum(exps, axis=1, keepdims=True) def forward(self, features: np.ndarray) -> np.ndarray: \"\"\" Performs the forward pass of the MLP. Args: features (np.ndarray): Input data of shape (batch_size, input_size). Returns: np.ndarray: Predicted class probabilities of shape (batch_size, output_size). \"\"\" # If features is a single row, reshape it to (1, input_size) if features.ndim == 1: features = features.reshape(1, -1) # Store features as an instance variable self.features: np.ndarray = features # Compute linear combination for hidden layer self.z1: np.ndarray = np.dot(features, self.w1) + self.b1 # Shape: (batch_size, hidden_size) # Apply ReLU activation self.a1: np.ndarray = self.relu(self.z1) # Shape: (batch_size, hidden_size) # Compute linear combination for output layer self.z2: np.ndarray = np.dot(self.a1, self.w2) + self.b2 # Shape: (batch_size, output_size) # Apply softmax activation to get probabilities self.probs: np.ndarray = self.softmax(self.z2) # Shape: (batch_size, output_size) return self.probs def loss(self, predictions: np.ndarray, labels: np.ndarray) -> float: \"\"\" Computes the cross-entropy loss between the predictions and the actual labels. Args: predictions (np.ndarray): Predicted probabilities of shape (batch_size, output_size). labels (np.ndarray): Actual labels of shape (batch_size,). Returns: float: The cross-entropy loss for the batch. \"\"\" # Store labels as an instance variable self.labels: np.ndarray = labels # Number of samples in the batch batch_size: int = predictions.shape[0] # Clip predictions to avoid log(0) predictions = np.clip(predictions, 1e-12, 1.0) # Compute the cross-entropy loss correct_log_probs: np.ndarray = -np.log(predictions[range(batch_size), labels]) loss: float = np.sum(correct_log_probs) / batch_size return loss def backward(self, learning_rate: float = 1e-3) -> None: \"\"\" Performs the backward pass of the MLP and updates the weights and biases. Args: learning_rate (float): Learning rate for gradient descent. \"\"\" # Number of samples in the batch batch_size: int = self.features.shape[0] # Compute the gradient of the loss with respect to z2 (output layer pre-activation) delta2: np.ndarray = self.probs.copy() delta2[range(batch_size), self.labels] -= 1 delta2 /= batch_size # Compute gradients for w2 and b2 dw2: np.ndarray =", "source": "mlp.py"}, {"content": "np.dot(self.a1.T, delta2) db2: np.ndarray = np.sum(delta2, axis=0) # Compute the gradient of the loss with respect to a1 (hidden layer activation) delta1: np.ndarray = np.dot(delta2, self.w2.T) delta1[self.z1 <= 0] = 0 # Apply ReLU derivative # Compute gradients for w1 and b1 dw1: np.ndarray = np.dot(self.features.T, delta1) db1: np.ndarray = np.sum(delta1, axis=0) # Update weights and biases self.w1 -= learning_rate * dw1 self.w1_update: np.ndarray = learning_rate * dw1 self.b1 -= learning_rate * db1 self.b1_update: np.ndarray = learning_rate * db1 self.w2 -= learning_rate * dw2 self.w2_update: np.ndarray = learning_rate * dw2 self.b2 -= learning_rate * db2 self.b2_update: np.ndarray = learning_rate * db2 def __repr__(self) -> str: \"\"\" Returns a string representation of the MLPTwoLayers instance. Returns: str: String representation including input size, hidden size, and output size. \"\"\" return f\"MLPTwoLayers(input_size={self.w1.shape[0]}, hidden_size={self.w1.shape[1]}, output_size={self.w2.shape[1]})\"", "source": "mlp.py"}, {"content": "from numpy import ndarray import numpy as np from typing import Optional, List, Union, Tuple, Dict from copy import deepcopy def assert_same_shape(array: ndarray, array_grad: ndarray): assert array.shape == array_grad.shape, \\ ''' Two ndarrays should have the same shape; instead, first ndarray's shape is {0} and second ndarray's shape is {1}. '''.format(tuple(array_grad.shape), tuple(array.shape)) return None def permute_data(X: ndarray, y: ndarray): ''' Permute X and y, using the same permutation, along axis=0 ''' perm = np.random.permutation(X.shape[0]) return X[perm], y[perm] class Operation(object): ''' Base class for an \"operation\" in a neural network. ''' def __init__(self): pass def forward(self, input_: ndarray): ''' Stores input in the self._input instance variable Calls the self._output() function. ''' self.input_ = input_ self.output = self._output() return self.output def backward(self, output_grad: ndarray) -> ndarray: ''' Calls the self._input_grad() function. Checks that the appropriate shapes match. ''' assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) assert_same_shape(self.input_, self.input_grad) return self.input_grad def _output(self) -> ndarray: ''' The _output method must be defined for each Operation. ''' raise NotImplementedError() def _input_grad(self, output_grad: ndarray) -> ndarray: ''' The _input_grad method must be defined for each Operation. ''' raise NotImplementedError() class ParamOperation(Operation): ''' An Operation with parameters. ''' def __init__(self, param: ndarray) -> ndarray: ''' The ParamOperation method ''' super().__init__() self.param = param def backward(self, output_grad: ndarray) -> ndarray: ''' Calls self._input_grad and self._param_grad. Checks appropriate shapes. ''' assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) self.param_grad = self._param_grad(output_grad) assert_same_shape(self.input_, self.input_grad) assert_same_shape(self.param, self.param_grad) return self.input_grad def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Every subclass of ParamOperation must implement _param_grad. ''' raise NotImplementedError() class WeightMultiply(ParamOperation): ''' Weight multiplication operation for a neural network. ''' def __init__(self, W: ndarray): ''' Initialize Operation with self.param = W. ''' super().__init__(W) def _output(self) -> ndarray: ''' Compute output. ''' return np.dot(self.input_, self.param) def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' return np.dot(output_grad, np.transpose(self.param, (1, 0))) def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Compute parameter gradient. ''' return np.dot(np.transpose(self.input_, (1, 0)), output_grad) class BiasAdd(ParamOperation): ''' Compute bias addition. ''' def __init__(self, B: ndarray): ''' Initialize Operation with self.param = B. Check appropriate shape. ''' assert B.shape[0] == 1 super().__init__(B) def _output(self) -> ndarray: ''' Compute output. ''' return self.input_ + self.param def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' return np.ones_like(self.input_) * output_grad def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Compute parameter gradient. ''' param_grad = np.ones_like(self.param) * output_grad return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1]) class Sigmoid(Operation): ''' Sigmoid activation function. ''' def __init__(self) -> None: '''Pass''' super().__init__() def _output(self) -> ndarray: ''' Compute output. ''' return 1.0/(1.0+np.exp(-1.0 * self.input_)) def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' sigmoid_backward = self.output * (1.0 - self.output) input_grad = sigmoid_backward * output_grad return input_grad class Linear(Operation): ''' \"Identity\" activation function ''' def __init__(self) -> None: '''Pass''' super().__init__() def _output(self) -> ndarray: '''Pass through''' return self.input_ def _input_grad(self, output_grad: ndarray) -> ndarray: '''Pass through''' return output_grad class Layer(object): ''' A \"layer\" of neurons in a neural network. ''' def __init__(self, neurons: int): ''' The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer ''' self.neurons = neurons self.first = True self.params: List[ndarray] = []", "source": "mlp_backup.py"}, {"content": "self.param_grads: List[ndarray] = [] self.operations: List[Operation] = [] def _setup_layer(self, num_in: int) -> None: ''' The _setup_layer function must be implemented for each layer. ''' raise NotImplementedError() def forward(self, input_: ndarray) -> ndarray: ''' Passes input forward through a series of operations. ''' if self.first: self._setup_layer(input_) self.first = False self.input_ = input_ for operation in self.operations: input_ = operation.forward(input_) self.output = input_ return self.output def backward(self, output_grad: ndarray) -> ndarray: ''' Passes output_grad backward through a series of operations. Checks appropriate shapes. ''' assert_same_shape(self.output, output_grad) for operation in reversed(self.operations): output_grad = operation.backward(output_grad) input_grad = output_grad self._param_grads() return input_grad def _param_grads(self) -> ndarray: ''' Extracts the _param_grads from a layer's operations. ''' self.param_grads = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.param_grads.append(operation.param_grad) def _params(self) -> ndarray: ''' Extracts the _params from a layer's operations. ''' self.params = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.params.append(operation.param) class Dense(Layer): ''' A fully connected layer that inherits from \"Layer.\" ''' def __init__(self,neurons: int, activation: Operation = Sigmoid()) -> None: ''' Requires an activation function upon initialization. ''' super().__init__(neurons) self.activation = activation def _setup_layer(self, input_: ndarray) -> None: ''' Defines the operations of a fully connected layer. ''' if self.seed: np.random.seed(self.seed) self.params = [] # weights self.params.append(np.random.randn(input_.shape[1], self.neurons)) # bias self.params.append(np.random.randn(1, self.neurons)) self.operations = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation] return None class Loss(object): ''' The \"loss\" of a neural network. ''' def __init__(self): '''Pass''' pass def forward(self, prediction: ndarray, target: ndarray) -> float: ''' Computes the actual loss value. ''' assert_same_shape(prediction, target) self.prediction = prediction self.target = target loss_value = self._output() return loss_value def backward(self) -> ndarray: ''' Computes gradient of the loss value with respect to the input to the loss function. ''' self.input_grad = self._input_grad() assert_same_shape(self.prediction, self.input_grad) return self.input_grad def _output(self) -> float: ''' Every subclass of \"Loss\" must implement the _output function. ''' raise NotImplementedError() def _input_grad(self) -> ndarray: ''' Every subclass of \"Loss\" must implement the _input_grad function. ''' raise NotImplementedError() class MeanSquaredError(Loss): def __init__(self): '''Pass''' super().__init__() def _output(self) -> float: ''' Computes the per-observation squared error loss. ''' loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0] return loss def _input_grad(self) -> ndarray: ''' Computes the loss gradient with respect to the input for MSE loss. ''' return 2.0 * (self.prediction - self.target) / self.prediction.shape[0] class NeuralNetwork(object): ''' The class for a neural network. ''' def __init__(self, layers: List[Layer], loss: Loss, seed: float = 1): ''' Neural networks need layers, and a loss. ''' self.layers = layers self.loss = loss self.seed = seed if seed: for layer in self.layers: setattr(layer, \"seed\", self.seed) def forward(self, x_batch: ndarray) -> ndarray: ''' Passes data forward through a series of layers. ''' x_out = x_batch for layer in self.layers: x_out = layer.forward(x_out) return x_out def backward(self, loss_grad: ndarray) -> None: ''' Passes data backward through a series of layers. ''' grad = loss_grad for layer in reversed(self.layers): grad = layer.backward(grad) return None def train_batch(self, x_batch: ndarray, y_batch: ndarray) -> float: ''' Passes data forward through the layers. Computes the loss. Passes data backward through the layers. ''' predictions = self.forward(x_batch) loss = self.loss.forward(predictions, y_batch) self.backward(self.loss.backward()) return loss", "source": "mlp_backup.py"}, {"content": "def params(self): ''' Gets the parameters for the network. ''' for layer in self.layers: yield from layer.params def param_grads(self): ''' Gets the gradient of the loss with respect to the parameters for the network. ''' for layer in self.layers: yield from layer.param_grads class Optimizer(object): ''' Base class for a neural network optimizer. ''' def __init__(self, lr: float = 0.01): ''' Every optimizer must have an initial learning rate. ''' self.lr = lr def step(self) -> None: ''' Every optimizer must implement the \"step\" function. ''' pass class SGD(Optimizer): ''' Stochastic gradient descent optimizer. ''' def __init__(self, lr: float = 0.01) -> None: '''Pass''' super().__init__(lr) def step(self): ''' For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment based on the learning rate. ''' for (param, param_grad) in zip(self.net.params(), self.net.param_grads()): param -= self.lr * param_grad class Trainer(object): ''' Trains a neural network. ''' def __init__(self, net: NeuralNetwork, optim: Optimizer): ''' Requires a neural network and an optimizer in order for training to occur. Assign the neural network as an instance variable to the optimizer. ''' self.net = net self.optim = optim self.best_loss = 1e9 setattr(self.optim, 'net', self.net) def generate_batches(self, X: ndarray, y: ndarray, size: int = 32) -> Tuple[ndarray]: ''' Generates batches for training ''' assert X.shape[0] == y.shape[0], \\ ''' features and target must have the same number of rows, instead features has {0} and target has {1} '''.format(X.shape[0], y.shape[0]) N = X.shape[0] for ii in range(0, N, size): X_batch, y_batch = X[ii:ii+size], y[ii:ii+size] yield X_batch, y_batch def fit(self, X_train: ndarray, y_train: ndarray, X_test: ndarray, y_test: ndarray, epochs: int=100, eval_every: int=10, batch_size: int=32, seed: int = 1, restart: bool = True) -> None: ''' Fits the neural network on the training data for a certain number of epochs. Every \"eval_every\" epochs, it evaluates the neural network on the testing data. ''' np.random.seed(seed) if restart: for layer in self.net.layers: layer.first = True self.best_loss = 1e9 for e in range(epochs): if (e+1) % eval_every == 0: # for early stopping last_model = deepcopy(self.net) X_train, y_train = permute_data(X_train, y_train) batch_generator = self.generate_batches(X_train, y_train, batch_size) for ii, (X_batch, y_batch) in enumerate(batch_generator): self.net.train_batch(X_batch, y_batch) self.optim.step() if (e+1) % eval_every == 0: test_preds = self.net.forward(X_test) loss = self.net.loss.forward(test_preds, y_test) print(f\"Validation loss after {e+1} epochs is {loss:.3f}\") if loss < self.best_loss: print(f\"Validation loss after {e+1} epochs is {loss:.3f}\") self.best_loss = loss else: print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\") self.net = last_model # ensure self.optim is still updating self.net setattr(self.optim, 'net', self.net) break", "source": "mlp_backup.py"}, {"content": "# Standard library imports from typing import Optional, List, Union, Tuple, Dict # Third-party library imports import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, PolynomialFeatures, FunctionTransformer from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.base import BaseEstimator, TransformerMixin from sklearn.feature_selection import SelectPercentile, mutual_info_classif from sklearn.impute import SimpleImputer class Datapipeline: def __init__(self): self.preprocessor = None self.numeric_features = None self.log_features = None self.nominal_features = None self.ordinal_features = None self.ordinal_categories = None self.binary_features = None self.interaction_features = None self.poly_features = None self.percentile = None def set_features( self, numeric_features: Optional[List[str]] = None, log_features: Optional[List[str]] = None, nominal_features: Optional[List[str]] = None, ordinal_features: Optional[List[str]] = None, ordinal_categories: Optional[List[List[str]]] = None, binary_features: Optional[List[str]] = None, interaction_features: Optional[List[str]] = None, poly_features: Optional[List[str]] = None, percentile: Optional[int] = None ): self.numeric_features = numeric_features self.log_features = log_features self.nominal_features = nominal_features self.ordinal_features = ordinal_features self.ordinal_categories = ordinal_categories self.binary_features = binary_features self.interaction_features = interaction_features self.poly_features = poly_features self.percentile = percentile self.test_size = 0.1 self.sample_size = None self.return_as_df = False def set_cleaning_params( self, drop_duplicates: bool = False, target_column: str = None, columns_to_drop: List[str] = None, replacements: Dict[str, Dict] = None, columns_to_impute: List[str] = None ): self.drop_duplicates = drop_duplicates self.target_column = target_column self.columns_to_drop = columns_to_drop self.replacements = replacements self.columns_to_impute = columns_to_impute def set_transform_params( self, test_size: float = 0.1, sample_size: Optional[int] = None, return_as_df: bool = False ): self.test_size = test_size self.sample_size = sample_size self.return_as_df = return_as_df @staticmethod def drop_unnecessary_columns(df: pd.DataFrame, columns_to_drop: List[str]) -> pd.DataFrame: \"\"\" Drop unnecessary columns from the DataFrame. :param df: The input pandas DataFrame. :param columns_to_drop: List of columns to drop. :return: The DataFrame with unnecessary columns dropped. \"\"\" return df.drop(columns=columns_to_drop) @staticmethod def replace_column_values(df: pd.DataFrame, column_name: str, replacements: Dict) -> pd.DataFrame: \"\"\" Replace column values with more meaningful values. :param df: The input pandas DataFrame. :param column_name: The name of the column to replace values in. :param replacements: A dictionary of replacements {old_value: new_value}. :return: The DataFrame with replaced values. \"\"\" df[column_name] = df[column_name].replace(replacements) return df @staticmethod def impute_missing_values(df: pd.DataFrame, column_name: str) -> pd.DataFrame: \"\"\" Impute missing values in a column with the mode. :param df: The input pandas DataFrame. :param column_name: The name of the column to impute. :return: The DataFrame with imputed values. \"\"\" imputer = SimpleImputer(strategy='most_frequent') df[column_name] = imputer.fit_transform(df[[column_name]]) return df def data_cleaning(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Perform data cleaning steps on the dataframe. This function performs several data cleaning steps on the input DataFrame, including: - Converting the target variable to a binary format. - Dropping unnecessary columns. - Replacing column values with more meaningful values. - Finding the mode of a column and replacing None values with the mode. - Dropping duplicate rows if specified. :param df: The input pandas DataFrame to be cleaned. :return: The cleaned DataFrame. \"\"\" # Drop unnecessary columns if self.columns_to_drop: df = Datapipeline.drop_unnecessary_columns(df, self.columns_to_drop) # Replace column values with more meaningful values if self.replacements: for column_name, replacement_dict in self.replacements.items(): df = Datapipeline.replace_column_values(df, column_name, replacement_dict) # Impute missing values if self.columns_to_impute: for column_name in self.columns_to_impute: df = Datapipeline.impute_missing_values(df, column_name) # Drop duplicate rows if drop_duplicates is True if self.drop_duplicates: df = df.drop_duplicates()", "source": "mlp_datapipeline.py"}, {"content": "return df def transform(self, data_path: str) -> Tuple[Union[np.ndarray, pd.DataFrame], Union[np.ndarray, pd.DataFrame], np.ndarray, np.ndarray]: \"\"\" Transform the input dataset by applying a series of preprocessing steps. :param data_path: Path to the dataset (CSV file). :return: A tuple containing: - X_train_transformed: Transformed training features as numpy arrays or DataFrames based on return_as_df. - X_test_transformed: Transformed testing features as numpy arrays or DataFrames based on return_as_df. - y_train: Training target values as numpy arrays. - y_test: Testing target values as numpy arrays. \"\"\" # Load the dataset df = pd.read_csv(data_path) # Clean the data df = self.data_cleaning(df) # Sample a variable number of rows, if specified if self.sample_size is not None: df = df.sample(n=self.sample_size, random_state=42) # Separate the target variable from the features y = df[self.target_column].values df.drop(self.target_column, axis=1, inplace=True) return df.to_numpy(), y class LogTransformerWithFeatureNames(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that applies a log1p transformation to specified features and preserves their names in the output. Attributes: feature_names (list of str): List of feature names to be transformed. transformer (FunctionTransformer): The internal transformer that applies the log1p transformation. \"\"\" def __init__(self, feature_names: List[str]): \"\"\" Initializes the transformer with the specified feature names. :param feature_names: List of feature names to be transformed. \"\"\" self.feature_names = feature_names self.transformer = FunctionTransformer(np.log1p, validate=False) def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'LogTransformerWithFeatureNames': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" self.transformer.fit(X, y) return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the log1p transformation to the input data. :param X: Input data to be transformed. :return: Transformed data with log1p applied to specified features. \"\"\" transformed_X = self.transformer.transform(X[self.feature_names].values) # Convert back to DataFrame after transformation transformed_X_df = pd.DataFrame(transformed_X, columns=self.get_feature_names_out(X.columns)) return transformed_X_df def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, with \"log_\" prefixed to the original names. \"\"\" return [f\"log_{feature}\" for feature in self.feature_names] class BinarizationFeatureTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that creates binary features indicating the presence of some variables. Attributes: features (list of str): List of feature names to be transformed into binary indicators. \"\"\" def __init__(self, features: List[str]): \"\"\" Initializes the transformer with the specified features. :param features: List of feature names to be transformed into binary indicators. \"\"\" self.features = features def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'BinarizationFeatureTransformer': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the binary transformation to the input data, creating 'has_{feature}' columns. :param X: Input data to be transformed. :return: Transformed data with binary presence indicators for specified features. \"\"\" X_transformed = X.copy() for feature in self.features: X_transformed[f'has_{feature}'] = (X_transformed[feature] > 0).astype(int) return X_transformed.drop(columns=self.features) def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features:", "source": "mlp_datapipeline.py"}, {"content": "Input feature names. :return: List of output feature names, where each original feature is prefixed with \"has_\". \"\"\" return [f'has_{feature}' for feature in self.features] class InteractionFeatureTransformer(BaseEstimator, TransformerMixin): \"\"\" Custom transformer that creates a new feature as the product of two specified features. Attributes: feature_1 (str): Name of the first feature. feature_2 (str): Name of the second feature. new_feature_name (str): Name of the new interaction feature created by multiplying feature_1 and feature_2. \"\"\" def __init__(self, feature_1: str, feature_2: str, new_feature_name: str): \"\"\" Initializes the transformer with the specified features and the name for the new interaction feature. :param feature_1: Name of the first feature. :param feature_2: Name of the second feature. :param new_feature_name: Name of the new interaction feature. \"\"\" self.feature_1 = feature_1 self.feature_2 = feature_2 self.new_feature_name = new_feature_name def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None) -> 'InteractionFeatureTransformer': \"\"\" Fit the transformer to the data. This method is necessary to comply with the scikit-learn API. :param X: Input data. :param y: Target values (not used). :return: The fitted transformer instance. \"\"\" return self def transform(self, X: pd.DataFrame) -> pd.DataFrame: \"\"\" Apply the interaction transformation to the input data, creating a new feature. :param X: Input data to be transformed. :return: Transformed data with the new interaction feature added. \"\"\" X_transformed = X.copy() X_transformed[self.new_feature_name] = X_transformed[self.feature_1] * X_transformed[self.feature_2] return X_transformed def get_feature_names_out(self, input_features: Optional[List[str]] = None) -> List[str]: \"\"\" Get the output feature names after transformation. :param input_features: Input feature names. :return: List of output feature names, including the new interaction feature. \"\"\" return list(input_features) + [self.new_feature_name]", "source": "mlp_datapipeline.py"}, {"content": "# Standard library imports from typing import Optional, List, Union, Tuple, Dict, Generator from copy import deepcopy # Third-party library imports import numpy as np from numpy import ndarray def assert_same_shape(array: ndarray, array_grad: ndarray): assert array.shape == array_grad.shape, \\ ''' Two ndarrays should have the same shape; instead, first ndarray's shape is {0} and second ndarray's shape is {1}. '''.format(tuple(array_grad.shape), tuple(array.shape)) return None def permute_data(X: ndarray, y: ndarray): ''' Permute X and y, using the same permutation, along axis=0 ''' perm = np.random.permutation(X.shape[0]) return X[perm], y[perm] def compute_accuracy(predictions: ndarray, targets: ndarray) -> float: ''' Computes the accuracy of the predictions. ''' pred_labels = np.argmax(predictions, axis=1) accuracy = np.mean(pred_labels == targets) return accuracy class Operation(object): ''' Base class for an \"operation\" in a neural network. ''' def __init__(self): pass def forward(self, input_: ndarray): ''' Stores input in the self._input instance variable Calls the self._output() function. ''' self.input_ = input_ self.output = self._output() return self.output def backward(self, output_grad: ndarray) -> ndarray: ''' Calls the self._input_grad() function. Checks that the appropriate shapes match. ''' assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) assert_same_shape(self.input_, self.input_grad) return self.input_grad def _output(self) -> ndarray: ''' The _output method must be defined for each Operation. ''' raise NotImplementedError() def _input_grad(self, output_grad: ndarray) -> ndarray: ''' The _input_grad method must be defined for each Operation. ''' raise NotImplementedError() class ParamOperation(Operation): ''' An Operation with parameters. ''' def __init__(self, param: ndarray) -> ndarray: ''' The ParamOperation method ''' super().__init__() self.param = param def backward(self, output_grad: ndarray) -> ndarray: ''' Calls self._input_grad and self._param_grad. Checks appropriate shapes. ''' assert_same_shape(self.output, output_grad) self.input_grad = self._input_grad(output_grad) self.param_grad = self._param_grad(output_grad) assert_same_shape(self.input_, self.input_grad) assert_same_shape(self.param, self.param_grad) return self.input_grad def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Every subclass of ParamOperation must implement _param_grad. ''' raise NotImplementedError() class WeightMultiply(ParamOperation): ''' Weight multiplication operation for a neural network. ''' def __init__(self, W: ndarray): ''' Initialize Operation with self.param = W. ''' super().__init__(W) def _output(self) -> ndarray: ''' Compute output. ''' return np.dot(self.input_, self.param) def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' return np.dot(output_grad, np.transpose(self.param, (1, 0))) def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Compute parameter gradient. ''' return np.dot(np.transpose(self.input_, (1, 0)), output_grad) class BiasAdd(ParamOperation): ''' Compute bias addition. ''' def __init__(self, B: ndarray): ''' Initialize Operation with self.param = B. Check appropriate shape. ''' assert B.shape[0] == 1 super().__init__(B) def _output(self) -> ndarray: ''' Compute output. ''' return self.input_ + self.param def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' return np.ones_like(self.input_) * output_grad def _param_grad(self, output_grad: ndarray) -> ndarray: ''' Compute parameter gradient. ''' param_grad = np.ones_like(self.param) * output_grad return np.sum(param_grad, axis=0).reshape(1, param_grad.shape[1]) class Sigmoid(Operation): ''' Sigmoid activation function. ''' def __init__(self) -> None: '''Pass''' super().__init__() def _output(self) -> ndarray: ''' Compute output. ''' return 1.0/(1.0+np.exp(-1.0 * self.input_)) def _input_grad(self, output_grad: ndarray) -> ndarray: ''' Compute input gradient. ''' sigmoid_backward = self.output * (1.0 - self.output) input_grad = sigmoid_backward * output_grad return input_grad class Linear(Operation): ''' \"Identity\" activation function ''' def __init__(self) -> None: '''Pass''' super().__init__() def _output(self) -> ndarray: '''Pass through''' return self.input_ def _input_grad(self, output_grad: ndarray) -> ndarray: '''Pass through''' return output_grad class Layer(object): ''' A \"layer\"", "source": "mlp_modular.py"}, {"content": "of neurons in a neural network. ''' def __init__(self, neurons: int): ''' The number of \"neurons\" roughly corresponds to the \"breadth\" of the layer ''' self.neurons = neurons self.first = True self.params: List[ndarray] = [] self.param_grads: List[ndarray] = [] self.operations: List[Operation] = [] def _setup_layer(self, num_in: int) -> None: ''' The _setup_layer function must be implemented for each layer. ''' raise NotImplementedError() def forward(self, input_: ndarray) -> ndarray: ''' Passes input forward through a series of operations. ''' if self.first: self._setup_layer(input_) self.first = False self.input_ = input_ for operation in self.operations: input_ = operation.forward(input_) self.output = input_ return self.output def backward(self, output_grad: ndarray) -> ndarray: ''' Passes output_grad backward through a series of operations. Checks appropriate shapes. ''' assert_same_shape(self.output, output_grad) for operation in reversed(self.operations): output_grad = operation.backward(output_grad) input_grad = output_grad self._param_grads() return input_grad def _param_grads(self) -> ndarray: ''' Extracts the _param_grads from a layer's operations. ''' self.param_grads = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.param_grads.append(operation.param_grad) def _params(self) -> ndarray: ''' Extracts the _params from a layer's operations. ''' self.params = [] for operation in self.operations: if issubclass(operation.__class__, ParamOperation): self.params.append(operation.param) class Dense(Layer): ''' A fully connected layer that inherits from \"Layer.\" ''' def __init__(self,neurons: int, activation: Operation = Sigmoid()) -> None: ''' Requires an activation function upon initialization. ''' super().__init__(neurons) self.activation = activation def _setup_layer(self, input_: ndarray) -> None: ''' Defines the operations of a fully connected layer. ''' if self.seed: np.random.seed(self.seed) self.params = [] # weights self.params.append(np.random.randn(input_.shape[1], self.neurons)) # bias self.params.append(np.random.randn(1, self.neurons)) self.operations = [WeightMultiply(self.params[0]), BiasAdd(self.params[1]), self.activation] return None class Loss(object): ''' The \"loss\" of a neural network. ''' def __init__(self): '''Pass''' pass def forward(self, prediction: ndarray, target: ndarray) -> float: ''' Computes the actual loss value. ''' assert_same_shape(prediction, target) self.prediction = prediction self.target = target loss_value = self._output() return loss_value def backward(self) -> ndarray: ''' Computes gradient of the loss value with respect to the input to the loss function. ''' self.input_grad = self._input_grad() assert_same_shape(self.prediction, self.input_grad) return self.input_grad def _output(self) -> float: ''' Every subclass of \"Loss\" must implement the _output function. ''' raise NotImplementedError() def _input_grad(self) -> ndarray: ''' Every subclass of \"Loss\" must implement the _input_grad function. ''' raise NotImplementedError() class MeanSquaredError(Loss): def __init__(self): '''Pass''' super().__init__() def _output(self) -> float: ''' Computes the per-observation squared error loss. ''' loss = np.sum(np.power(self.prediction - self.target, 2)) / self.prediction.shape[0] return loss def _input_grad(self) -> ndarray: ''' Computes the loss gradient with respect to the input for MSE loss. ''' return 2.0 * (self.prediction - self.target) / self.prediction.shape[0] class NeuralNetwork(object): ''' The class for a neural network. ''' def __init__(self, layers: List[Layer], loss: Loss, seed: float = 1): ''' Neural networks need layers, and a loss. ''' self.layers = layers self.loss = loss self.seed = seed if seed: for layer in self.layers: setattr(layer, \"seed\", self.seed) def forward(self, x_batch: ndarray) -> ndarray: ''' Passes data forward through a series of layers. ''' x_out = x_batch for layer in self.layers: x_out = layer.forward(x_out) return x_out def backward(self, loss_grad: ndarray) -> None: ''' Passes data backward through a series of layers. ''' grad = loss_grad for layer in reversed(self.layers): grad = layer.backward(grad) return None", "source": "mlp_modular.py"}, {"content": "def train_batch(self, x_batch: ndarray, y_batch: ndarray) -> float: ''' Passes data forward through the layers. Computes the loss. Passes data backward through the layers. ''' predictions = self.forward(x_batch) print(f\"Predictions shape: {predictions.shape}\") print(f\"Targets shape: {y_batch.shape}\") loss = self.loss.forward(predictions, y_batch) self.backward(self.loss.backward()) return loss def params(self): ''' Gets the parameters for the network. ''' for layer in self.layers: yield from layer.params def param_grads(self): ''' Gets the gradient of the loss with respect to the parameters for the network. ''' for layer in self.layers: yield from layer.param_grads class Optimizer(object): ''' Base class for a neural network optimizer. ''' def __init__(self, lr: float = 0.01): ''' Every optimizer must have an initial learning rate. ''' self.lr = lr def step(self) -> None: ''' Every optimizer must implement the \"step\" function. ''' pass class SGD(Optimizer): ''' Stochastic gradient descent optimizer. ''' def __init__(self, lr: float = 0.01) -> None: '''Pass''' super().__init__(lr) def step(self): ''' For each parameter, adjust in the appropriate direction, with the magnitude of the adjustment based on the learning rate. ''' for (param, param_grad) in zip(self.net.params(), self.net.param_grads()): param -= self.lr * param_grad class Trainer(object): ''' Trains a neural network. ''' def __init__(self, net: NeuralNetwork, optim: Optimizer): ''' Requires a neural network and an optimizer in order for training to occur. Assign the neural network as an instance variable to the optimizer. ''' self.net = net self.optim = optim self.best_loss = 1e9 setattr(self.optim, 'net', self.net) def generate_batches(self, X: ndarray, y: ndarray, size: int = 32) -> Generator[Tuple[ndarray, ndarray], None, None]: ''' Generates batches for training ''' assert X.shape[0] == y.shape[0], \\ ''' features and target must have the same number of rows, instead features has {0} and target has {1} '''.format(X.shape[0], y.shape[0]) N = X.shape[0] for ii in range(0, N, size): X_batch, y_batch = X[ii:ii+size], y[ii:ii+size] yield X_batch, y_batch def fit(self, X_train: ndarray, y_train: ndarray, X_test: ndarray, y_test: ndarray, epochs: int=100, eval_every: int=10, batch_size: int=32, seed: int = 1, restart: bool = True) -> None: ''' Fits the neural network on the training data for a certain number of epochs. Every \"eval_every\" epochs, it evaluates the neural network on the testing data. ''' np.random.seed(seed) if restart: for layer in self.net.layers: layer.first = True self.best_loss = 1e9 for e in range(epochs): if (e+1) % eval_every == 0: # for early stopping last_model = deepcopy(self.net) X_train, y_train = permute_data(X_train, y_train) print(f\"After permutation X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\") batch_generator = self.generate_batches(X_train, y_train, batch_size) for ii, (X_batch, y_batch) in enumerate(batch_generator): print(f\"Batch {ii} X_batch shape: {X_batch.shape}, y_batch shape: {y_batch.shape}\") self.net.train_batch(X_batch, y_batch) self.optim.step() if (e+1) % eval_every == 0: test_preds = self.net.forward(X_test) print(f\"test_preds shape: {test_preds.shape}\") loss = self.net.loss.forward(test_preds, y_test) print(f\"Validation loss after {e+1} epochs is {loss:.3f}\") if loss < self.best_loss: print(f\"Validation loss after {e+1} epochs is {loss:.3f}\") self.best_loss = loss else: print(f\"\"\"Loss increased after epoch {e+1}, final loss was {self.best_loss:.3f}, using the model from epoch {e+1-eval_every}\"\"\") self.net = last_model # ensure self.optim is still updating self.net setattr(self.optim, 'net', self.net) break class CrossEntropyLoss(Loss): def __init__(self): '''Pass''' super().__init__() def _output(self) -> float: ''' Computes the cross-entropy loss. ''' m = self.target.shape[0] p = self.prediction # Convert one-hot encoded targets to class indices if", "source": "mlp_modular.py"}, {"content": "len(self.target.shape) == 2: self.target = np.argmax(self.target, axis=1) log_likelihood = -np.log(p[range(m), self.target]) loss = np.sum(log_likelihood) / m return loss def _input_grad(self) -> ndarray: ''' Computes the gradient of the cross-entropy loss with respect to the input. ''' m = self.target.shape[0] # Convert one-hot encoded targets to class indices if len(self.target.shape) == 2: self.target = np.argmax(self.target, axis=1) grad = self.prediction.copy() grad[range(m), self.target] -= 1 grad = grad / m return grad", "source": "mlp_modular.py"}, {"content": "# Standard library imports import os import urllib.parse # Third-party library imports import hydra import matplotlib.pyplot as plt import mlflow import mlflow.keras import numpy as np import optuna import pandas as pd import tensorflow as tf from dotenv import load_dotenv from hydra import initialize, compose from imblearn.over_sampling import SMOTE from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, TensorSpec from omegaconf import DictConfig, OmegaConf from sklearn.metrics import ( auc, classification_report, confusion_matrix, precision_recall_curve, roc_auc_score, precision_score, f1_score, recall_score ) from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sqlalchemy import create_engine from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Input from tensorflow.keras.metrics import Recall from tensorflow.keras.models import Sequential from tensorflow.keras.optimizers import Adam, AdamW # Local imports import datapipeline as dp # Set the MLflow tracking URI mlflow.set_tracking_uri(\"http://localhost:5000\") # Define the main function using Hydra for configuration @hydra.main(config_path=\"../conf\", config_name=\"config\", version_base=None) def main(cfg: DictConfig): # Load and preprocess data data_path = os.path.join(os.path.dirname(__file__), '../data/creditcard.csv') object = dp.Datapipeline() df = object.load_data(data_path=data_path) # Define numeric features numeric_features = df.select_dtypes(include=[np.number]).columns.to_list() object.set_cleaning_params(target_column='Class') object.set_features(numeric_features=numeric_features) object.set_transform_params(return_as_df=False) X_train, X_test, y_train, y_test = object.transform(data_path=data_path) # Perform SMOTE on the training data smote = SMOTE(random_state=42) X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train) # Split the resampled training data for validation X_train_split, X_val_split, y_train_split, y_val_split = train_test_split( X_train_resampled, y_train_resampled, test_size=0.2, random_state=42 ) # Calculate class proportions proportion_false = np.mean(y_train_resampled == 0) proportion_true = np.mean(y_train_resampled == 1) # Calculate weights inversely proportional to the class proportions weight_for_0 = 1 / proportion_false weight_for_1 = 1 / proportion_true # Normalize the weights total_weight = weight_for_0 + weight_for_1 weight_for_0 /= total_weight weight_for_1 /= total_weight print(f\"Weight for class 0 (False): {weight_for_0}\") print(f\"Weight for class 1 (True): {weight_for_1}\") # Define the custom weighted binary cross-entropy loss function def weighted_binary_crossentropy(y_true, y_pred): # Apply the weights to the loss weight_vector = y_true * weight_for_1 + (1.0 - y_true) * weight_for_0 bce = tf.keras.losses.binary_crossentropy(y_true, y_pred) weighted_bce = weight_vector * bce return tf.reduce_mean(weighted_bce) # Define the objective function for Optuna def objective(trial): # Suggest hyperparameters learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True) batch_size = trial.suggest_int(\"batch_size\", 16, 128, log=True) dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5) units_1 = trial.suggest_int(\"units_1\", 32, 128, log=True) units_2 = trial.suggest_int(\"units_2\", 32, 128, log=True) units_3 = trial.suggest_int(\"units_3\", 16, 64, log=True) # Define the model model = Sequential([ Input(shape=(X_train_resampled.shape[1],)), # Input layer Dropout(dropout_rate), # Dropout layer after input Dense(units_1, activation='relu'), # First hidden layer Dropout(dropout_rate), # Dropout layer Dense(units_2, activation='relu'), # Second hidden layer Dropout(dropout_rate), # Dropout layer Dense(units_3, activation='relu'), # Third hidden layer Dropout(dropout_rate), # Dropout layer Dense(1, activation='sigmoid') # Output layer for binary classification ]) # Compile the model model.compile( optimizer=AdamW(learning_rate=learning_rate), loss=weighted_binary_crossentropy, metrics=['accuracy', Recall(name='recall')] ) # Create a unique filename for the model checkpoint checkpoint_dir = \"model_checkpoints\" os.makedirs(checkpoint_dir, exist_ok=True) checkpoint_filename = os.path.join(checkpoint_dir, f'best_model_trial_{trial.number}.keras') # Implement early stopping and model checkpoint early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) model_checkpoint = ModelCheckpoint(checkpoint_filename, save_best_only=True, monitor='val_loss') # Start an MLflow run with mlflow.start_run(): # Log parameters mlflow.log_param(\"optimizer\", \"AdamW\") mlflow.log_param(\"loss\", \"weighted_binary_crossentropy\") mlflow.log_param(\"learning_rate\", learning_rate) mlflow.log_param(\"batch_size\", batch_size) mlflow.log_param(\"dropout_rate\", dropout_rate) mlflow.log_param(\"units_1\", units_1) mlflow.log_param(\"units_2\", units_2) mlflow.log_param(\"units_3\", units_3) # Train the model history = model.fit( X_train_split, y_train_split, epochs=5, batch_size=batch_size, validation_data=(X_val_split, y_val_split), callbacks=[early_stopping, model_checkpoint] ) # Get the actual number of epochs run actual_epochs = len(history.history['accuracy']) # Log the", "source": "optuna_hydra_script.py"}, {"content": "actual number of epochs mlflow.log_param(\"actual_epochs\", actual_epochs) # Log metrics for each epoch for epoch in range(actual_epochs): mlflow.log_metric(\"accuracy\", history.history['accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch) mlflow.log_metric(\"recall\", history.history['recall'][epoch], step=epoch) mlflow.log_metric(\"val_recall\", history.history['val_recall'][epoch], step=epoch) mlflow.log_metric(\"loss\", history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch) # Evaluate the model on the validation set val_predictions_prob = model.predict(X_val_split) val_predictions = (val_predictions_prob > 0.5).astype(\"int32\") # Calculate additional metrics val_recall = recall_score(y_val_split, val_predictions) precision = precision_score(y_val_split, val_predictions) f1 = f1_score(y_val_split, val_predictions) roc_auc = roc_auc_score(y_val_split, val_predictions_prob) precision_vals, recall_vals, _ = precision_recall_curve(y_val_split, val_predictions_prob) pr_auc = auc(recall_vals, precision_vals) # Log final metrics after training mlflow.log_metric(\"final_val_recall\", val_recall) mlflow.log_metric(\"final_precision\", precision) mlflow.log_metric(\"final_f1_score\", f1) mlflow.log_metric(\"final_roc_auc\", roc_auc) mlflow.log_metric(\"final_pr_auc\", pr_auc) # Create model signature input_dim = X_train_resampled.shape[1] input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, input_dim))]) output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 1))]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) # Log the model mlflow.keras.log_model( model=model, artifact_path=\"model\", signature=signature ) # Return the validation loss and negative validation recall for Optuna to optimize return history.history['val_loss'][-1], -val_recall # Create an Optuna study for multi-objective optimization study = optuna.create_study(directions=[\"minimize\", \"maximize\"]) # Run the optimization study.optimize(objective, n_trials=10) # Print the best hyperparameters for each of the best trials print(\"Best hyperparameters for each objective:\") for i, trial in enumerate(study.best_trials): print(f\"Objective {i + 1}: {trial.params}\") if __name__ == \"__main__\": main()", "source": "optuna_hydra_script.py"}, {"content": "import sys import os import pandas as pd import numpy as np import pytest sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../src'))) from datapipeline import Datapipeline @pytest.fixture def sample_data(): data = { 'numeric_feature1': [1.0, 2.0, 3.0, 4.0], 'numeric_feature2': [10.0, 20.0, 30.0, 40.0], 'log_feature': [1, 10, 100, 1000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [1, 2, 3, 4], 'interaction_feature2': [5, 6, 7, 8], 'poly_feature': [1, 2, 3, 4], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_data.csv', index=False) return 'sample_data.csv' @pytest.fixture def sample_train_data(): data = { 'numeric_feature1': [1.0, 2.0, 3.0, 4.0], 'numeric_feature2': [10.0, 20.0, 30.0, 40.0], 'log_feature': [1, 10, 100, 1000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [1, 2, 3, 4], 'interaction_feature2': [5, 6, 7, 8], 'poly_feature': [1, 2, 3, 4], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_train_data.csv', index=False) return 'sample_train_data.csv' @pytest.fixture def sample_test_data(): data = { 'numeric_feature1': [5.0, 6.0, 7.0, 8.0], 'numeric_feature2': [50.0, 60.0, 70.0, 80.0], 'log_feature': [5, 50, 500, 5000], 'nominal_feature': ['A', 'B', 'A', 'B'], 'ordinal_feature': ['low', 'medium', 'high', 'medium'], 'binary_feature': [0, 1, 0, 1], 'interaction_feature1': [5, 6, 7, 8], 'interaction_feature2': [9, 10, 11, 12], 'poly_feature': [5, 6, 7, 8], 'income_group': ['low', 'medium', 'high', 'medium'] } df = pd.DataFrame(data) df.to_csv('sample_test_data.csv', index=False) return 'sample_test_data.csv' def test_transform(sample_data): # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.5, sample_size=None, return_as_df=True ) # Transform the data X_train, X_test, y_train, y_test = pipeline.transform(sample_data) # Print the transformed data print(\"X_train:\") print(X_train) print(\"X_test:\") print(X_test) print(\"y_train:\", y_train) print(\"y_test:\", y_test) # Assertions to check the correctness of the transformations assert isinstance(X_train, pd.DataFrame), \"X_train should be a DataFrame\" assert isinstance(X_test, pd.DataFrame), \"X_test should be a DataFrame\" assert isinstance(y_train, np.ndarray), \"y_train should be a numpy array\" assert isinstance(y_test, np.ndarray), \"y_test should be a numpy array\" assert 'income_group' not in X_train.columns, \"Target column should not be in X_train\" assert 'income_group' not in X_test.columns, \"Target column should not be in X_test\" assert len(y_train) == int(len(pd.read_csv(sample_data)) * (1 - pipeline.test_size)), \"y_train should have the correct number of rows based on test_size\" assert len(y_test) == len(pd.read_csv(sample_data)) * pipeline.test_size, \"y_test should have the correct number of rows based on test_size\" def test_transform_train_data(sample_train_data): # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.5, sample_size=None, return_as_df=True ) # Transform the training data X_train, y_train = pipeline.transform_train_data(sample_train_data) # Print the transformed training data print(\"Transformed Training Data:\") print(\"X_train:\") print(X_train) print(\"y_train:\", y_train) # Test 1: Check if the transformed data is a DataFrame assert isinstance(X_train, pd.DataFrame), \"X_train should be a DataFrame\" assert isinstance(y_train, np.ndarray), \"y_train should be a numpy array\" # Test 2: Check if the target variable has been correctly separated assert 'income_group' not in X_train.columns, \"Target column should not be in X_train\" assert len(y_train) ==", "source": "test_datapipeline.py"}, {"content": "len(pd.read_csv(sample_train_data)), \"y_train should have the correct number of rows as the train data set\" def test_transform_test_data(sample_train_data, sample_test_data): # Instantiate the Datapipeline class pipeline = Datapipeline() # Set features pipeline.set_features( numeric_features=['numeric_feature1', 'numeric_feature2'], log_features=['log_feature'], nominal_features=['nominal_feature'], ordinal_features=['ordinal_feature'], ordinal_categories=[['low', 'medium', 'high']], binary_features=['binary_feature'], interaction_features=['interaction_feature1', 'interaction_feature2'], poly_features=['poly_feature'], percentile=None ) # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=False, target_column='income_group', columns_to_drop=None, replacements=None, columns_to_impute=None ) # Set transform parameters pipeline.set_transform_params( test_size=0.5, sample_size=None, return_as_df=True ) # Transform the training data first to fit the preprocessor pipeline.transform_train_data(sample_train_data) # Transform the testing data X_test, y_test = pipeline.transform_test_data(sample_test_data) # Print the transformed testing data print(\"Transformed Testing Data:\") print(\"X_test:\") print(X_test) print(\"y_test:\", y_test) # Test 1: Check if the transformed data is a DataFrame assert isinstance(X_test, pd.DataFrame), \"X_test should be a DataFrame\" assert isinstance(y_test, np.ndarray), \"y_test should be a numpy array\" # Test 2: Check if the target variable has been correctly separated assert 'income_group' not in X_test.columns, \"Target column should not be in X_test\" assert len(y_test) == len(pd.read_csv(sample_test_data)), \"y_test should have the correct number of rows as the test data set\" def test_data_cleaning(): # Sample data data = { 'feature1': [1, 2, 2, None], 'feature2': ['A', 'B', 'B', 'A'], 'target': ['positive', 'negative', 'positive', 'negative'], 'unnecessary_column': [10, 20, 20, 10] } df = pd.DataFrame(data) # Instantiate the Datapipeline class pipeline = Datapipeline() # Set cleaning parameters pipeline.set_cleaning_params( drop_duplicates=True, target_column='target', columns_to_drop=['unnecessary_column'], replacements={'feature2': {'A': 'Alpha', 'B': 'Beta'}}, columns_to_impute=['feature1'] ) # Clean the data cleaned_df = pipeline.data_cleaning(df) # Print the cleaned DataFrame print(cleaned_df) # Assertions to check the correctness of the cleaning assert 'unnecessary_column' not in cleaned_df.columns, \"Unnecessary column should be dropped\" assert cleaned_df['feature2'].tolist() == ['Alpha', 'Beta', 'Beta', 'Alpha'], \"Values in 'feature2' should be replaced\" assert cleaned_df['feature1'].isnull().sum() == 0, \"Missing values in 'feature1' should be imputed\" assert cleaned_df.duplicated().sum() == 0, \"Duplicate rows should be dropped\" def test_create_preprocessor(): # Sample data data = { 'age': [25, 45, 35, 50], 'hours_per_week': [40, 50, 60, 20], 'wage_per_hour': [15, 20, 25, 30], 'capital_gains': [0, 1000, 500, 0], 'capital_losses': [0, 0, 0, 100], 'dividends_from_stocks': [0, 200, 0, 0], 'marital_status': ['Never-married', 'Married', 'Divorced', 'Widowed'], 'occupation': ['Tech-support', 'Craft-repair', 'Other-service', 'Sales'], 'education': ['Bachelors degree(BA AB BS)', 'High school graduate', 'Some college but no degree', 'Masters degree(MA MS MEng MEd MSW MBA)'], 'full_or_part_time_employment_stat': ['Full-time schedules', 'PT for econ reasons usually FT', 'Not in labor force', 'Unemployed full-time'], 'sex': ['Male', 'Female', 'Female', 'Male'], 'weeks_worked_in_year': [52, 40, 30, 20], 'num_persons_worked_for_employer': [1, 2, 3, 4] } df = pd.DataFrame(data) # Define Feature Lists numeric_features = ['age', 'hours_per_week'] log_features = ['wage_per_hour', 'capital_gains', 'capital_losses', 'dividends_from_stocks'] nominal_features = ['marital_status', 'occupation'] ordinal_features = ['education', 'full_or_part_time_employment_stat'] binary_features = ['capital_gains'] interaction_features = ['weeks_worked_in_year', 'num_persons_worked_for_employer'] poly_features = ['age'] # Define Ordinal Categories education_order = [ 'Less than 1st grade', '1st 2nd 3rd or 4th grade', '5th or 6th grade', '7th and 8th grade', '9th grade', '10th grade', '11th grade', '12th grade no diploma', 'High school graduate', 'Some college but no degree', 'Associates degree-occup /vocational', 'Associates degree-academic program', 'Bachelors degree(BA AB BS)', 'Masters degree(MA MS MEng MEd MSW MBA)', 'Prof school degree (MD DDS DVM LLB JD)', 'Doctorate degree(PhD EdD)', 'Children' ] employment_order = [ 'Children or Armed Forces', 'Not in labor force', 'Unemployed part- time', 'Unemployed full-time', 'PT for econ reasons", "source": "test_datapipeline.py"}, {"content": "usually PT', 'PT for econ reasons usually FT', 'PT for non-econ reasons usually FT', 'Full-time schedules' ] ordinal_categories = [education_order, employment_order] # Instantiate the Datapipeline class pipeline = Datapipeline() # Set feature lists pipeline.set_features( numeric_features=numeric_features, log_features=log_features, nominal_features=nominal_features, ordinal_features=ordinal_features, ordinal_categories=ordinal_categories, binary_features=binary_features, interaction_features=interaction_features, poly_features=poly_features, percentile=None # Set percentile for feature selection ) # Create the preprocessor preprocessor = pipeline.create_preprocessor() # Fit and transform the sample data using the created preprocessor preprocessed_data = preprocessor.fit_transform(df) # Get Feature Names feature_names = preprocessor.get_feature_names_out() # Create DataFrame with Transformed Data transformed_df = pd.DataFrame(preprocessed_data, columns=feature_names) # Print the transformed data print(transformed_df) # Assertions to check the correctness of the preprocessing assert isinstance(transformed_df, pd.DataFrame), \"Transformed data should be a DataFrame\" assert len(transformed_df) == len(df), \"Transformed data should have the same number of rows as the original data\" assert all(col in transformed_df.columns for col in feature_names), \"All feature names should be in the transformed DataFrame\"", "source": "test_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "import os import numpy as np import tensorflow as tf from tensorflow.keras import layers, models from tensorflow.keras.preprocessing import image_dataset_from_directory from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, roc_auc_score, precision_recall_curve, auc from tensorflow.keras.metrics import SparseCategoricalAccuracy from tensorflow.keras.metrics import Recall import mlflow import mlflow.keras from mlflow.models.signature import ModelSignature from mlflow.types.schema import Schema, TensorSpec import optuna from omegaconf import DictConfig, OmegaConf from hydra.utils import to_absolute_path import matplotlib.pyplot as plt import seaborn as sns import hydra class TensorFoodTrainer: def __init__(self, cfg: DictConfig): self.cfg = cfg self.data_dir = cfg.data.data_dir self.img_height = cfg.data.img_height self.img_width = cfg.data.img_width self.batch_size = cfg.data.batch_size self.train_dataset = None self.validation_dataset = None self.class_names = None self.model = None self.base_model = None self.history = None self.history_fine = None def load_data(self): print(f\"Current working directory: {os.getcwd()}\") data_dir_absolute = to_absolute_path(self.cfg.data.data_dir) print(f\"Data directory absolute path: {data_dir_absolute}\") self.train_dataset = image_dataset_from_directory( data_dir_absolute, validation_split=0.2, subset=\"training\", seed=123, image_size=(self.img_height, self.img_width), batch_size=self.batch_size ) self.validation_dataset = image_dataset_from_directory( data_dir_absolute, validation_split=0.2, subset=\"validation\", seed=123, image_size=(self.img_height, self.img_width), batch_size=self.batch_size ) self.class_names = self.train_dataset.class_names data_augmentation = tf.keras.Sequential([ layers.RandomFlip(\"horizontal_and_vertical\"), layers.RandomRotation(0.2), layers.RandomZoom(0.2), layers.RandomWidth(0.2), layers.RandomHeight(0.2), layers.RandomTranslation(0.2, 0.2), layers.RandomContrast(0.2), layers.RandomBrightness(factor=0.2), ]) self.train_dataset = self.train_dataset.map( lambda x, y: (data_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE ) self.train_dataset = self.train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE) self.validation_dataset = self.validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE) # Print shapes of images and labels for images, labels in self.train_dataset.take(1): print(f\"Train images shape: {images.shape}\") print(f\"Train labels shape: {labels.shape}\") for images, labels in self.validation_dataset.take(1): print(f\"Validation images shape: {images.shape}\") print(f\"Validation labels shape: {labels.shape}\") def get_class_names(self): return self.class_names def build_model(self, trial=None): self.base_model = getattr(tf.keras.applications, self.cfg.model.base_model)( input_shape=(self.img_height, self.img_width, 3), include_top=self.cfg.model.include_top, weights=self.cfg.model.weights ) self.base_model.trainable = False inputs = tf.keras.Input(shape=(self.img_height, self.img_width, 3)) x = tf.keras.applications.vgg16.preprocess_input(inputs) x = self.base_model(x, training=False) x = layers.GlobalAveragePooling2D()(x) units_1 = trial.suggest_int(\"units_1\", 32, 128, log=True) if trial else self.cfg.model.units_1 units_2 = trial.suggest_int(\"units_2\", 32, 128, log=True) if trial else self.cfg.model.units_2 dropout_rate = trial.suggest_float(\"dropout_rate\", 0.2, 0.5) if trial else self.cfg.model.dropout_rate learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True) if trial else self.cfg.model.learning_rate x = layers.Dense(units_1, activation='relu')(x) x = layers.Dropout(dropout_rate)(x) x = layers.Dense(units_2, activation='relu')(x) x = layers.Dropout(dropout_rate)(x) outputs = layers.Dense(12, activation='softmax')(x) self.model = models.Model(inputs, outputs) self.model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=[SparseCategoricalAccuracy()] ) self.model.summary() def train_model(self): mlflow.set_tracking_uri(self.cfg.mlflow_tracking_uri) experiment_name = self.cfg.mlflow_exp_name if not mlflow.get_experiment_by_name(experiment_name): mlflow.create_experiment(experiment_name) mlflow.set_experiment(experiment_name) early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True) model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss') with mlflow.start_run(): mlflow.log_param(\"optimizer\", \"adam\") mlflow.log_param(\"loss\", \"sparse_categorical_crossentropy\") mlflow.log_param(\"batch_size\", self.cfg.data.batch_size) mlflow.log_param(\"epochs\", self.cfg.optuna.epochs) self.history = self.model.fit( self.train_dataset, validation_data=self.validation_dataset, epochs=self.cfg.optuna.epochs, callbacks=[early_stopping, model_checkpoint] ) actual_epochs = len(self.history.history['sparse_categorical_accuracy']) mlflow.log_param(\"actual_epochs\", actual_epochs) for epoch in range(actual_epochs): mlflow.log_metric(\"sparse_categorical_accuracy\", self.history.history['sparse_categorical_accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_sparse_categorical_accuracy\", self.history.history['val_sparse_categorical_accuracy'][epoch], step=epoch) mlflow.log_metric(\"loss\", self.history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", self.history.history['val_loss'][epoch], step=epoch) input_dim = self.cfg.data.img_height * self.cfg.data.img_width * 3 input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, input_dim))]) output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 12))]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) mlflow.keras.log_model( model=self.model, artifact_path=\"model\", signature=signature ) def fine_tune_model(self, fine_tune_epochs): self.base_model.trainable = True for layer in self.base_model.layers[:10]: layer.trainable = False self.model.compile( optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='sparse_categorical_crossentropy', metrics=[SparseCategoricalAccuracy()] ) total_epochs = len(self.history.epoch) + fine_tune_epochs with mlflow.start_run(): mlflow.log_param(\"optimizer\", \"adam (fine-tuning)\") mlflow.log_param(\"learning_rate\", 1e-5) mlflow.log_param(\"fine_tune_epochs\", fine_tune_epochs) self.history_fine = self.model.fit( self.train_dataset, validation_data=self.validation_dataset, epochs=total_epochs, initial_epoch=self.history.epoch[-1], callbacks=[ tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True), tf.keras.callbacks.ModelCheckpoint('best_model_fine_tuned.keras', save_best_only=True, monitor='val_loss') ] ) actual_epochs = len(self.history_fine.history['sparse_categorical_accuracy']) mlflow.log_param(\"actual_epochs_fine_tune\", actual_epochs) for epoch in range(actual_epochs): mlflow.log_metric(\"sparse_categorical_accuracy_fine_tune\", self.history_fine.history['sparse_categorical_accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_sparse_categorical_accuracy_fine_tune\", self.history_fine.history['val_sparse_categorical_accuracy'][epoch], step=epoch) mlflow.log_metric(\"loss_fine_tune\", self.history_fine.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_loss_fine_tune\", self.history_fine.history['val_loss'][epoch], step=epoch) input_dim = self.img_height * self.img_width * 3 input_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, input_dim))]) output_schema = Schema([TensorSpec(np.dtype(np.float32), (-1, 12))]) signature = ModelSignature(inputs=input_schema, outputs=output_schema) mlflow.keras.log_model( model=self.model, artifact_path=\"model_fine_tuned\", signature=signature ) def visualize_results(self): val_images, val_labels = [], [] for images, labels in self.validation_dataset: val_images.append(images)", "source": "train.py"}, {"content": "val_labels.append(labels) val_images = np.concatenate(val_images) val_labels = np.concatenate(val_labels) predictions = self.model.predict(val_images) predicted_labels = np.argmax(predictions, axis=1) cm = confusion_matrix(val_labels, predicted_labels) plt.figure(figsize=(10, 8)) sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=self.class_names, yticklabels=self.class_names) plt.xlabel('Predicted') plt.ylabel('True') plt.title('Confusion Matrix') plt.show() misclassified_indices = np.where(predicted_labels != val_labels)[0] plt.figure(figsize=(15, 15)) for i, idx in enumerate(misclassified_indices[:9]): ax = plt.subplot(3, 3, i + 1) plt.imshow(val_images[idx].astype(\"uint8\")) plt.title(f\"True: {self.class_names[val_labels[idx]]}, Pred: {self.class_names[predicted_labels[idx]]}\") plt.axis(\"off\") plt.show() val_loss, val_accuracy = self.model.evaluate(self.validation_dataset) print(f\"Validation accuracy: {val_accuracy * 100:.2f}%\") acc = self.history.history['sparse_categorical_accuracy'] val_acc = self.history.history['val_sparse_categorical_accuracy'] loss = self.history.history['loss'] val_loss = self.history.history['val_loss'] epochs_range = range(len(acc)) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='Training Accuracy') plt.plot(epochs_range, val_acc, label='Validation Accuracy') plt.legend(loc='lower right') plt.title('Training and Validation Accuracy') plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='Training Loss') plt.plot(epochs_range, val_loss, label='Validation Loss') plt.legend(loc='upper right') plt.title('Training and Validation Loss') plt.show() acc = self.history.history['sparse_categorical_accuracy'] + self.history_fine.history['sparse_categorical_accuracy'] val_acc = self.history.history['val_sparse_categorical_accuracy'] + self.history_fine.history['val_sparse_categorical_accuracy'] loss = self.history.history['loss'] + self.history_fine.history['loss'] val_loss = self.history.history['val_loss'] + self.history_fine.history['val_loss'] epochs_range = range(len(acc)) plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.plot(epochs_range, acc, label='Training Accuracy') plt.plot(epochs_range, val_acc, label='Validation Accuracy') plt.legend(loc='lower right') plt.title('Training and Validation Accuracy') plt.subplot(1, 2, 2) plt.plot(epochs_range, loss, label='Training Loss') plt.plot(epochs_range, val_loss, label='Validation Loss') plt.legend(loc='upper right') plt.title('Training and Validation Loss') plt.show() self.model.save('tensorfood_vgg16_model.keras') def objective(self, trial): self.build_model(trial) self.train_model() # Remove the epochs argument # Evaluate the model on the validation set val_images, val_labels = [], [] for images, labels in self.validation_dataset: val_images.append(images) val_labels.append(labels) val_images = np.concatenate(val_images) val_labels = np.concatenate(val_labels) predictions = self.model.predict(val_images) predicted_labels = np.argmax(predictions, axis=1) val_recall = recall_score(val_labels, predicted_labels, average='macro') val_loss = self.history.history['val_loss'][-1] return val_loss, -val_recall @hydra.main(config_path=\"../conf\", config_name=\"config\", version_base=\"1.1\") def main(cfg: DictConfig): print(OmegaConf.to_yaml(cfg)) trainer = TensorFoodTrainer(cfg) trainer.load_data() # Create an Optuna study for multi-objective optimization study = optuna.create_study(directions=cfg.optuna.directions) # Run the optimization study.optimize(trainer.objective, n_trials=cfg.optuna.n_trials) # Print the best hyperparameters for each of the best trials print(\"Best hyperparameters for each objective:\") for i, trial in enumerate(study.best_trials): print(f\"Objective {i + 1}: {trial.params}\") trainer.build_model(study.best_trials[0]) trainer.train_model() trainer.fine_tune_model(fine_tune_epochs=cfg.optuna.fine_tune_epochs) trainer.visualize_results() if __name__ == \"__main__\": main()", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim import numpy as np class CNNModel(nn.Module): def __init__(self, input_size, num_channels, kernel_size, output_size, lookback, feature_size=None, cnn_type='1D'): super(CNNModel, self).__init__() self.cnn_type = cnn_type self.device = self.get_device() if self.cnn_type == '1D': self.conv1 = nn.Conv1d(in_channels=input_size, out_channels=num_channels, kernel_size=kernel_size) self.pool = nn.MaxPool1d(kernel_size=2) self.fc1 = nn.Linear(num_channels * ((lookback - kernel_size + 1) // 2), output_size) elif self.cnn_type == '2D': self.conv1 = nn.Conv2d(in_channels=1, out_channels=num_channels, kernel_size=kernel_size) self.pool = nn.MaxPool2d(kernel_size=2) conv_output_size = ((lookback - kernel_size[0] + 1) // 2) * ((feature_size - kernel_size[1] + 1) // 2) self.fc1 = nn.Linear(num_channels * conv_output_size, output_size) else: raise ValueError(\"cnn_type must be either '1D' or '2D'\") def get_device(self): if torch.cuda.is_available(): num_gpus = torch.cuda.device_count() if num_gpus > 1: return torch.device('cuda:1') else: return torch.device('cuda:0') else: return torch.device('cpu') def forward(self, x): if self.cnn_type == '1D': x = x.permute(0, 2, 1) # Change shape to (batch_size, input_size, sequence_length) x = self.pool(torch.relu(self.conv1(x))) elif self.cnn_type == '2D': x = x.unsqueeze(1) # Add channel dimension: (batch_size, 1, lookback, feature_size) x = self.pool(torch.relu(self.conv1(x))) x = x.view(x.size(0), -1) x = self.fc1(x) return x def fit(self, dataloader, num_epochs, learning_rate): criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) self.train() for epoch in range(num_epochs): for features, labels in dataloader: features, labels = features.to(self.device), labels.to(self.device) # Reshape labels to match the output shape labels = labels.view(-1, 1) outputs = self(features) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(self.device) outputs = self(features) predictions.append(outputs.cpu().numpy()) return np.concatenate(predictions, axis=0) def evaluate(self, dataloader): self.eval() criterion = nn.MSELoss() mse, mae, rmse, mape, smape = 0, 0, 0, 0, 0 n_samples = 0 with torch.no_grad(): for features, labels in dataloader: features, labels = features.to(self.device), labels.to(self.device) # Reshape labels to match the output shape labels = labels.view(-1, 1) outputs = self(features) mse += criterion(outputs, labels).item() mae += torch.mean(torch.abs(outputs - labels)).item() rmse += torch.sqrt(criterion(outputs, labels)).item() mape += torch.mean(torch.abs((labels - outputs) / labels)).item() smape += torch.mean(2 * torch.abs(labels - outputs) / (torch.abs(labels) + torch.abs(outputs))).item() n_samples += len(labels) mse /= len(dataloader) mae /= len(dataloader) rmse /= len(dataloader) mape /= len(dataloader) smape /= len(dataloader) return { 'mse': mse, 'mae': mae, 'rmse': rmse, 'mape': mape, 'smape': smape }", "source": "cnn_model.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim import numpy as np class CNNModel(nn.Module): def __init__(self, input_size, num_channels, kernel_size, output_size, lookback, feature_size=None, cnn_type='1D', num_conv_layers=1, num_fc_layers=1): super(CNNModel, self).__init__() self.cnn_type = cnn_type self.device = self.get_device() self.num_conv_layers = num_conv_layers self.num_fc_layers = num_fc_layers layers = [] if self.cnn_type == '1D': in_channels = input_size current_size = lookback for _ in range(num_conv_layers): if current_size < kernel_size: kernel_size = current_size layers.append(nn.Conv1d(in_channels=in_channels, out_channels=num_channels, kernel_size=kernel_size)) layers.append(nn.ReLU()) if current_size < 2: pool_size = current_size else: pool_size = 2 layers.append(nn.MaxPool1d(kernel_size=pool_size)) in_channels = num_channels current_size = (current_size - kernel_size + 1) // pool_size self.conv_layers = nn.Sequential(*layers) conv_output_size = num_channels * current_size elif self.cnn_type == '2D': in_channels = 1 current_size = (lookback, feature_size) for _ in range(num_conv_layers): if current_size[0] < kernel_size[0] or current_size[1] < kernel_size[1]: kernel_size = (min(current_size[0], kernel_size[0]), min(current_size[1], kernel_size[1])) layers.append(nn.Conv2d(in_channels=in_channels, out_channels=num_channels, kernel_size=kernel_size)) layers.append(nn.ReLU()) if current_size[0] < 2 or current_size[1] < 2: pool_size = (min(current_size[0], 2), min(current_size[1], 2)) else: pool_size = (2, 2) layers.append(nn.MaxPool2d(kernel_size=pool_size)) in_channels = num_channels current_size = ((current_size[0] - kernel_size[0] + 1) // pool_size[0], (current_size[1] - kernel_size[1] + 1) // pool_size[1]) self.conv_layers = nn.Sequential(*layers) conv_output_size = num_channels * current_size[0] * current_size[1] else: raise ValueError(\"cnn_type must be either '1D' or '2D'\") fc_layers = [] in_features = conv_output_size for _ in range(num_fc_layers - 1): fc_layers.append(nn.Linear(in_features, in_features // 2)) fc_layers.append(nn.ReLU()) in_features = in_features // 2 fc_layers.append(nn.Linear(in_features, output_size)) self.fc_layers = nn.Sequential(*fc_layers) def get_device(self): if torch.cuda.is_available(): num_gpus = torch.cuda.device_count() if num_gpus > 1: return torch.device('cuda:1') else: return torch.device('cuda:0') else: return torch.device('cpu') def forward(self, x): if self.cnn_type == '1D': x = x.permute(0, 2, 1) # Change shape to (batch_size, input_size, sequence_length) elif self.cnn_type == '2D': x = x.unsqueeze(1) # Add channel dimension: (batch_size, 1, lookback, feature_size) x = self.conv_layers(x) x = x.view(x.size(0), -1) x = self.fc_layers(x) return x def fit(self, train_dataloader, num_epochs, learning_rate, val_dataloader=None, early_stopping_patience=5): criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) self.train() best_loss = float('inf') patience_counter = 0 for epoch in range(num_epochs): epoch_loss = 0 for features, labels in train_dataloader: features, labels = features.to(self.device), labels.to(self.device) # Reshape labels to match the output shape labels = labels.view(-1, 1) outputs = self(features) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() epoch_loss += loss.item() epoch_loss /= len(train_dataloader) print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}') if val_dataloader: self.eval() val_loss = 0 with torch.no_grad(): for features, labels in val_dataloader: features, labels = features.to(self.device), labels.to(self.device) labels = labels.view(-1, 1) outputs = self(features) loss = criterion(outputs, labels) val_loss += loss.item() val_loss /= len(val_dataloader) print(f'Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_loss:.4f}') if val_loss < best_loss: best_loss = val_loss patience_counter = 0 best_model = self.state_dict() else: patience_counter += 1 if patience_counter >= early_stopping_patience: print(\"Early stopping triggered\") break else: if epoch_loss < best_loss: best_loss = epoch_loss patience_counter = 0 best_model = self.state_dict() else: patience_counter += 1 if patience_counter >= early_stopping_patience: print(\"Early stopping triggered\") break self.load_state_dict(best_model) return best_loss def predict_model(model, dataloader): model.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(model.device) outputs = model(features) predictions.append(outputs.cpu().numpy()) return np.concatenate(predictions, axis=0) def evaluate_model(model, dataloader): model.eval() criterion = nn.MSELoss() mse, mae, rmse, mape, smape = 0, 0, 0, 0, 0 n_samples = 0 with torch.no_grad(): for features, labels in dataloader: features, labels = features.to(model.device), labels.to(model.device) # Reshape labels to match the", "source": "cnn_model_v2.py"}, {"content": "output shape labels = labels.view(-1, 1) outputs = model(features) mse += criterion(outputs, labels).item() mae += torch.mean(torch.abs(outputs - labels)).item() rmse += torch.sqrt(criterion(outputs, labels)).item() mape += torch.mean(torch.abs((labels - outputs) / labels)).item() smape += torch.mean(2 * torch.abs(labels - outputs) / (torch.abs(labels) + torch.abs(outputs))).item() n_samples += len(labels) mse /= len(dataloader) mae /= len(dataloader) rmse /= len(dataloader) mape /= len(dataloader) smape /= len(dataloader) return { 'mse': mse, 'mae': mae, 'rmse': rmse, 'mape': mape, 'smape': smape }", "source": "cnn_model_v2.py"}, {"content": "import pandas as pd class DataPipeline: def __init__(self): pass def load_data(self, file_path): \"\"\"Load data from a CSV file.\"\"\" return pd.read_csv(file_path) def combine_datetime(self, df): \"\"\"Combine year, month, day, and hour columns into a single datetime column.\"\"\" df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) return df def set_datetime_index(self, df): \"\"\"Set the datetime column as the index.\"\"\" df.set_index('datetime', inplace=True) return df def drop_unnecessary_columns(self, df): \"\"\"Drop the original year, month, day, and hour columns.\"\"\" df.drop(columns=['No', 'year', 'month', 'day', 'hour'], inplace=True) return df def ensure_correct_dtypes(self, df): \"\"\"Ensure that the columns have the correct data types.\"\"\" df = df.astype({ 'pm2.5': 'float64', 'DEWP': 'float64', 'TEMP': 'float64', 'PRES': 'float64', 'cbwd': 'category', 'Iws': 'float64', 'Is': 'int64', 'Ir': 'int64' }) return df def fill_missing_values_moving_average(self, df, window_size): \"\"\"Fill missing values using moving average and backfill if necessary.\"\"\" numeric_cols = df.select_dtypes(include=['number']).columns for feature in numeric_cols: previous_missing = df[feature].isnull().sum() while df[feature].isnull().sum() > 0: df[feature] = df[feature].fillna(df[feature].rolling(window=window_size, min_periods=1).mean()) current_missing = df[feature].isnull().sum() print(f\"Missing values for {feature} after iteration: {current_missing}\") # If the number of missing values doesn't change, backfill if current_missing == previous_missing: df[feature] = df[feature].fillna(method='bfill') print(f\"Backfilled missing values for {feature}. Remaining missing values: {df[feature].isnull().sum()}\") break previous_missing = current_missing return df def handle_missing_data(self, df, method='ffill', window_size=24): \"\"\"Handle missing data using the specified method (ffill, bfill, or moving average).\"\"\" if method == 'ffill': df = df.ffill() elif method == 'bfill': df = df.bfill() elif method == 'moving_average': df = self.fill_missing_values_moving_average(df, window_size) return df def downsample_data(self, df, frequency='10D'): \"\"\"Downsample the data to the specified frequency.\"\"\" df_numeric = df.select_dtypes(include=['float64', 'int64']) df_downsampled = df_numeric.resample(frequency).mean() return df_downsampled def upsample_data(self, df, frequency='H'): \"\"\"Upsample the data to the specified frequency and interpolate missing values.\"\"\" df_upsampled = df.resample(frequency).interpolate(method='linear') return df_upsampled def create_lagged_features(self, df, features, lags=24, apply_differencing=False): \"\"\"Create lagged features for the specified columns.\"\"\" lagged_features = {} for feature in features: lagged_features[feature] = df[feature] # Include the unlagged feature for lag in range(1, lags + 1): lagged_features[f'{feature}_lag_{lag}'] = df[feature].shift(lag) if apply_differencing: # Difference the target variable before lagging df['pm2.5_diff'] = df['pm2.5'].diff() target_column = 'pm2.5_diff' else: target_column = 'pm2.5' lagged_data = pd.concat(lagged_features, axis=1) lagged_data['target'] = df[target_column].shift(-1) lagged_data.dropna(inplace=True) return lagged_data def run_data_pipeline(self, csv_path, missing_data_method='ffill', downsample_freq='10D', upsample_freq='H', window_size=24, feature_engineering=False, lags=24, apply_differencing=False): \"\"\"Run the data pipeline.\"\"\" df = self.load_data(csv_path) df = self.combine_datetime(df) df = self.set_datetime_index(df) df = self.drop_unnecessary_columns(df) df = self.ensure_correct_dtypes(df) df = self.handle_missing_data(df, method=missing_data_method, window_size=window_size) df_downsampled = self.downsample_data(df, frequency=downsample_freq) df_upsampled = self.upsample_data(df_downsampled, frequency=upsample_freq) if feature_engineering: # Feature engineering: create lagged features features = ['DEWP', 'TEMP', 'PRES', 'Iws', 'Is', 'Ir'] df_upsampled = self.create_lagged_features(df_upsampled, features, lags=lags, apply_differencing=apply_differencing) return df_upsampled # Example usage # if __name__ == \"__main__\": # pipeline = DataPipeline() # file_path = 'data/beijing+pm2+5+data/PRSA_data_2010.1.1-2014.12.31.csv' # lagged_data = pipeline.run_data_pipeline(file_path, missing_data_method='moving_average', feature_engineering=True, lags=48, apply_differencing=True) # print(lagged_data.head())", "source": "datapipeline.py"}, {"content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from .datapipeline import DataPipeline from .ml_model import ForecastModel def run_experiment(data_path, horizons=[1, 2, 3, 6, 12, 24], apply_differencing=False): # Initialize the data pipeline pipeline = DataPipeline() # Run the data pipeline to preprocess the data lagged_data = pipeline.run_data_pipeline(data_path, missing_data_method='moving_average', feature_engineering=True, lags=12, apply_differencing=apply_differencing) # Split the data into features and target X = lagged_data.drop(columns=['target']) y = lagged_data['target'] # Define the split point (e.g., 80% of the data for training, 20% for testing) split_point = int(len(lagged_data) * 0.8) # Create train and test sets based on the split point X_train, X_test = X[:split_point], X[split_point:] y_train, y_test = y[:split_point], y[split_point:] metrics_dict = {} predictions_dict = {} for horizon in horizons: # Adjust the target for the forecast horizon y_train_horizon = y_train.shift(-horizon).dropna() y_test_horizon = y_test.shift(-horizon).dropna() # Align the features with the adjusted target X_train_horizon = X_train.iloc[:len(y_train_horizon)] X_test_horizon = X_test.iloc[:len(y_test_horizon)] # Initialize the forecast model model = ForecastModel() # Fit the model on the training data model.fit(X_train_horizon, y_train_horizon) # Evaluate the model on both training and testing data evaluation_results = model.evaluate(X_train_horizon, y_train_horizon, X_test_horizon, y_test_horizon) # Record the errors for the current forecast horizon metrics_dict[f\"horizon_{horizon}\"] = { \"train_mse\": evaluation_results['train_mse'], \"train_mae\": evaluation_results['train_mae'], \"train_rmse\": evaluation_results['train_rmse'], \"train_mape\": evaluation_results['train_mape'], \"train_smape\": evaluation_results['train_smape'], \"test_mse\": evaluation_results['test_mse'], \"test_mae\": evaluation_results['test_mae'], \"test_rmse\": evaluation_results['test_rmse'], \"test_mape\": evaluation_results['test_mape'], \"test_smape\": evaluation_results['test_smape'] } # Generate predictions for the test set predictions = model.predict(X_test_horizon) predictions_dict[f\"horizon_{horizon}\"] = predictions return model, metrics_dict, predictions_dict, y_test def plot_predictions_subplots(y_test, predictions_dict, horizons, apply_differencing=False): \"\"\" Plot the actual and predicted values for different forecast horizons in subplots. Parameters: - y_test (pd.Series): Actual target values for the test set. - predictions_dict (dict): Dictionary containing predictions for each forecast horizon. - horizons (list): List of forecast horizons. - apply_differencing (bool): Whether differencing was applied to the target variable. \"\"\" num_horizons = len(horizons) fig, axes = plt.subplots(num_horizons, 1, figsize=(14, 7 * num_horizons), sharex=True) for i, horizon in enumerate(horizons): predictions = predictions_dict[f\"horizon_{horizon}\"] y_test_horizon = y_test.shift(-horizon).dropna() results = pd.DataFrame({'Actual': y_test_horizon, 'Predicted': predictions}, index=y_test_horizon.index) sns.lineplot(data=results['Actual'], color='blue', linewidth=2.5, label='Actual', ax=axes[i]) sns.lineplot(data=results['Predicted'], color='red', linewidth=2.5, label='Predicted', ax=axes[i]) title = f'Actual vs Predicted {\"Differenced \" if apply_differencing else \"\"}PM2.5 Values for Horizon {horizon}' ylabel = f'{\"Differenced \" if apply_differencing else \"\"}PM2.5' axes[i].set_title(title) axes[i].set_xlabel('Datetime') axes[i].set_ylabel(ylabel) axes[i].legend() plt.tight_layout() plt.show() # Example usage if __name__ == \"__main__\": data_path = 'data/beijing+pm2+5+data/PRSA_data_2010.1.1-2014.12.31.csv' horizons = [1, 2, 3, 6, 12, 24] apply_differencing = True # Set to True or False based on your requirement model, metrics, predictions, y_test = run_experiment(data_path, horizons, apply_differencing) print(metrics) plot_predictions_subplots(y_test, predictions, horizons, apply_differencing)", "source": "ml_experiment.py"}, {"content": "from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error import numpy as np class ForecastModel: def __init__(self, n_estimators=100, random_state=42): \"\"\" Initializes the ForecastModel with a Random Forest Regressor. Parameters: - n_estimators (int): Number of trees in the forest. - random_state (int): Seed for reproducibility. \"\"\" self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state) def fit(self, X, y): \"\"\" Fits the Random Forest model to the training data. Parameters: - X (pd.DataFrame or np.ndarray): Feature matrix for training. - y (pd.Series or np.ndarray): Target vector for training. \"\"\" self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): \"\"\" Evaluates the model on both training and testing data using multiple metrics. Parameters: - X_train (pd.DataFrame or np.ndarray): Feature matrix for training data. - y_train (pd.Series or np.ndarray): Target vector for training data. - X_test (pd.DataFrame or np.ndarray): Feature matrix for testing data. - y_test (pd.Series or np.ndarray): Target vector for testing data. Returns: - dict: A dictionary containing MSE, MAE, RMSE, MAPE, and sMAPE for both training and testing sets. \"\"\" # Predictions on training data y_train_pred = self.model.predict(X_train) train_mse = mean_squared_error(y_train, y_train_pred) train_mae = mean_absolute_error(y_train, y_train_pred) train_rmse = np.sqrt(train_mse) train_mape = mean_absolute_percentage_error(y_train, y_train_pred) * 100 train_smape = self.symmetric_mean_absolute_percentage_error(y_train, y_train_pred) * 100 # Predictions on testing data y_test_pred = self.model.predict(X_test) test_mse = mean_squared_error(y_test, y_test_pred) test_mae = mean_absolute_error(y_test, y_test_pred) test_rmse = np.sqrt(test_mse) test_mape = mean_absolute_percentage_error(y_test, y_test_pred) * 100 test_smape = self.symmetric_mean_absolute_percentage_error(y_test, y_test_pred) * 100 return { 'train_mse': train_mse, 'train_mae': train_mae, 'train_rmse': train_rmse, 'train_mape': train_mape, 'train_smape': train_smape, 'test_mse': test_mse, 'test_mae': test_mae, 'test_rmse': test_rmse, 'test_mape': test_mape, 'test_smape': test_smape } def predict(self, X): \"\"\" Generates predictions using the trained model. Parameters: - X (pd.DataFrame or np.ndarray): Feature matrix for which to generate predictions. Returns: - np.ndarray: Predicted values. \"\"\" return self.model.predict(X) def calculate_error(self, y_true, y_pred): \"\"\" Calculates error metrics between true and predicted values. Parameters: - y_true (pd.Series or np.ndarray): Actual target values. - y_pred (np.ndarray): Predicted target values. Returns: - dict: A dictionary containing MSE, MAE, RMSE, MAPE, and sMAPE. \"\"\" mse = mean_squared_error(y_true, y_pred) mae = mean_absolute_error(y_true, y_pred) rmse = np.sqrt(mse) # Optional Metrics # Uncomment if you wish to include these metrics if np.any(y_true == 0): raise ValueError(\"y_true contains zero values, MAPE is undefined.\") mape = mean_absolute_percentage_error(y_true, y_pred) * 100 smape = self.symmetric_mean_absolute_percentage_error(y_true, y_pred) * 100 return { 'mse': mse, 'mae': mae, 'rmse': rmse, 'mape': mape, 'smape': smape } def symmetric_mean_absolute_percentage_error(self, y_true, y_pred): \"\"\" Calculates the Symmetric Mean Absolute Percentage Error (sMAPE). Parameters: - y_true (np.ndarray): Actual target values. - y_pred (np.ndarray): Predicted target values. Returns: - float: sMAPE value. \"\"\" denominator = (np.abs(y_true) + np.abs(y_pred)) / 2 # To avoid division by zero mask = denominator != 0 return np.mean(2.0 * np.abs(y_pred - y_true) / denominator[mask])", "source": "ml_model.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim import numpy as np class RNNModel(nn.Module): def __init__(self, input_size, hidden_size, num_layers, output_size): super(RNNModel, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True) self.fc = nn.Linear(hidden_size, output_size) self.device = self.get_device() def get_device(self): if torch.cuda.is_available(): num_gpus = torch.cuda.device_count() if num_gpus > 1: return torch.device('cuda:1') else: return torch.device('cuda:0') else: return torch.device('cpu') def forward(self, x): h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(self.device) out, _ = self.gru(x, h0) out = self.fc(out[:, -1, :]) return out def fit(self, dataloader, num_epochs, learning_rate): criterion = nn.MSELoss() optimizer = optim.Adam(self.parameters(), lr=learning_rate) self.train() for epoch in range(num_epochs): for features, labels in dataloader: features, labels = features.to(self.device), labels.to(self.device) # Reshape labels to match the output shape labels = labels.view(-1, 1) outputs = self(features) loss = criterion(outputs, labels) optimizer.zero_grad() loss.backward() optimizer.step() print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}') def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for features, _ in dataloader: features = features.to(self.device) outputs = self(features) predictions.append(outputs.cpu().numpy()) return np.concatenate(predictions, axis=0) def evaluate(self, dataloader): self.eval() criterion = nn.MSELoss() mse, mae, rmse, mape, smape = 0, 0, 0, 0, 0 n_samples = 0 with torch.no_grad(): for features, labels in dataloader: features, labels = features.to(self.device), labels.to(self.device) # Reshape labels to match the output shape labels = labels.view(-1, 1) outputs = self(features) mse += criterion(outputs, labels).item() mae += torch.mean(torch.abs(outputs - labels)).item() rmse += torch.sqrt(criterion(outputs, labels)).item() mape += torch.mean(torch.abs((labels - outputs) / labels)).item() smape += torch.mean(2 * torch.abs(labels - outputs) / (torch.abs(labels) + torch.abs(outputs))).item() n_samples += len(labels) mse /= len(dataloader) mae /= len(dataloader) rmse /= len(dataloader) mape /= len(dataloader) smape /= len(dataloader) return { 'mse': mse, 'mae': mae, 'rmse': rmse, 'mape': mape, 'smape': smape }", "source": "rnn_model.py"}, {"content": "import os # Set the environment variable to avoid OpenMP runtime error os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\" import pandas as pd import torch from torch.utils.data import DataLoader from datapipeline import DataPipeline from cnn_model_v2 import CNNModel, predict_model, evaluate_model from windowing import WindowGenerator import matplotlib.pyplot as plt import mlflow import optuna import hydra # import shap from hydra import initialize, compose from hydra.utils import to_absolute_path from omegaconf import DictConfig, OmegaConf from mlflow.models.signature import infer_signature # Print the current working directory print(\"Current working directory:\", os.getcwd()) # Define functions # def log_shap_values(model, dataloader): # # Get a batch of data # batch = next(iter(dataloader)) # inputs, _ = batch # inputs = inputs.to(model.device) # # Reshape inputs for SHAP # batch_size, sequence_length, num_features = inputs.shape # inputs_reshaped = inputs.view(batch_size * sequence_length, num_features) # # Create a SHAP explainer # explainer = shap.DeepExplainer(model, inputs) # # Calculate SHAP values # shap_values = explainer.shap_values(inputs, check_additivity=False) # # Move inputs to CPU and convert to NumPy array for logging # inputs_cpu = inputs.cpu().numpy() # # Log SHAP values to MLflow # mlflow.shap.log_explanation(explainer, inputs_cpu) def prepare_data(cleaned_data, lookback, lookahead, label_col, batch_size): # Split the data into training, validation, and testing sets while maintaining temporal order test_split_index = int(len(cleaned_data) * 0.8) val_split_index = int(len(cleaned_data[:test_split_index]) * 0.8) train_data = cleaned_data[:val_split_index] val_data = cleaned_data[val_split_index:test_split_index] test_data = cleaned_data[test_split_index:] # Create datasets and dataloaders train_dataset = WindowGenerator(train_data, lookback, lookahead, label_col) val_dataset = WindowGenerator(val_data, lookback, lookahead, label_col) test_dataset = WindowGenerator(test_data, lookback, lookahead, label_col) train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False) return train_dataloader, val_dataloader, test_dataloader, test_data def train_and_evaluate_model(model, train_dataloader, val_dataloader, test_dataloader, num_epochs, learning_rate): # Train the model model.to(model.device) best_val_loss = model.fit(train_dataloader, num_epochs, learning_rate, val_dataloader=val_dataloader) # Evaluate the model on the training set train_metrics = evaluate_model(model, train_dataloader) print(f'Training Metrics: {train_metrics}') # Evaluate the model on the test set test_metrics = evaluate_model(model, test_dataloader) print(f'Test Metrics: {test_metrics}') # Make predictions on the test set predictions = predict_model(model, test_dataloader) return train_metrics, test_metrics, predictions, best_val_loss def plot_predictions(ground_truth, predictions, lookback, lookahead, label_col, title): # Extract ground truth values for the test set ground_truth_values = ground_truth[label_col].values[lookback + lookahead - 1:] # Plot predictions vs ground truth plt.figure(figsize=(12, 6)) plt.plot(ground_truth_values, label='Ground Truth') plt.plot(predictions, label='Predictions') plt.xlabel('Time') plt.ylabel('PM2.5') plt.legend() plt.title(title) # Log the plot to MLflow mlflow.log_figure(plt.gcf(), \"predictions_plot.png\") plt.close() def compare_models(train_metrics, test_metrics, previous_train_metrics, previous_test_metrics): print(\"\\nComparison with Previous Model:\") for metric in train_metrics.keys(): print(f\"{metric.upper()}:\") print(f\" Current Model - Train: {train_metrics[metric]:.4f}, Test: {test_metrics[metric]:.4f}\") print(f\" Previous Model - Train: {previous_train_metrics[metric]:.4f}, Test: {previous_test_metrics[metric]:.4f}\") if test_metrics[metric] < previous_test_metrics[metric]: print(f\" The current model performs better on {metric.upper()} than the previous model.\") else: print(f\" The previous model performs better on {metric.upper()} than the current model.\") def objective(trial, cfg, data, lookback): lookahead = cfg.lookahead batch_size = cfg.train_bs num_epochs = cfg.epochs learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True) num_channels = trial.suggest_int('num_channels', 8, 128) cnn_type = cfg.cnn_type label_col = 'pm2.5' # Add the label_col argument # Set a fixed kernel size kernel_size = 3 dataset = WindowGenerator(data, lookback=lookback, lookahead=lookahead, label_col=label_col) dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False) model = CNNModel(input_size=data.shape[1] - 1, num_channels=num_channels, kernel_size=kernel_size, output_size=1, lookback=lookback, cnn_type=cnn_type) model.to(model.device) model.fit(dataloader, num_epochs, learning_rate) metrics = evaluate_model(model, dataloader) # Log each trial to MLflow with mlflow.start_run(run_name=f\"lookback_{lookback}_trial_{trial.number}\", nested=True): mlflow.log_params({", "source": "run_cnn_pipeline.py"}, {"content": "'learning_rate': learning_rate, 'num_channels': num_channels, 'lookback': lookback }) mlflow.log_metrics({ 'mse': metrics['mse'], 'mae': metrics['mae'], 'rmse': metrics['rmse'], 'mape': metrics['mape'], 'smape': metrics['smape'] }) return [metrics['mse'], metrics['mae'], metrics['rmse'], metrics['mape'], metrics['smape']] @hydra.main(config_path=\"../conf\", config_name=\"config\", version_base=\"1.1\") def main(cfg: DictConfig): mlflow.set_tracking_uri(cfg.mlflow_tracking_uri) mlflow.set_experiment(cfg.mlflow_exp_name) # Run the data pipeline pipeline = DataPipeline() file_path = to_absolute_path(cfg.data_dir_path) cleaned_data = pipeline.run_data_pipeline(file_path, missing_data_method='moving_average', feature_engineering=False, apply_differencing=False) lookback_values = cfg.lookbacks # Read lookback values from config for lookback in lookback_values: print(f\"Running for lookback: {lookback}\") # Define parameters lookahead = cfg.lookahead label_col = 'pm2.5' batch_size = cfg.train_bs num_epochs = cfg.epochs learning_rate = cfg.learning_rate # Prepare data train_dataloader, val_dataloader, test_dataloader, test_data = prepare_data(cleaned_data, lookback, lookahead, label_col, batch_size) # Define model parameters input_size = cleaned_data.shape[1] - 1 # Number of features excluding the label column # Set a fixed kernel size kernel_size = 3 # Hyperparameter optimization with Optuna study = optuna.create_study(directions=cfg.optuna.directions) study.optimize(lambda trial: objective(trial, cfg, cleaned_data, lookback), n_trials=cfg.optuna.n_trials) print(f\"Best trials: {study.best_trials}\") for i, trial in enumerate(study.best_trials): print(f\"Trial {i}:\") for metric_name, value in zip(['mse', 'mae', 'rmse', 'mape', 'smape'], trial.values): print(f\" {metric_name.upper()}: {value:.4f}\") print(f\" Params: {trial.params}\") # Train the model with the best parameters from the Optuna trials best_trial = study.best_trials[0] # Assuming you want to use the first best trial best_params = best_trial.params cnn_model = CNNModel(input_size=input_size, num_channels=best_params['num_channels'], kernel_size=kernel_size, output_size=1, lookback=lookback, cnn_type=cfg.cnn_type) cnn_model.to(cnn_model.device) train_metrics, test_metrics, predictions, best_val_loss = train_and_evaluate_model(cnn_model, train_dataloader, val_dataloader, test_dataloader, num_epochs, best_params['learning_rate']) # Log best trial to MLflow with mlflow.start_run(run_name=f\"lookback_{lookback}_best_trial\"): mlflow.log_params({**best_params, 'lookback': lookback}) mlflow.log_metrics({f\"best_{key}\": value for key, value in zip(['mse', 'mae', 'rmse', 'mape', 'smape'], best_trial.values)}) # Infer model signature input_example = next(iter(train_dataloader))[0].to(cnn_model.device) input_example = input_example[:1] # Take a single example input_example_np = input_example.cpu().numpy() # Convert to numpy array signature = infer_signature(input_example_np, cnn_model(input_example).cpu().detach().numpy()) mlflow.pytorch.log_model(cnn_model, \"model\", signature=signature, input_example=input_example_np) # Log the model architecture mlflow.log_text(str(cnn_model), \"model_architecture.txt\") # Log training and validation losses mlflow.log_metrics({'train_loss': train_metrics['mse'], 'val_loss': best_val_loss}) # # Log SHAP values # log_shap_values(cnn_model, test_dataloader) # Plot and log predictions plot_predictions(test_data, predictions, lookback, lookahead, label_col, f\"Predictions vs Ground Truth (Lookback: {lookback})\") # End the current run mlflow.end_run() if __name__ == \"__main__\": main()", "source": "run_cnn_pipeline.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead, label_col): if isinstance(data, pd.DataFrame): self.data = data else: raise ValueError(\"Data must be a pandas DataFrame\") self.lookback = lookback self.lookahead = lookahead self.label_col = label_col self.length = len(data) - lookback - lookahead + 1 def __len__(self): return self.length def __getitem__(self, idx): start = idx end = start + self.lookback lookahead_idx = end + self.lookahead - 1 features = self.data.iloc[start:end].drop(columns=[self.label_col]).values # Shape: (lookback, features) label = self.data.iloc[lookahead_idx][self.label_col] # Shape: () features = torch.tensor(features, dtype=torch.float32) label = torch.tensor(label, dtype=torch.float32) # Adjust label dimensions to (1, 1) label = label.unsqueeze(0).unsqueeze(0) return features, label", "source": "windowing.py"}, {"content": "# import numpy as np # # Define the encoder hidden states # S0 = np.array([0.3, 0.11, 0.9, 0.5]) # S1 = np.array([0.8, 0.3, 0.7, 0.1]) # S2 = np.array([0.5, 0.3, 0.4, 0.8]) # # Define the decoder hidden state at time step 1 # T1 = np.array([0.2, 0.7, 0.9, 0.3]) # # Stack the encoder hidden states # S = np.stack([S0, S1, S2]) # def softmax(self, x): # e_x = np.exp(x - np.max(x)) # stability improvement # return e_x / e_x.sum() # def compute_context_vector(self): # # Step 1: Compute alignment scores (dot product) # alignment_scores = np.dot(self.S, self.T1) # # Step 2: Apply softmax to the alignment scores to get the attention weights # attention_weights = self.softmax(alignment_scores) # # Step 3: Compute the context vector as a weighted sum of the encoder hidden states # context_vector = np.sum(attention_weights[:, np.newaxis] * self.S, axis=0) # # Round the context vector to 2 decimal places and convert to list # return [np.round(context_vector, 2).tolist()] # if __name__ == \"__main__\": # cv = ContextVector() # print(\"Context Vector:\", cv.compute_context_vector()) class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import os import pandas as pd import argparse import logging class DataLoader: def __init__(self, data_dir='data'): self.data_dir = data_dir os.makedirs(self.data_dir, exist_ok=True) logging.basicConfig(level=logging.INFO) self.logger = logging.getLogger(__name__) def download_data(self, splits): for split_name, split_path in splits.items(): self.logger.info(f\"Downloading {split_name} data...\") df = pd.read_parquet(f\"hf://datasets/dair-ai/emotion/{split_path}\") df.to_parquet(os.path.join(self.data_dir, f\"{split_name}.parquet\")) self.logger.info(f\"Downloaded and saved {split_name} data to {self.data_dir}/{split_name}.parquet\") if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Download and save data splits.\") parser.add_argument('--data_dir', type=str, default='data', help=\"Directory to save the downloaded data.\") args = parser.parse_args() splits = { 'train': 'split/train-00000-of-00001.parquet', 'validation': 'split/validation-00000-of-00001.parquet', 'test': 'split/test-00000-of-00001.parquet' } data_loader = DataLoader(data_dir=args.data_dir) data_loader.download_data(splits)", "source": "data_loader.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification, Trainer, TrainingArguments from torch.utils.data import DataLoader, Dataset import logging # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class EmotionDataset(Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: val[idx] for key, val in self.encodings.items()} item['labels'] = self.labels[idx] return item def __len__(self): return len(self.labels) class EmotionClassifier(nn.Module): def __init__(self, hidden_size, num_classes): super(EmotionClassifier, self).__init__() self.fc = nn.Linear(hidden_size, num_classes) def forward(self, x): return self.fc(x) class EncoderOnlyModel: def __init__(self, model_name='distilbert-base-uncased', num_labels=None, use_label_mapping=True, labels=None): self.tokenizer = DistilBertTokenizer.from_pretrained(model_name) self.encoder = DistilBertModel.from_pretrained(model_name) self.use_label_mapping = use_label_mapping self.label_mapping = { 0: 0, # sadness -> Negative 1: 1, # joy -> Positive 2: 0, # love -> Negative 3: 0, # anger -> Negative 4: 0, # fear -> Negative 5: 1 # surprise -> Positive } if self.use_label_mapping: num_labels = len(set(self.label_mapping.values())) elif labels is not None: num_labels = len(labels.unique()) else: raise ValueError(\"Number of labels must be provided if not using label mapping and labels are not provided.\") self.classifier = EmotionClassifier(hidden_size=self.encoder.config.hidden_size, num_classes=num_labels) self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') self.encoder.to(self.device) self.classifier.to(self.device) self.criterion = nn.CrossEntropyLoss() self.optimizer = optim.Adam(self.classifier.parameters(), lr=1e-4) def tokenize_text(self, df): logger.info(\"Tokenizing text...\") return self.tokenizer(df['text'].tolist(), padding=True, truncation=True, max_length=128, return_tensors='pt') def map_labels(self, labels): if self.use_label_mapping: logger.info(\"Mapping labels...\") return labels.map(self.label_mapping) return labels def create_dataloader(self, df, labels, batch_size=8): encodings = self.tokenize_text(df) mapped_labels = self.map_labels(labels) labels_tensor = torch.tensor(mapped_labels.values) dataset = EmotionDataset(encodings, labels_tensor) return DataLoader(dataset, batch_size=batch_size, shuffle=True) def train(self, train_dataloader, epochs=3): logger.info(\"Starting training...\") self.classifier.train() for epoch in range(epochs): total_loss = 0 for batch in train_dataloader: inputs = batch['input_ids'].to(self.device) attention_mask = batch['attention_mask'].to(self.device) labels = batch['labels'].to(self.device) with torch.no_grad(): outputs = self.encoder(input_ids=inputs, attention_mask=attention_mask) hidden_states = outputs.last_hidden_state[:, 0, :].to(self.device) # Use the [CLS] token's hidden state logits = self.classifier(hidden_states) loss = self.criterion(logits, labels) self.optimizer.zero_grad() loss.backward() self.optimizer.step() total_loss += loss.item() logger.info(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_dataloader)}\") def evaluate(self, eval_dataloader): logger.info(\"Starting evaluation...\") self.classifier.eval() correct = 0 total = 0 with torch.no_grad(): for batch in eval_dataloader: inputs = batch['input_ids'].to(self.device) attention_mask = batch['attention_mask'].to(self.device) labels = batch['labels'].to(self.device) outputs = self.encoder(input_ids=inputs, attention_mask=attention_mask) hidden_states = outputs.last_hidden_state[:, 0, :].to(self.device) logits = self.classifier(hidden_states) _, predicted = torch.max(logits, 1) total += labels.size(0) correct += (predicted == labels).sum().item() accuracy = 100 * correct / total logger.info(f\"Validation Accuracy: {accuracy}%\") return accuracy def fine_tune(self, train_dataset, val_dataset, output_dir='./results', epochs=3, batch_size=8, learning_rate=2e-5): logger.info(\"Starting fine-tuning...\") # Determine the number of labels if self.use_label_mapping: num_labels = len(set(self.label_mapping.values())) else: # Infer the number of labels from the training dataset num_labels = len(set(train_dataset.labels.numpy())) model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels).to(self.device) training_args = TrainingArguments( output_dir=output_dir, evaluation_strategy=\"epoch\", learning_rate=learning_rate, per_device_train_batch_size=batch_size, per_device_eval_batch_size=batch_size, num_train_epochs=epochs, weight_decay=0.01, ) trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, ) trainer.train() trainer.evaluate()", "source": "encoder_only_model.py"}, {"content": "import pandas as pd import torch from transformers import BertTokenizer, BertModel from tqdm import tqdm import logging def generate_bert_embeddings(df): \"\"\" Generate BERT embeddings for the given DataFrame. Parameters ---------- df : pd.DataFrame The input DataFrame containing cleaned reviews and sentiment. Returns ------- pd.DataFrame The DataFrame containing BERT embeddings and sentiment. \"\"\" # Configure logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') logging.info(\"Starting the BERT embeddings generation process.\") # Initialize BERT model and tokenizer logging.info(\"Initializing BERT model and tokenizer.\") tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') model = BertModel.from_pretrained('bert-base-uncased') # Function to generate BERT embeddings for a given text def get_bert_embeddings(text): inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512) with torch.no_grad(): outputs = model(**inputs) cls_embeddings = outputs.last_hidden_state[:, 0, :] return cls_embeddings.squeeze().numpy() # Generate BERT Embeddings for the DataFrame embeddings = [] logging.info(\"Generating BERT embeddings.\") for text in tqdm(df['cleaned_review'], desc=\"Generating BERT embeddings\"): embedding = get_bert_embeddings(text) embeddings.append(embedding) logging.info(\"BERT embeddings generation completed.\") # Convert embeddings to DataFrame embeddings_df = pd.DataFrame(embeddings, columns=[f'embedding_{i}' for i in range(embeddings[0].shape[0])]) embeddings_df['sentiment'] = df['sentiment'].values # Log the shape and first few rows of the embeddings DataFrame logging.info(f\"Embeddings DataFrame shape: {embeddings_df.shape}\") logging.info(f\"Embeddings DataFrame (first 5 rows):\\n{embeddings_df.head()}\") return embeddings_df", "source": "generate_embeddings.py"}, {"content": "import spacy from spacy.tokens import Doc # Register the 'language' extension attribute if not Doc.has_extension(\"language\"): Doc.set_extension(\"language\", default=None) @spacy.Language.component(\"language_detector\") def language_detector(doc): # Dummy implementation of language detection doc._.language = \"en\" return doc", "source": "language_detector.py"}, {"content": "import os import pandas as pd from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.feature_extraction.text import CountVectorizer from sklearn.ensemble import RandomForestClassifier from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss from xgboost import XGBClassifier import mlflow import hydra from hydra.utils import to_absolute_path from omegaconf import DictConfig import optuna import logging from dotenv import load_dotenv import joblib from sklearn.impute import SimpleImputer import shap import matplotlib.pyplot as plt import scipy.sparse # Load environment variables from .env file load_dotenv() # Custom tokenizer function def custom_tokenizer(text): return text.split() # Custom preprocessor function def custom_preprocessor(text): return text class ModelTrainer: def __init__(self, cfg: DictConfig): self.cfg = cfg self.logger = logging.getLogger(__name__) self.logger.info(\"Loading data for modeling...\") # Verify the file path and encoding file_path = to_absolute_path(cfg.data.data_path) self.logger.info(f\"Reading CSV file from path: {file_path}\") # Read the CSV file and print the first few rows for debugging self.data_for_modeling = pd.read_csv(file_path, encoding='utf-8') self.logger.info(f\"Data read from CSV (first 5 rows):\\n{self.data_for_modeling.head()}\") # Check for NaN values in the DataFrame if self.data_for_modeling.isnull().values.any(): self.logger.warning(\"NaN values detected in the DataFrame after reading the CSV file.\") self.logger.info(f\"DataFrame info:\\n{self.data_for_modeling.info()}\") # Determine if the data contains BERT embeddings or tokens if 'embedding_0' in self.data_for_modeling.columns: self.is_bert_embeddings = True self.X = self.data_for_modeling.filter(regex='^embedding_') self.logger.info(\"Detected BERT embeddings in the data.\") self.logger.info(f\"Filtered embeddings data (first 5 rows):\\n{self.X.head()}\") else: self.is_bert_embeddings = False self.X = self.data_for_modeling['tokens'] self.logger.info(\"Detected tokenized text in the data.\") self.y = self.data_for_modeling['sentiment'] self.logger.info(f\"Sentiment labels (first 5 rows):\\n{self.y.head()}\") # Handle NaN values self.logger.info(\"Checking for NaN values in the data...\") if self.X.isnull().values.any(): self.logger.info(\"NaN values detected. Imputing missing values...\") imputer = SimpleImputer(strategy='mean') self.X = imputer.fit_transform(self.X) else: self.logger.info(\"No NaN values detected.\") self.logger.info(\"Splitting data into train, validation, and test sets...\") self.X_train, self.X_test, self.y_train, self.y_test = train_test_split( self.X, self.y, test_size=cfg.training.test_size, random_state=cfg.training.random_seed ) self.X_train, self.X_val, self.y_train, self.y_val = train_test_split( self.X_train, self.y_train, test_size=cfg.training.val_size, random_state=cfg.training.random_seed ) self.model = None def train_model(self, model_type='logistic_regression', **params): self.logger.info(f\"Training model of type: {model_type} with parameters: {params}\") if self.is_bert_embeddings: # No need for vectorizer if using BERT embeddings self.logger.info(\"Using BERT embeddings for training.\") if model_type == 'logistic_regression': self.model = LogisticRegression(max_iter=params.get('max_iter', 1000), solver=params.get('solver', 'lbfgs'), C=params.get('C', 1.0)) elif model_type == 'xgboost': self.model = XGBClassifier(n_estimators=params.get('n_estimators', 100), learning_rate=params.get('learning_rate', 0.1), max_depth=params.get('max_depth', 6), use_label_encoder=params.get('use_label_encoder', False), eval_metric=params.get('eval_metric', 'logloss')) elif model_type == 'random_forest': self.model = RandomForestClassifier( n_estimators=params.get('n_estimators', 100), max_depth=params.get('max_depth', None), min_samples_split=params.get('min_samples_split', 2), min_samples_leaf=params.get('min_samples_leaf', 1), bootstrap=params.get('bootstrap', True), max_features=params.get('max_features', 'sqrt'), criterion=params.get('criterion', 'gini'), max_leaf_nodes=params.get('max_leaf_nodes', None), min_weight_fraction_leaf=params.get('min_weight_fraction_leaf', 0.0) ) else: raise ValueError(\"Unsupported model type. Choose 'logistic_regression', 'xgboost', or 'random_forest'.\") else: # Use vectorizer if using tokens self.logger.info(\"Using tokenized text for training.\") if model_type == 'logistic_regression': self.model = Pipeline([ ('vectorizer', CountVectorizer(tokenizer=custom_tokenizer, preprocessor=custom_preprocessor, token_pattern=None)), ('classifier', LogisticRegression(max_iter=params.get('max_iter', 1000), solver=params.get('solver', 'lbfgs'), C=params.get('C', 1.0))) ]) elif model_type == 'xgboost': self.model = Pipeline([ ('vectorizer', CountVectorizer(tokenizer=custom_tokenizer, preprocessor=custom_preprocessor, token_pattern=None)), ('classifier', XGBClassifier(n_estimators=params.get('n_estimators', 100), learning_rate=params.get('learning_rate', 0.1), max_depth=params.get('max_depth', 6), use_label_encoder=params.get('use_label_encoder', False), eval_metric=params.get('eval_metric', 'logloss'))) ]) elif model_type == 'random_forest': self.model = Pipeline([ ('vectorizer', CountVectorizer(tokenizer=custom_tokenizer, preprocessor=custom_preprocessor, token_pattern=None)), ('classifier', RandomForestClassifier( n_estimators=params.get('n_estimators', 100), max_depth=params.get('max_depth', None), min_samples_split=params.get('min_samples_split', 2), min_samples_leaf=params.get('min_samples_leaf', 1), bootstrap=params.get('bootstrap', True), max_features=params.get('max_features', 'sqrt'), criterion=params.get('criterion', 'gini'), max_leaf_nodes=params.get('max_leaf_nodes', None), min_weight_fraction_leaf=params.get('min_weight_fraction_leaf', 0.0) )) ]) else: raise ValueError(\"Unsupported model type. Choose 'logistic_regression', 'xgboost', or 'random_forest'.\") self.model.fit(self.X_train, self.y_train) self.logger.info(\"Model training completed.\") def evaluate_model(self, X, y): y_pred = self.model.predict(X) y_prob = self.model.predict_proba(X) metrics = { 'accuracy': accuracy_score(y, y_pred), 'precision': precision_score(y, y_pred, average='binary', pos_label=1), 'recall': recall_score(y, y_pred, average='binary', pos_label=1), 'f1':", "source": "model_training.py"}, {"content": "f1_score(y, y_pred, average='binary', pos_label=1), 'log_loss': log_loss(y, y_prob) } return metrics def log_metrics(self, metrics, stage): mlflow.log_metrics({f\"{stage}_accuracy\": metrics['accuracy'], f\"{stage}_precision\": metrics['precision'], f\"{stage}_recall\": metrics['recall'], f\"{stage}_f1\": metrics['f1'], f\"{stage}_log_loss\": metrics['log_loss']}) self.logger.info(f\"{stage.capitalize()} metrics: {metrics}\") def objective(self, trial): model_type = trial.suggest_categorical('model_type', ['logistic_regression', 'xgboost', 'random_forest']) params = {} if model_type == 'logistic_regression': params = { 'max_iter': trial.suggest_int('max_iter', 100, 1000), 'solver': trial.suggest_categorical('solver', ['lbfgs', 'liblinear']), 'C': trial.suggest_loguniform('C', 1e-4, 1e2), 'penalty': trial.suggest_categorical('penalty', ['l1', 'l2', 'elasticnet', 'none']), 'fit_intercept': trial.suggest_categorical('fit_intercept', [True, False]), 'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]) } elif model_type == 'xgboost': params = { 'n_estimators': trial.suggest_int('n_estimators', 50, 500), 'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1), 'max_depth': trial.suggest_int('max_depth', 3, 10), 'subsample': trial.suggest_uniform('subsample', 0.5, 1.0), 'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0), 'gamma': trial.suggest_loguniform('gamma', 1e-8, 1.0), 'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 1.0), 'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 1.0), '_label_encoder': False, 'eval_metric': 'logloss' } elif model_type == 'random_forest': params = { 'n_estimators': trial.suggest_int('n_estimators', 50, 500), 'max_depth': trial.suggest_int('max_depth', 3, 20), 'min_samples_split': trial.suggest_int('min_samples_split', 2, 10), 'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10), 'bootstrap': trial.suggest_categorical('bootstrap', [True, False]), 'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]), 'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']), 'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 10, 100), 'min_weight_fraction_leaf': trial.suggest_uniform('min_weight_fraction_leaf', 0.0, 0.5) } with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True): self.train_model(model_type=model_type, **params) train_metrics = self.evaluate_model(self.X_train, self.y_train) val_metrics = self.evaluate_model(self.X_val, self.y_val) self.log_metrics(train_metrics, 'train') self.log_metrics(val_metrics, 'val') mlflow.log_params(params) mlflow.log_param('model_type', model_type) mlflow.log_param('trial_number', trial.number) return val_metrics['f1'] def generate_shap_plots(self, model, X_train, X_test): # Ensure the img directory exists os.makedirs('img', exist_ok=True) self.logger.info(\"Generating SHAP plots...\") if isinstance(model, Pipeline): vectorizer = model.named_steps['vectorizer'] classifier = model.named_steps['classifier'] X_train_transformed = vectorizer.transform(X_train) X_test_transformed = vectorizer.transform(X_test) else: classifier = model X_train_transformed = X_train X_test_transformed = X_test # Choose the appropriate SHAP explainer based on the model type if isinstance(classifier, (RandomForestClassifier, XGBClassifier)): explainer = shap.TreeExplainer(classifier) else: explainer = shap.LinearExplainer(classifier, X_train_transformed, feature_perturbation=\"interventional\") # Convert sparse matrix to dense format for SHAP explanations if isinstance(X_train_transformed, (scipy.sparse.csr_matrix, scipy.sparse.csc_matrix)): X_train_transformed = X_train_transformed.toarray() X_test_transformed = X_test_transformed.toarray() # Log SHAP explanations directly to MLflow with mlflow.start_run(nested=True): mlflow.shap.log_explanation(classifier.predict, X_test_transformed) # Save the SHAP explainer to MLflow (only for LinearExplainer) if not isinstance(explainer, shap.TreeExplainer): try: mlflow.shap.save_explainer(explainer, path=\"shap_explainer\") except AttributeError as e: self.logger.warning(f\"Could not save explainer: {e}\") # Generate summary plot (beeswarm plot) shap_values = explainer.shap_values(X_test_transformed) shap.summary_plot(shap_values, X_test_transformed, feature_names=vectorizer.get_feature_names_out() if isinstance(model, Pipeline) else None) mlflow.log_figure(plt.gcf(), \"shap_summary_plot.png\") plt.close() # Generate other SHAP visualizations (force plot, dependence plot, etc.) shap.force_plot(explainer.expected_value, shap_values[1], feature_names=vectorizer.get_feature_names_out() if isinstance(model, Pipeline) else None, matplotlib=True) mlflow.log_figure(plt.gcf(), \"shap_force_plot.png\") plt.close() # Example: dependence plot for a specific feature if isinstance(model, Pipeline): feature_names = vectorizer.get_feature_names_out() shap.dependence_plot(0, shap_values, X_test_transformed, feature_names=feature_names) mlflow.log_figure(plt.gcf(), \"shap_dependence_plot.png\") plt.close() # Example: waterfall plot for a specific prediction shap.waterfall_plot(shap.Explanation(values=shap_values[0], base_values=explainer.expected_value, data=X_test_transformed[0], feature_names=feature_names)) mlflow.log_figure(plt.gcf(), \"shap_waterfall_plot.png\") plt.close() # Generate decision plot for a specific prediction shap.decision_plot(explainer.expected_value, shap_values[0], feature_names=feature_names) mlflow.log_figure(plt.gcf(), \"shap_decision_plot.png\") plt.close() self.logger.info(\"SHAP plots generation completed.\") @hydra.main(config_path=\"../conf\", config_name=\"config\", version_base=\"1.1\") def main(cfg: DictConfig): logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) mlflow.set_tracking_uri(cfg.mlflow_tracking_uri) mlflow.set_experiment(cfg.mlflow_exp_name) logger.info(\"Initializing ModelTrainer...\") trainer = ModelTrainer(cfg) logger.info(\"Starting Optuna study...\") study = optuna.create_study(direction='maximize') study.optimize(trainer.objective, n_trials=cfg.optuna.n_trials) best_trial = study.best_trial logger.info(f\"Best trial: {best_trial}\") with mlflow.start_run(run_name=\"best_trial\"): mlflow.log_params(best_trial.params) model_type = best_trial.params.pop('model_type') trainer.train_model(model_type=model_type, **best_trial.params) train_metrics = trainer.evaluate_model(trainer.X_train, trainer.y_train) val_metrics = trainer.evaluate_model(trainer.X_val, trainer.y_val) test_metrics = trainer.evaluate_model(trainer.X_test, trainer.y_test) trainer.log_metrics(train_metrics, 'train') trainer.log_metrics(val_metrics, 'val') trainer.log_metrics(test_metrics, 'test') # Ensure the models directory exists models_dir = os.path.join(\"..\", \"models\") os.makedirs(models_dir, exist_ok=True) # Save the best model model_name = f\"best_model_{model_type}_trial_{best_trial.number}.joblib\" model_path = os.path.join(models_dir, model_name) joblib.dump(trainer.model, model_path) logger.info(f\"Best model saved at {model_path}\") # Generate SHAP plots for the best model if isinstance(trainer.model, Pipeline): trainer.generate_shap_plots(trainer.model, trainer.X_train, trainer.X_test) else: trainer.generate_shap_plots(trainer.model, trainer.X_train, trainer.X_test) # #", "source": "model_training.py"}, {"content": "Generate SHAP plots for the best model # trainer.generate_shap_plots(trainer.model, trainer.X_train, trainer.X_test) if __name__ == \"__main__\": main()", "source": "model_training.py"}, {"content": "import argparse import pandas as pd import logging from encoder_only_model import EncoderOnlyModel # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def main(args): # Load the datasets logger.info(\"Loading datasets...\") train_df = pd.read_parquet(args.train_file) val_df = pd.read_parquet(args.val_file) test_df = pd.read_parquet(args.test_file) # Print unique labels in the training dataset logger.info(f\"Unique labels in the training dataset: {train_df['label'].unique()}\") logger.info(f\"Unique labels in the validation dataset: {val_df['label'].unique()}\") logger.info(f\"Unique labels in the test dataset: {test_df['label'].unique()}\") # Infer number of labels if args.use_label_mapping: num_labels = 2 # Based on the label mapping provided else: num_labels = len(train_df['label'].unique()) # Initialize the model logger.info(\"Initializing the model...\") model = EncoderOnlyModel(model_name='distilbert-base-uncased', num_labels=num_labels, use_label_mapping=args.use_label_mapping, labels=train_df['label']) # Create DataLoaders logger.info(\"Creating DataLoaders...\") train_dataloader = model.create_dataloader(train_df, train_df['label'], batch_size=args.batch_size) val_dataloader = model.create_dataloader(val_df, val_df['label'], batch_size=args.batch_size) test_dataloader = model.create_dataloader(test_df, test_df['label'], batch_size=args.batch_size) if args.fine_tune: # Fine-tune the model logger.info(\"Fine-tuning the model...\") model.fine_tune(train_dataloader.dataset, val_dataloader.dataset, output_dir=args.output_dir, epochs=args.epochs, batch_size=args.batch_size, learning_rate=args.learning_rate) else: # Train the model logger.info(\"Training the model...\") model.train(train_dataloader, epochs=args.epochs) # Evaluate the model logger.info(\"Evaluating the model...\") model.evaluate(val_dataloader) if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Train and evaluate an encoder-only model.\") parser.add_argument('--train_file', type=str, required=True, help=\"Path to the training data file.\") parser.add_argument('--val_file', type=str, required=True, help=\"Path to the validation data file.\") parser.add_argument('--test_file', type=str, required=True, help=\"Path to the test data file.\") parser.add_argument('--fine_tune', action='store_true', help=\"Flag to perform fine-tuning.\") parser.add_argument('--output_dir', type=str, default='./results', help=\"Directory to save the fine-tuning results.\") parser.add_argument('--epochs', type=int, default=3, help=\"Number of epochs for training.\") parser.add_argument('--batch_size', type=int, default=8, help=\"Batch size for training.\") parser.add_argument('--learning_rate', type=float, default=2e-5, help=\"Learning rate for training.\") parser.add_argument('--use_label_mapping', action='store_true', help=\"Flag to use label mapping.\") args = parser.parse_args() main(args)", "source": "run_model.py"}, {"content": "import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, RobertaModel import torch import logging import numpy as np from tqdm import tqdm import os import argparse import mlflow import mlflow.pytorch from omegaconf import OmegaConf from dotenv import load_dotenv import torch.nn as nn from sklearn.linear_model import LogisticRegression from torch.utils.data import DataLoader # Load environment variables from .env file load_dotenv() # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) # Load configuration config_path = os.path.join(os.path.dirname(__file__), '../conf/config.yaml') config = OmegaConf.load(config_path) # Check if CUDA is available and set the device device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') logger.info(f\"Using device: {device}\") def load_data(): # Load the dataset logger.info(\"Loading dataset...\") df = pd.read_csv(config.data.data_path) # Split the data into features and labels X = df['cleaned_review'] y = df['sentiment'] # Split the data into train, validation, and test sets logger.info(\"Splitting data into train, validation, and test sets...\") X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=config.training.test_size, random_state=config.training.random_seed) X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=config.training.val_size, random_state=config.training.random_seed) return X_train, X_val, X_test, y_train, y_val, y_test def tokenize_data(X_train, X_val, X_test): # Initialize the tokenizer logger.info(\"Initializing RoBERTa tokenizer...\") tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # Tokenize the text data logger.info(\"Tokenizing text data...\") train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=512) val_encodings = tokenizer(list(X_val), truncation=True, padding=True, max_length=512) test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512) return train_encodings, val_encodings, test_encodings, tokenizer class MovieDataset(torch.utils.data.Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item['labels'] = self.labels[idx] return item def __len__(self): return len(self.labels) def calculate_metrics(y_true, y_pred, y_prob): metrics = { 'accuracy': accuracy_score(y_true, y_pred), 'precision': precision_score(y_true, y_pred, average='binary', pos_label=1), 'recall': recall_score(y_true, y_pred, average='binary', pos_label=1), 'f1': f1_score(y_true, y_pred, average='binary', pos_label=1), 'log_loss': log_loss(y_true, y_prob) } return metrics def log_metrics(metrics, stage): mlflow.log_metrics({ f\"{stage}_accuracy\": metrics['accuracy'], f\"{stage}_precision\": metrics['precision'], f\"{stage}_recall\": metrics['recall'], f\"{stage}_f1\": metrics['f1'], f\"{stage}_log_loss\": metrics['log_loss'] }) logger.info(f\"{stage.capitalize()} metrics: {metrics}\") def train_from_scratch(): X_train, X_val, X_test, y_train, y_val, y_test = load_data() train_encodings, val_encodings, test_encodings, tokenizer = tokenize_data(X_train, X_val, X_test) train_labels = torch.tensor(y_train.values) val_labels = torch.tensor(y_val.values) test_labels = torch.tensor(y_test.values) train_dataset = MovieDataset(train_encodings, train_labels) val_dataset = MovieDataset(val_encodings, val_labels) test_dataset = MovieDataset(test_encodings, test_labels) # Define training arguments logger.info(\"Defining training arguments...\") training_args = TrainingArguments( output_dir=config.training.output_dir, num_train_epochs=config.training.num_train_epochs, per_device_train_batch_size=config.training.per_device_train_batch_size, per_device_eval_batch_size=config.training.per_device_eval_batch_size, warmup_steps=config.training.warmup_steps, weight_decay=config.training.weight_decay, logging_dir=config.training.logging_dir, logging_steps=config.training.logging_steps, save_steps=config.training.save_steps, save_total_limit=config.training.save_total_limit, ) # Check if the model has already been trained and saved model_dir = config.model_checkpoint_path tokenizer_dir = config.model_checkpoint_path if os.path.exists(model_dir) and os.path.exists(tokenizer_dir): logger.info(\"Loading the saved model and tokenizer...\") model = RobertaForSequenceClassification.from_pretrained(model_dir) tokenizer = RobertaTokenizer.from_pretrained(tokenizer_dir) model.to(device) else: # Initialize the RoBERTa model and tokenizer logger.info(\"Initializing RoBERTa model for sequence classification...\") model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2) model.to(device) # Initialize the Trainer logger.info(\"Initializing Trainer...\") trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, ) # Train the model with tqdm progress bar logger.info(\"Training the model...\") for epoch in range(training_args.num_train_epochs): logger.info(f\"Epoch {epoch + 1}/{training_args.num_train_epochs}\") trainer.train() logger.info(\"Evaluating the model...\") results = trainer.evaluate() logger.info(f\"Evaluation results after epoch {epoch + 1}: {results}\") # Ensure the directories exist os.makedirs(model_dir, exist_ok=True) os.makedirs(tokenizer_dir, exist_ok=True) # Save the model logger.info(\"Saving the model...\") model.save_pretrained(model_dir) tokenizer.save_pretrained(tokenizer_dir) # Initialize the Trainer with the loaded model trainer = Trainer( model=model, args=training_args, eval_dataset=test_dataset, ) # Get predictions and probabilities logger.info(\"Making predictions on the test set...\") predictions =", "source": "sentiment_analysis_transformer.py"}, {"content": "trainer.predict(test_dataset) y_pred = predictions.predictions.argmax(axis=1) y_prob = predictions.predictions # Calculate metrics metrics = calculate_metrics(test_labels, y_pred, y_prob) logger.info(f\"Test set metrics: {metrics}\") # Log metrics to MLflow log_metrics(metrics, 'test') # Log the model to MLflow mlflow.pytorch.log_model(model, \"model\") print(metrics) def use_pretrained_model(): # Load the pretrained model and tokenizer logger.info(\"Loading pretrained model and tokenizer...\") model = RobertaForSequenceClassification.from_pretrained('textattack/roberta-base-imdb') tokenizer = RobertaTokenizer.from_pretrained('textattack/roberta-base-imdb') model.to(device) # Load data _, _, X_test, _, _, y_test = load_data() # Tokenize the test data logger.info(\"Tokenizing test data...\") test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=512) test_labels = torch.tensor(y_test.values) test_dataset = MovieDataset(test_encodings, test_labels) # Update the TrainingArguments initialization in use_pretrained_model function training_args = TrainingArguments( output_dir=config.training.output_dir, # Add this line per_device_eval_batch_size=config.training.per_device_eval_batch_size, ) # Initialize the Trainer trainer = Trainer( model=model, args=training_args, eval_dataset=test_dataset, ) # Get predictions and probabilities logger.info(\"Making predictions on the test set...\") predictions = trainer.predict(test_dataset) y_pred = predictions.predictions.argmax(axis=1) y_prob = predictions.predictions # Calculate metrics metrics = calculate_metrics(test_labels, y_pred, y_prob) logger.info(f\"Test set metrics: {metrics}\") # Log metrics to MLflow log_metrics(metrics, 'test') # Log the model to MLflow mlflow.pytorch.log_model(model, \"model\") print(metrics) def retrain_head(): X_train, X_val, X_test, y_train, y_val, y_test = load_data() train_encodings, val_encodings, test_encodings, tokenizer = tokenize_data(X_train, X_val, X_test) train_labels = torch.tensor(y_train.values) val_labels = torch.tensor(y_val.values) test_labels = torch.tensor(y_test.values) train_dataset = MovieDataset(train_encodings, train_labels) val_dataset = MovieDataset(val_encodings, val_labels) test_dataset = MovieDataset(test_encodings, test_labels) # Define training arguments logger.info(\"Defining training arguments...\") training_args = TrainingArguments( output_dir=config.training.output_dir, num_train_epochs=config.training.num_train_epochs, per_device_train_batch_size=config.training.per_device_train_batch_size, per_device_eval_batch_size=config.training.per_device_eval_batch_size, warmup_steps=config.training.warmup_steps, weight_decay=config.training.weight_decay, logging_dir=config.training.logging_dir, logging_steps=config.training.logging_steps, save_steps=config.training.save_steps, save_total_limit=config.training.save_total_limit, ) # Initialize the RoBERTa model and tokenizer logger.info(\"Initializing RoBERTa model for sequence classification...\") model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=2) model.to(device) # Freeze the base layers logger.info(\"Freezing base layers...\") for param in model.roberta.parameters(): param.requires_grad = False # Initialize the Trainer logger.info(\"Initializing Trainer...\") trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, ) # Train the model with tqdm progress bar logger.info(\"Training the model...\") for epoch in range(training_args.num_train_epochs): logger.info(f\"Epoch {epoch + 1}/{training_args.num_train_epochs}\") trainer.train() logger.info(\"Evaluating the model...\") results = trainer.evaluate() logger.info(f\"Evaluation results after epoch {epoch + 1}: {results}\") # Ensure the directories exist model_dir = config.model_checkpoint_path tokenizer_dir = config.model_checkpoint_path os.makedirs(model_dir, exist_ok=True) os.makedirs(tokenizer_dir, exist_ok=True) # Save the model logger.info(\"Saving the model...\") model.save_pretrained(model_dir) tokenizer.save_pretrained(tokenizer_dir) # Initialize the Trainer with the loaded model trainer = Trainer( model=model, args=training_args, eval_dataset=test_dataset, ) # Get predictions and probabilities logger.info(\"Making predictions on the test set...\") predictions = trainer.predict(test_dataset) y_pred = predictions.predictions.argmax(axis=1) y_prob = predictions.predictions # Calculate metrics metrics = calculate_metrics(test_labels, y_pred, y_prob) logger.info(f\"Test set metrics: {metrics}\") # Log metrics to MLflow log_metrics(metrics, 'test') # Log the model to MLflow mlflow.pytorch.log_model(model, \"model\") print(metrics) class LogisticRegressionHead(nn.Module): def __init__(self, input_dim, num_labels): super(LogisticRegressionHead, self).__init__() self.linear = nn.Linear(input_dim, num_labels) def forward(self, x): return self.linear(x) class FeedforwardHead(nn.Module): def __init__(self, input_dim, hidden_dim, num_labels): super(FeedforwardHead, self).__init__() self.fc1 = nn.Linear(input_dim, hidden_dim) self.relu = nn.ReLU() self.fc2 = nn.Linear(hidden_dim, num_labels) def forward(self, x): x = self.fc1(x) x = self.relu(x) x = self.fc2(x) return x class CustomModel(nn.Module): def __init__(self, base_model, head): super(CustomModel, self).__init__() self.base_model = base_model self.head = head def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None): logger.info(f\"Input IDs: {input_ids.shape}\") outputs = self.base_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids) pooled_output = outputs[1] # Pooled output from the base model logger.info(f\"Pooled Output: {pooled_output.shape}\") logits = self.head(pooled_output) logger.info(f\"Logits: {logits.shape}\") loss = None if labels is not None: # Ensure the labels are of the correct shape labels = labels.view(-1) # Use", "source": "sentiment_analysis_transformer.py"}, {"content": "CrossEntropyLoss for classification loss_fct = nn.CrossEntropyLoss() # Determine the number of output features based on the head type if isinstance(self.head, LogisticRegressionHead): num_labels = self.head.linear.out_features elif isinstance(self.head, FeedforwardHead): num_labels = self.head.fc2.out_features else: raise ValueError(\"Unknown head type\") loss = loss_fct(logits.view(-1, num_labels), labels) if loss is not None: return {'loss': loss, 'logits': logits} else: return {'logits': logits} def compute_metrics_function(eval_pred): logits, labels = eval_pred predictions = np.argmax(logits, axis=1) metrics = { 'accuracy': accuracy_score(labels, predictions), 'precision': precision_score(labels, predictions, average='binary'), 'recall': recall_score(labels, predictions, average='binary'), 'f1': f1_score(labels, predictions, average='binary') } # Log metrics to MLflow mlflow.log_metrics(metrics) return metrics def use_different_head(head_type): X_train, X_val, X_test, y_train, y_val, y_test = load_data() train_encodings, val_encodings, test_encodings, tokenizer = tokenize_data(X_train, X_val, X_test) train_labels = torch.tensor(y_train.values) val_labels = torch.tensor(y_val.values) test_labels = torch.tensor(y_test.values) train_dataset = MovieDataset(train_encodings, train_labels) val_dataset = MovieDataset(val_encodings, val_labels) test_dataset = MovieDataset(test_encodings, test_labels) # Define training arguments logger.info(\"Defining training arguments...\") training_args = TrainingArguments( output_dir=config.training.output_dir, num_train_epochs=config.training.num_train_epochs, per_device_train_batch_size=config.training.per_device_train_batch_size, per_device_eval_batch_size=config.training.per_device_eval_batch_size, warmup_steps=config.training.warmup_steps, weight_decay=config.training.weight_decay, logging_dir=config.training.logging_dir, logging_steps=config.training.logging_steps, save_steps=config.training.save_steps, save_total_limit=config.training.save_total_limit, ) # Initialize the RoBERTa model and tokenizer logger.info(\"Initializing RoBERTa model with a different head...\") base_model = RobertaModel.from_pretrained('roberta-base') # Body # Choose a head based on the head_type argument input_dim = base_model.config.hidden_size num_labels = 2 if head_type == 'logistic': head = LogisticRegressionHead(input_dim, num_labels) elif head_type == 'feedforward': head = FeedforwardHead(input_dim, hidden_dim=128, num_labels=num_labels) else: raise ValueError(f\"Unknown head type: {head_type}\") # Create the custom model model = CustomModel(base_model, head) model.to(device) # def compute_metrics_function(eval_pred): # logits, labels = eval_pred # predictions = np.argmax(logits, axis=1) # metrics = { # 'accuracy': accuracy_score(labels, predictions), # 'precision': precision_score(labels, predictions, average='binary'), # 'recall': recall_score(labels, predictions, average='binary'), # 'f1': f1_score(labels, predictions, average='binary') # } # # Log metrics to MLflow # mlflow.log_metrics(metrics) # return metrics # Initialize the Trainer logger.info(\"Initializing Trainer...\") trainer = Trainer( model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset, compute_metrics=compute_metrics_function # Ensure metrics are calculated correctly ) # Train the model with tqdm progress bar logger.info(\"Training the model...\") trainer.train() logger.info(\"Evaluating the model...\") results = trainer.evaluate() logger.info(f\"Evaluation results: {results}\") # Ensure the directories exist model_dir = config.model_checkpoint_path tokenizer_dir = config.model_checkpoint_path os.makedirs(model_dir, exist_ok=True) os.makedirs(tokenizer_dir, exist_ok=True) # Save the model logger.info(\"Saving the model...\") torch.save(model.state_dict(), os.path.join(model_dir, 'pytorch_model.bin')) tokenizer.save_pretrained(tokenizer_dir) # Initialize the Trainer with the loaded model trainer = Trainer( model=model, args=training_args, eval_dataset=test_dataset, compute_metrics=compute_metrics_function # Ensure metrics are calculated correctly ) # Get predictions and probabilities logger.info(\"Making predictions on the test set...\") predictions = trainer.predict(test_dataset) y_pred = predictions.predictions.argmax(axis=1) y_prob = predictions.predictions # Calculate metrics metrics = calculate_metrics(test_labels, y_pred, y_prob) logger.info(f\"Test set metrics: {metrics}\") # Log metrics to MLflow log_metrics(metrics, 'test') # Log the model to MLflow mlflow.pytorch.log_model(model, \"model\") print(metrics) if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Sentiment Analysis with Transformers\") parser.add_argument('--task', type=str, required=True, choices=['train_from_scratch', 'use_pretrained_model', 'retrain_head', 'use_different_head'], help=\"Task to perform\") parser.add_argument('--head', type=str, default='logistic', choices=['logistic', 'feedforward'], help=\"Type of head to use for the model\") args = parser.parse_args() mlflow.set_tracking_uri(config.mlflow_tracking_uri) mlflow.set_experiment(config.mlflow_exp_name) with mlflow.start_run(run_name=config.mlflow_run_name): if args.task == 'train_from_scratch': train_from_scratch() elif args.task == 'use_pretrained_model': use_pretrained_model() elif args.task == 'retrain_head': retrain_head() elif args.task == 'use_different_head': use_different_head(args.head)", "source": "sentiment_analysis_transformer.py"}, {"content": "import os import numpy as np import pandas as pd import re import string from bs4 import BeautifulSoup, MarkupResemblesLocatorWarning import spacy from textblob import TextBlob import contractions import emoji from num2words import num2words from typing import Tuple, Dict from tqdm import tqdm import warnings import logging # Initialize logging # logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Ignore the specific BeautifulSoup warning warnings.filterwarnings(\"ignore\", category=MarkupResemblesLocatorWarning) # # Load spaCy model # nlp = spacy.load(\"en_core_web_sm\") # nlp.add_pipe('language_detector', last=True) chat_word = { 'AFAIK': 'As Far As I Know', 'AFK': 'Away From Keyboard', 'ASAP': 'As Soon As Possible', 'ATK': 'At The Keyboard', 'ATM': 'At The Moment', 'A3': 'Anytime, Anywhere, Anyplace', 'BAK': 'Back At Keyboard', 'BBL': 'Be Back Later', 'BBS': 'Be Back Soon', 'BFN': 'Bye For Now', 'B4N': 'Bye For Now', 'BRB': 'Be Right Back', 'BRT': 'Be Right There', 'BTW': 'By The Way', 'B4': 'Before', 'CU': 'See You', 'CUL8R': 'See You Later', 'CYA': 'See You', 'FAQ': 'Frequently Asked Questions', 'FC': 'Fingers Crossed', 'FWIW': \"For What It's Worth\", 'FYI': 'For Your Information', 'GAL': 'Get A Life', 'GG': 'Good Game', 'GN': 'Good Night', 'GMTA': 'Great Minds Think Alike', 'GR8': 'Great!', 'G9': 'Genius', 'IC': 'I See', 'ICQ': 'I Seek you (also a chat program)', 'ILU': 'ILU: I Love You', 'IMHO': 'In My Honest/Humble Opinion', 'IMO': 'In My Opinion', 'IOW': 'In Other Words', 'IRL': 'In Real Life', 'KISS': 'Keep It Simple, Stupid', 'LDR': 'Long Distance Relationship', 'LMAO': 'Laugh My A.. Off', 'LOL': 'Laughing Out Loud', 'LTNS': 'Long Time No See', 'L8R': 'Later', 'MTE': 'My Thoughts Exactly', 'M8': 'Mate', 'NRN': 'No Reply Necessary', 'OIC': 'Oh I See', 'PITA': 'Pain In The A..', 'PRT': 'Party', 'PRW': 'Parents Are Watching', 'QPSA?': 'Que Pasa?', 'ROFL': 'Rolling On The Floor Laughing', 'ROFLOL': 'Rolling On The Floor Laughing Out Loud', 'ROTFLMAO': 'Rolling On The Floor Laughing My A.. Off', 'SK8': 'Skate', 'STATS': 'Your sex and age', 'ASL': 'Age, Sex, Location', 'THX': 'Thank You', 'TTFN': 'Ta-Ta For Now!', 'TTYL': 'Talk To You Later', 'U': 'You', 'U2': 'You Too', 'U4E': 'Yours For Ever', 'WB': 'Welcome Back', 'WTF': 'What The F...', 'WTG': 'Way To Go!', 'WUF': 'Where Are You From?', 'W8': 'Wait...', '7K': 'Sick:-D Laugher', 'TFW': 'That feeling when', 'MFW': 'My face when', 'MRW': 'My reaction when', 'IFYP': 'I feel your pain', 'TNTL': 'Trying not to laugh', 'JK': 'Just kidding', 'IDC': \"I don't care\", 'ILY': 'I love you', 'IMU': 'I miss you', 'ADIH': 'Another day in hell', 'ZZZ': 'Sleeping, bored, tired', 'WYWH': 'Wish you were here', # 'TIME': 'Tears in my eyes', 'BAE': 'Before anyone else', 'FIMH': 'Forever in my heart', 'BSAAW': 'Big smile and a wink', 'BWL': 'Bursting with laughter', 'BFF': 'Best friends forever', 'CSL': \"Can't stop laughing\" } class TextCleaner: def __init__(self): self.changes_log: Dict[str, Dict[str, str]] = {} # Load spaCy model self.nlp = spacy.load(\"en_core_web_sm\") # Register and add the language_detector component if not already added if \"language_detector\" not in self.nlp.pipe_names: self.nlp.add_pipe(\"language_detector\", last=True) logging.info(\"TextCleaner initialized with spaCy model and language detector.\") def log_changes(self, original: str, cleaned: str, step_name: str) -> None: \"\"\" Log changes between original and cleaned text. Parameters ---------- original : str The original text before cleaning. cleaned : str The cleaned text after applying the", "source": "text_cleaner.py"}, {"content": "cleaning step. step_name : str The name of the cleaning step. \"\"\" if original != cleaned: self.changes_log[step_name] = {'before': original, 'after': cleaned} logging.debug(f\"Change in step '{step_name}': {self.changes_log[step_name]}\") def regex_cleaning(self, text: str) -> str: \"\"\" Apply a series of regex-based cleaning steps to the input text. This function performs the following steps: 1. Remove URLs 2. Remove content between square brackets 3. Remove punctuation and non-alphanumeric characters 4. Remove extra spaces and tabs 5. Remove single alphanumeric characters Parameters ---------- text : str The input text. Returns ------- str The cleaned text after applying all regex-based cleaning steps. \"\"\" logging.info(\"Applying regex-based cleaning steps.\") # Remove URLs text = re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text) # Remove content between square brackets text = re.sub(r'\\[[^]]*\\]', '', text) # Remove punctuation and non-alphanumeric characters text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text) text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text) text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) # Remove extra spaces and tabs text = re.sub(r'\\s+', ' ', text).strip() # Remove single alphanumeric characters text = re.sub(r'\\b\\w\\b', '', text) return text def remove_non_english(self, text: str) -> str: \"\"\" Remove text that is not in English. \"\"\" logging.info(\"Removing non-English text.\") if not text.strip(): # Check if the text is empty or contains only whitespace return text doc = self.nlp(text) if doc._.language == 'en': return text else: return '' def to_lower_case(self, text: str) -> str: \"\"\" Convert text to lower case. Parameters ---------- text : str The input text. Returns ------- str The text converted to lower case. \"\"\" logging.info(\"Converting text to lower case.\") return text.lower() def remove_html_tags(self, text: str) -> str: \"\"\" Remove HTML tags from the input text. Parameters ---------- text : str The input text. Returns ------- str The text with HTML tags removed. \"\"\" logging.info(\"Removing HTML tags from text.\") # Check if the text looks like a filename or URL if re.match(r'^[a-zA-Z]:\\\\|^\\/|^http', text): return text return BeautifulSoup(text, \"html.parser\").get_text() def convert_digits_to_words(self, text: str) -> str: \"\"\" Convert numeric digits in the input text to their word representations. Parameters ---------- text : str The input text. Returns ------- str The text with digits converted to words. \"\"\" logging.info(\"Converting digits to words.\") return \" \".join([num2words(w) if w.isdigit() else w for w in text.split()]) def expand_contractions(self, text: str) -> str: \"\"\" Expand contractions in the input text to their full forms. Parameters ---------- text : str The input text. Returns ------- str The text with contractions expanded. \"\"\" logging.info(\"Expanding contractions in text.\") return contractions.fix(text) def correct_spelling(self, text: str) -> str: \"\"\" Correct the spelling of words in the input text using TextBlob. Parameters ---------- text : str The input text. Returns ------- str The text with corrected spelling. \"\"\" logging.info(\"Correcting spelling in text.\") text_blob = TextBlob(text) corrected_text = text_blob.correct().string return corrected_text def remove_stop_words(self, text: str) -> str: \"\"\" Remove stopwords from the input text using spaCy. Parameters ---------- text : str The input text. Returns ------- str The text with stopwords removed. \"\"\" logging.info(\"Removing stopwords from text.\") doc = self.nlp(text) return ' '.join([token.text for token in doc if not token.is_stop]) def handle_emojis(self, text: str) -> str: \"\"\" Convert emojis in the input text to their text", "source": "text_cleaner.py"}, {"content": "representations. Parameters ---------- text : str The input text. Returns ------- str The text with emojis converted to text. \"\"\" logging.info(\"Handling emojis in text.\") return emoji.demojize(text) def short_conv(self, text: str) -> str: \"\"\" Convert chat abbreviations in the input text to their full forms. Parameters ---------- text : str The input text. Returns ------- str The text with chat abbreviations converted to full forms. \"\"\" logging.info(\"Converting chat abbreviations in text.\") new_text = [] for w in text.split(): if w.upper() in chat_word: new_text.append(chat_word[w.upper()]) else: new_text.append(w) return \" \".join(new_text) def apply_spelling_correction(self, df: pd.DataFrame, text_column: str, save_path: str, start_part: int = 0) -> pd.DataFrame: \"\"\" Apply spelling correction to the specified text column in a DataFrame, with progress tracking via tqdm and ability to resume from last saved part. Parameters ---------- df : pd.DataFrame The input DataFrame. text_column : str The column containing text to correct. save_path : str Path to save each processed part. start_part : int, optional The part number to resume from if process is stopped. Returns ------- pd.DataFrame The DataFrame with corrected text in the specified column. \"\"\" logging.info(\"Starting spelling correction process.\") # Ensure the save path directory exists os.makedirs(save_path, exist_ok=True) # Split the DataFrame into 10 parts df_parts = np.array_split(df, 10) # Correct spelling part by part with progress tracking corrected_parts = [] for i, part in enumerate(df_parts[start_part:], start=start_part): logging.info(f\"Correcting part {i+1} of {len(df_parts)}.\") # Apply spelling correction with progress tracking for each row part[text_column] = [ self.correct_spelling(text) for text in tqdm(part[text_column], desc=f\"Correcting part {i+1}\", total=len(part)) ] # Save the processed part to disk to allow resuming later part_save_path = os.path.join(save_path, f\"corrected_part_{i}.csv\") part.to_csv(part_save_path, index=False) logging.info(f\"Saved corrected part {i+1} to {part_save_path}.\") # Add to corrected parts list corrected_parts.append(part) # If resuming, read the previously processed parts from disk if start_part > 0: for i in range(start_part): part_save_path = os.path.join(save_path, f\"corrected_part_{i}.csv\") part = pd.read_csv(part_save_path) corrected_parts.append(part) logging.info(f\"Loaded previously corrected part {i+1} from {part_save_path}.\") # Concatenate the parts back together corrected_df = pd.concat(corrected_parts) logging.info(\"Spelling correction process completed.\") return corrected_df def clean_review_with_log(self, review: str) -> Tuple[str, Dict[str, str]]: \"\"\" Clean a single review and return the cleaned text and change log. Parameters ---------- review : str The review text to clean. Returns ------- Tuple[str, Dict[str, str]] A tuple containing the cleaned text and a log of changes made during the cleaning process. \"\"\" logging.info(\"Cleaning a single review.\") if review is None: logging.error(\"Review is None.\") return None, {'error': 'Review is None'} try: cleaned_text, changes_log = self.clean_text(review) logging.info(\"Review cleaned successfully.\") return cleaned_text, changes_log except Exception as e: logging.error(f\"Error cleaning review: {e}\") return None, {'error': str(e)} def clean_text(self, text: str) -> Tuple[str, Dict[str, Dict[str, str]]]: \"\"\" Clean the input text by applying a series of text cleaning methods. Parameters ---------- text : str The input text. Returns ------- Tuple[str, Dict[str, Dict[str, str]]] A tuple containing the cleaned text and a log of changes made during the cleaning process. \"\"\" logging.info(\"Starting text cleaning process.\") # Reset changes log for each new text input self.changes_log = {} # List of cleaning steps to apply cleaning_steps = [ ('expand_contractions', self.expand_contractions), ('to_lower_case', self.to_lower_case), ('short_conv', self.short_conv), ('remove_html_tags', self.remove_html_tags), ('regex_cleaning', self.regex_cleaning), ('remove_stop_words', self.remove_stop_words), ('drop_duplicates_and_missing',", "source": "text_cleaner.py"}, {"content": "self.drop_duplicates_and_missing), # New step added here # ('correct_spelling', self.correct_spelling), ('convert_digits_to_words', self.convert_digits_to_words), ('handle_emojis', self.handle_emojis), # ('remove_non_english', self.remove_non_english) # handled in text_processing.py ] # Apply each cleaning step and log changes for step_name, cleaning_function in cleaning_steps: original_text = text text = cleaning_function(text) self.log_changes(original_text, text, step_name) logging.info(\"Text cleaning process completed.\") return text, self.changes_log def drop_duplicates_and_missing(self, text: str) -> str: \"\"\" Drop duplicates and missing values in the text. Parameters ---------- text : str The input text. Returns ------- str The text after dropping duplicates and missing values. \"\"\" logging.info(\"Dropping duplicates and missing values in text.\") # Assuming text is a DataFrame column, convert it to a DataFrame for processing df = pd.DataFrame({'text': [text]}) # Drop duplicates and missing values df.drop_duplicates(inplace=True) df.dropna(inplace=True) # Return the cleaned text return df['text'].iloc[0] if not df.empty else ''", "source": "text_cleaner.py"}, {"content": "import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.feature_extraction.text import CountVectorizer from yellowbrick.text import FreqDistVisualizer import spacy from collections import Counter from tqdm import tqdm import logging import nltk # Ensure you have the necessary NLTK data files nltk.download('punkt') nltk.download('averaged_perceptron_tagger') nltk.download('stopwords') # Initialize logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Load the spaCy model nlp = spacy.load(\"en_core_web_sm\") nlp.max_length = 10000000 # Increase the max length limit class TextEDA: \"\"\" A class used to perform exploratory data analysis (EDA) on text data. Attributes ---------- df : pd.DataFrame The DataFrame containing the text data. Methods ------- check_missing_data() Prints the number of missing values in each column of the DataFrame. check_duplicated_data() Prints the number of duplicated rows in the DataFrame. check_artifacts(column) Prints the counts of various artifacts (e.g., HTML tags, URLs) in the specified text column. analyze_label_distribution(column) Prints the distribution of labels in the specified column. plot_freq_dist(text_column, sentiment_column, sentiment_label, n=10) Plots a frequency distribution of the top n terms in the specified text column for the given sentiment label. get_ngram_freq(n, text_column, sentiment_column, sentiment_label, frequency=10) Calculates the frequency of n-grams in the specified text column for the given sentiment label and creates a seaborn barplot. get_most_common_pos_tag(text_column, sentiment_column, sentiment_label, tag) Retrieves the most common words with a specific part-of-speech tag from the specified text column for the given sentiment label. process_and_plot_word_counts(text_column, sentiment_column, sentiment_label, percentile=0.95) Creates a word count column, filters the DataFrame by the specified percentile of word counts, and plots a histogram of word counts for the given sentiment label. \"\"\" def __init__(self, df: pd.DataFrame): \"\"\" Initializes the TextEDA class with the DataFrame containing the text data. Parameters ---------- df : pd.DataFrame The DataFrame containing the text data. \"\"\" self.df = df logging.info(\"TextEDA initialized with DataFrame.\") def check_missing_data(self) -> None: \"\"\" Prints the number of missing values in each column of the DataFrame. \"\"\" missing_data = self.df.isnull().sum() logging.info(\"Missing data in each column:\\n%s\", missing_data) def check_duplicated_data(self) -> None: \"\"\" Prints the number of duplicated rows in the DataFrame. \"\"\" duplicated_data = self.df.duplicated().sum() logging.info(\"Number of duplicated rows: %d\", duplicated_data) def check_artifacts(self, column: str) -> None: \"\"\" Prints the counts of various artifacts (e.g., HTML tags, URLs) in the specified text column. Parameters ---------- column : str The name of the text column to check for artifacts. \"\"\" artifacts = { \"HTML tags\": r\"<[^>]+>\", \"URLs\": r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"Email addresses\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\", \"Special characters\": r\"[@#$%^&*]\", \"Escape sequences\": r\"\\\\[ntr]\", \"Non-ASCII characters\": r\"[^\\x00-\\x7F]\", \"Repeated punctuation\": r\"([!?.]){2,}\", \"Emojis\": r\"[^\\w\\s,]\" } artifact_counts = {name: self.df[column].str.contains(pattern).sum() for name, pattern in artifacts.items()} logging.info(\"Artifact counts in the text column:\\n%s\", artifact_counts) def analyze_label_distribution(self, column: str) -> None: \"\"\" Prints the distribution of labels in the specified column. Parameters ---------- column : str The name of the column containing the labels. \"\"\" label_distribution = self.df[column].value_counts() logging.info(\"Label distribution:\\n%s\", label_distribution) def plot_freq_dist(self, text_column: str, sentiment_column: str, sentiment_label: int, n=10, file_name=None) -> None: \"\"\" Plots a frequency distribution of the top n terms in the specified text column for the given sentiment label. Parameters ---------- text_column : str The name of the text column to analyze. sentiment_column : str The name of the column containing sentiment labels. sentiment_label", "source": "text_eda.py"}, {"content": ": int The sentiment label to filter the DataFrame by. n : int, optional The number of top terms to plot (default is 10). file_name : str, optional The name of the file to save the plot (default is None). \"\"\" logging.info(\"Plotting frequency distribution for sentiment label %d...\", sentiment_label) df_filtered = self.df[self.df[sentiment_column] == sentiment_label] vectorizer = CountVectorizer() docs = vectorizer.fit_transform(df_filtered[text_column]) features = vectorizer.get_feature_names_out() visualizer = FreqDistVisualizer(features=features, n=n) visualizer.fit(docs) title = f\"Frequency Distribution of Top {n} Tokens for Sentiment {sentiment_label}\" visualizer.ax.set_title(title) if file_name: visualizer.show(outpath=file_name) visualizer.poof() logging.info(\"Frequency distribution plot saved to %s\", file_name if file_name else \"display\") def get_ngram_freq(self, n: int, text_column: str, sentiment_column: str, sentiment_label: int, frequency=10, file_name=None) -> pd.DataFrame: \"\"\" Calculates the frequency of n-grams in the specified text column for the given sentiment label and creates a seaborn barplot. Parameters ---------- n : int The desired n-gram size. text_column : str The name of the text column to analyze. sentiment_column : str The name of the column containing sentiment labels. sentiment_label : int The sentiment label to filter the DataFrame by. frequency : int, optional The number of top n-grams to plot (default is 10). file_name : str, optional The name of the file to save the plot (default is None). Returns ------- pd.DataFrame A DataFrame with n-gram frequencies sorted by frequency. \"\"\" logging.info(\"Calculating %d-gram frequencies for sentiment label %d...\", n, sentiment_label) df_filtered = self.df[self.df[sentiment_column] == sentiment_label] vec = CountVectorizer(stop_words='english', ngram_range=(n, n)) bow = vec.fit_transform(df_filtered[text_column]) count_values = bow.sum(axis=0).A1 ngram_freq = pd.DataFrame({\"ngram\": vec.get_feature_names_out(), \"frequency\": count_values}) ngram_freq.sort_values(by=\"frequency\", ascending=False, inplace=True) top_ngrams = ngram_freq.head(frequency) plt.figure(figsize=(10, 6)) sns.barplot(x=\"frequency\", y=\"ngram\", data=top_ngrams, palette=\"viridis\") plt.xlabel(\"Frequency\") plt.ylabel(\"N-gram\") plt.title(f\"{n}-grams for Sentiment {sentiment_label}\") if file_name: plt.savefig(file_name) plt.show() logging.info(\"%d-gram frequency plot saved to %s\", n, file_name if file_name else \"display\") return ngram_freq def get_most_common_pos_tags_spacy(self, text_column: str, sentiment_column: str, sentiment_label: int, tags: list, file_name_prefix=None) -> None: \"\"\" Retrieves the most common words with specific parts-of-speech tags from the specified text column for the given sentiment label. Parameters ---------- text_column : str The name of the text column to analyze. sentiment_column : str The name of the column containing sentiment labels. sentiment_label : int The sentiment label to filter the DataFrame by. tags : list A list of desired parts-of-speech tags (e.g., ['ADJ', 'ADV', 'NOUN', 'VERB']). file_name_prefix : str, optional The prefix of the file name to save the plots (default is None). \"\"\" logging.info(\"Retrieving most common POS tags for sentiment label %d...\", sentiment_label) df_filtered = self.df[self.df[sentiment_column] == sentiment_label] stop = nlp.Defaults.stop_words corpus = df_filtered[text_column].values.tolist() # Process the text in smaller chunks chunk_size = 500000 # Reduce chunk size to avoid exceeding max length limit for tag in tags: filtered_words = [] for i in tqdm(range(0, len(corpus), chunk_size), desc=f\"Processing text chunks for {tag}\"): chunk = \" \".join(corpus[i:i + chunk_size]) if len(chunk) > nlp.max_length: # Further split the chunk if it exceeds the max length sub_chunks = [chunk[j:j + nlp.max_length] for j in range(0, len(chunk), nlp.max_length)] for sub_chunk in sub_chunks: doc = nlp(sub_chunk) filtered_words.extend([token.text for token in doc if token.pos_ == tag and token.text.lower() not in stop]) else: doc = nlp(chunk) filtered_words.extend([token.text for token in doc if token.pos_ == tag and token.text.lower() not in stop]) most_common", "source": "text_eda.py"}, {"content": "= Counter(filtered_words).most_common(10) words, frequency = zip(*most_common) plt.figure(figsize=(10, 6)) sns.barplot(x=list(frequency), y=list(words), palette='viridis') plt.xlabel('Frequency') plt.ylabel('Word') plt.title(f\"Most Common {tag} for Sentiment {sentiment_label}\") if file_name_prefix: plt.savefig(f\"{file_name_prefix}_{tag}.png\") plt.show() logging.info(\"Most common %s plot saved to %s\", tag, f\"{file_name_prefix}_{tag}.png\" if file_name_prefix else \"display\") def process_and_plot_word_counts(self, text_column: str, sentiment_column: str, sentiment_label: int, percentile: float = 0.95, file_name=None) -> None: \"\"\" Creates a word count column, filters the DataFrame by the specified percentile of word counts, and plots a histogram of word counts for the given sentiment label. Parameters ---------- text_column : str The name of the text column to analyze. sentiment_column : str The name of the column containing sentiment labels. sentiment_label : int The sentiment label to filter the DataFrame by. percentile : float, optional The percentile to filter word counts by (default is 0.95). file_name : str, optional The name of the file to save the plot (default is None). \"\"\" logging.info(\"Processing and plotting word counts for sentiment label %d...\", sentiment_label) df_filtered = self.df[self.df[sentiment_column] == sentiment_label] df_filtered[\"word_count\"] = df_filtered[text_column].map(lambda x: len(x.split())) word_count_desc = df_filtered['word_count'].describe(percentiles=[percentile]) percentile_value = word_count_desc[f'{int(percentile * 100)}%'] df_filtered = df_filtered[df_filtered['word_count'] < percentile_value] logging.info(\"Filtered DataFrame by the %dth percentile of word counts for sentiment %d.\", int(percentile * 100), sentiment_label) sns.histplot(data=df_filtered, x='word_count', hue=sentiment_column) plt.legend(labels=['Positive', 'Negative']) plt.title(f\"Word Count up to {int(percentile * 100)}th Percentile for Sentiment {sentiment_label}\") if file_name: plt.savefig(file_name) plt.show() logging.info(\"Word count histogram saved to %s\", file_name if file_name else \"display\")", "source": "text_eda.py"}, {"content": "import argparse import logging import torch from transformers import GPT2LMHeadModel, GPT2Tokenizer, set_seed # Set up logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) def load_model(model_name): \"\"\"Load a pre-trained decoder-only model and tokenizer.\"\"\" logger.info(f\"Loading model: {model_name}\") model = GPT2LMHeadModel.from_pretrained(model_name) tokenizer = GPT2Tokenizer.from_pretrained(model_name) return model, tokenizer def generate_text(model, tokenizer, context, max_length=50, decoding_method='greedy', device='cpu'): \"\"\"Generate text using the specified decoding method.\"\"\" inputs = tokenizer.encode(context, return_tensors='pt').to(device) attention_mask = torch.ones(inputs.shape, device=device) # Create an attention mask if decoding_method == 'greedy': outputs = model.generate(inputs, max_length=max_length, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id) elif decoding_method == 'beam': outputs = model.generate(inputs, max_length=max_length, num_beams=5, early_stopping=True, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id) elif decoding_method == 'sampling': outputs = model.generate(inputs, max_length=max_length, do_sample=True, top_k=50, attention_mask=attention_mask, pad_token_id=tokenizer.eos_token_id) else: raise ValueError(f\"Unknown decoding method: {decoding_method}\") return tokenizer.decode(outputs[0], skip_special_tokens=True) def main(args): # Set seed for reproducibility set_seed(42) # Determine the device to use (GPU if available, otherwise CPU) device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') logger.info(f\"Using device: {device}\") # Load the model and tokenizer model, tokenizer = load_model(args.model_name) model.to(device) # Generate text for different contexts and decoding methods contexts = [ \"Once upon a time\", \"In a galaxy far, far away\", \"The quick brown fox\" ] decoding_methods = ['greedy', 'beam', 'sampling'] results = {} for context in contexts: results[context] = {} for method in decoding_methods: logger.info(f\"Generating text for context: '{context}' using method: '{method}'\") generated_text = generate_text(model, tokenizer, context, max_length=args.max_length, decoding_method=method, device=device) results[context][method] = generated_text logger.info(f\"Generated text: {generated_text}\") # Save results to a file with utf-8 encoding with open(args.output_file, 'w', encoding='utf-8') as f: for context, methods in results.items(): f.write(f\"Context: {context}\\n\") for method, text in methods.items(): f.write(f\"Method: {method}\\n\") f.write(f\"Generated Text: {text}\\n\\n\") if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Text generation using pre-trained decoder-only transformer models.\") parser.add_argument('--model_name', type=str, default='gpt2', help=\"Name of the pre-trained model to use.\") parser.add_argument('--max_length', type=int, default=50, help=\"Maximum length of the generated text.\") parser.add_argument('--output_file', type=str, default='generated_texts.txt', help=\"File to save the generated texts.\") args = parser.parse_args() main(args)", "source": "text_generation.py"}, {"content": "from text_cleaner import TextCleaner from nltk.stem import SnowballStemmer from typing import List import pandas as pd from tqdm import tqdm import logging import argparse from generate_embeddings import generate_bert_embeddings # Initialize logging logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s') # Initialize the SnowballStemmer stemmer = SnowballStemmer(language='english') def clean_tokenize_and_normalize(df: pd.DataFrame, text_column: str, method: str = 'lemmatize') -> pd.DataFrame: \"\"\" Clean the input text, tokenize it, and either stem or lemmatize the tokens. Parameters ---------- df : pd.DataFrame The input DataFrame containing the text column. text_column : str The name of the column containing the text to clean. method : str, optional The method to use for normalization ('lemmatize' or 'stem'). Default is 'lemmatize'. Returns ------- pd.DataFrame The DataFrame with cleaned and tokenized text. \"\"\" logging.info(\"Initializing TextCleaner...\") text_cleaner = TextCleaner() logging.info(\"Cleaning and tokenizing text...\") tqdm.pandas(desc=\"Cleaning reviews\") df['cleaned_review'], df['change_log'] = zip(*df[text_column].progress_apply(text_cleaner.clean_review_with_log)) logging.info(\"Dropping missing values and duplicates...\") df.dropna(subset=['cleaned_review'], inplace=True) df.drop_duplicates(subset=['cleaned_review'], inplace=True) logging.info(\"Removing non-English text...\") tqdm.pandas(desc=\"Removing non-English reviews\") df['cleaned_review'] = df['cleaned_review'].progress_apply(text_cleaner.remove_non_english) logging.info(\"Dropping missing values and duplicates again...\") df.dropna(subset=['cleaned_review'], inplace=True) df.drop_duplicates(subset=['cleaned_review'], inplace=True) return df def tokenize_and_normalize(df: pd.DataFrame, method: str = 'lemmatize') -> pd.DataFrame: \"\"\" Tokenize and normalize the text in the DataFrame. Parameters ---------- df : pd.DataFrame The input DataFrame containing the cleaned text. method : str, optional The method to use for normalization ('lemmatize' or 'stem'). Default is 'lemmatize'. Returns ------- pd.DataFrame The DataFrame with tokenized and normalized text. \"\"\" text_cleaner = TextCleaner() logging.info(\"Tokenizing and normalizing text...\") tqdm.pandas(desc=\"Tokenizing reviews\") def tokenize_and_normalize_text(text: str) -> List[str]: if text is None: return [] doc = text_cleaner.nlp(text) if method == 'lemmatize': return [token.lemma_ for token in doc] elif method == 'stem': return [stemmer.stem(token.text) for token in doc] else: raise ValueError(\"Method must be 'lemmatize' or 'stem'\") df['tokens'] = df['cleaned_review'].progress_apply(tokenize_and_normalize_text) logging.info(\"Returning DataFrame with tokens and sentiment...\") return df if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Process text data and optionally generate BERT embeddings.\") parser.add_argument('input_file', type=str, help=\"Path to the input CSV file containing reviews and sentiment.\") parser.add_argument('--requires_contextual_embedding', action='store_true', help=\"Flag to generate BERT embeddings.\") parser.add_argument('--preprocessed_file', type=str, help=\"Path to the preprocessed CSV file to skip cleaning step.\") args = parser.parse_args() logging.info(\"Starting text processing script...\") if args.preprocessed_file: logging.info(f\"Reading preprocessed CSV file from {args.preprocessed_file}...\") df = pd.read_csv(args.preprocessed_file) else: logging.info(f\"Reading CSV file from {args.input_file}...\") df = pd.read_csv(args.input_file) logging.info(\"Renaming columns...\") df.rename(columns={'text': 'review', 'label': 'sentiment'}, inplace=True) logging.info(\"Mapping sentiment values to binary...\") df['sentiment'] = df['sentiment'].map({'pos': 1, 'neg': 0}) logging.info(\"Cleaning, tokenizing, and normalizing the reviews...\") df = clean_tokenize_and_normalize(df, text_column='review') # Save the cleaned DataFrame to a temporary CSV file temp_cleaned_file = 'data/temp_cleaned_reviews.csv' df.to_csv(temp_cleaned_file, index=False) logging.info(f\"Cleaned data saved to {temp_cleaned_file}\") if args.requires_contextual_embedding: logging.info(\"Generating BERT embeddings...\") embeddings_df = generate_bert_embeddings(df) # Log the embeddings DataFrame before dropping the sentiment column logging.info(f\"Generated embeddings DataFrame (first 5 rows):\\n{embeddings_df.head()}\") # Keep only the embeddings and the sentiment column embeddings_df = embeddings_df[['sentiment'] + [col for col in embeddings_df.columns if col.startswith('embedding_')]] # Log the embeddings DataFrame after keeping only the necessary columns logging.info(f\"Embeddings DataFrame with sentiment (first 5 rows):\\n{embeddings_df.head()}\") # Save the embeddings DataFrame output_file = 'data/full_data_with_bert_embeddings.csv' embeddings_df.to_csv(output_file, index=False) logging.info(f\"Embeddings DataFrame with sentiment saved to {output_file}\") else: logging.info(\"Tokenizing and normalizing text...\") df = tokenize_and_normalize(df, method='lemmatize') logging.info(\"Saving the full DataFrame to a CSV file...\") output_file = 'data/full_data_with_tokens.csv' df.to_csv(output_file, index=False) logging.info(f\"Full DataFrame with tokens saved to", "source": "text_processing.py"}, {"content": "{output_file}\") print(f\"Full DataFrame with tokens saved to {output_file}\")", "source": "text_processing.py"}, {"content": "from datasets import load_dataset from utils import setup_logging, get_device, load_model_and_tokenizer # Set up logging logger = setup_logging() # Load the CNN/DailyMail dataset logger.info(\"Loading CNN/DailyMail dataset...\") dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\") # Load T5 model and tokenizer logger.info(\"Loading T5 model and tokenizer...\") t5_model, t5_tokenizer = load_model_and_tokenizer(\"t5-small\") # Load BART model and tokenizer logger.info(\"Loading BART model and tokenizer...\") bart_model, bart_tokenizer = load_model_and_tokenizer(\"facebook/bart-large-cnn\") # Determine the device to use (GPU if available, otherwise CPU) device = get_device() logger.info(f\"Using device: {device}\") # Move models to the device t5_model.to(device) bart_model.to(device) def summarize_text_t5(text, model, tokenizer, max_length=150): logger.info(\"Summarizing text using T5...\") input_ids = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device) summary_ids = model.generate(input_ids, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True) summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True) logger.info(\"T5 summarization complete.\") return summary def summarize_text_bart(text, model, tokenizer, max_length=150): logger.info(\"Summarizing text using BART...\") input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True).to(device) summary_ids = model.generate(input_ids, max_length=max_length, min_length=30, length_penalty=2.0, num_beams=4, early_stopping=True) summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True) logger.info(\"BART summarization complete.\") return summary # Select a few examples from the dataset examples = dataset['test'].select(range(3)) for example in examples: text = example['article'] reference_summary = example['highlights'] logger.info(\"Generating summaries for a new example...\") # Generate summaries using T5 t5_summary = summarize_text_t5(text, t5_model, t5_tokenizer) # Generate summaries using BART bart_summary = summarize_text_bart(text, bart_model, bart_tokenizer) logger.info(\"Summaries generated.\") print(f\"Original Text: {text}\\n\") print(f\"Reference Summary: {reference_summary}\\n\") print(f\"T5 Summary: {t5_summary}\\n\") print(f\"BART Summary: {bart_summary}\\n\") print(\"=\"*80)", "source": "text_summarization.py"}, {"content": "import logging import pandas as pd from sklearn.decomposition import LatentDirichletAllocation from sklearn.feature_extraction.text import CountVectorizer from sklearn.preprocessing import StandardScaler import argparse from tqdm import tqdm # Function to perform topic modeling def perform_topic_modeling(df, text_column, n_topics=10): vectorizer = CountVectorizer(max_features=1000) X = vectorizer.fit_transform(df[text_column]) lda = LatentDirichletAllocation(n_components=n_topics, random_state=42) # Wrap the fit_transform call with tqdm document_topic_matrix = lda.fit_transform(tqdm(X, desc=\"Performing topic modeling\")) topic_columns = [f'topic_{i}' for i in range(n_topics)] topic_df = pd.DataFrame(document_topic_matrix, columns=topic_columns) return topic_df def main(args): logging.basicConfig(level=logging.INFO) # Load the cleaned text CSV logging.info(f\"Reading cleaned text CSV file from {args.cleaned_text_file}...\") cleaned_text_df = pd.read_csv(args.cleaned_text_file) # Load the BERT embeddings CSV logging.info(f\"Reading BERT embeddings CSV file from {args.bert_embeddings_file}...\") bert_embeddings_df = pd.read_csv(args.bert_embeddings_file) # Perform topic modeling on cleaned reviews logging.info(\"Performing topic modeling on cleaned reviews...\") n_topics = 10 # Set the number of topics topic_df = perform_topic_modeling(cleaned_text_df, text_column='cleaned_review', n_topics=n_topics) # Combine the topic proportions with the BERT embeddings logging.info(\"Combining topic proportions with BERT embeddings...\") combined_df = pd.concat([bert_embeddings_df, topic_df], axis=1) # Save the unscaled combined DataFrame unscaled_file = 'data/combined_features_unscaled.csv' combined_df.to_csv(unscaled_file, index=False) logging.info(f\"Unscaled combined features DataFrame saved to {unscaled_file}\") # Scale the combined features logging.info(\"Scaling the combined features...\") scaler = StandardScaler() scaled_features = scaler.fit_transform(combined_df.drop(columns=['sentiment'])) scaled_df = pd.DataFrame(scaled_features, columns=combined_df.columns.drop('sentiment')) scaled_df['sentiment'] = combined_df['sentiment'].values # Save the scaled combined DataFrame scaled_file = 'data/combined_features_scaled.csv' scaled_df.to_csv(scaled_file, index=False) logging.info(f\"Scaled combined features DataFrame saved to {scaled_file}\") if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Perform topic modeling and combine features with BERT embeddings.\") parser.add_argument('cleaned_text_file', type=str, help='Path to the cleaned text CSV file.') parser.add_argument('bert_embeddings_file', type=str, help='Path to the BERT embeddings CSV file.') args = parser.parse_args() main(args)", "source": "topic_modeling.py"}, {"content": "import logging import torch from transformers import AutoModelForSeq2SeqLM, AutoTokenizer def setup_logging(): logging.basicConfig(level=logging.INFO) return logging.getLogger(__name__) def get_device(): return torch.device('cuda' if torch.cuda.is_available() else 'cpu') def load_model_and_tokenizer(model_name): model = AutoModelForSeq2SeqLM.from_pretrained(model_name) tokenizer = AutoTokenizer.from_pretrained(model_name) return model, tokenizer", "source": "utils.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import os import argparse from dotenv import load_dotenv from langchain_openai import AzureChatOpenAI from langchain.schema import HumanMessage # Load environment variables load_dotenv() openai_api_base = os.getenv('ENDPOINT') openai_api_key = os.getenv('API_KEY') deployment_name = os.getenv('MODEL') openai_api_version = os.getenv('API_VERSION') openai_api_type = os.getenv('API_TYPE') # Initialize Azure OpenAI client llm = AzureChatOpenAI( azure_endpoint=openai_api_base, openai_api_version=openai_api_version, deployment_name=deployment_name, openai_api_key=openai_api_key, openai_api_type=openai_api_type, temperature=0.7, max_tokens=4096, top_p=0.9 ) # Function to generate response from a prompt def generate_response_from_file(file_path): with open(file_path, 'r') as file: prompt = file.read().strip() response = llm.invoke([HumanMessage(content=prompt)]) return response.content.strip() # Main function def main(example_file): response = generate_response_from_file(example_file) print(response) # Command-line argument parsing if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Generate responses from a prompt file.\") parser.add_argument(\"example_file\", type=str, help=\"File containing the prompt.\") args = parser.parse_args() main(args.example_file)", "source": "prompts.py"}, {"content": "import os import pandas as pd from dotenv import load_dotenv import fitz # PyMuPDF from sentence_transformers import SentenceTransformer import numpy as np import faiss from langchain_openai import AzureChatOpenAI from langchain.schema import HumanMessage # Load environment variables load_dotenv() openai_api_base = os.getenv('ENDPOINT') openai_api_key = os.getenv('API_KEY') deployment_name = os.getenv('MODEL') openai_api_version = os.getenv('API_VERSION') openai_api_type = os.getenv('API_TYPE') # Load the CSV files batch_13_results = pd.read_csv('data/AIAP_BATCH_13_TECHNICAL_ASSESSMENT_RESULTS.CSV') batch_14_results = pd.read_csv('data/AIAP_BATCH_14_TECHNICAL_ASSESSMENT_RESULTS.CSV') # Function to extract text from PDFs def extract_text_from_pdf(pdf_path): text = \"\" with fitz.open(pdf_path) as doc: for page in doc: text += page.get_text() return text # Extract text from the PDFs batch_13_text = extract_text_from_pdf('data/AIAP_Batch 13_Technical_Assessment.PDF') batch_14_text = extract_text_from_pdf('data/AIAP_Batch 14_Technical_Assessment.PDF') # Convert CSV content to text batch_13_csv_text = batch_13_results.to_string(index=False) batch_14_csv_text = batch_14_results.to_string(index=False) # Combine all text data all_texts = [batch_13_text, batch_14_text, batch_13_csv_text, batch_14_csv_text] # Load the pre-trained SBERT model model = SentenceTransformer('all-MiniLM-L6-v2') # Generate embeddings for all text data embeddings = model.encode(all_texts) # Create a FAISS index dimension = embeddings.shape[1] index = faiss.IndexFlatL2(dimension) index.add(embeddings) # Save the FAISS index faiss.write_index(index, \"vector_store.faiss\") # Function to retrieve documents using FAISS def retrieve_documents(query, k=5): query_embedding = model.encode([query]) distances, indices = index.search(query_embedding, k) return [all_texts[i] for i in indices[0]] # Initialize Azure OpenAI client llm = AzureChatOpenAI( azure_endpoint=openai_api_base, openai_api_version=openai_api_version, deployment_name=deployment_name, openai_api_key=openai_api_key, openai_api_type=openai_api_type, temperature=0.7, top_p=0.9, max_tokens=4096 # Adjusting max tokens ) # Define the questions and expected answers questions_and_answers = [ (\"When is the deadline for AIAP Batch 13?\", \"The deadline for AIAP Batch 13 is 1900hrs on 16th January 2023.\"), (\"What is the difference between AIAP Batch 13 and 14?\", \"The difference between AIAP Batch 13 and Batch 14 could include various aspects such as: Participants, Curriculum, Projects, Mentorship and Resources, Duration and Schedule, Evaluation Criteria.\"), (\"The total number of candidates who passed the two technical assessments.\", \"23\"), (\"Which candidate is present in both technical assessments?\", \"Charmander\"), (\"Who is the assessor of Shuckle?\", \"Latios\") ] # Function to validate the answer using heuristics and confidence scores def validate_answer(question, generated_answer, context): # Example heuristic: Check if the answer contains certain keywords keywords = [\"deadline\", \"difference\", \"total\", \"candidate\", \"assessor\"] if any(keyword in generated_answer.lower() for keyword in keywords): return True # Example confidence score validation (assuming the model provides confidence scores) # This is a placeholder as the actual implementation depends on the model's API confidence_score = get_confidence_score(generated_answer, context) if confidence_score > 0.7: # Threshold for confidence score return True return False # Placeholder function to get confidence score (implementation depends on the model's API) def get_confidence_score(generated_answer, context): # This is a mock implementation. Replace with actual API call to get confidence score. return 0.8 # Example confidence score # Generate answers using the QA chain with retries max_retries = 3 for question, _ in questions_and_answers: # Ignore expected_answer as it's not known for attempt in range(max_retries): # Retrieve relevant documents retrieved_docs = retrieve_documents(question) context = \" \".join(retrieved_docs) # Ensure the context length does not exceed the token limit max_context_length = 4096 # Adjust this value as needed if len(context) > max_context_length: context = context[:max_context_length] # Generate response using Azure OpenAI prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\" response = llm([HumanMessage(content=prompt)]) generated_answer = response.content.strip() # Validate the generated answer", "source": "rag.py"}, {"content": "if validate_answer(question, generated_answer, context): print(f\"Question: {question}\") print(f\"Answer: {generated_answer}\\n\") break else: print(f\"Attempt {attempt + 1} failed. Retrying...\") else: print(f\"Failed to generate a valid answer for: {question}\")", "source": "rag.py"}, {"content": "import os from dotenv import load_dotenv from langchain_openai import AzureChatOpenAI from langchain.schema import HumanMessage from sentence_transformers import SentenceTransformer import faiss import numpy as np import pandas as pd import fitz # PyMuPDF # Load environment variables from .env file load_dotenv() # Azure OpenAI credentials openai_api_base = os.getenv('ENDPOINT') openai_api_key = os.getenv('API_KEY') deployment_name = os.getenv('MODEL') openai_api_version = os.getenv('API_VERSION') openai_api_type = os.getenv('API_TYPE') # Initialize Azure OpenAI client llm = AzureChatOpenAI( azure_endpoint=openai_api_base, openai_api_version=openai_api_version, deployment_name=deployment_name, openai_api_key=openai_api_key, openai_api_type=openai_api_type, temperature=0.7, top_p=0.9, max_tokens=4096 # Adjusting max tokens ) # Function to extract text from PDFs def extract_text_from_pdf(pdf_path): text = \"\" with fitz.open(pdf_path) as doc: for page in doc: text += page.get_text() return text # Extract text from the PDFs batch_13_text = extract_text_from_pdf('data/AIAP_Batch 13_Technical_Assessment.PDF') batch_14_text = extract_text_from_pdf('data/AIAP_Batch 14_Technical_Assessment.PDF') # Load the CSV files batch_13_results = pd.read_csv('data/AIAP_BATCH_13_TECHNICAL_ASSESSMENT_RESULTS.CSV') batch_14_results = pd.read_csv('data/AIAP_BATCH_14_TECHNICAL_ASSESSMENT_RESULTS.CSV') # Convert CSV content to text batch_13_csv_text = batch_13_results.to_string(index=False) batch_14_csv_text = batch_14_results.to_string(index=False) # Combine all text data all_texts = [batch_13_text, batch_14_text, batch_13_csv_text, batch_14_csv_text] # Load the pre-trained SBERT model model = SentenceTransformer('all-MiniLM-L6-v2') # Generate embeddings for all text data embeddings = model.encode(all_texts) # Create a FAISS index dimension = embeddings.shape[1] index = faiss.IndexFlatL2(dimension) index.add(embeddings) # Function to retrieve documents using FAISS def retrieve_documents(query, k=5): query_embedding = model.encode([query]) distances, indices = index.search(query_embedding, k) return [all_texts[i] for i in indices[0]] # Define the ReAct process with iterative reasoning def react_process(question, iterations=3): context = \"\" for _ in range(iterations): # Step 1: Reasoning - Generate intermediate thoughts reasoning_prompt = f\"Question: {question}\\nGenerate intermediate thoughts to solve the problem.\" reasoning_response = llm([HumanMessage(content=reasoning_prompt)]) thoughts = reasoning_response.content # Step 2: Acting - Retrieve relevant documents based on thoughts retrieved_docs = retrieve_documents(thoughts) context += \" \".join(retrieved_docs) + \" \" # Step 3: Reasoning - Generate final answer based on accumulated context final_prompt = f\"Context: {context}\\n\\nQuestion: {question}\\nAnswer:\" final_response = llm([HumanMessage(content=final_prompt)]) return final_response.content # Example questions questions = [ \"When is the deadline for AIAP Batch 13?\", \"What is the difference between AIAP Batch 13 and 14?\", \"The total number of candidates who passed the two technical assessments.\", \"Which candidate is present in both technical assessments?\", \"Who is the assessor of Shuckle?\" ] # Generate answers using the ReAct process for question in questions: answer = react_process(question) print(f\"Question: {question}\") print(f\"Answer: {answer}\\n\")", "source": "react.py"}, {"content": "import argparse import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import ( StandardScaler, OneHotEncoder, ) from sklearn.impute import SimpleImputer from src.feature_engineering.create_feature import map_categories_to_numbers def transform(data_path: str) -> pd.DataFrame: \"\"\" Load raw data from a specified file path and transform it into a format suitable for training a machine learning model. The function performs the following steps: 1. Loads the raw data from a CSV file. 2. Removes the \"id\" column, drops duplicates, and replaces missing values. 3. Maps categories in the \"education\" column to numerical values using a predefined mapping dictionary. 4. Engineers new features: - Creates a \"non_employment_income\" feature by combining \"capital_gains\", \"capital_losses\", and \"dividends_from_stocks\". - Drops columns no longer needed after feature engineering. 5. Identifies and separates numerical, nominal, and target features. 6. Splits the data into training and test sets. 7. Encodes the target variable. 8. Preprocesses numeric and nominal features: - Imputes missing values. - Scales numeric features. - Encodes categorical features using one-hot encoding. 9. Transforms the input features into a final format for model training. :param data_path: str The path to the CSV file containing the raw data. :return: tuple A tuple containing: - X_train_trans: pd.DataFrame Transformed training input features. - X_test_trans: pd.DataFrame Transformed test input features. - y_train_trans: np.ndarray Encoded training target variable. - y_test_trans: np.ndarray Encoded test target variable. \"\"\" df_raw = pd.read_csv(data_path) # Remove \"id\" column and drop duplicates df = df_raw.drop([\"id\"], axis=1).drop_duplicates() df.replace([\"None\", \"?\"], np.nan, inplace=True) df.reset_index(drop=True, inplace=True) # Engineer features category_mapping = { \"Children\": 0, \"Less than 1st grade\": 0.5, \"1st 2nd 3rd or 4th grade\": 2.5, \"5th or 6th grade\": 5.5, \"7th and 8th grade\": 7.5, \"9th grade\": 9, \"10th grade\": 10, \"11th grade\": 11, \"12th grade no diploma\": 12, \"High school graduate\": 12, \"Some college but no degree\": 14, \"Associates degree-academic program\": 14, \"Associates degree-occup /vocational\": 14, \"Bachelors degree(BA AB BS)\": 16, \"Masters degree(MA MS MEng MEd MSW MBA)\": 18, \"Prof school degree (MD DDS DVM LLB JD)\": 20, \"Doctorate degree(PhD EdD)\": 21, } df = map_categories_to_numbers( df, \"education\", \"years_of_education\", category_mapping ) df.drop(columns=[\"education\"], inplace=True) df[\"non_employment_income\"] = ( df[\"capital_gains\"] - df[\"capital_losses\"] + df[\"dividends_from_stocks\"] ) df.drop( columns=[\"capital_gains\", \"capital_losses\", \"dividends_from_stocks\"], inplace=True, ) nom_feat = [ \"enroll_in_edu_inst_last_wk\", \"marital_stat\", \"race\", \"country_of_birth_father\", \"country_of_birth_mother\", \"country_of_birth_self\", \"citizenship\", \"class_of_worker\", \"detailed_industry_recode\", \"detailed_occupation_recode\", \"major_industry_code\", \"major_occupation_code\", \"hispanic_origin\", \"member_of_a_labor_union\", \"reason_for_unemployment\", \"full_or_part_time_employment_stat\", \"detailed_household_and_family_stat\", \"detailed_household_summary_in_household\", \"family_members_under_18\", \"own_business_or_self_employed\", \"tax_filer_stat\", \"region_of_previous_residence\", \"state_of_previous_residence\", \"migration_code_change_in_msa\", \"migration_code_change_in_reg\", \"fill_inc_questionnaire_for_veteran_s_admin\", \"veterans_benefits\", \"migration_code_move_within_reg\", \"live_in_this_house_1_year_ago\", \"migration_prev_res_in_sunbelt\", \"sex\", ] num_feat = [ \"age\", \"wage_per_hour\", \"num_persons_worked_for_employer\", \"weeks_worked_in_year\", \"year\", \"years_of_education\", \"non_employment_income\", ] # ord_feat = [\"education\"] target = \"income_group\" # Define X as input features and y as the outcome variable X = df.drop(target, axis=1) y = df[target] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42 ) # Encode y encoder = LabelEncoder() y_train_trans = encoder.fit_transform(y_train) y_test_trans = encoder.transform(y_test) # Preprocessing step for numeric features num_transformer = Pipeline( steps=[ (\"imputer\", SimpleImputer(strategy=\"mean\")), (\"scaler\", StandardScaler()), ] ) # Preprocessing step for nominal features nom_transformer = Pipeline( steps=[ ( \"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\"), ), (\"encoder\", OneHotEncoder(sparse_output=False)), ] ) # Preprocessing pipeline preprocessor = ColumnTransformer( transformers=[", "source": "datapipeline.py"}, {"content": "(\"num\", num_transformer, num_feat), (\"nom\", nom_transformer, nom_feat), # (\"ord\", ord_transformer, ord_feat), ], remainder=\"passthrough\", ) X_train_trans = preprocessor.fit_transform(X_train) X_test_trans = preprocessor.transform(X_test) # Convert transformed data to DataFrame encoded_columns = preprocessor.get_feature_names_out() X_train_trans = pd.DataFrame(X_train_trans, columns=encoded_columns) X_test_trans = pd.DataFrame(X_test_trans, columns=encoded_columns) return X_train_trans, X_test_trans, y_train_trans, y_test_trans if __name__ == \"__main__\": # Set up argument parser and help message parser = argparse.ArgumentParser( description=\"Preprocess data from a CSV file.\" ) parser.add_argument( \"data_path\", type=str, help=\"Path to the CSV file containing the data.\" ) args = parser.parse_args() # Call the transform function with the provided data path transform(args.data_path)", "source": "datapipeline.py"}, {"content": "import numpy as np from typing import Optional, Tuple, List, Union class Node: def __init__(self, gini: float, num_samples: int, num_samples_per_class: List[int], predicted_class: int): \"\"\" Initializes a node in the decision tree. Parameters: gini (float): Gini impurity of the node. num_samples (int): Number of samples at the node. num_samples_per_class (List[int]): Number of samples for each class at the node. predicted_class (int): Predicted class for samples at this node. \"\"\" self.gini = gini self.num_samples = num_samples self.num_samples_per_class = num_samples_per_class self.predicted_class = predicted_class self.feature_index: Optional[int] = None self.threshold: Optional[float] = None self.left: Optional[Node] = None self.right: Optional[Node] = None class DecisionTree: def __init__(self, max_depth: int = None): \"\"\" Initializes the DecisionTree. Parameters: max_depth (int, optional): Maximum depth of the tree. If None, the tree expands until all leaves are pure or contain less than two samples. \"\"\" self.max_depth = max_depth self.root: Optional[Node] = None @staticmethod def _gini_impurity(y: Union[np.ndarray, List[int]]) -> float: \"\"\" Calculates the Gini impurity for a given set of labels. Parameters: y (Union[np.ndarray, List[int]]): Class labels. Returns: float: Gini impurity. \"\"\" m = len(y) if m == 0: return 0.0 class_counts = np.bincount(y, minlength=2) proportions = class_counts / m return 1.0 - np.sum(proportions ** 2) def fit(self, X: np.ndarray, y: np.ndarray): \"\"\" Fits the decision tree to the data. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. \"\"\" self.root = self._grow_tree(X, y) def _grow_tree(self, X: np.ndarray, y: np.ndarray, depth: int = 0) -> Node: \"\"\" Recursively grows the decision tree. Parameters: X (np.ndarray): Feature matrix at the current node. y (np.ndarray): Target vector at the current node. depth (int): Depth of the current node in the tree. Returns: Node: The constructed node. \"\"\" # Calculate values for the current node num_samples_per_class = [np.sum(y == i) for i in range(2)] predicted_class = np.argmax(num_samples_per_class) node = Node( gini=self._gini_impurity(y), # Use the reusable static method num_samples=len(y), num_samples_per_class=num_samples_per_class, predicted_class=predicted_class, ) # Check stopping conditions and attempt to split if (self.max_depth is None or depth < self.max_depth) and len(set(y)) > 1: idx, thr = self._best_split(X, y) if idx is not None: indices_left = X[:, idx] < thr X_left, y_left = X[indices_left], y[indices_left] X_right, y_right = X[~indices_left], y[~indices_left] node.feature_index = idx node.threshold = thr node.left = self._grow_tree(X_left, y_left, depth + 1) node.right = self._grow_tree(X_right, y_right, depth + 1) return node @staticmethod def _gini(y_left: Union[np.ndarray, List[int]], y_right: Union[np.ndarray, List[int]]) -> float: \"\"\" Calculates the Gini impurity for a split. Parameters: y_left (Union[np.ndarray, List[int]]): Class labels for the left split. y_right (Union[np.ndarray, List[int]]): Class labels for the right split. Returns: float: Gini impurity of the split. \"\"\" m_left, m_right = len(y_left), len(y_right) if m_left == 0 or m_right == 0: return 0.0 # Use the reusable method with self for consistency gini_left = DecisionTree._gini_impurity(y_left) gini_right = DecisionTree._gini_impurity(y_right) # Weighted average of the Gini impurity of both splits gini_total = (m_left * gini_left + m_right * gini_right) / (m_left + m_right) return gini_total def _best_split(self, X: np.ndarray, y: np.ndarray) -> Tuple[Optional[int], Optional[float]]: \"\"\" Finds the best split for a node. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. Returns: tuple: Index of the best feature to split and the best", "source": "decision_tree.py"}, {"content": "threshold value. \"\"\" m, n = X.shape if m <= 1: return None, None best_gini = float('inf') best_idx, best_thr = None, None for idx in range(n): thresholds, classes = zip(*sorted(zip(X[:, idx], y))) for i in range(1, m): y_left, y_right = classes[:i], classes[i:] gini_value = self._gini(y_left, y_right) if thresholds[i] == thresholds[i - 1]: continue if gini_value < best_gini: best_gini = gini_value best_idx = idx best_thr = (thresholds[i] + thresholds[i - 1]) / 2 return best_idx, best_thr def predict(self, X: np.ndarray) -> np.ndarray: \"\"\" Predicts class labels for samples in X. Parameters: X (np.ndarray): Feature matrix. Returns: np.ndarray: Predicted class labels. \"\"\" return np.array([self._predict(inputs) for inputs in X]) def _predict(self, inputs: np.ndarray) -> int: \"\"\" Predicts class label for a single sample. Parameters: inputs (np.ndarray): Feature vector for a single sample. Returns: int: Predicted class label. \"\"\" node = self.root while node.left: if inputs[node.feature_index] < node.threshold: node = node.left else: node = node.right return node.predicted_class", "source": "decision_tree.py"}, {"content": "import pandas as pd from typing import Dict, Any from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score from sklearn.model_selection import StratifiedKFold, cross_val_score class Model: def __init__(self): # init your model here \"\"\" Initialize the Model class with a RandomForestClassifier instance. Attributes: estimator (RandomForestClassifier): A RandomForestClassifier estimator initialized with a seed value of 42. \"\"\" self.estimator: RandomForestClassifier = RandomForestClassifier( random_state=42 ) def train( self, params: Dict[str, Any], X_train: pd.DataFrame, y_train: pd.Series ) -> float: \"\"\" Train the RandomForestClassifier with the given parameters and training data. Parameters: params (Dict[str, Any]): A dictionary of hyperparameters for the RandomForestClassifier. X_train (pd.DataFrame): Training feature set. y_train (pd.Series): Training target labels. Returns: float: The mean F1 score from cross-validation on the training data. \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.estimator.set_params(**params) self.estimator.fit(X_train, y_train) # Perform stratified 5-fold cross-validation and compute mean F1 score cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) f1_scores = cross_val_score( self.estimator, X_train, y_train, cv=cv, scoring=\"f1\" ) return f1_scores.mean() def evaluate(self, X_test: pd.DataFrame, y_test: pd.Series) -> float: \"\"\" Evaluate the trained RandomForestClassifier on the test data. Parameters: X_test (pd.DataFrame): Test feature set. y_test (pd.Series): Test target labels. Returns: float: The F1 score for the predictions on the test data. \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score y_pred = self.estimator.predict(X_test) f1 = f1_score(y_test, y_pred) return f1 def get_default_params(self) -> Dict[str, Any]: \"\"\" Retrieve the default parameters for training the RandomForestClassifier. Returns: Dict[str, Any]: A dictionary containing the default hyperparameters for the RandomForestClassifier. \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return { \"max_depth\": 30, \"min_samples_leaf\": 1, \"min_samples_split\": 8, \"n_estimators\": 100, }", "source": "model.py"}, {"content": "import numpy as np from typing import List from .decision_tree import DecisionTree class RandomForest: def __init__(self, n_trees: int = 5, max_depth: int = None): \"\"\" Initializes the RandomForest. Parameters: n_trees (int): The number of trees in the forest. max_depth (int, optional): Maximum depth for each decision tree. If None, trees will expand until leaves are pure or contain less than two samples. \"\"\" self.n_trees = n_trees self.max_depth = max_depth self.trees: List[DecisionTree] = [] def fit(self, X: np.ndarray, y: np.ndarray): \"\"\" Fits the random forest to the data. Parameters: X (np.ndarray): Feature matrix. y (np.ndarray): Target vector. \"\"\" self.trees = [] n_samples = X.shape[0] for _ in range(self.n_trees): # Bootstrap sampling: sample with replacement indices = np.random.choice(n_samples, size=n_samples, replace=True) X_bootstrap = X[indices] y_bootstrap = y[indices] # Initialize a DecisionTree and fit to the bootstrap sample tree = DecisionTree(max_depth=self.max_depth) tree.fit(X_bootstrap, y_bootstrap) # Append the trained tree to the list of trees self.trees.append(tree) def predict(self, X: np.ndarray) -> np.ndarray: \"\"\" Predicts class labels for samples in X. Parameters: X (np.ndarray): Feature matrix. Returns: np.ndarray: Predicted class labels. \"\"\" # Collect predictions from each tree tree_predictions = np.array([tree.predict(X) for tree in self.trees]) # Majority voting mechanism: Find the most common prediction for each sample return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)", "source": "random_forest.py"}, {"content": "import pandas as pd # Query tables for column names def get_table_columns(tables: list[str], conn) -> dict: \"\"\" Retrieve column names for a list of tables from a database. This function queries the database for the names of columns in each specified table and returns a dictionary where the keys are table names and the values are lists of column names. Parameters: tables (list[str]): A list of table names to query for column names. Returns: - dict: A dictionary where each key is a table name, and the corresponding value is a list of column names present in that table. Example ------- >>> import sqlite3 >>> conn = sqlite3.connect('example.db') >>> tables = ['employees', 'departments'] >>> get_table_columns(tables, conn) { 'employees': ['id', 'name', 'position', 'salary'], 'departments': ['dept_id', 'dept_name'] } \"\"\" # Dictionary to store the column names table_columns = {} # Loop through each table and get the column names for table in tables: query = f\"\"\" SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table}'; \"\"\" df_col_names = pd.read_sql_query(sql=query, con=conn) # Add the table and column names to the dictionary table_columns[table] = sum(df_col_names.values.tolist(), []) return table_columns", "source": "query.py"}, {"content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2_contingency from IPython.display import display # Seach columns for values (including ability to handle NaN and None values) def search_columns_for_values( df: pd.DataFrame, search_list: list, columns: list[str] = None ) -> dict[str, list[str]]: \"\"\" Search for specific values within specified columns of a DataFrame, handling NaN and None values. This function checks each column in the given DataFrame (or only the specified columns) for values present in the `search_list`. It handles special cases for `NaN` and `None` values to ensure they are correctly identified if they appear in both the column and the `search_list`. Parameters: - df (pd.DataFrame): The DataFrame in which to search for values. - search_list (list): A list of values to search for within the DataFrame columns. columns (list[str], optional): A list of column names to restrict the search to. If not provided, the function will search all columns. Returns: - dict[str, list[str]]: A dictionary where the keys are column names and the values are lists of matching values found in those columns. Example >>> df = pd.DataFrame({ ... 'A': [1, None, 3], ... 'B': [4, 5, np.nan], ... 'C': ['x', 'y', 'z'] ... }) >>> search_list = [None, 5, 'y', np.nan] >>> search_columns_for_values(df, search_list) Column 'A' contains: [None] Column 'B' contains: [5, nan] Column 'C' contains: ['y'] {'A': [None], 'B': [5, nan], 'C': ['y']} \"\"\" # Dictionary to store the columns and matching values matching_columns = {} # Iterate over the columns for column in columns if columns else df.columns: column_values = df[column] matching_values = set(column_values.dropna()) & set( [val for val in search_list if not pd.isna(val)] ) if pd.isna(column_values).any(): # Handle None or NaN specifically if it's in the search_list element_types = [type(element) for element in set(column_values)] if None in search_list and None in set(column_values): matching_values.add(None) if np.nan in search_list and float in element_types: matching_values.add(np.nan) # If there are any matches, add them to the dictionary if matching_values: matching_columns[column] = list(matching_values) # Print the matching columns and values for column, matches in matching_columns.items(): print(f\"Column '{column}' contains: {matches}\") return matching_columns def check_missing_values( df: pd.DataFrame, columns_to_check: list[str], missing_values_list: list[str] = None, ) -> None: \"\"\" Check for missing values in specified columns of a DataFrame and display the count of rows with missing values for each combination. This function identifies rows with missing values in the specified columns of a DataFrame. Missing values can be determined based on a provided list (`missing_values_list`) or by using the default method that checks for `NaN` values. The function displays the count of rows with missing values for each unique combination and the total number of rows with at least one missing value. Parameters: df (pd.DataFrame): The DataFrame to check for missing values. columns_to_check (list[str]): A list of column names to check for missing values. missing_values_list (list[str], optional): A list of values that should be considered as missing. If not provided, the function defaults to checking for `NaN` values. Returns: - None: The function does not return any value; instead, it prints the count of rows with missing values for", "source": "utils.py"}, {"content": "each combination and the total number of rows with at least one missing value. Example ------- >>> df = pd.DataFrame({ ... 'A': [1, None, 3, 'N/A'], ... 'B': [4, 5, np.nan, 6], ... 'C': ['x', 'y', None, 'z'] ... }) >>> columns_to_check = ['A', 'B', 'C'] >>> missing_values_list = [None, 'N/A'] >>> check_missing_values(df, columns_to_check, missing_values_list) Count of rows with missing values for each combination: A B C None False True 1 N/A False False 1 dtype: int64 Total rows with at least one missing value: 2 \"\"\" if missing_values_list: missing_mask = df[columns_to_check].isin(missing_values_list) else: missing_mask = df[columns_to_check].isnull() # Filter out rows where all selected columns have no missing values rows_with_any_missing = missing_mask[columns_to_check].any(axis=1) # Calculate the total number of rows with missing values total_missing_rows = rows_with_any_missing.sum() # Group by the mask and count the number of rows for each combination missing_combinations = ( missing_mask[rows_with_any_missing].groupby(columns_to_check).size() ) print(\"Count of rows with missing values for each combination:\") display(missing_combinations) print(f\"Total rows with at least one missing value: {total_missing_rows}\") # Categorize features to categorical, numerical and binary def categorize_features( df: pd.DataFrame, ) -> tuple[list[str], list[str], list[str]]: \"\"\" Categorizes the features (columns) of a pandas DataFrame into categorical, numerical, and binary. This function iterates through each column of the input DataFrame and classifies the columns into three categories based on their data types and unique values: - Binary features: Columns with no more than two unique values, including categorical columns with exactly two unique values. - Numerical features: Columns with more than two unique values that are not of type 'category'. - Categorical features: Columns with data type 'category' and more than two unique values. Parameters: - df (pd.DataFrame): The input DataFrame whose features are to be categorized. Returns: - Tuple[List[str], List[str], List[str]]: A tuple containing three lists: 1. cat_feat (List[str]): The list of names of categorical features with more than two unique values. 2. num_feat (List[str]): The list of names of numerical features. 3. bin_feat (List[str]): The list of names of binary features, including both numerical and categorical types. \"\"\" cat_feat = [] num_feat = [] bin_feat = [] for column in df.columns: if df[column].dtype == \"object\" or df[column].dtype == \"category\": if df[column].nunique() <= 2: bin_feat.append(column) else: cat_feat.append(column) else: num_feat.append(column) return cat_feat, num_feat, bin_feat def list_duplicate_counts(df: pd.DataFrame) -> None: # Filter the dataframe to keep only rows that are duplicated duplicates = df[df.duplicated(keep=False)] # Count the number of occurrences of each duplicated row duplicate_groups = ( duplicates.groupby(list(df.columns)).size().reset_index(name=\"count\") ) # Count the number of instances of rows that are duplicated once, twice, thrice, and so on instance_counts = ( duplicate_groups[\"count\"] .value_counts() .sort_index() .reset_index(name=\"Occurrences\") ) instance_counts.columns = [\"Count\", \"Occurrences\"] # Display the formatted table print() print(\"Duplicate counts and the number of occurrences:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(instance_counts.to_string(index=False)) # Display the characteristics of a dataframe def summarize_dataframe( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, target_column: str = None, ) -> None: \"\"\" Prints a summary of a pandas DataFrame including its shape, counts of missing values, duplicates, data types, and lists of categorical, numerical, and binary features. This function leverages the `categorize_features` function to classify features (excluding the identifier column by default)", "source": "utils.py"}, {"content": "into categorical, numerical, and binary based on their data types and unique value counts. It then prints the total number of observations and features, counts of missing values per column, number of duplicate rows, data types of each column, and lists of feature names categorized by their types. Parameters: - df (pd.DataFrame): The DataFrame to summarize. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - target_column (str, optional): The name of the target column. Defaults to None. Returns: - None: This function does not return a value; it prints the summary to the console. \"\"\" cat_feat, num_feat, bin_feat = categorize_features(df) if exclude_id_column: if id_column in cat_feat: cat_feat.remove(id_column) elif id_column in num_feat: num_feat.remove(id_column) elif id_column in bin_feat: bin_feat.remove(id_column) # Display the shape of the dataframe print(f\"Dataframe shape: {df.shape}\") print(f\"{df.shape[0]} rows\") print(f\"{df.shape[1]} columns\") # List missing values print(\"\\nNo. of missing values per column:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.isnull().sum()) # Count the number of rows with missing values missing_rows_count = df.isna().any(axis=1).sum() print(\"\\nNo. (percent) of rows with missing values:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(f\"{missing_rows_count} ({missing_rows_count/df.shape[0]*100:.2f}%)\") # Get the number of duplicates and display it num_duplicates = df.duplicated().sum() print(\"\\nNo. of duplicates:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df) if num_duplicates > 0 else None if id_column in df.columns: # List duplicates excluding key value or identifier column df_subset = df[df.columns.difference([id_column])] num_subset_duplicates = df_subset.duplicated().sum() print(\"\\nNo. of duplicates (excluding identifier column):\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_subset_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df_subset) if num_subset_duplicates > 0 else None # Display the number if duplicated values in the identifier column print(\"\\nNo. of duplicated identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].duplicated().sum()) # Display the number of unique values in the identifier column print(\"\\nNo. of unique identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].nunique()) # List data types print(\"\\nData types:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.dtypes) # For the feature type list, exclude the target column if it exists if target_column: if target_column in cat_feat: cat_feat.remove(target_column) elif target_column in num_feat: num_feat.remove(target_column) elif target_column in bin_feat: bin_feat.remove(target_column) # List categorical features print(\"\\nCategorical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(cat_feat) == 0: print(\"\u2014No categorical features\u2014\") else: for feature in cat_feat: print(feature) # List numerical features print(\"\\nNumerical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(num_feat) > 0: for feature in num_feat: print(feature) else: print(\"\u2014No numerical features\u2014\") # List binary features print(\"\\nBinary features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(bin_feat) == 0: print(\"\u2014No binary features\u2014\") else: for feature in bin_feat: print(feature) # List target print(\"\\nTarget:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if target_column: print(target_column) else: print(\"\u2014No target\u2014\") # List and count unique values in each categorical feature of a dataframe def list_unique_values( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, include_counts: bool = True, ) -> None: \"\"\" Prints the unique values for each categorical or binary feature in a DataFrame, including or excluding a specified identifier column. This function identifies categorical and binary features, by default, excluding a specified identifier column (such as a primary key), and then for each relevant feature, prints the feature name followed by its unique values, along with the count and proportion of each category. If the feature contains None or", "source": "utils.py"}, {"content": "NaN values, they are included in the output; otherwise, the unique values are sorted before being printed. The function also prints the count of unique values for each feature. Parameters: - df (pd.DataFrame): The DataFrame containing the features to analyze. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - include_counts (bool, optional): Include the count and proportion of unique values for each feature. Defaults to True. Returns: - None: This function does not return a value; it prints the summary to the console. Example usage: >>> df = pd.DataFrame({ 'ID': [1, 2, 3, 4], 'Color': ['Red', 'Blue', 'Green', 'Red'], 'Size': ['Small', 'Medium', 'Large', 'Medium'], 'Is_New': [1, 0, 1, 0] }) >>> list_unique_values(df, exclude_id_column=True, id_column='ID') Unique Values: Color \u203e\u203e\u203e\u203e\u203e ['Blue', 'Green', 'Red'] No.of unique values: 3 Counts and Proportions: Blue: 1 (25.0%) Green: 1 (25.0%) Red: 2 (50.0%) Size \u203e\u203e\u203e\u203e ['Large', 'Medium', 'Small'] No.of unique values: 3 Counts and Proportions: Large: 1 (25.0%) Medium: 2 (50.0%) Small: 1 (25.0%) Is_New \u203e\u203e\u203e\u203e\u203e\u203e [0, 1] No.of unique values: 2 Counts and Proportions: 0: 2 (50.0%) 1: 2 (50.0%) \"\"\" cat_feat, _, bin_feat = categorize_features(df) id_column = id_column if exclude_id_column else None feat_list = [x for x in cat_feat + bin_feat if x != id_column] print(\"Unique Values:\\n\") for feature in feat_list: print(feature) print(len(feature) * \"\u203e\") unique_values = df[feature].unique() if ( unique_values.dtype.name == \"category\" and unique_values.dtype.ordered ): print(unique_values) else: none_present = False nan_present = False unique_set = set(unique_values) if None in unique_set: none_present = True unique_set.remove(None) if np.nan in unique_set: nan_present = True unique_set.remove(np.nan) unique_list = sorted(unique_set) if none_present: unique_list.insert(0, None) if nan_present: unique_list.insert(0, np.nan) print(unique_list) # Print the number of unique values print(f\"No.of unique values: {df[feature].nunique()}\") # Calculate and print counts and proportions if include_counts: counts = df[feature].value_counts(dropna=False) total = len(df[feature]) proportions = (counts / total) * 100 result_df = pd.DataFrame( {\"Count\": counts, \"Proportion (%)\": proportions.round(2)} ) display(result_df) print() def create_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, ) -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Creates a cross-tabulation of counts and percentages for a target feature grouped by another feature within a provided DataFrame. This function computes a cross-tabulation table that counts occurrences of each category of the `count_target` grouped by each category of the `by_feature`. It also calculates the percentage representation of each `count_target` category within groups defined by `by_feature`. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` by which the `count_target` values are to be grouped. - p_threshold (float, optional): A threshold value used for determining statistical significance, primarily used in subsequent analysis (not directly affecting the cross-tabulation itself). Defaults to 0.05. Returns: - Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames: 1. The first DataFrame contains the absolute counts of `count_target` grouped by `by_feature`. 2. The second DataFrame contains the percentages of these counts relative to the total", "source": "utils.py"}, {"content": "counts for each `by_feature` group. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> counts, percentages = create_cross_tab(df, 'employee_turnover', 'department') Notes: Ensure that `df` contains the columns specified by `count_target` and `by_feature`. The function assumes these columns contain categorical data suitable for cross-tabulation. \"\"\" # Tabulate absolute numbers and percentages ctab = pd.crosstab(df[by_feature], df[count_target]) ctab_percent = ctab.divide(ctab.sum(axis=1), axis=0) * 100 return ctab, ctab_percent def plot_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Plot a cross-tabulation of counts and percentages of a target feature grouped by another feature, displayed as side-by-side bar charts in a single figure. This function generates two subplots: the first shows the absolute counts and the second shows the percentages. It supports customization of bar colors and label orientations. Args: - df (pd.DataFrame): The DataFrame containing the data to be plotted. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` to group the `count_target` values by. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function directly displays the plot and does not return any value. Example: >>> plot_cross_tab( df=data, count_target='scam_call', by_feature='country_prefix', colors=[\"#1f77b4\", \"#ff7f0e\"], label=True, rotate_label=False, rotate_xlabel=True ) Notes: Ensure that the DataFrame `df` includes the specified `count_target` and `by_feature` columns, and that the `colors` list (if provided) has enough color codes to differentiate the groups. \"\"\" # Perform cross tabulation cross_tab, cross_tab_percentage = create_cross_tab( df, count_target, by_feature ) # Set up the figure and axes for the subplot fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6)) fig.suptitle( f\"Count and Percentage of {count_target} by {by_feature}\", fontsize=18, y=1.06, ) # Plot 1: Absolute counts if colors: cross_tab.plot( kind=\"bar\", stacked=True, ax=axes[0], color=colors, legend=False ) else: cross_tab.plot(kind=\"bar\", stacked=True, ax=axes[0], legend=False) axes[0].set_ylabel(\"Count\") axes[0].set_xlabel(by_feature) axes[0].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the absolute counts if label: for rect in axes[0].patches: height = rect.get_height() y = rect.get_y() if height > 0: # Only place labels for non-empty segments axes[0].text( rect.get_x() + rect.get_width() / 2, y + height / 2, f\"{height:.0f}\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Plot 2: Percentages if colors: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], color=colors, legend=False ) else: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], legend=False ) axes[1].set_ylabel(\"Percentage\") axes[1].set_xlabel(by_feature) axes[1].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the percentages if label: for rect in axes[1].patches: height = rect.get_height() y = rect.get_y() percentage = height if height > 0: # Only place labels for non-empty segments axes[1].text( rect.get_x()", "source": "utils.py"}, {"content": "+ rect.get_width() / 2, y + height / 2, f\"{percentage:.1f}%\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Collect handles and labels from each subplot handles, legend_labels = [], [] for ax in axes: for handle, legend_label in zip(*ax.get_legend_handles_labels()): if legend_label not in legend_labels: handles.append(handle) legend_labels.append(legend_label) fig.legend( handles, legend_labels, title=count_target, loc=\"upper center\", ncols=len(legend_labels), bbox_to_anchor=(0.5, 1.01), frameon=False, ) plt.tight_layout() plt.show() def check_association( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, include_plots: bool = True, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Analyzes the statistical association between two categorical variables using the Chi-Square test. The function performs cross-tabulation of the count and percentage distribution of the `count_target`grouped by the `by_feature`, displays these statistics, and then conducts and reports the results of the Chi-Square test to determine if the association is statistically significant. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the categorical data for counting. - by_feature (str): The name of the column in `df` that represents the categorical groupings to be analyzed against `count_target`. - p_threshold (float, optional): The threshold for determining statistical significance in the Chi-Square test. Defaults to 0.05. - include_plots (bool, optional): Whether to include the cross-tabulation plots. Defaults to True. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function does not return any value but prints to the console, the cross-tabulation results, Chi-Square test results, and a statement about the statistical significance. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> check_association(df, 'employee_turnover', 'department') Notes: It is important that both `count_target` and `by_feature` columns exist in the DataFrame and are properly formatted as categorical data. The function outputs detailed results directly to the console, making it suitable for exploratory data analysis in interactive environments. \"\"\" # Plot cross tabulation if include_plots is True if include_plots: plot_cross_tab( df=df, count_target=count_target, by_feature=by_feature, colors=colors, rotate_xlabel=rotate_xlabel, label=label, rotate_label=rotate_label, ) # Perform cross tabulation ctab, ctab_percent = create_cross_tab( df, count_target, by_feature, p_threshold ) # Display the cross-tabulation of absolute counts and percentages print( f\"Investigating the association between {count_target} and {by_feature}\" ) print(f\"Absolute count of {count_target} values by {by_feature}:\") display(ctab) print(f\"\\nPercentage of {count_target} values by {by_feature}:\") display(ctab_percent) print() # Perform the Chi-Square test chi2, p, dof, expected = chi2_contingency(ctab) print(f\"Chi-square statistic: {chi2}\") print(f\"P threshold: {p_threshold}\") print(f\"P-value: {p}\") print(f\"Degrees of freedom: {dof}\") print(\"Expected frequencies:\") print(expected) print() # Interpret Chi-Square test based on the p-value if p < p_threshold: print( f\"There is a significant association between {count_target} and {by_feature}.\"", "source": "utils.py"}, {"content": ") else: print( f\"There is no significant association between {count_target} and {by_feature}.\" ) def get_unique_values_by_category( df: pd.DataFrame, col1: str, col2: str ) -> dict[str, list[str]]: \"\"\" Retrieves unique values from one column grouped by categories from another column. Parameters: - df (pd.DataFrame): The DataFrame containing the data. - col1 (str): The name of the column used to group categories. Each unique value in this column represents a category. - col2 (str): The name of the column from which unique values are extracted for each category in `col1`. Returns: - dict[str, list[str]]: A dictionary where each key is a unique value from `col1`, and the corresponding value is a list of unique values from `col2` that are associated with that category. Example: -------- Suppose `df` is a DataFrame with the following columns: 'Department' and 'Employee'. This function can be used to retrieve a list of unique employees in each department. >>> df = pd.DataFrame({ ... 'Department': ['HR', 'IT', 'HR', 'Finance', 'IT', 'Finance'], ... 'Employee': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Dave'] ... }) >>> get_unique_values_by_category(df, 'Department', 'Employee') {'HR': ['Alice'], 'IT': ['Bob'], 'Finance': ['Charlie', 'Dave']} \"\"\" unique_values_by_category = {} for value in df[col1].unique(): unique_values_by_category[value] = ( df[df[col1] == value][col2].unique().tolist() ) print(f\"{col1}: {value}\") print( f\"{len(unique_values_by_category[value])} {col2}: {unique_values_by_category[value]}\\n\" ) return unique_values_by_category", "source": "utils.py"}, {"content": "import pandas as pd def map_categories_to_numbers( df: pd.DataFrame, categorical_col: str, new_col: str, category_mapping: dict[str, int], ) -> pd.DataFrame: \"\"\" Map categories in a categorical column to numerical values using a specified mapping dictionary. This function takes a DataFrame and a categorical column and creates a new column by mapping the categories in the original column to numerical values according to a provided dictionary. Parameters: df (pd.DataFrame): The DataFrame containing the categorical column to be mapped. categorical_col (str): The name of the categorical column to be converted to numerical values. new_col (str): The name of the new column to store the mapped numerical values. category_mapping (dict[str, int]): A dictionary that defines the mapping from categories (keys) to numerical values (values). Returns: - pd.DataFrame: The DataFrame with the new column added, containing the numerical representation of the categorical data. Example ------- >>> df = pd.DataFrame({'education': ['High school graduate', 'Bachelors degree(BA AB BS)', 'Less than 1st grade']}) >>> category_mapping = { ... \"Less than 1st grade\": 0.5, ... \"High school graduate\": 12, ... \"Bachelors degree(BA AB BS)\": 16 ... } >>> map_categories_to_numbers(df, 'education', 'education_num', category_mapping) education education_num 0 High school graduate 12.0 1 Bachelors degree(BA AB BS) 16.0 2 Less than 1st grade 0.5 \"\"\" # Map the categorical column to the new numerical column df[new_col] = df[categorical_col].map(category_mapping) return df", "source": "create_feature.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import os import urllib import pandas as pd import logging from dotenv import load_dotenv from sqlalchemy import create_engine from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split from sklearn.pipeline import Pipeline # Configure logging logger = logging.getLogger(__name__) class DataPipeline: def __init__(self, args): # Load environment variables from .env file load_dotenv() # Secrets and connection parameters self.server = os.getenv(\"SERVER\") self.database = os.getenv(\"DATABASE\") self.username = os.getenv(\"USERNAME\") self.password = os.getenv(\"PASSWORD\") self.driver = os.getenv(\"DRIVER\") self.params = urllib.parse.quote_plus( f\"DRIVER={self.driver};\" f\"SERVER={self.server};\" f\"DATABASE={self.database};\" f\"UID={self.username};\" f\"PWD={self.password};\" ) # SQLAlchemy engine for database connection self.engine = create_engine( f\"mssql+pyodbc:///?odbc_connect={self.params}\" ) # Attributes for data loading and splitting self.table_name = args[\"table_name\"] self.random_seed = args[\"random_seed\"] self.target = args[\"target\"] self.test_size = args[\"test_size\"] # Attributes for file paths self.raw_data_path = os.path.join( args[\"raw_data_directory\"], args[\"raw_data_file\"] ) self.clean_data_path = os.path.join( args[\"clean_data_directory\"], args[\"clean_data_file\"] ) self.train_data_path = os.path.join( args[\"split_data_directory\"], args[\"train_data_file\"] ) self.val_data_path = os.path.join( args[\"split_data_directory\"], args[\"val_data_file\"] ) self.test_data_path = os.path.join( args[\"split_data_directory\"], args[\"test_data_file\"] ) self.X_train_path = os.path.join( args[\"transformed_data_directory\"], args[\"X_train_data_file\"] ) self.y_train_path = os.path.join( args[\"transformed_data_directory\"], args[\"y_train_data_file\"] ) self.X_val_path = os.path.join( args[\"transformed_data_directory\"], args[\"X_val_data_file\"] ) self.y_val_path = os.path.join( args[\"transformed_data_directory\"], args[\"y_val_data_file\"] ) self.X_test_path = os.path.join( args[\"transformed_data_directory\"], args[\"X_test_data_file\"] ) self.y_test_path = os.path.join( args[\"transformed_data_directory\"], args[\"y_test_data_file\"] ) # Data transformation pipeline self.scaler = Pipeline(steps=[(\"scaler\", StandardScaler())]) logger.info(f\"DataPipeline initialized with table: {self.table_name}\") def load_data(self): \"\"\" Load data from SQL database table into a pandas DataFrame and save to the raw data path. \"\"\" logger.info(f\"Loading data from table '{self.table_name}'...\") query = f\"SELECT * FROM {self.table_name};\" df = pd.read_sql_query(sql=query, con=self.engine) df.to_csv(self.raw_data_path, index=False) logger.info(f\"Data loaded and saved to {self.raw_data_path}.\") return df def clean_data(self): \"\"\" Remove duplicate rows from the raw data and save the cleaned data. \"\"\" logger.info(f\"Cleaning data from {self.raw_data_path}...\") df = pd.read_csv(self.raw_data_path) df_clean = df.drop_duplicates() df_clean.to_csv(self.clean_data_path, index=False) logger.info(f\"Data cleaned and saved to {self.clean_data_path}.\") return df_clean def split_data(self, test_size=None, random_state=None): \"\"\" Split the cleaned data into training and testing sets and save to the respective paths. \"\"\" if test_size is None: test_size = self.test_size if random_state is None: random_state = self.random_seed logger.info(\"Splitting data into train, validation and test sets...\") df = pd.read_csv(self.clean_data_path) temp_df, test_df = train_test_split( df, test_size=test_size, random_state=random_state ) train_df, val_df = train_test_split( temp_df, test_size=test_size, random_state=random_state ) train_df.to_csv(self.train_data_path, index=False) val_df.to_csv(self.val_data_path, index=False) test_df.to_csv(self.test_data_path, index=False) logger.info(f\"Train data saved to {self.train_data_path}.\") logger.info(f\"Validation data saved to {self.val_data_path}.\") logger.info(f\"Test data saved to {self.test_data_path}.\") return train_df, val_df, test_df def transform_train_data(self): \"\"\" Load, preprocess, and save the transformed training data. \"\"\" logger.info(\"Transforming training data...\") train_df = pd.read_csv(self.train_data_path) X_train = train_df.drop(columns=[self.target]) y_train = train_df[self.target] X_train_scaled = self.scaler.fit_transform(X_train) pd.DataFrame(X_train_scaled).to_csv(self.X_train_path, index=False) pd.DataFrame(y_train).to_csv(self.y_train_path, index=False) logger.info( f\"Training data transformed and saved to {self.X_train_path} and {self.y_train_path}.\" ) return X_train_scaled, y_train def transform_val_data(self): \"\"\" Load, preprocess, and save the transformed validation data. \"\"\" logger.info(\"Transforming validation data...\") val_df = pd.read_csv(self.val_data_path) X_val = val_df.drop(columns=[self.target]) y_val = val_df[self.target] X_val_scaled = self.scaler.transform(X_val) pd.DataFrame(X_val_scaled).to_csv(self.X_val_path, index=False) pd.DataFrame(y_val).to_csv(self.y_val_path, index=False) logger.info( f\"Test data transformed and saved to {self.X_val_path} and {self.y_val_path}.\" ) return X_val_scaled, y_val def transform_test_data(self): \"\"\" Load, preprocess, and save the transformed test data. \"\"\" logger.info(\"Transforming test data...\") test_df = pd.read_csv(self.test_data_path) X_test = test_df.drop(columns=[self.target]) y_test = test_df[self.target] X_test_scaled = self.scaler.transform(X_test) pd.DataFrame(X_test_scaled).to_csv(self.X_test_path, index=False) pd.DataFrame(y_test).to_csv(self.y_test_path, index=False) logger.info( f\"Test data transformed and saved to {self.X_test_path} and {self.y_test_path}.\" ) return X_test_scaled, y_test", "source": "datapipeline.py"}, {"content": "import os import hydra import pandas as pd from omegaconf import DictConfig from hydra.core.hydra_config import HydraConfig from datapipeline import DataPipeline from model import Model @hydra.main(version_base=None, config_path=\"../conf\", config_name=\"main.yaml\") def main(args: DictConfig): # Hydra sets the working directory to the run directory run_dir = HydraConfig.get().run.dir # Define dynamic directories based on Hydra's run directory raw_data_directory = os.path.join(run_dir, args.raw_data_directory) clean_data_directory = os.path.join(run_dir, args.clean_data_directory) split_data_directory = os.path.join(run_dir, args.split_data_directory) transformed_data_directory = os.path.join( run_dir, args.transformed_data_directory ) model_directory = os.path.join(run_dir, args.model_directory) plot_directory = os.path.join(run_dir, args.plot_directory) # Create necessary directories os.makedirs(raw_data_directory, exist_ok=True) os.makedirs(clean_data_directory, exist_ok=True) os.makedirs(split_data_directory, exist_ok=True) os.makedirs(transformed_data_directory, exist_ok=True) os.makedirs(model_directory, exist_ok=True) os.makedirs(plot_directory, exist_ok=True) # Prepare arguments for DataPipeline datapipeline_args = { \"raw_data_directory\": raw_data_directory, \"clean_data_directory\": clean_data_directory, \"split_data_directory\": split_data_directory, \"transformed_data_directory\": transformed_data_directory, \"raw_data_file\": args.raw_data_file, \"clean_data_file\": args.clean_data_file, \"train_data_file\": args.train_data_file, \"test_data_file\": args.test_data_file, \"X_train_data_file\": args.X_train_data_file, \"y_train_data_file\": args.y_train_data_file, \"X_val_data_file\": args.X_val_data_file, \"y_val_data_file\": args.y_val_data_file, \"X_test_data_file\": args.X_test_data_file, \"y_test_data_file\": args.y_test_data_file, \"table_name\": args.table_name, \"target\": args.target, \"test_size\": args.test_size, \"random_seed\": args.random_seed, } # Prepare arguments for Model model_args = { \"random_seed\": args.random_seed, \"neurons\": args.neurons, \"hidden_activation\": args.hidden_activation, \"output_activation\": args.output_activation, \"batch_normalization\": args.batch_normalization, \"dropout\": args.dropout, \"dropout_amount\": args.dropout_amount, \"early_stopping\": args.early_stopping, \"patience\": args.patience, \"optimizer\": args.optimizer, \"loss\": args.loss, \"metrics\": args.metrics, \"batch_size\": args.batch_size, \"epochs\": args.epochs, \"handle_imbalance\": args.handle_imbalance, \"model_directory\": model_directory, \"model_file\": args.model_file, \"plot_directory\": plot_directory, \"plot_file\": args.plot_file, } # Initialize and run DataPipeline data_pipeline = DataPipeline(datapipeline_args) data_pipeline.load_data() data_pipeline.clean_data() data_pipeline.split_data() X_train_scaled, y_train = data_pipeline.transform_train_data() y_train = y_train.values.ravel() X_val_scaled, y_val = data_pipeline.transform_val_data() y_val = y_val.values.ravel() X_test_scaled, y_test = data_pipeline.transform_test_data() y_test = y_test.values.ravel() # Initialize and build the model model = Model(model_args) built_model = model.build_model(input_shape=(X_train_scaled.shape[1],)) # Train the model history = model.train_model( built_model, X_train_scaled, y_train, X_val_scaled, y_val ) # Evaluate the model model.evaluate_model(built_model, X_test_scaled, y_test) model.generate_confusion_matrix(built_model, X_test_scaled, y_test) model.plot_training_loss(history) # Save the model model.save_model(built_model) if __name__ == \"__main__\": main()", "source": "main.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import os import logging import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow.keras as keras from sklearn.metrics import classification_report, confusion_matrix from sklearn.utils import class_weight from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, Input from tensorflow.keras.callbacks import EarlyStopping from omegaconf import OmegaConf # Configure the logger logger = logging.getLogger(__name__) class Model: def __init__(self, args): self.random_seed = args[\"random_seed\"] self.neurons = OmegaConf.to_container(args[\"neurons\"], resolve=True) self.hidden_activation = args[\"hidden_activation\"] self.output_activation = args[\"output_activation\"] self.batch_normalization = args[\"batch_normalization\"] self.dropout = args[\"dropout\"] self.dropout_amount = args[\"dropout_amount\"] self.early_stopping = args[\"early_stopping\"] self.patience = args[\"patience\"] self.optimizer = args[\"optimizer\"] self.loss = args[\"loss\"] self.metrics = OmegaConf.to_container(args[\"metrics\"], resolve=True) self.batch_size = args[\"batch_size\"] self.epochs = args[\"epochs\"] self.handle_imbalance = args[\"handle_imbalance\"] self.model_directory = args[\"model_directory\"] self.model_file = args[\"model_file\"] self.plot_directory = args[\"plot_directory\"] self.plot_file = args[\"plot_file\"] def build_model(self, input_shape): \"\"\" Build and compile the neural network model based on the provided arguments. \"\"\" # Set random seed for reproducibility keras.utils.set_random_seed(self.random_seed) tf.config.experimental.enable_op_determinism() model = Sequential() model.add(Input(shape=input_shape)) # Add hidden layers for i in range(len(self.neurons)): model.add( Dense(self.neurons[i], activation=self.hidden_activation) ) if self.batch_normalization: model.add(BatchNormalization()) logger.debug(f\"Added BatchNormalization after layer {i+1}\") if self.dropout: model.add(Dropout(self.dropout_amount)) logger.debug( f\"Added Dropout({self.dropout_amount}) after layer {i+1}\" ) # Output layer model.add(Dense(1, activation=self.output_activation)) # Compile the model model.compile( optimizer=self.optimizer, loss=self.loss, metrics=self.metrics ) logger.info(\"Model architecture:\") model.summary(print_fn=lambda x: logger.info(x)) return model def train_model(self, model, X_train, y_train, X_val, y_val): \"\"\" Train the model using early stopping (optional) and class weights if imbalance handling is enabled. \"\"\" class_weight_dict = None if self.handle_imbalance: # Compute class weights to handle imbalance class_weights = class_weight.compute_class_weight( class_weight=\"balanced\", classes=np.unique(y_train), y=y_train ) class_weight_dict = dict(enumerate(class_weights)) logger.info(f\"Computed class weights: {class_weight_dict}\") callbacks = [] if self.early_stopping: early_stopping = EarlyStopping( monitor=\"val_loss\", patience=self.patience, restore_best_weights=True, ) callbacks.append(early_stopping) logger.info(\"EarlyStopping callback added.\") history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=self.epochs, batch_size=self.batch_size, class_weight=class_weight_dict, callbacks=callbacks, verbose=1, ) logger.info(\"Model training completed.\") return history def evaluate_model(self, model, X_test, y_test): \"\"\" Evaluate the model on test data and log precision and recall. \"\"\" results = model.evaluate(X_test, y_test, return_dict=True, verbose=0) precision = results.get(\"precision\", \"N/A\") recall = results.get(\"recall\", \"N/A\") logger.info(f\"Test Precision: {precision:.4f}\") logger.info(f\"Test Recall: {recall:.4f}\") def generate_confusion_matrix(self, model, X_test, y_test): \"\"\" Generate and log a confusion matrix and classification report. \"\"\" y_pred_proba = model.predict(X_test, verbose=0) y_pred = (y_pred_proba > 0.5).astype(int) cm = confusion_matrix(y_test, y_pred) report = classification_report( y_test, y_pred, target_names=[\"Class 0\", \"Class 1\"] ) logger.info(\"Confusion Matrix:\") logger.info(f\"\\n{cm}\") logger.info(\"Classification Report:\") logger.info(f\"\\n{report}\") def plot_training_loss(self, history): \"\"\" Plot training and validation loss over epochs and save the plot to a file. \"\"\" plt.figure(figsize=(8, 6)) plt.plot(history.history[\"loss\"], label=\"Training Loss\") plt.plot(history.history[\"val_loss\"], label=\"Validation Loss\") plt.title(\"Training and Validation Loss Over Epochs\") plt.xlabel(\"Epochs\") plt.ylabel(\"Loss\") plt.legend(loc=\"upper right\") plt.grid(True) # Define the path to save the plot plot_path = os.path.join(self.plot_directory, self.plot_file) plt.savefig(plot_path) plt.close() logger.info(f\"Training and validation loss plot saved to {plot_path}\") def save_model(self, model): \"\"\" Save the trained model to the specified path. \"\"\" os.makedirs(self.model_directory, exist_ok=True) model_path = os.path.join(self.model_directory, self.model_file) model.save(model_path) logger.info(f\"Model saved to {model_path}\")", "source": "model.py"}, {"content": "import pandas as pd # Query tables for column names def get_table_columns(tables: list[str], conn) -> dict: \"\"\" Retrieve column names for a list of tables from a database. This function queries the database for the names of columns in each specified table and returns a dictionary where the keys are table names and the values are lists of column names. Parameters: ---------- tables (list[str]): A list of table names to query for column names. conn: Connection object to the database. Returns: ------- - dict: A dictionary where each key is a table name, and the corresponding value is a list of column names present in that table. Example ------- >>> import sqlite3 >>> conn = sqlite3.connect('example.db') >>> tables = ['employees', 'departments'] >>> get_table_columns(tables, conn) { 'employees': ['id', 'name', 'position', 'salary'], 'departments': ['dept_id', 'dept_name'] } \"\"\" # Dictionary to store the column names table_columns = {} # Loop through each table and get the column names for table in tables: query = f\"\"\" SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table}'; \"\"\" df_col_names = pd.read_sql_query(sql=query, con=conn) # Add the table and column names to the dictionary table_columns[table] = sum(df_col_names.values.tolist(), []) return table_columns def count_missing_values(tables: list[str], conn) -> None: # Loop through each table and get the column names for table in tables: table_columns = get_table_columns([table], conn) # Query each column in each table for missing values for table, columns in table_columns.items(): print(f\"Table: {table}\") for column in columns: query = f\"\"\" SELECT SUM( CASE WHEN {column} IS NULL THEN 1 ELSE 0 END ) AS {column}_missing FROM {table}; \"\"\" print( f\"{pd.read_sql_query(sql=query, con=conn).iloc[0,0]} missing values in column '{column}'\" ) print()", "source": "query.py"}, {"content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2_contingency from IPython.display import display # Seach columns for values (including ability to handle NaN and None values) def search_columns_for_values( df: pd.DataFrame, search_list: list, columns: list[str] = None ) -> dict[str, list[str]]: \"\"\" Search for specific values within specified columns of a DataFrame, handling NaN and None values. This function checks each column in the given DataFrame (or only the specified columns) for values present in the `search_list`. It handles special cases for `NaN` and `None` values to ensure they are correctly identified if they appear in both the column and the `search_list`. Parameters: - df (pd.DataFrame): The DataFrame in which to search for values. - search_list (list): A list of values to search for within the DataFrame columns. columns (list[str], optional): A list of column names to restrict the search to. If not provided, the function will search all columns. Returns: - dict[str, list[str]]: A dictionary where the keys are column names and the values are lists of matching values found in those columns. Example >>> df = pd.DataFrame({ ... 'A': [1, None, 3], ... 'B': [4, 5, np.nan], ... 'C': ['x', 'y', 'z'] ... }) >>> search_list = [None, 5, 'y', np.nan] >>> search_columns_for_values(df, search_list) Column 'A' contains: [None] Column 'B' contains: [5, nan] Column 'C' contains: ['y'] {'A': [None], 'B': [5, nan], 'C': ['y']} \"\"\" # Dictionary to store the columns and matching values matching_columns = {} # Iterate over the columns for column in columns if columns else df.columns: column_values = df[column] matching_values = set(column_values.dropna()) & set( [val for val in search_list if not pd.isna(val)] ) if pd.isna(column_values).any(): # Handle None or NaN specifically if it's in the search_list element_types = [type(element) for element in set(column_values)] if None in search_list and None in set(column_values): matching_values.add(None) if np.nan in search_list and float in element_types: matching_values.add(np.nan) # If there are any matches, add them to the dictionary if matching_values: matching_columns[column] = list(matching_values) # Print the matching columns and values for column, matches in matching_columns.items(): print(f\"Column '{column}' contains: {matches}\") return matching_columns def check_missing_values( df: pd.DataFrame, columns_to_check: list[str], missing_values_list: list[str] = None, ) -> None: \"\"\" Check for missing values in specified columns of a DataFrame and display the count of rows with missing values for each combination. This function identifies rows with missing values in the specified columns of a DataFrame. Missing values can be determined based on a provided list (`missing_values_list`) or by using the default method that checks for `NaN` values. The function displays the count of rows with missing values for each unique combination and the total number of rows with at least one missing value. Parameters: df (pd.DataFrame): The DataFrame to check for missing values. columns_to_check (list[str]): A list of column names to check for missing values. missing_values_list (list[str], optional): A list of values that should be considered as missing. If not provided, the function defaults to checking for `NaN` values. Returns: - None: The function does not return any value; instead, it prints the count of rows with missing values for", "source": "utils.py"}, {"content": "each combination and the total number of rows with at least one missing value. Example ------- >>> df = pd.DataFrame({ ... 'A': [1, None, 3, 'N/A'], ... 'B': [4, 5, np.nan, 6], ... 'C': ['x', 'y', None, 'z'] ... }) >>> columns_to_check = ['A', 'B', 'C'] >>> missing_values_list = [None, 'N/A'] >>> check_missing_values(df, columns_to_check, missing_values_list) Count of rows with missing values for each combination: A B C None False True 1 N/A False False 1 dtype: int64 Total rows with at least one missing value: 2 \"\"\" if missing_values_list: missing_mask = df[columns_to_check].isin(missing_values_list) else: missing_mask = df[columns_to_check].isnull() # Filter out rows where all selected columns have no missing values rows_with_any_missing = missing_mask[columns_to_check].any(axis=1) # Calculate the total number of rows with missing values total_missing_rows = rows_with_any_missing.sum() # Group by the mask and count the number of rows for each combination missing_combinations = ( missing_mask[rows_with_any_missing].groupby(columns_to_check).size() ) print(\"Count of rows with missing values for each combination:\") display(missing_combinations) print(f\"Total rows with at least one missing value: {total_missing_rows}\") # Categorize features to categorical, numerical and binary def categorize_features( df: pd.DataFrame, ) -> tuple[list[str], list[str], list[str]]: \"\"\" Categorizes the features (columns) of a pandas DataFrame into categorical, numerical, and binary. This function iterates through each column of the input DataFrame and classifies the columns into three categories based on their data types and unique values: - Binary features: Columns with no more than two unique values, including categorical columns with exactly two unique values. - Numerical features: Columns with more than two unique values that are not of type 'category'. - Categorical features: Columns with data type 'category' and more than two unique values. Parameters: - df (pd.DataFrame): The input DataFrame whose features are to be categorized. Returns: - Tuple[List[str], List[str], List[str]]: A tuple containing three lists: 1. cat_feat (List[str]): The list of names of categorical features with more than two unique values. 2. num_feat (List[str]): The list of names of numerical features. 3. bin_feat (List[str]): The list of names of binary features, including both numerical and categorical types. \"\"\" cat_feat = [] num_feat = [] bin_feat = [] for column in df.columns: if df[column].dtype == \"object\" or df[column].dtype == \"category\": if df[column].nunique() <= 2: bin_feat.append(column) else: cat_feat.append(column) else: num_feat.append(column) return cat_feat, num_feat, bin_feat def list_duplicate_counts(df: pd.DataFrame) -> None: # Filter the dataframe to keep only rows that are duplicated duplicates = df[df.duplicated(keep=False)] # Count the number of occurrences of each duplicated row duplicate_groups = ( duplicates.groupby(list(df.columns)).size().reset_index(name=\"count\") ) # Count the number of instances of rows that are duplicated once, twice, thrice, and so on instance_counts = ( duplicate_groups[\"count\"] .value_counts() .sort_index() .reset_index(name=\"Occurrences\") ) instance_counts.columns = [\"Count\", \"Occurrences\"] # Display the formatted table print() print(\"Duplicate counts and the number of occurrences:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(instance_counts.to_string(index=False)) # Display the characteristics of a dataframe def summarize_dataframe( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, target_column: str = None, ) -> None: \"\"\" Prints a summary of a pandas DataFrame including its shape, counts of missing values, duplicates, data types, and lists of categorical, numerical, and binary features. This function leverages the `categorize_features` function to classify features (excluding the identifier column by default)", "source": "utils.py"}, {"content": "into categorical, numerical, and binary based on their data types and unique value counts. It then prints the total number of observations and features, counts of missing values per column, number of duplicate rows, data types of each column, and lists of feature names categorized by their types. Parameters: - df (pd.DataFrame): The DataFrame to summarize. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - target_column (str, optional): The name of the target column. Defaults to None. Returns: - None: This function does not return a value; it prints the summary to the console. \"\"\" cat_feat, num_feat, bin_feat = categorize_features(df) if exclude_id_column: if id_column in cat_feat: cat_feat.remove(id_column) elif id_column in num_feat: num_feat.remove(id_column) elif id_column in bin_feat: bin_feat.remove(id_column) # Display the shape of the dataframe print(f\"Dataframe shape: {df.shape}\") print(f\"{df.shape[0]} rows\") print(f\"{df.shape[1]} columns\") # List missing values print(\"\\nNo. of missing values per column:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.isnull().sum()) # Count the number of rows with missing values missing_rows_count = df.isna().any(axis=1).sum() print(\"\\nNo. (percent) of rows with missing values:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(f\"{missing_rows_count} ({missing_rows_count/df.shape[0]*100:.2f}%)\") # Get the number of duplicates and display it num_duplicates = df.duplicated().sum() print(\"\\nNo. of duplicates:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df) if num_duplicates > 0 else None if id_column in df.columns: # List duplicates excluding key value or identifier column df_subset = df[df.columns.difference([id_column])] num_subset_duplicates = df_subset.duplicated().sum() print(\"\\nNo. of duplicates (excluding identifier column):\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_subset_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df_subset) if num_subset_duplicates > 0 else None # Display the number if duplicated values in the identifier column print(\"\\nNo. of duplicated identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].duplicated().sum()) # Display the number of unique values in the identifier column print(\"\\nNo. of unique identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].nunique()) # List data types print(\"\\nData types:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.dtypes) # For the feature type list, exclude the target column if it exists if target_column: if target_column in cat_feat: cat_feat.remove(target_column) elif target_column in num_feat: num_feat.remove(target_column) elif target_column in bin_feat: bin_feat.remove(target_column) # List categorical features print(\"\\nCategorical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(cat_feat) == 0: print(\"\u2014No categorical features\u2014\") else: for feature in cat_feat: print(feature) # List numerical features print(\"\\nNumerical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(num_feat) > 0: for feature in num_feat: print(feature) else: print(\"\u2014No numerical features\u2014\") # List binary features print(\"\\nBinary features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(bin_feat) == 0: print(\"\u2014No binary features\u2014\") else: for feature in bin_feat: print(feature) # List target print(\"\\nTarget:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if target_column: print(target_column) else: print(\"\u2014No target\u2014\") # List and count unique values in each categorical feature of a dataframe def list_unique_values( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, include_counts: bool = True, ) -> None: \"\"\" Prints the unique values for each categorical or binary feature in a DataFrame, including or excluding a specified identifier column. This function identifies categorical and binary features, by default, excluding a specified identifier column (such as a primary key), and then for each relevant feature, prints the feature name followed by its unique values, along with the count and proportion of each category. If the feature contains None or", "source": "utils.py"}, {"content": "NaN values, they are included in the output; otherwise, the unique values are sorted before being printed. The function also prints the count of unique values for each feature. Parameters: - df (pd.DataFrame): The DataFrame containing the features to analyze. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - include_counts (bool, optional): Include the count and proportion of unique values for each feature. Defaults to True. Returns: - None: This function does not return a value; it prints the summary to the console. Example usage: >>> df = pd.DataFrame({ 'ID': [1, 2, 3, 4], 'Color': ['Red', 'Blue', 'Green', 'Red'], 'Size': ['Small', 'Medium', 'Large', 'Medium'], 'Is_New': [1, 0, 1, 0] }) >>> list_unique_values(df, exclude_id_column=True, id_column='ID') Unique Values: Color \u203e\u203e\u203e\u203e\u203e ['Blue', 'Green', 'Red'] No.of unique values: 3 Counts and Proportions: Blue: 1 (25.0%) Green: 1 (25.0%) Red: 2 (50.0%) Size \u203e\u203e\u203e\u203e ['Large', 'Medium', 'Small'] No.of unique values: 3 Counts and Proportions: Large: 1 (25.0%) Medium: 2 (50.0%) Small: 1 (25.0%) Is_New \u203e\u203e\u203e\u203e\u203e\u203e [0, 1] No.of unique values: 2 Counts and Proportions: 0: 2 (50.0%) 1: 2 (50.0%) \"\"\" cat_feat, _, bin_feat = categorize_features(df) id_column = id_column if exclude_id_column else None feat_list = [x for x in cat_feat + bin_feat if x != id_column] print(\"Unique Values:\\n\") for feature in feat_list: print(feature) print(len(feature) * \"\u203e\") unique_values = df[feature].unique() if ( unique_values.dtype.name == \"category\" and unique_values.dtype.ordered ): print(unique_values) else: none_present = False nan_present = False unique_set = set(unique_values) if None in unique_set: none_present = True unique_set.remove(None) if np.nan in unique_set: nan_present = True unique_set.remove(np.nan) unique_list = sorted(unique_set) if none_present: unique_list.insert(0, None) if nan_present: unique_list.insert(0, np.nan) print(unique_list) # Print the number of unique values print(f\"No.of unique values: {df[feature].nunique()}\") # Calculate and print counts and proportions if include_counts: counts = df[feature].value_counts(dropna=False) total = len(df[feature]) proportions = (counts / total) * 100 result_df = pd.DataFrame( {\"Count\": counts, \"Proportion (%)\": proportions.round(2)} ) display(result_df) print() def create_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, ) -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Creates a cross-tabulation of counts and percentages for a target feature grouped by another feature within a provided DataFrame. This function computes a cross-tabulation table that counts occurrences of each category of the `count_target` grouped by each category of the `by_feature`. It also calculates the percentage representation of each `count_target` category within groups defined by `by_feature`. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` by which the `count_target` values are to be grouped. - p_threshold (float, optional): A threshold value used for determining statistical significance, primarily used in subsequent analysis (not directly affecting the cross-tabulation itself). Defaults to 0.05. Returns: - Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames: 1. The first DataFrame contains the absolute counts of `count_target` grouped by `by_feature`. 2. The second DataFrame contains the percentages of these counts relative to the total", "source": "utils.py"}, {"content": "counts for each `by_feature` group. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> counts, percentages = create_cross_tab(df, 'employee_turnover', 'department') Notes: Ensure that `df` contains the columns specified by `count_target` and `by_feature`. The function assumes these columns contain categorical data suitable for cross-tabulation. \"\"\" # Tabulate absolute numbers and percentages ctab = pd.crosstab(df[by_feature], df[count_target]) ctab_percent = ctab.divide(ctab.sum(axis=1), axis=0) * 100 return ctab, ctab_percent def plot_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Plot a cross-tabulation of counts and percentages of a target feature grouped by another feature, displayed as side-by-side bar charts in a single figure. This function generates two subplots: the first shows the absolute counts and the second shows the percentages. It supports customization of bar colors and label orientations. Args: - df (pd.DataFrame): The DataFrame containing the data to be plotted. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` to group the `count_target` values by. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function directly displays the plot and does not return any value. Example: >>> plot_cross_tab( df=data, count_target='scam_call', by_feature='country_prefix', colors=[\"#1f77b4\", \"#ff7f0e\"], label=True, rotate_label=False, rotate_xlabel=True ) Notes: Ensure that the DataFrame `df` includes the specified `count_target` and `by_feature` columns, and that the `colors` list (if provided) has enough color codes to differentiate the groups. \"\"\" # Perform cross tabulation cross_tab, cross_tab_percentage = create_cross_tab( df, count_target, by_feature ) # Set up the figure and axes for the subplot fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6)) fig.suptitle( f\"Count and Percentage of {count_target} by {by_feature}\", fontsize=18, y=1.06, ) # Plot 1: Absolute counts if colors: cross_tab.plot( kind=\"bar\", stacked=True, ax=axes[0], color=colors, legend=False ) else: cross_tab.plot(kind=\"bar\", stacked=True, ax=axes[0], legend=False) axes[0].set_ylabel(\"Count\") axes[0].set_xlabel(by_feature) axes[0].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the absolute counts if label: for rect in axes[0].patches: height = rect.get_height() y = rect.get_y() if height > 0: # Only place labels for non-empty segments axes[0].text( rect.get_x() + rect.get_width() / 2, y + height / 2, f\"{height:.0f}\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Plot 2: Percentages if colors: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], color=colors, legend=False ) else: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], legend=False ) axes[1].set_ylabel(\"Percentage\") axes[1].set_xlabel(by_feature) axes[1].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the percentages if label: for rect in axes[1].patches: height = rect.get_height() y = rect.get_y() percentage = height if height > 0: # Only place labels for non-empty segments axes[1].text( rect.get_x()", "source": "utils.py"}, {"content": "+ rect.get_width() / 2, y + height / 2, f\"{percentage:.1f}%\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Collect handles and labels from each subplot handles, legend_labels = [], [] for ax in axes: for handle, legend_label in zip(*ax.get_legend_handles_labels()): if legend_label not in legend_labels: handles.append(handle) legend_labels.append(legend_label) fig.legend( handles, legend_labels, title=count_target, loc=\"upper center\", ncols=len(legend_labels), bbox_to_anchor=(0.5, 1.01), frameon=False, ) plt.tight_layout() plt.show() def check_association( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, include_plots: bool = True, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Analyzes the statistical association between two categorical variables using the Chi-Square test. The function performs cross-tabulation of the count and percentage distribution of the `count_target`grouped by the `by_feature`, displays these statistics, and then conducts and reports the results of the Chi-Square test to determine if the association is statistically significant. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the categorical data for counting. - by_feature (str): The name of the column in `df` that represents the categorical groupings to be analyzed against `count_target`. - p_threshold (float, optional): The threshold for determining statistical significance in the Chi-Square test. Defaults to 0.05. - include_plots (bool, optional): Whether to include the cross-tabulation plots. Defaults to True. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function does not return any value but prints to the console, the cross-tabulation results, Chi-Square test results, and a statement about the statistical significance. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> check_association(df, 'employee_turnover', 'department') Notes: It is important that both `count_target` and `by_feature` columns exist in the DataFrame and are properly formatted as categorical data. The function outputs detailed results directly to the console, making it suitable for exploratory data analysis in interactive environments. \"\"\" # Plot cross tabulation if include_plots is True if include_plots: plot_cross_tab( df=df, count_target=count_target, by_feature=by_feature, colors=colors, rotate_xlabel=rotate_xlabel, label=label, rotate_label=rotate_label, ) # Perform cross tabulation ctab, ctab_percent = create_cross_tab( df, count_target, by_feature, p_threshold ) # Display the cross-tabulation of absolute counts and percentages print( f\"Investigating the association between {count_target} and {by_feature}\" ) print(f\"Absolute count of {count_target} values by {by_feature}:\") display(ctab) print(f\"\\nPercentage of {count_target} values by {by_feature}:\") display(ctab_percent) print() # Perform the Chi-Square test chi2, p, dof, expected = chi2_contingency(ctab) print(f\"Chi-square statistic: {chi2}\") print(f\"P threshold: {p_threshold}\") print(f\"P-value: {p}\") print(f\"Degrees of freedom: {dof}\") print(\"Expected frequencies:\") print(expected) print() # Interpret Chi-Square test based on the p-value if p < p_threshold: print( f\"There is a significant association between {count_target} and {by_feature}.\"", "source": "utils.py"}, {"content": ") else: print( f\"There is no significant association between {count_target} and {by_feature}.\" ) def get_unique_values_by_category( df: pd.DataFrame, col1: str, col2: str ) -> dict[str, list[str]]: \"\"\" Retrieves unique values from one column grouped by categories from another column. Parameters: - df (pd.DataFrame): The DataFrame containing the data. - col1 (str): The name of the column used to group categories. Each unique value in this column represents a category. - col2 (str): The name of the column from which unique values are extracted for each category in `col1`. Returns: - dict[str, list[str]]: A dictionary where each key is a unique value from `col1`, and the corresponding value is a list of unique values from `col2` that are associated with that category. Example: -------- Suppose `df` is a DataFrame with the following columns: 'Department' and 'Employee'. This function can be used to retrieve a list of unique employees in each department. >>> df = pd.DataFrame({ ... 'Department': ['HR', 'IT', 'HR', 'Finance', 'IT', 'Finance'], ... 'Employee': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Dave'] ... }) >>> get_unique_values_by_category(df, 'Department', 'Employee') {'HR': ['Alice'], 'IT': ['Bob'], 'Finance': ['Charlie', 'Dave']} \"\"\" unique_values_by_category = {} for value in df[col1].unique(): unique_values_by_category[value] = ( df[df[col1] == value][col2].unique().tolist() ) print(f\"{col1}: {value}\") print( f\"{len(unique_values_by_category[value])} {col2}: {unique_values_by_category[value]}\\n\" ) return unique_values_by_category", "source": "utils.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100] ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [ -128, -200, -24], [ -127, 573, 28], [ 384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "class DataPipeline: def __init__(self): # Your code here pass def run_data_pipeline(self, csv_path): # Your code here return cleaned_data", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "class ForecastModel: def __init__(self): self.model = ml_model_of_your_choice def fit(self, X, y): self.model.fit(X, y) def evaluate(model, X_train, y_train, X_test, y_test): y_train_pred = model.predict(X_train) train_error = metric_of_your_choice y_test_pred = model.predict(X_test) test_error = metric_of_your_choice return train_error, test_error def predict(self, X): return self.model.predict(X)", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.stats import chi2_contingency from IPython.display import display # Seach columns for values (including ability to handle NaN and None values) def search_columns_for_values( df: pd.DataFrame, search_list: list, columns: list[str] = None ) -> dict[str, list[str]]: \"\"\" Search for specific values within specified columns of a DataFrame, handling NaN and None values. This function checks each column in the given DataFrame (or only the specified columns) for values present in the `search_list`. It handles special cases for `NaN` and `None` values to ensure they are correctly identified if they appear in both the column and the `search_list`. Parameters: - df (pd.DataFrame): The DataFrame in which to search for values. - search_list (list): A list of values to search for within the DataFrame columns. columns (list[str], optional): A list of column names to restrict the search to. If not provided, the function will search all columns. Returns: - dict[str, list[str]]: A dictionary where the keys are column names and the values are lists of matching values found in those columns. Example >>> df = pd.DataFrame({ ... 'A': [1, None, 3], ... 'B': [4, 5, np.nan], ... 'C': ['x', 'y', 'z'] ... }) >>> search_list = [None, 5, 'y', np.nan] >>> search_columns_for_values(df, search_list) Column 'A' contains: [None] Column 'B' contains: [5, nan] Column 'C' contains: ['y'] {'A': [None], 'B': [5, nan], 'C': ['y']} \"\"\" # Dictionary to store the columns and matching values matching_columns = {} # Iterate over the columns for column in columns if columns else df.columns: column_values = df[column] matching_values = set(column_values.dropna()) & set( [val for val in search_list if not pd.isna(val)] ) if pd.isna(column_values).any(): # Handle None or NaN specifically if it's in the search_list element_types = [type(element) for element in set(column_values)] if None in search_list and None in set(column_values): matching_values.add(None) if np.nan in search_list and float in element_types: matching_values.add(np.nan) # If there are any matches, add them to the dictionary if matching_values: matching_columns[column] = list(matching_values) # Print the matching columns and values for column, matches in matching_columns.items(): print(f\"Column '{column}' contains: {matches}\") return matching_columns def check_missing_values( df: pd.DataFrame, columns_to_check: list[str], missing_values_list: list[str] = None, ) -> None: \"\"\" Check for missing values in specified columns of a DataFrame and display the count of rows with missing values for each combination. This function identifies rows with missing values in the specified columns of a DataFrame. Missing values can be determined based on a provided list (`missing_values_list`) or by using the default method that checks for `NaN` values. The function displays the count of rows with missing values for each unique combination and the total number of rows with at least one missing value. Parameters: df (pd.DataFrame): The DataFrame to check for missing values. columns_to_check (list[str]): A list of column names to check for missing values. missing_values_list (list[str], optional): A list of values that should be considered as missing. If not provided, the function defaults to checking for `NaN` values. Returns: - None: The function does not return any value; instead, it prints the count of rows with missing values for", "source": "utils.py"}, {"content": "each combination and the total number of rows with at least one missing value. Example ------- >>> df = pd.DataFrame({ ... 'A': [1, None, 3, 'N/A'], ... 'B': [4, 5, np.nan, 6], ... 'C': ['x', 'y', None, 'z'] ... }) >>> columns_to_check = ['A', 'B', 'C'] >>> missing_values_list = [None, 'N/A'] >>> check_missing_values(df, columns_to_check, missing_values_list) Count of rows with missing values for each combination: A B C None False True 1 N/A False False 1 dtype: int64 Total rows with at least one missing value: 2 \"\"\" if missing_values_list: missing_mask = df[columns_to_check].isin(missing_values_list) else: missing_mask = df[columns_to_check].isnull() # Filter out rows where all selected columns have no missing values rows_with_any_missing = missing_mask[columns_to_check].any(axis=1) # Calculate the total number of rows with missing values total_missing_rows = rows_with_any_missing.sum() # Group by the mask and count the number of rows for each combination missing_combinations = ( missing_mask[rows_with_any_missing].groupby(columns_to_check).size() ) print(\"Count of rows with missing values for each combination:\") display(missing_combinations) print(f\"Total rows with at least one missing value: {total_missing_rows}\") # Categorize features to categorical, numerical and binary def categorize_features( df: pd.DataFrame, ) -> tuple[list[str], list[str], list[str]]: \"\"\" Categorizes the features (columns) of a pandas DataFrame into categorical, numerical, and binary. This function iterates through each column of the input DataFrame and classifies the columns into three categories based on their data types and unique values: - Binary features: Columns with no more than two unique values, including categorical columns with exactly two unique values. - Numerical features: Columns with more than two unique values that are not of type 'category'. - Categorical features: Columns with data type 'category' and more than two unique values. Parameters: - df (pd.DataFrame): The input DataFrame whose features are to be categorized. Returns: - Tuple[List[str], List[str], List[str]]: A tuple containing three lists: 1. cat_feat (List[str]): The list of names of categorical features with more than two unique values. 2. num_feat (List[str]): The list of names of numerical features. 3. bin_feat (List[str]): The list of names of binary features, including both numerical and categorical types. \"\"\" cat_feat = [] num_feat = [] bin_feat = [] for column in df.columns: if df[column].dtype == \"object\" or df[column].dtype == \"category\": if df[column].nunique() <= 2: bin_feat.append(column) else: cat_feat.append(column) else: num_feat.append(column) return cat_feat, num_feat, bin_feat def list_duplicate_counts(df: pd.DataFrame) -> None: # Filter the dataframe to keep only rows that are duplicated duplicates = df[df.duplicated(keep=False)] # Count the number of occurrences of each duplicated row duplicate_groups = ( duplicates.groupby(list(df.columns)).size().reset_index(name=\"count\") ) # Count the number of instances of rows that are duplicated once, twice, thrice, and so on instance_counts = ( duplicate_groups[\"count\"] .value_counts() .sort_index() .reset_index(name=\"Occurrences\") ) instance_counts.columns = [\"Count\", \"Occurrences\"] # Display the formatted table print() print(\"Duplicate counts and the number of occurrences:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(instance_counts.to_string(index=False)) # Display the characteristics of a dataframe def summarize_dataframe( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, target_column: str = None, ) -> None: \"\"\" Prints a summary of a pandas DataFrame including its shape, counts of missing values, duplicates, data types, and lists of categorical, numerical, and binary features. This function leverages the `categorize_features` function to classify features (excluding the identifier column by default)", "source": "utils.py"}, {"content": "into categorical, numerical, and binary based on their data types and unique value counts. It then prints the total number of observations and features, counts of missing values per column, number of duplicate rows, data types of each column, and lists of feature names categorized by their types. Parameters: - df (pd.DataFrame): The DataFrame to summarize. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - target_column (str, optional): The name of the target column. Defaults to None. Returns: - None: This function does not return a value; it prints the summary to the console. \"\"\" cat_feat, num_feat, bin_feat = categorize_features(df) if exclude_id_column: if id_column in cat_feat: cat_feat.remove(id_column) elif id_column in num_feat: num_feat.remove(id_column) elif id_column in bin_feat: bin_feat.remove(id_column) # Display the shape of the dataframe print(f\"Dataframe shape: {df.shape}\") print(f\"{df.shape[0]} rows\") print(f\"{df.shape[1]} columns\") # List missing values print(\"\\nNo. of missing values per column:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.isnull().sum()) # Count the number of rows with missing values missing_rows_count = df.isna().any(axis=1).sum() print(\"\\nNo. (percent) of rows with missing values:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(f\"{missing_rows_count} ({missing_rows_count/df.shape[0]*100:.2f}%)\") # Get the number of duplicates and display it num_duplicates = df.duplicated().sum() print(\"\\nNo. of duplicates:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df) if num_duplicates > 0 else None if id_column in df.columns: # List duplicates excluding key value or identifier column df_subset = df[df.columns.difference([id_column])] num_subset_duplicates = df_subset.duplicated().sum() print(\"\\nNo. of duplicates (excluding identifier column):\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_subset_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df_subset) if num_subset_duplicates > 0 else None # Display the number if duplicated values in the identifier column print(\"\\nNo. of duplicated identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].duplicated().sum()) # Display the number of unique values in the identifier column print(\"\\nNo. of unique identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].nunique()) # List data types print(\"\\nData types:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.dtypes) # For the feature type list, exclude the target column if it exists if target_column: if target_column in cat_feat: cat_feat.remove(target_column) elif target_column in num_feat: num_feat.remove(target_column) elif target_column in bin_feat: bin_feat.remove(target_column) # List categorical features print(\"\\nCategorical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(cat_feat) == 0: print(\"\u2014No categorical features\u2014\") else: for feature in cat_feat: print(feature) # List numerical features print(\"\\nNumerical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(num_feat) > 0: for feature in num_feat: print(feature) else: print(\"\u2014No numerical features\u2014\") # List binary features print(\"\\nBinary features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(bin_feat) == 0: print(\"\u2014No binary features\u2014\") else: for feature in bin_feat: print(feature) # List target print(\"\\nTarget:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if target_column: print(target_column) else: print(\"\u2014No target\u2014\") # List and count unique values in each categorical feature of a dataframe def list_unique_values( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, include_counts: bool = True, ) -> None: \"\"\" Prints the unique values for each categorical or binary feature in a DataFrame, including or excluding a specified identifier column. This function identifies categorical and binary features, by default, excluding a specified identifier column (such as a primary key), and then for each relevant feature, prints the feature name followed by its unique values, along with the count and proportion of each category. If the feature contains None or", "source": "utils.py"}, {"content": "NaN values, they are included in the output; otherwise, the unique values are sorted before being printed. The function also prints the count of unique values for each feature. Parameters: - df (pd.DataFrame): The DataFrame containing the features to analyze. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - include_counts (bool, optional): Include the count and proportion of unique values for each feature. Defaults to True. Returns: - None: This function does not return a value; it prints the summary to the console. Example usage: >>> df = pd.DataFrame({ 'ID': [1, 2, 3, 4], 'Color': ['Red', 'Blue', 'Green', 'Red'], 'Size': ['Small', 'Medium', 'Large', 'Medium'], 'Is_New': [1, 0, 1, 0] }) >>> list_unique_values(df, exclude_id_column=True, id_column='ID') Unique Values: Color \u203e\u203e\u203e\u203e\u203e ['Blue', 'Green', 'Red'] No.of unique values: 3 Counts and Proportions: Blue: 1 (25.0%) Green: 1 (25.0%) Red: 2 (50.0%) Size \u203e\u203e\u203e\u203e ['Large', 'Medium', 'Small'] No.of unique values: 3 Counts and Proportions: Large: 1 (25.0%) Medium: 2 (50.0%) Small: 1 (25.0%) Is_New \u203e\u203e\u203e\u203e\u203e\u203e [0, 1] No.of unique values: 2 Counts and Proportions: 0: 2 (50.0%) 1: 2 (50.0%) \"\"\" cat_feat, _, bin_feat = categorize_features(df) id_column = id_column if exclude_id_column else None feat_list = [x for x in cat_feat + bin_feat if x != id_column] print(\"Unique Values:\\n\") for feature in feat_list: print(feature) print(len(feature) * \"\u203e\") unique_values = df[feature].unique() if ( unique_values.dtype.name == \"category\" and unique_values.dtype.ordered ): print(unique_values) else: none_present = False nan_present = False unique_set = set(unique_values) if None in unique_set: none_present = True unique_set.remove(None) if np.nan in unique_set: nan_present = True unique_set.remove(np.nan) unique_list = sorted(unique_set) if none_present: unique_list.insert(0, None) if nan_present: unique_list.insert(0, np.nan) print(unique_list) # Print the number of unique values print(f\"No.of unique values: {df[feature].nunique()}\") # Calculate and print counts and proportions if include_counts: counts = df[feature].value_counts(dropna=False) total = len(df[feature]) proportions = (counts / total) * 100 result_df = pd.DataFrame( {\"Count\": counts, \"Proportion (%)\": proportions.round(2)} ) display(result_df) print() def create_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, ) -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\" Creates a cross-tabulation of counts and percentages for a target feature grouped by another feature within a provided DataFrame. This function computes a cross-tabulation table that counts occurrences of each category of the `count_target` grouped by each category of the `by_feature`. It also calculates the percentage representation of each `count_target` category within groups defined by `by_feature`. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` by which the `count_target` values are to be grouped. - p_threshold (float, optional): A threshold value used for determining statistical significance, primarily used in subsequent analysis (not directly affecting the cross-tabulation itself). Defaults to 0.05. Returns: - Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing two DataFrames: 1. The first DataFrame contains the absolute counts of `count_target` grouped by `by_feature`. 2. The second DataFrame contains the percentages of these counts relative to the total", "source": "utils.py"}, {"content": "counts for each `by_feature` group. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> counts, percentages = create_cross_tab(df, 'employee_turnover', 'department') Notes: Ensure that `df` contains the columns specified by `count_target` and `by_feature`. The function assumes these columns contain categorical data suitable for cross-tabulation. \"\"\" # Tabulate absolute numbers and percentages ctab = pd.crosstab(df[by_feature], df[count_target]) ctab_percent = ctab.divide(ctab.sum(axis=1), axis=0) * 100 return ctab, ctab_percent def plot_cross_tab( df: pd.DataFrame, count_target: str, by_feature: str, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Plot a cross-tabulation of counts and percentages of a target feature grouped by another feature, displayed as side-by-side bar charts in a single figure. This function generates two subplots: the first shows the absolute counts and the second shows the percentages. It supports customization of bar colors and label orientations. Args: - df (pd.DataFrame): The DataFrame containing the data to be plotted. - count_target (str): The name of the column in `df` that contains the target values to count. - by_feature (str): The name of the column in `df` to group the `count_target` values by. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function directly displays the plot and does not return any value. Example: >>> plot_cross_tab( df=data, count_target='scam_call', by_feature='country_prefix', colors=[\"#1f77b4\", \"#ff7f0e\"], label=True, rotate_label=False, rotate_xlabel=True ) Notes: Ensure that the DataFrame `df` includes the specified `count_target` and `by_feature` columns, and that the `colors` list (if provided) has enough color codes to differentiate the groups. \"\"\" # Perform cross tabulation cross_tab, cross_tab_percentage = create_cross_tab( df, count_target, by_feature ) # Set up the figure and axes for the subplot fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6)) fig.suptitle( f\"Count and Percentage of {count_target} by {by_feature}\", fontsize=18, y=1.06, ) # Plot 1: Absolute counts if colors: cross_tab.plot( kind=\"bar\", stacked=True, ax=axes[0], color=colors, legend=False ) else: cross_tab.plot(kind=\"bar\", stacked=True, ax=axes[0], legend=False) axes[0].set_ylabel(\"Count\") axes[0].set_xlabel(by_feature) axes[0].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the absolute counts if label: for rect in axes[0].patches: height = rect.get_height() y = rect.get_y() if height > 0: # Only place labels for non-empty segments axes[0].text( rect.get_x() + rect.get_width() / 2, y + height / 2, f\"{height:.0f}\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Plot 2: Percentages if colors: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], color=colors, legend=False ) else: cross_tab_percentage.plot( kind=\"bar\", stacked=True, ax=axes[1], legend=False ) axes[1].set_ylabel(\"Percentage\") axes[1].set_xlabel(by_feature) axes[1].tick_params(axis=\"x\", rotation=90 if rotate_xlabel else 0) # Adding labels for the percentages if label: for rect in axes[1].patches: height = rect.get_height() y = rect.get_y() percentage = height if height > 0: # Only place labels for non-empty segments axes[1].text( rect.get_x()", "source": "utils.py"}, {"content": "+ rect.get_width() / 2, y + height / 2, f\"{percentage:.1f}%\", ha=\"center\", va=\"center\", rotation=90 if rotate_label else 0, ) # Collect handles and labels from each subplot handles, legend_labels = [], [] for ax in axes: for handle, legend_label in zip(*ax.get_legend_handles_labels()): if legend_label not in legend_labels: handles.append(handle) legend_labels.append(legend_label) fig.legend( handles, legend_labels, title=count_target, loc=\"upper center\", ncols=len(legend_labels), bbox_to_anchor=(0.5, 1.01), frameon=False, ) plt.tight_layout() plt.show() def check_association( df: pd.DataFrame, count_target: str, by_feature: str, p_threshold: float = 0.05, include_plots: bool = True, colors: list[str] = None, rotate_xlabel: bool = False, label: bool = True, rotate_label: bool = False, ) -> None: \"\"\" Analyzes the statistical association between two categorical variables using the Chi-Square test. The function performs cross-tabulation of the count and percentage distribution of the `count_target`grouped by the `by_feature`, displays these statistics, and then conducts and reports the results of the Chi-Square test to determine if the association is statistically significant. Args: - df (pd.DataFrame): The DataFrame containing the data to be analyzed. - count_target (str): The name of the column in `df` that contains the categorical data for counting. - by_feature (str): The name of the column in `df` that represents the categorical groupings to be analyzed against `count_target`. - p_threshold (float, optional): The threshold for determining statistical significance in the Chi-Square test. Defaults to 0.05. - include_plots (bool, optional): Whether to include the cross-tabulation plots. Defaults to True. - colors (List[str], optional): A list of color hex codes to use for the bars in the plots. If not provided, default matplotlib colors are used. - rotate_xlabel (bool, optional): If True, rotates the x-axis labels to be vertical. Defaults to False. - label (bool, optional): If True, adds numerical labels to the bars in both subplots. Defaults to True. - rotate_label (bool, optional): If True, rotates the numerical labels inside the bars to be vertical. Defaults to False. Returns: - None: This function does not return any value but prints to the console, the cross-tabulation results, Chi-Square test results, and a statement about the statistical significance. Example: >>> df = pd.DataFrame({ 'department': ['sales', 'sales', 'hr', 'hr', 'it', 'it'], 'employee_turnover': ['left', 'stayed', 'left', 'stayed', 'left', 'stayed'] }) >>> check_association(df, 'employee_turnover', 'department') Notes: It is important that both `count_target` and `by_feature` columns exist in the DataFrame and are properly formatted as categorical data. The function outputs detailed results directly to the console, making it suitable for exploratory data analysis in interactive environments. \"\"\" # Plot cross tabulation if include_plots is True if include_plots: plot_cross_tab( df=df, count_target=count_target, by_feature=by_feature, colors=colors, rotate_xlabel=rotate_xlabel, label=label, rotate_label=rotate_label, ) # Perform cross tabulation ctab, ctab_percent = create_cross_tab( df, count_target, by_feature, p_threshold ) # Display the cross-tabulation of absolute counts and percentages print( f\"Investigating the association between {count_target} and {by_feature}\" ) print(f\"Absolute count of {count_target} values by {by_feature}:\") display(ctab) print(f\"\\nPercentage of {count_target} values by {by_feature}:\") display(ctab_percent) print() # Perform the Chi-Square test chi2, p, dof, expected = chi2_contingency(ctab) print(f\"Chi-square statistic: {chi2}\") print(f\"P threshold: {p_threshold}\") print(f\"P-value: {p}\") print(f\"Degrees of freedom: {dof}\") print(\"Expected frequencies:\") print(expected) print() # Interpret Chi-Square test based on the p-value if p < p_threshold: print( f\"There is a significant association between {count_target} and {by_feature}.\"", "source": "utils.py"}, {"content": ") else: print( f\"There is no significant association between {count_target} and {by_feature}.\" ) def get_unique_values_by_category( df: pd.DataFrame, col1: str, col2: str ) -> dict[str, list[str]]: \"\"\" Retrieves unique values from one column grouped by categories from another column. Parameters: - df (pd.DataFrame): The DataFrame containing the data. - col1 (str): The name of the column used to group categories. Each unique value in this column represents a category. - col2 (str): The name of the column from which unique values are extracted for each category in `col1`. Returns: - dict[str, list[str]]: A dictionary where each key is a unique value from `col1`, and the corresponding value is a list of unique values from `col2` that are associated with that category. Example: -------- Suppose `df` is a DataFrame with the following columns: 'Department' and 'Employee'. This function can be used to retrieve a list of unique employees in each department. >>> df = pd.DataFrame({ ... 'Department': ['HR', 'IT', 'HR', 'Finance', 'IT', 'Finance'], ... 'Employee': ['Alice', 'Bob', 'Alice', 'Charlie', 'Bob', 'Dave'] ... }) >>> get_unique_values_by_category(df, 'Department', 'Employee') {'HR': ['Alice'], 'IT': ['Bob'], 'Finance': ['Charlie', 'Dave']} \"\"\" unique_values_by_category = {} for value in df[col1].unique(): unique_values_by_category[value] = ( df[df[col1] == value][col2].unique().tolist() ) print(f\"{col1}: {value}\") print( f\"{len(unique_values_by_category[value])} {col2}: {unique_values_by_category[value]}\\n\" ) return unique_values_by_category", "source": "utils.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import pandas as pd # Query tables for column names def get_table_columns(tables: list[str], conn) -> dict: \"\"\" Retrieve column names for a list of tables from a database. This function queries the database for the names of columns in each specified table and returns a dictionary where the keys are table names and the values are lists of column names. Parameters: ---------- tables (list[str]): A list of table names to query for column names. conn: Connection object to the database. Returns: ------- - dict: A dictionary where each key is a table name, and the corresponding value is a list of column names present in that table. Example ------- >>> import sqlite3 >>> conn = sqlite3.connect('example.db') >>> tables = ['employees', 'departments'] >>> get_table_columns(tables, conn) { 'employees': ['id', 'name', 'position', 'salary'], 'departments': ['dept_id', 'dept_name'] } \"\"\" # Dictionary to store the column names table_columns = {} # Loop through each table and get the column names for table in tables: query = f\"\"\" SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table}'; \"\"\" df_col_names = pd.read_sql_query(sql=query, con=conn) # Add the table and column names to the dictionary table_columns[table] = sum(df_col_names.values.tolist(), []) return table_columns def count_missing_values(tables: list[str], conn) -> None: # Loop through each table and get the column names for table in tables: table_columns = get_table_columns([table], conn) # Query each column in each table for missing values for table, columns in table_columns.items(): print(f\"Table: {table}\") for column in columns: query = f\"\"\" SELECT SUM( CASE WHEN {column} IS NULL THEN 1 ELSE 0 END ) AS {column}_missing FROM {table}; \"\"\" print( f\"{pd.read_sql_query(sql=query, con=conn).iloc[0,0]} missing values in column '{column}'\" ) print()", "source": "query.py"}, {"content": "import pandas as pd import numpy as np from IPython.display import display # Categorize features to categorical, numerical and binary def categorize_features( df: pd.DataFrame, ) -> tuple[list[str], list[str], list[str]]: \"\"\" Categorizes the features (columns) of a pandas DataFrame into categorical, numerical, and binary. This function iterates through each column of the input DataFrame and classifies the columns into three categories based on their data types and unique values: - Binary features: Columns with no more than two unique values, including categorical columns with exactly two unique values. - Numerical features: Columns with more than two unique values that are not of type 'category'. - Categorical features: Columns with data type 'category' and more than two unique values. Parameters: - df (pd.DataFrame): The input DataFrame whose features are to be categorized. Returns: - Tuple[List[str], List[str], List[str]]: A tuple containing three lists: 1. cat_feat (List[str]): The list of names of categorical features with more than two unique values. 2. num_feat (List[str]): The list of names of numerical features. 3. bin_feat (List[str]): The list of names of binary features, including both numerical and categorical types. \"\"\" cat_feat = [] num_feat = [] bin_feat = [] for column in df.columns: if df[column].dtype == \"object\" or df[column].dtype == \"category\": if df[column].nunique() <= 2: bin_feat.append(column) else: cat_feat.append(column) else: num_feat.append(column) return cat_feat, num_feat, bin_feat def list_duplicate_counts(df: pd.DataFrame) -> None: # Filter the dataframe to keep only rows that are duplicated duplicates = df[df.duplicated(keep=False)] # Count the number of occurrences of each duplicated row duplicate_groups = ( duplicates.groupby(list(df.columns)).size().reset_index(name=\"count\") ) # Count the number of instances of rows that are duplicated once, twice, thrice, and so on instance_counts = ( duplicate_groups[\"count\"] .value_counts() .sort_index() .reset_index(name=\"Occurrences\") ) instance_counts.columns = [\"Count\", \"Occurrences\"] # Display the formatted table print() print(\"Duplicate counts and the number of occurrences:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(instance_counts.to_string(index=False)) # Display the characteristics of a dataframe def summarize_dataframe( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, target_column: str = None, ) -> None: \"\"\" Prints a summary of a pandas DataFrame including its shape, counts of missing values, duplicates, data types, and lists of categorical, numerical, and binary features. This function leverages the `categorize_features` function to classify features (excluding the identifier column by default) into categorical, numerical, and binary based on their data types and unique value counts. It then prints the total number of observations and features, counts of missing values per column, number of duplicate rows, data types of each column, and lists of feature names categorized by their types. Parameters: - df (pd.DataFrame): The DataFrame to summarize. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - target_column (str, optional): The name of the target column. Defaults to None. Returns: - None: This function does not return a value; it prints the summary to the console. \"\"\" cat_feat, num_feat, bin_feat = categorize_features(df) if exclude_id_column: if id_column in cat_feat: cat_feat.remove(id_column) elif id_column in num_feat: num_feat.remove(id_column) elif id_column in bin_feat: bin_feat.remove(id_column) # Display the shape of the dataframe print(f\"Dataframe shape:", "source": "utils.py"}, {"content": "{df.shape}\") print(f\"{df.shape[0]} rows\") print(f\"{df.shape[1]} columns\") # List missing values print(\"\\nNo. of missing values per column:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.isnull().sum()) # Count the number of rows with missing values missing_rows_count = df.isna().any(axis=1).sum() print(\"\\nNo. (percent) of rows with missing values:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(f\"{missing_rows_count} ({missing_rows_count/df.shape[0]*100:.2f}%)\") # Get the number of duplicates and display it num_duplicates = df.duplicated().sum() print(\"\\nNo. of duplicates:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df) if num_duplicates > 0 else None if id_column in df.columns: # List duplicates excluding key value or identifier column df_subset = df[df.columns.difference([id_column])] num_subset_duplicates = df_subset.duplicated().sum() print(\"\\nNo. of duplicates (excluding identifier column):\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(num_subset_duplicates) # If there are duplicates, count and display the occurrences list_duplicate_counts(df_subset) if num_subset_duplicates > 0 else None # Display the number if duplicated values in the identifier column print(\"\\nNo. of duplicated identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].duplicated().sum()) # Display the number of unique values in the identifier column print(\"\\nNo. of unique identifiers:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df[id_column].nunique()) # List data types print(\"\\nData types:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") print(df.dtypes) # For the feature type list, exclude the target column if it exists if target_column: if target_column in cat_feat: cat_feat.remove(target_column) elif target_column in num_feat: num_feat.remove(target_column) elif target_column in bin_feat: bin_feat.remove(target_column) # List categorical features print(\"\\nCategorical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(cat_feat) == 0: print(\"\u2014No categorical features\u2014\") else: for feature in cat_feat: print(feature) # List numerical features print(\"\\nNumerical features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(num_feat) > 0: for feature in num_feat: print(feature) else: print(\"\u2014No numerical features\u2014\") # List binary features print(\"\\nBinary features:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if len(bin_feat) == 0: print(\"\u2014No binary features\u2014\") else: for feature in bin_feat: print(feature) # List target print(\"\\nTarget:\") print(\"\u203e\u203e\u203e\u203e\u203e\u203e\u203e\") if target_column: print(target_column) else: print(\"\u2014No target\u2014\") # List and count unique values in each categorical feature of a dataframe def list_unique_values( df: pd.DataFrame, exclude_id_column: bool = True, id_column: str = None, include_counts: bool = True, ) -> None: \"\"\" Prints the unique values for each categorical or binary feature in a DataFrame, including or excluding a specified identifier column. This function identifies categorical and binary features, by default, excluding a specified identifier column (such as a primary key), and then for each relevant feature, prints the feature name followed by its unique values, along with the count and proportion of each category. If the feature contains None or NaN values, they are included in the output; otherwise, the unique values are sorted before being printed. The function also prints the count of unique values for each feature. Parameters: - df (pd.DataFrame): The DataFrame containing the features to analyze. - exclude_id_column (bool, optional): Exclude the identifier column from the analysis. Defaults to True. - id_column (str, optional): The name of the identifier column to exclude from the analysis. Defaults to None. - include_counts (bool, optional): Include the count and proportion of unique values for each feature. Defaults to True. Returns: - None: This function does not return a value; it prints the summary to the console. Example usage: >>> df = pd.DataFrame({ 'ID': [1, 2, 3, 4], 'Color': ['Red', 'Blue', 'Green', 'Red'], 'Size': ['Small', 'Medium', 'Large', 'Medium'], 'Is_New': [1, 0, 1, 0] }) >>> list_unique_values(df, exclude_id_column=True, id_column='ID') Unique Values: Color \u203e\u203e\u203e\u203e\u203e ['Blue', 'Green', 'Red'] No.of unique values: 3 Counts", "source": "utils.py"}, {"content": "and Proportions: Blue: 1 (25.0%) Green: 1 (25.0%) Red: 2 (50.0%) Size \u203e\u203e\u203e\u203e ['Large', 'Medium', 'Small'] No.of unique values: 3 Counts and Proportions: Large: 1 (25.0%) Medium: 2 (50.0%) Small: 1 (25.0%) Is_New \u203e\u203e\u203e\u203e\u203e\u203e [0, 1] No.of unique values: 2 Counts and Proportions: 0: 2 (50.0%) 1: 2 (50.0%) \"\"\" cat_feat, _, bin_feat = categorize_features(df) id_column = id_column if exclude_id_column else None feat_list = [x for x in cat_feat + bin_feat if x != id_column] print(\"Unique Values:\\n\") for feature in feat_list: print(feature) print(len(feature) * \"\u203e\") unique_values = df[feature].unique() if ( unique_values.dtype.name == \"category\" and unique_values.dtype.ordered ): print(unique_values) else: none_present = False nan_present = False unique_set = set(unique_values) if None in unique_set: none_present = True unique_set.remove(None) if np.nan in unique_set: nan_present = True unique_set.remove(np.nan) unique_list = sorted(unique_set) if none_present: unique_list.insert(0, None) if nan_present: unique_list.insert(0, np.nan) print(unique_list) # Print the number of unique values print(f\"No.of unique values: {df[feature].nunique()}\") # Calculate and print counts and proportions if include_counts: counts = df[feature].value_counts(dropna=False) total = len(df[feature]) proportions = (counts / total) * 100 result_df = pd.DataFrame( {\"Count\": counts, \"Proportion (%)\": proportions.round(2)} ) display(result_df) print()", "source": "utils.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "\"\"\" Data Transformation module @author: YH-Yeo \"\"\" import pandas as pd from scipy.stats.contingency import association from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder,StandardScaler from sklearn.model_selection import train_test_split def transform(data_path: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: \"\"\" Description of the function Parameters ---------- data_path: absolute path to the downloaded CSV file (obtain using: os.path.join(os.path.dirname(__file__), relative path) Returns ------- df: cleaned Dataframe with engineered features num_feat: list of numeric features cat_feat: list of categorical features \"\"\" df = load_dataset(data_path) drop_dup(df) drop_miss(df) num_feat = [\"age\", \"num_persons_worked_for_employer\", \"capital_gains\", \"capital_losses\", \"dividends_from_stocks\", \"weeks_worked_in_year\", \"wage_per_hour\"] target_feat = \"income_group\" cat_feat = [feature for feature in df if feature not in num_feat + [target_feat]] df, cat_feat = drop_cat_feat(df, cat_feat) df[target_feat] = df[target_feat].replace({\"- 50000.\": 0, \"50000+.\": 1}) # necessary for using Panda's corr function later df, cat_feat = feat_eng_edu(df, cat_feat) df, cat_feat = feat_eng_region(df, cat_feat) num_feat, cat_feat = feat_sel(df, cat_feat, num_feat, target_feat, 8) ord_feat = [\"qualification\"] nom_feat = [feature for feature in cat_feat if feature not in ord_feat] x = df[num_feat + nom_feat + ord_feat] y = df[target_feat] x_transformed = x_preprocessing(x, num_feat, nom_feat, ord_feat) X_train_, X_test_, y_train_, y_test_ = train_test_split(x_transformed, y, test_size=0.2, stratify=y, random_state=123) return X_train_, X_test_, y_train_, y_test_ #raise NotImplementedError def load_dataset(path: str) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- path: relative path to the downloaded CSV file (if absolute needed, obtain using: os.path.join(os.path.dirname(__file__), relative path) Returns ------- df: DataFrame containing the dataset \"\"\" df = pd.read_csv(path) return df def drop_dup(df: pd.DataFrame) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- df: DataFrame with duplicated rows Returns ------- df: DataFrame without duplicated rows \"\"\" # Drop the duplicated data-points (assuming complete duplication) df.drop([\"Unnamed: 0\", \"id\"], axis=1, inplace=True) # irrelevant column preventing the use of drop_duplicates df.drop_duplicates(inplace=True) df.reset_index(drop=True, inplace=True) return df def drop_miss(df: pd.DataFrame) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- df: DataFrame with missing data Returns ------- df: Dataframe where rows with missing data have been removed \"\"\" # Drop all rows with missing data df.dropna(inplace=True) df.reset_index(drop=True, inplace=True) return df def drop_cat_feat(df: pd.DataFrame, categories: list) -> tuple[pd.DataFrame, list]: \"\"\" Description of the function Parameters ---------- df: DataFrame with columns aggregated by another categories: List of categorical features Returns ------- df: DataFrame without columns aggregated by another \"\"\" df.drop(columns=[\"detailed_industry_recode\", \"detailed_occupation_recode\", \"detailed_household_and_family_stat\"], axis=1, inplace=True) df.drop(columns=[\"country_of_birth_self\"], axis=1, inplace=True) # aggregated by \"citizenship\", which also provides information on nationality and is more useful df.drop(columns=[\"state_of_previous_residence\", \"migration_code_change_in_msa\", \"migration_code_move_within_reg\"], axis=1, inplace=True) # aggregated by or less useful than \"migration_code_change_in_reg\" cat_feat = [feature for feature in categories if feature not in [\"detailed_industry_recode\", \"detailed_occupation_recode\", \"detailed_household_and_family_stat\", \"country_of_birth_self\", \"state_of_previous_residence\", \"migration_code_change_in_msa\", \"migration_code_move_within_reg\"]] return df, cat_feat def feat_eng_edu(df: pd.DataFrame, categories: list) -> tuple[pd.DataFrame, list]: \"\"\" Description of the function Parameters ---------- df: Cleaned DataFrame with \"education\" column categories: List of categorical features Returns ------- df: Cleaned DataFrame with \"qualification\" column \"\"\" df[\"qualification\"] = df.education.replace([\"Less than 1st grade\", \"1st 2nd 3rd or 4th grade\", \"5th or 6th grade\", \"7th and 8th grade\", \"9th grade\", \"10th grade\", \"11th grade\", \"12th grade no diploma\"], \"No Qualification\").replace([\"High school graduate\", \"Some college but no degree\"], \"Diploma\").replace([\"Associates degree-academic program\", \"Associates degree-occup /vocational\"], \"Associates Degree\").replace([\"Masters degree(MA MS MEng MEd MSW MBA)\", \"Prof school degree (MD DDS", "source": "datapipeline.py"}, {"content": "DVM LLB JD)\"], \"Graduate Degree\") df.drop(columns=[\"education\"], axis=1, inplace=True) categories.remove(\"education\") categories.append(\"qualification\") return df, categories def feat_eng_region(df: pd.DataFrame, categories: list) -> tuple[pd.DataFrame, list]: \"\"\" Description of the function Parameters ---------- df: Cleaned DataFrame with \"region_birth_father\" and \"region_birth_mother\" columns categories: List of categorical features Returns ------- df: Cleaned DataFrame with \"region_birth_father\"and \"region_birth_mother\" columns \"\"\" df[\"region_birth_father\"] = df.country_of_birth_father.replace(['United-States', 'Canada', 'Mexico'], \"North America\").replace(['Cuba', 'Dominican-Republic', 'Nicaragua', 'Guatemala', 'El-Salvador', 'Honduras', 'Panama'], \"Central America\").replace(['Peru', 'Columbia', 'Ecuador'], \"South America\").replace(['Puerto-Rico', 'Outlying-U S (Guam USVI etc)'], \"Puerto Rico or US Outlying\").replace(['Trinadad&Tobago', 'Jamaica', 'Haiti'], \"Carribean\").replace(['England', 'Germany', 'Hungary', 'Ireland', 'Italy', 'Poland', 'Greece', 'Yugoslavia', 'Scotland', 'France', 'Holand-Netherlands', 'Portugal'], \"Europe\").replace(['Hong Kong', 'China', 'India', 'Iran', 'South Korea', 'Vietnam', 'Philippines', 'Japan', 'Laos', 'Taiwan', 'Cambodia', 'Thailand'], \"Asia\") df.drop(columns=[\"country_of_birth_father\"], axis=1, inplace=True) df[\"region_birth_mother\"] = df.country_of_birth_mother.replace(['United-States', 'Canada', 'Mexico'], \"North America\").replace(['Cuba', 'Dominican-Republic', 'Nicaragua', 'Guatemala', 'El-Salvador', 'Honduras', 'Panama'], \"Central America\").replace(['Peru', 'Columbia', 'Ecuador'], \"South America\").replace(['Puerto-Rico', 'Outlying-U S (Guam USVI etc)'], \"Puerto Rico or US Outlying\").replace(['Trinadad&Tobago', 'Jamaica', 'Haiti'], \"Carribean\").replace(['England', 'Germany', 'Hungary', 'Ireland', 'Italy', 'Poland', 'Greece', 'Yugoslavia', 'Scotland', 'France', 'Holand-Netherlands', 'Portugal'], \"Europe\").replace(['Hong Kong', 'China', 'India', 'Iran', 'South Korea', 'Vietnam', 'Philippines', 'Japan', 'Laos', 'Taiwan', 'Cambodia', 'Thailand'], \"Asia\") df.drop(columns=[\"country_of_birth_mother\"], axis=1, inplace=True) categories.remove(\"country_of_birth_father\") categories.remove(\"country_of_birth_mother\") categories.extend([\"region_birth_father\", \"region_birth_mother\"]) return df, categories def feat_sel(df: pd.DataFrame, categories: list, num: list, target: str, num_cat_selected: int) -> tuple[list, list]: \"\"\" Description of the function Parameters ---------- df: DataFrame to perform Feature Selection on categories: List of categorical features num: List of numerical features target: Target feature num_cat_selected: number of categorical features to use in model Returns ------- num_feat: List of final selected numerical features cat_feat: List of final selected categorical features \"\"\" assoc_dict = {} for feature in categories: feature_crosstab = pd.crosstab(df[feature], df[target], margins=True, margins_name=\"subtotal\") assoc_dict[feature] = str(association(feature_crosstab)) assoc = pd.DataFrame(assoc_dict.items(), columns=['Categorical Feature', \"Cramer's V Association with Target feature\"]) assoc.sort_values(\"Cramer's V Association with Target feature\", ascending=False, inplace=True) selected_cat = assoc[\"Categorical Feature\"].head(num_cat_selected).tolist() num.remove(\"capital_losses\") num.remove(\"wage_per_hour\") return num, selected_cat def x_preprocessing(df: pd.DataFrame, numerical: list, nominal: list, ordinal: list) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- df: input DataFrame numerical: list of numerical features nominal: list of nominal features ordinal: list of Ordinal features Returns ------- df: input Dataframe with numerical features scaled and nominal and Ordinal features encoded \"\"\" preprocessor = ColumnTransformer([(\"Numerical\", StandardScaler(), numerical), (\"Nominal\", OneHotEncoder(sparse_output=False), nominal), (\"Ordinal\", OrdinalEncoder(categories=[[\"Children\", \"No Qualification\", \"Diploma\", \"Associates Degree\", \"Bachelors degree(BA AB BS)\", \"Graduate Degree\", \"Doctorate degree(PhD EdD)\"]]), ordinal)], remainder='passthrough' ) df_transformed = preprocessor.fit_transform(df) df_colnames = preprocessor.get_feature_names_out() return pd.DataFrame(df_transformed, columns=df_colnames)", "source": "datapipeline.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = RandomForestClassifier() def train(self, params, X_train, y_train): \"\"\" Description of the function. :param params: :param X_train: :param y_train: :return: \"\"\" self.model.set_params(**params) self.model.fit(X_train, y_train) y_pred_train = self.model.predict(X_train) f1_train = f1_score(y_train, y_pred_train, average='weighted') return f1_train def evaluate(self, X_test, y_test): \"\"\" Description of the function. :param X_test: ...... :param y_test: ...... :return: ...... \"\"\" y_pred_test = self.model.predict(X_test) f1_test = f1_score(y_test, y_pred_test, average='weighted') return f1_test def get_default_params(self): \"\"\" Description of the function. :return: ...... \"\"\" default_params = {'n_estimators': 200, 'max_depth': 20, 'min_samples_leaf': 10, 'max_features': 'sqrt', 'random_state': 123 } #'min_samples_split': 2,'bootstrap': True, return default_params", "source": "model.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import pandas as pd from sklearn.utils import resample from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split class DataPipeline(): def __init__(self, std: bool =True, balance: bool =False) -> None: \"\"\" Parameters ---------- std : bool, optional Whether to standardize the data, by default True resample : bool, optional Whether to resample the data to make it less imbalanced, by default False \"\"\" self.scaler = None self.balance = balance if std: self.scaler = StandardScaler() def balancing(self, df: pd.DataFrame, factor: int) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- df: imbalanced DataFrame factor: resampling factor Returns ------- data_resampled: balanced DataFrame \"\"\" data_maj = df[df[\"Class\"]==0] data_mino = df[df[\"Class\"]==1] data_maj_downsampled = resample(data_maj, replace=False, n_samples=round(len(data_maj)/factor)) data_mino_upsampled = resample(data_mino, n_samples=len(data_mino)*factor) data_resampled = pd.concat([data_maj_downsampled, data_mino_upsampled]) return data_resampled def transform_data(self, data_path: str, factor: int =1) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: \"\"\" Description of the function Parameters ---------- data_path: absolute path to the downloaded CSV file (obtain using: os.path.join(os.path.dirname(__file__), relative path) factor: resampling factor, if applicable (optional) Returns ------- df: cleaned Dataframe with engineered features \"\"\" data = pd.read_csv(data_path, index_col=0) self.rem_outliers(data) self.drop_dup(data) data[\"Class\"] = data[\"Class\"].replace({False: 0, True: 1}) X = data.iloc[:, :-1] y = data.iloc[:, -1:] X_scaled = self.scaler.fit_transform(X.values) X_transformed = pd.DataFrame(X_scaled) X_train_, X_test_, y_train_, y_test_ = train_test_split(X_transformed, y, test_size=0.2, stratify=y, random_state=123) if self.balance: data_train = pd.concat([X_train_, y_train_], axis=1) balanced = self.balancing(data_train, factor) X_train_ = balanced.iloc[:, :-1] y_train_ = balanced.iloc[:, -1:] return X_train_, X_test_, y_train_, y_test_ def rem_outliers(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Description of the function Parameters ---------- df: DataFrame with outliers in Amount feature Returns ------- df: Dataframe with outliers removed \"\"\" # Drop all rows with missing data df = df.loc[df[\"Amount\"]<15000, :] return df def drop_dup(self, df: pd.DataFrame) -> pd.DataFrame: \"\"\" Parameters ---------- df: DataFrame with duplicated rows Returns ----- df: DataFrame without duplicated rows \"\"\" # Drop the duplicated data-points (assuming complete duplication) df.drop_duplicates(inplace=True) df.reset_index(drop=True, inplace=True) return df", "source": "datapipeline.py"}, {"content": "class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): raise NotImplementedError def forward(self, features): \"\"\" Takes in the features returns the prediction \"\"\" raise NotImplementedError def loss(self, predictions, label): \"\"\" Takes in the predictions and label returns the training loss \"\"\" raise NotImplementedError def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" raise NotImplementedError", "source": "mlp.py"}, {"content": "class Datapipeline(): def __init__(self): raise NotImplementedError def transform(self): raise NotImplementedError", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [80, -685, -1028], [-618, 573, -126], [265, 391, -100] ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch class CNNModel(torch.nn.Module): def __init__(self, your_args): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "cnn_model.py"}, {"content": "import pandas as pd from sklearn.preprocessing import StandardScaler class DataPipeline: def __init__(self, test_size, X_lag: dict, horizon: int =1) -> None: self.TEST_SIZE = test_size self.X_LAG = X_lag self.HORIZON = horizon def run_data_pipeline(self, data_path: str) -> tuple[pd.DataFrame, pd.Series, pd.DataFrame, pd.Series]: df = pd.read_csv(data_path, index_col=0) df[\"time\"] = pd.to_datetime(df[[\"year\", \"month\", \"day\", \"hour\"]]) df.set_index(\"time\", inplace=True) df.fillna(method='bfill', inplace=True) df, X = self.create_lagged_X(df, self.X_LAG) df[\"Target\"] = df[\"pm2.5\"].shift(1-self.HORIZON) df.dropna(inplace=True) df = df[X+[\"Target\"]] split_index = int(len(df) * (1 - self.TEST_SIZE)) df_train, df_test = df[:split_index], df[split_index:] scaler = StandardScaler() df_train.iloc[:, :] = scaler.fit_transform(df_train.iloc[:, :]) df_test.iloc[:, :] = scaler.transform(df_test.iloc[:, :]) X_train, y_train = df_train.drop(\"Target\", axis=1), df_train[\"Target\"] X_test, y_test = df_test.drop(\"Target\", axis=1), df_test[\"Target\"] return X_train, y_train, X_test, y_test def create_lagged_X(self, df: pd.DataFrame, X_lag: dict) -> tuple[pd.DataFrame, list]: X_col = [] for feature, max_lag in X_lag.items(): X_col.extend([f'{feature}_lag{i}' for i in range(1, max_lag+1)]) for lag in range(1, max_lag+1): df[f'{feature}_lag{lag}'] = df[feature].shift(lag) return df, X_col", "source": "datapipeline.py"}, {"content": "from src.data_pipeline import Datapipeline from src.evaluation import evaluate from src.model import Model def run_experiment(data_path, lags=[]): # Read data # Perform data split for lag in lags: # Fit and evaluate over for each lag value metrics_dict[f\"lag_{lag}\"] = { \"train_error\": train_error \"test_error\": test_error } return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error class ForecastModel: def __init__(self, n_estimators: int =100) -> None: self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=123) def fit(self, X: pd.DataFrame, y: pd.DataFrame) -> None: self.model.fit(X, y) def evaluate(self, X_train: pd.DataFrame, y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> tuple[float, float]: y_train_pred = self.predict(X_train) train_error = mean_squared_error(y_train, y_train_pred) y_test_pred = self.predict(X_test) test_error = mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, x: pd.DataFrame) -> float: return self.model.predict(x) def feature_importances(self): if hasattr(self.model, 'feature_importances_'): return self.model.feature_importances_ else: return None", "source": "ml_model.py"}, {"content": "import torch class RNNModel(torch.nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size): # Your code here pass def forward(self, x): # Your code here pass def fit(self, dataloader): # Your code here pass def predict(self, dataloader): # Your code here pass def evaluate(self, dataloader): # Your code here return mse", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): # Your code here pass def __len__(self): return self.length def __getitem__(self, idx): # Your code here return features, labels", "source": "windowing.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [[0.53, 0.23, 0.68, 0.45]]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import warnings from typing import Tuple import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from category_encoders.count import CountEncoder from sklearn.compose import ColumnTransformer from sklearn.pipeline import make_pipeline from sklearn.impute import SimpleImputer from sklearn.preprocessing import MinMaxScaler, OrdinalEncoder def transform(data_path) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]: \"\"\" Description of the function. :param data_path: file path to the data file :return: \"\"\" df = pd.read_csv(data_path) id_var = [\"id\"] # variables to be considered as categorical regroup_vars = [ \"detailed_industry_recode\", \"detailed_occupation_recode\", \"year\", \"own_business_or_self_employed\", \"veterans_benefits\", ] categorical_vars = [ var for var in df.select_dtypes(include=\"object\").columns.tolist() + regroup_vars if var != \"income_group\" ] # convert to categorical for var in regroup_vars: df[var] = pd.Categorical(df[var]) numeric_vars = [ var for var in df.select_dtypes(include=\"int64\").columns.tolist() if var not in (id_var + regroup_vars) ] # Define X as input features and y as the outcome variable X = df[numeric_vars + categorical_vars] y = df[[\"income_group\"]] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42, stratify=df[\"income_group\"] ) # Build a preprocessing step for numeric features prep_num = make_pipeline(MinMaxScaler()) # Build a preprocessing step for nominal features ordinal_vars = [\"year\"] categorical_vars = [var for var in categorical_vars if var not in ordinal_vars] prep_cat = make_pipeline( SimpleImputer(strategy=\"constant\", fill_value=\"?\"), # HashingEncoder(n_components=10) CountEncoder( handle_unknown=\"value\", # handle_missing=\"value\" ), ) # Build a preprocessing step for ordinal features prep_ord = make_pipeline(OrdinalEncoder()) # If using sklearn's pipelines, use sklearn.compose.ColumnTransformer to combine these pipelines, # taking care to apply each of them to the correct set of columns. Any feature not modified by # any of the pipelines can be passed through to the output dataframe (if needed). warnings.simplefilter(action=\"ignore\", category=FutureWarning) ct = ColumnTransformer( [ (\"numeric_preprocess\", prep_num, numeric_vars), (\"categorical_preprocess\", prep_cat, categorical_vars), (\"ordinal_preprocess\", prep_ord, ordinal_vars), ], remainder=\"passthrough\", # verbose_feature_names_out=False, ) X_train = ct.fit_transform(X_train) X_test = ct.transform(X_test) # Encode y y_enc = OrdinalEncoder(categories=[[\"- 50000.\", \"50000+.\"]]) y_train = y_enc.fit_transform(y_train).reshape(-1) y_test = y_enc.transform(y_test).reshape(-1) return X_train, X_test, y_train, y_test", "source": "datapipeline.py"}, {"content": "import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler, OneHotEncoder from sklearn.feature_extraction import FeatureHasher from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline def transform(data_path): \"\"\" Loads data from a CSV file, cleans it, engineers features, preprocesses it, and splits it into training and testing sets. This function performs the following steps: 1. Loads the data from the specified path 2. Cleans the data by replacing placeholder values with NaN 3. Engineers new features 4. Splits the data into features (X) and target variable (y) 5. Converts the target variable to binary 6. Splits the data into training and testing sets 7. Applies preprocessing steps including imputation, scaling, one-hot encoding, and feature hashing 8. Returns the preprocessed training and testing data as numpy arrays :param data_path: String, path to the CSV file containing the data :return: Tuple of four numpy arrays (X_train, X_test, y_train, y_test) X_train and X_test are 2D arrays of preprocessed features y_train and y_test are 1D arrays of binary target variables \"\"\" # Load the data df = pd.read_csv(data_path) # Data Cleaning def replace_placeholder_values(df, placeholder_list): df.replace(placeholder_list, np.nan, inplace=True) return df placeholder_list = ['?', 'N/A', 'NA', '--'] df = replace_placeholder_values(df, placeholder_list) # Feature Engineering def education_to_years(education): edu_years = { 'Children': 0, 'Less than 1st grade': 0, '1st 2nd 3rd or 4th grade': 2, '5th or 6th grade': 5, '7th and 8th grade': 7, '9th grade': 9, '10th grade': 10, '11th grade': 11, '12th grade no diploma': 12, 'High school graduate': 12, 'Some college but no degree': 14, 'Associates degree-occup /vocational': 14, 'Associates degree-academic program': 14, 'Bachelors degree(BA AB BS)': 16, 'Masters degree(MA MS MEng MEd MSW MBA)': 18, 'Doctorate degree(PhD EdD)': 21, 'Prof school degree (MD DDS DVM LLB JD)': 20 } return edu_years.get(education, 0) df['education_years'] = df['education'].apply(education_to_years) df['income_education_ratio'] = (df['wage_per_hour'] + 0.01) / (df['education_years'] + 0.01) df['financial_sophistication'] = ((df['capital_gains'] > 0).astype(int) + (df['capital_losses'] > 0).astype(int) + (df['dividends_from_stocks'] > 0).astype(int)) df['work_life_balance'] = np.where( (df['age'] > 0) & (df['weeks_worked_in_year'] > 0), (df['weeks_worked_in_year'] * df['wage_per_hour']) / (df['age'] * 52 * 40), 0 ) # Clip the work_life_balance to avoid extreme values df['work_life_balance'] = np.clip(df['work_life_balance'], 0, 1) # Define X as input features and y as the outcome variable X = df.drop(['income_group'], axis=1) y = df['income_group'] # Convert target variable to binary y = (y == '50000+.').astype(int) # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42) # Define feature types numeric_features = [ 'age', 'detailed_industry_recode', 'detailed_occupation_recode', 'wage_per_hour', 'num_persons_worked_for_employer', 'own_business_or_self_employed', 'weeks_worked_in_year', 'year', 'capital_gains', 'capital_losses', 'dividends_from_stocks', 'veterans_benefits', 'education_years', 'income_education_ratio', 'financial_sophistication', 'work_life_balance' ] onehot_features = [ 'education', 'enroll_in_edu_inst_last_wk', 'marital_stat', 'race', 'sex', 'hispanic_origin', 'member_of_a_labor_union', 'reason_for_unemployment', 'full_or_part_time_employment_stat', 'detailed_household_and_family_stat', 'class_of_worker', 'detailed_household_summary_in_household', 'family_members_under_18', 'tax_filer_stat', 'migration_code_change_in_msa', 'migration_code_change_in_reg', 'fill_inc_questionnaire_for_veteran_s_admin', 'migration_code_move_within_reg', 'live_in_this_house_1_year_ago', 'migration_prev_res_in_sunbelt' ] hashing_features = [ 'country_of_birth_father', 'country_of_birth_mother', 'country_of_birth_self', 'citizenship', 'major_industry_code', 'major_occupation_code', 'region_of_previous_residence', 'state_of_previous_residence' ] # Build preprocessing steps numeric_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler()) ]) nominal_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore')) ]) hashing_transformer = Pipeline(steps=[ ('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('hasher', FeatureHasher(n_features=10, input_type='string')) ]) # Combine the preprocessing steps using ColumnTransformer preprocessor = ColumnTransformer( transformers=[ ('num', numeric_transformer, numeric_features), ('onehot', nominal_transformer, onehot_features), ('hash', hashing_transformer, hashing_features) ])", "source": "datapipeline_V1.py"}, {"content": "# Fit the preprocessor to the training data and transform both training and test data X_train_preprocessed = preprocessor.fit_transform(X_train) X_test_preprocessed = preprocessor.transform(X_test) return X_train_preprocessed, X_test_preprocessed, y_train, y_test # Example usage: # X_train, X_test, y_train, y_test = transform('path_to_your_data.csv')", "source": "datapipeline_V1.py"}, {"content": "import numpy as np from scipy import sparse class DecisionTree: @staticmethod def gini(left, right): \"\"\" Calculate the Gini impurity for a split dataset. Args: left (list): A list of labels for the left split right (list): A list of labels for the right split Returns: float: The weighted Gini impurity \"\"\" def gini_impurity(labels): if len(labels) == 0: return 0 class_counts = {} # counts occurence of each unique label in the input list. The numerator for p_i for label in labels: if label not in class_counts: class_counts[label] = 0 class_counts[label] += 1 impurity = 1 # formula of gini impurity, where starts with 1 for count in class_counts.values(): prob = count / len(labels) # obtain proportion of each class using class_count / total impurity -= prob ** 2 # subtract p_i^2 from 1 and for loop repeats for all classes return impurity left_impurity = gini_impurity(left) right_impurity = gini_impurity(right) total_samples = len(left) + len(right) weighted_impurity = (len(left) / total_samples) * left_impurity + \\ (len(right) / total_samples) * right_impurity return weighted_impurity def __init__(self, max_depth=None): \"\"\" Initialize the DecisionTree. Args: max_depth (int, optional): The maximum depth of the tree. Defaults to None. \"\"\" # Initialize any necessary attributes for the DecisionTree class self.max_depth = max_depth self.tree = None def fit(self, X, y): \"\"\" Build the decision tree using the training data. Args: X (array-like): m*n array of m training examples with n features y (array-like): m*1 array of labels \"\"\" # Convert X and y to numpy arrays if they're not already X = np.array(X) y = np.array(y) # Ensure y is 1D according to Args if y.ndim > 1: y = y.ravel() # If X is 1D, reshape it to 2D according to Args if X.ndim == 1: X = X.reshape(-1, 1) self.n_features = X.shape[1] #to use in other methods self.n_classes = len(np.unique(y)) #to use in other methods self.tree = self._grow_tree(X, y) def _grow_tree(self, X, y, depth=0): \"\"\" Recursively grow the decision tree. Args: X (numpy.ndarray): The feature matrix y (numpy.ndarray): The target labels depth (int): The current depth of the tree Returns: dict: A node of the decision tree (either a leaf or an internal node) \"\"\" n_samples, n_features = X.shape n_labels = len(np.unique(y)) # Stopping criteria to prevent overfitting and infinite recursion if (self.max_depth is not None and depth >= self.max_depth) or n_labels == 1 or n_samples < 2: # stop if max depth is no infinite and it exceeds the defined max depth or # also stop if all remaining samples in the node belong to same class (i.e. only 1 unique class) or # also stop is fewer than two samples left to split since need at least 2 samples to split meaningfully leaf_value = self._most_common_label(y) #determines the most common label among the remaining samples (see below) return {\"type\": \"leaf\", \"value\": leaf_value} # Find the best split best_split = self._best_split(X, y) # If no valid split is found, return a leaf node if best_split is None: leaf_value = self._most_common_label(y) return {\"type\": \"leaf\", \"value\": leaf_value} # Split the data left_indices = X[:, best_split[\"feature\"]] < best_split[\"threshold\"] right_indices = ~left_indices # Recursively build", "source": "decision_tree.py"}, {"content": "the left and right subtrees self._grow_tree(X[left_indices], y[left_indices], depth + 1) self._grow_tree(X[right_indices], y[right_indices], depth + 1) # return { # \"type\": \"node\", # \"feature\": best_split[\"feature\"], # \"threshold\": best_split[\"threshold\"], # \"left\": left, # \"right\": right # } def _best_split(self, X, y): \"\"\" Find the best split for a node. Args: X (numpy.ndarray): The feature matrix y (numpy.ndarray): The target labels Returns: dict or None: Information about the best split, or None if no valid split is found \"\"\" best_split = None # used to store information about the best split best_gini = float(\"inf\") # initialised to +ve inf so any split will be better for feature in range(self.n_features): thresholds = np.unique(X[:, feature]) #find all unique values in each feature and set as threshold for threshold in thresholds: #loops through each unique value of the current feature left_indices = X[:, feature] < threshold # boolean mask for the data, true if feature value < threshold right_indices = ~left_indices # boolean mask, opposite of left_indices if len(y[left_indices]) > 0 and len(y[right_indices]) > 0: # ensures no empty node, left and right split must have at least 1 sample gini = self.gini(y[left_indices], y[right_indices]) # calculate gini impurity using gini method if gini < best_gini: # if this split has lower Gini than best, update best_gini best_gini = gini best_split = { \"feature\": feature, \"threshold\": threshold, \"gini\": gini } return best_split def _most_common_label(self, y): #finds the most frequent label in a set of labels \"\"\" Find the most common label in a set of labels. Args: y (numpy.ndarray): Array of labels Returns: int: The most common label \"\"\" return np.bincount(y).argmax() def predict(self, X): \"\"\" Predict class labels for samples in X. Args: X (array-like): n_samples * n_features array of samples to predict Returns: array: Predicted class label for each sample in X \"\"\" # Ensure X is a 2D numpy array X = np.array(X) if X.ndim == 1: X = X.reshape(1, -1) # Make predictions for each sample return np.array([self._predict_sample(sample) for sample in X]) def _predict_sample(self, sample): \"\"\" Predict class label for a single sample by traversing the decision tree. Args: sample (array-like): 1D array representing a single sample Returns: int: Predicted class label for the sample \"\"\" node = self.tree # start at root of the tree i.e. top level node while node['type'] != 'leaf': # loop until reach a leaf note if sample[node['feature']] < node['threshold']: # check sample value less than threshold node = node['left'] # move to left node if yes else: node = node['right'] # move to right node if no return node['value'] # return value of leaf when reach leaf node and exit the loop # Add other methods for the DecisionTree class as needed", "source": "decision_tree.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = None def train(self, params, X_train, y_train): \"\"\" Train the Random Forest model. :param params: Dictionary of parameters for the Random Forest model :param X_train: Training features :param y_train: Training labels :return: F1 score on the training data \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it self.model = RandomForestClassifier(**params) self.model.fit(X_train, y_train) y_train_pred = self.model.predict(X_train) train_f1 = f1_score(y_train, y_train_pred) print(f\"Training F1 Score: {train_f1}\") return train_f1 def evaluate(self, X_test, y_test): \"\"\" Evaluate the trained Random Forest model. :param X_test: Test features :param y_test: Test labels :return: F1 score on the test data \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score if self.model is None: raise ValueError(\"Model has not been trained yet. Call train() first.\") y_pred = self.model.predict(X_test) test_f1 = f1_score(y_test, y_pred) print(f\"Test F1 Score: {test_f1}\") return test_f1 def get_default_params(self): \"\"\" Get default parameters for the Random Forest model. :return: Dictionary of default parameters \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return { 'n_estimators': 300, 'max_depth': 25, 'min_samples_split': 12, 'max_features': 'sqrt', 'random_state': 42 }", "source": "model.py"}, {"content": "import numpy as np from src.decision_tree import DecisionTree class RandomForest: \"\"\" Random Forest Classifier This class implements a Random Forest classifier, an ensemble learning method that operates by constructing multiple decision trees during training and outputting the class that is the mode of the classes output by individual trees for classification tasks. Attributes: n_trees (int): The number of trees in the forest. max_depth (int): The maximum depth of each tree. subsample_size (float): The proportion of samples to use for each tree. sample_with_replacement (bool): Whether to sample with replacement. feature_proportion (float): The proportion of features to consider for each tree. trees (list): The list of trained decision trees and their feature indices. \"\"\" def __init__(self, n_trees=5, max_depth=None, subsample_size=1.0, sample_with_replacement=True, feature_proportion=1.0): \"\"\" Initialize the RandomForest. Args: n_trees (int): Number of trees in the forest. max_depth (int): Maximum depth of each tree. If None, nodes are expanded until all leaves are pure. subsample_size (float): Proportion of samples to use for each tree (0.0 to 1.0). sample_with_replacement (bool): Whether to sample instances with replacement. feature_proportion (float): Proportion of features to consider for each tree (0.0 to 1.0). \"\"\" self.n_trees = n_trees self.max_depth = max_depth self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.feature_proportion = feature_proportion self.trees = [] def fit(self, X, y): \"\"\" Fit the random forest to the training data. This method trains multiple decision trees on bootstrapped samples of the data, with each tree considering a random subset of features. Args: X (numpy.ndarray): The input samples, shape (n_samples, n_features). y (numpy.ndarray): The target values, shape (n_samples,). \"\"\" self.trees = [] n_samples, n_features = X.shape # Calculate the number of samples and features for subsampling subsample_n = int(n_samples * self.subsample_size) n_features_subset = max(1, int(n_features * self.feature_proportion)) for _ in range(self.n_trees): # loop over each trees # Sample instances (with or without replacement) if self.sample_with_replacement: instance_indices = np.random.choice(n_samples, size=subsample_n, replace=True) else: # When sampling without replacement, ensure we don't ask for more samples than available size = min(subsample_n, n_samples) # min of subsample size defined or total number of samples instance_indices = np.random.choice(n_samples, size=size, replace=False) # Randomly select a subset of features for this tree feature_indices = np.random.choice(n_features, size=n_features_subset, replace=False) # Create subsets of X and y based on the sampled instances and features X_subsample = X[instance_indices][:, feature_indices] y_subsample = y[instance_indices] # Create and train a new decision tree tree = DecisionTree(max_depth=self.max_depth) # each tree is a decision tree with a subsample and proportion of feature tree.fit(X_subsample, y_subsample) # Store the tree along with its feature indices for later prediction self.trees.append((tree, feature_indices)) def predict(self, X): \"\"\" Predict class labels for samples in X. Args: X (numpy.ndarray): The input samples, shape (n_samples, n_features). Returns: numpy.ndarray: Predicted class labels, shape (n_samples,). \"\"\" # Make predictions with all trees tree_predictions = [] # initialise a list for tree, feature_indices in self.trees: # Use only the features that were selected for this tree X_subset = X[:, feature_indices] tree_predictions.append(tree.predict(X_subset)) # Stack predictions from all trees tree_predictions = np.stack(tree_predictions, axis=1) # stacking means in the array each list is the prediction for the sample X from all trees and you will have X number", "source": "random_forest.py"}, {"content": "of list # Use voting mechanism (majority vote) to determine final prediction return np.array([np.bincount(pred).argmax() for pred in tree_predictions]) # count the 0s and 1s in each and return the index (0,1) def predict_proba(self, X): \"\"\" Predict class probabilities for samples in X. Args: X (numpy.ndarray): The input samples, shape (n_samples, n_features). Returns: numpy.ndarray: Predicted class probabilities, shape (n_samples, n_classes). \"\"\" # Make predictions with all trees tree_predictions = [] for tree, feature_indices in self.trees: # Use only the features that were selected for this tree X_subset = X[:, feature_indices] tree_predictions.append(tree.predict(X_subset)) # Stack predictions from all trees tree_predictions = np.stack(tree_predictions, axis=1) # Calculate class probabilities n_samples = X.shape[0] # no. of rows in X n_classes = len(np.unique(tree_predictions)) # number of unique classes to predict probas = np.zeros((n_samples, n_classes)) # initialised array with zero with rows = n_samples and cols = n_classes for i in range(n_samples): # Count occurrences of each class for this sample across all trees class_counts = np.bincount(tree_predictions[i], minlength=n_classes) # count occurence of each class prediction across all trees for each sample # Convert counts to probabilities probas[i] = class_counts / self.n_trees # count of class divide by total number of trees return probas", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import RobustScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer class DataPipeline: \"\"\" A class for preprocessing data using sklearn pipelines. This class provides methods to load, transform, and preprocess training and test data for machine learning tasks. Attributes: preprocessor (ColumnTransformer): The sklearn preprocessor for transforming data. float_columns (list): List of column names with float data type. \"\"\" def __init__(self): self.preprocessor = None self.float_columns = None def log_transform(self, X, epsilon=1e-8): \"\"\" Apply logarithmic transformation to the input data. Args: X (numpy.ndarray): Input data to transform. epsilon (float): Small value to add before taking log to avoid log(0). Returns: numpy.ndarray: Log-transformed data. \"\"\" return np.log(X + 1 + epsilon) def create_preprocessor(self, float_columns): \"\"\" Create a ColumnTransformer preprocessor for the data. Args: float_columns (list): List of column names with float data type. Returns: ColumnTransformer: Preprocessor for transforming the data. \"\"\" return ColumnTransformer( transformers=[ ('num', Pipeline([ # ('log', FunctionTransformer(self.log_transform, validate=False)), ('scaler', RobustScaler()) ]), float_columns) ]) def transform_train_data(self, train_data_path): \"\"\" Load and preprocess the training data. Args: train_data_path (str): Path to the training data CSV file. Returns: tuple: (X_train, y_train) preprocessed features and labels as numpy arrays. \"\"\" # Load data df = pd.read_csv(train_data_path) # Separate features and target X = df.drop('Class', axis=1) y = df['Class'] # Identify float columns self.float_columns = X.select_dtypes(include=['float64']).columns.tolist() # Create and fit preprocessor self.preprocessor = self.create_preprocessor(self.float_columns) X_preprocessed = self.preprocessor.fit_transform(X) # Convert y to numpy array y_array = y.to_numpy() print(\"Training dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_array, return_counts=True)) return X_preprocessed, y_array def transform_test_data(self, test_data_path): \"\"\" Load and preprocess the test data using the preprocessor fit on the training data. Args: test_data_path (str): Path to the test data CSV file. Returns: tuple: (X_test, y_test) preprocessed features and labels as numpy arrays. Raises: ValueError: If the preprocessor is not fitted (transform_train_data not called first). \"\"\" if self.preprocessor is None: raise ValueError(\"Preprocessor not fitted. Run transform_train_data first.\") # Load data df = pd.read_csv(test_data_path) # Separate features and target X = df.drop('Class', axis=1) y = df['Class'] # Apply preprocessing X_preprocessed = self.preprocessor.transform(X) # Convert y to numpy array y_array = y.to_numpy() print(\"Test dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_array, return_counts=True)) return X_preprocessed, y_array # Example usage: # pipeline = DataPipeline() # X_train, y_train = pipeline.transform_train_data('path/to/train_data.csv') # X_test, y_test = pipeline.transform_test_data('path/to/test_data.csv')", "source": "datapipeline.py"}, {"content": "# pylint: disable=import-error import numpy as np from datapipeline import DataPipeline from resampling import resample_data from train_batchnorm import train_model, print_final_metrics from sklearn.metrics import classification_report, confusion_matrix def main(): \"\"\" Execute the main machine learning pipeline. This function performs the following steps: 1. Data Pipeline: Load and preprocess the training and test data. 2. Resampling: Balance the training data. 3. Model Training: Create and train a neural network model. 4. Evaluation: Evaluate the model's performance on the test set. The function prints progress updates and final metrics to the console. \"\"\" # Step 1: Data Pipeline print('Step 1: Running Data Pipeline') pipeline = DataPipeline() X_train, y_train = pipeline.transform_train_data('data/train_data.csv') X_test, y_test = pipeline.transform_test_data('data/test_data.csv') print(f'Training data shape: {X_train.shape}, Test data shape: {X_test.shape}') # Step 2: Resampling print('\\nStep 2: Resampling the training data') X_resampled, y_resampled = resample_data(X_train, y_train) print(f'Resampled training data shape: {X_resampled.shape}') print(f'Original class distribution: {np.bincount(y_train)}') print(f'Resampled class distribution: {np.bincount(y_resampled)}') # Step 3: Create and train the model model, history = train_model(X_resampled, y_resampled, X_test, y_test) # Print final metrics print_final_metrics(history) # Evaluate the model evaluation = model.evaluate(X_test, y_test, verbose=1) print(f\"Test Loss: {evaluation[0]}\") print(f\"Test AUC: {evaluation[1]}\") # If you need more detailed metrics y_pred = model.predict(X_test) y_pred_classes = (y_pred > 0.5).astype(int) # Threshold predictions print(\"\\nClassification Report:\") print(classification_report(y_test, y_pred_classes)) print(\"\\nConfusion Matrix:\") print(confusion_matrix(y_test, y_pred_classes)) if __name__ == \"__main__\": main()", "source": "main.py"}, {"content": "import numpy as np class MLPTwoLayers: \"\"\" A two-layer Multi-Layer Perceptron (MLP) implementation. This class implements a neural network with one hidden layer and an output layer. It uses ReLU activation for the hidden layer and softmax for the output layer. Attributes: n_x (int): Number of input features. n_h (int): Number of neurons in the hidden layer. n_y (int): Number of output classes. W1 (np.array): Weights for the hidden layer. b1 (np.array): Biases for the hidden layer. W2 (np.array): Weights for the output layer. b2 (np.array): Biases for the output layer. X (np.array): Input features (set during forward pass). Z1 (np.array): Pre-activation values of hidden layer (set during forward pass). A1 (np.array): Activation values of hidden layer (set during forward pass). Z2 (np.array): Pre-activation values of output layer (set during forward pass). A2 (np.array): Activation values of output layer (set during forward pass). Y (np.array): True labels in one-hot encoding (set during loss computation). \"\"\" # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): \"\"\" Initialize the MLP with given layer sizes. Args: input_size (int): Number of input features. hidden_size (int): Number of neurons in the hidden layer. output_size (int): Number of output classes. random_seed (int): Seed for random number generator. \"\"\" # Set the random seed for reproducibility np.random.seed(88) self.n_x = input_size self.n_h = hidden_size self.n_y = output_size # Initialize weights for the hidden layer # #https://www.deeplearning.ai/ai-notes/initialization/index.html self.W1 = np.random.randn(self.n_h, self.n_x) * np.sqrt(1. / self.n_x) # all weights are picked randomly from a normal distribution # with mean = 0 and variance = 1 / (n)[l-1] self.b1 = np.zeros((self.n_h, 1)) # bias are initialised as zeros # Initialize weights for the output layer self.W2 = np.random.randn(self.n_y, self.n_h) * np.sqrt(1. / self.n_h) self.b2 = np.zeros((self.n_y, 1)) def forward(self, features): \"\"\" Perform a forward pass through the network. Args: features (np.array): Input features of shape (n_samples, n_features). Returns: np.array: Output probabilities for each class, shape (n_classes, n_samples). \"\"\" # Ensure features is a 2D array when passing in one sample for test case. if features.ndim == 1: features = features.reshape(1, -1) # Store the input for use in backward pass self.X = features.T # Shape: (n_features, n_samples) #hidden layer self.Z1 = self.W1.dot(self.X) + self.b1 # y = wx + b ,transpose 'features' to be able to dot product # with W1 which is n_h by n_x dimensions () self.A1 = np.maximum(0, self.Z1) # activation function (ReLU) #output layer self.Z2 = self.W2.dot(self.A1) + self.b2 # Softmax activation since its multiclass (3 class) # includes numerical trick to avoid overflow issue # exp_scores = np.exp(self.Z2 - np.max(self.Z2, axis=0, keepdims=True)) # self.A2 = exp_scores / np.sum(exp_scores, axis=0, keepdims=True) # Softmax activation since its multiclass (3 class) - Alternative exp_scores = np.exp(self.Z2) self.A2 = exp_scores / np.sum(exp_scores, axis=0, keepdims=True) return self.A2 def loss(self, predictions, label): \"\"\" Compute the categorical cross-entropy loss. Args: predictions (np.array): Predicted probabilities from forward pass, shape (n_classes, n_samples). label (int or np.array): True class label(s). If int, it's treated as a single sample. If array, it should have shape (n_samples,). Returns: float: The computed average loss. \"\"\" # Store the", "source": "mlp.py"}, {"content": "true label for use in backward pass m = predictions.shape[1] self.Y = np.eye(self.n_y)[label].reshape(self.n_y, m) # Compute the categorical cross-entropy loss epsilon = 1e-15 # Small constant to avoid log(0) log_probs = np.log(predictions + epsilon) # Clip predictions to avoid log(0) # Compute the loss loss = -np.sum(self.Y * log_probs) / m return loss def backward(self): \"\"\" Perform a backward pass through the network. This method computes gradients and updates the network's parameters. It uses the stored values from the last forward pass and loss computation. The method applies gradient clipping and uses a fixed learning rate for updates. \"\"\" # Number of samples m = self.X.shape[1] # Gradient of the loss with respect to A2 dZ2 = self.A2 - self.Y # For softmax + cross-entropy, this simplifies to A2 - Y # Gradients for the output layer dW2 = (1 / m) * np.dot(dZ2, self.A1.T) db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) # Gradient of the loss with respect to A1 dA1 = np.dot(self.W2.T, dZ2) # Gradient of the loss with respect to Z1 (ReLU derivative) dZ1 = dA1 * (self.Z1 > 0) # ReLU derivative is 1 for Z1 > 0, 0 otherwise # Gradients for the hidden layer dW1 = (1 / m) * np.dot(dZ1, self.X.T) db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True) # # Gradient clipping # clip_value = 5 # dW1 = np.clip(dW1, -clip_value, clip_value) # dW2 = np.clip(dW2, -clip_value, clip_value) # Update weights and biases learning_rate = 1e-3 self.W1 -= learning_rate * dW1 self.b1 -= learning_rate * db1 self.W2 -= learning_rate * dW2 self.b2 -= learning_rate * db2", "source": "mlp.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import StandardScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer class Datapipeline: \"\"\" A class for preprocessing data using sklearn pipelines. This class provides methods to load, transform, and preprocess data for machine learning tasks. Attributes: preprocessor (ColumnTransformer): The sklearn preprocessor for transforming data. columns (list): List of column names to be preprocessed. \"\"\" def __init__(self): \"\"\" Initialize the Datapipeline class. \"\"\" self.preprocessor = None self.columns = None def create_preprocessor(self, columns): \"\"\" Create a ColumnTransformer preprocessor for the data. Args: columns (list): List of column names to be preprocessed. Returns: ColumnTransformer: Preprocessor for transforming the data. \"\"\" return ColumnTransformer( transformers=[ ('num', Pipeline([ ('scaler', StandardScaler()) ]), columns) ]) def transform(self, data_path): \"\"\" Load and preprocess the data. This method reads the CSV file, separates features and target, creates and fits the preprocessor, and transforms the data. Args: data_path (str): Path to the data CSV file. Returns: tuple: (X_preprocessed, y_array) preprocessed features and labels as numpy arrays. Prints: Dataset shape and class distribution. \"\"\" # Read the CSV file df = pd.read_csv(data_path) # Separate features and target X = df.drop(['y','id'], axis=1) y = df['y'] # Identify columns self.columns = X.columns.tolist() # Create and fit preprocessor self.preprocessor = self.create_preprocessor(self.columns) X_preprocessed = self.preprocessor.fit_transform(X) # Convert y to numpy array y_array = y.to_numpy() print(\"Dataset shape:\", X_preprocessed.shape) print(\"Class distribution:\", np.unique(y_array, return_counts=True)) return X_preprocessed, y_array", "source": "mlp_datapipeline.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import DataLoader, TensorDataset from sklearn.metrics import roc_auc_score import numpy as np import random def set_seed(seed_value): \"\"\" Set seed for reproducibility across multiple libraries. Args: seed_value (int): The seed value to use for random number generation. \"\"\" random.seed(seed_value) np.random.seed(seed_value) torch.manual_seed(seed_value) torch.cuda.manual_seed_all(seed_value) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False class NeuralNetwork(nn.Module): \"\"\" A custom neural network model for binary classification. This model consists of multiple fully connected layers with LeakyReLU activation, batch normalization, and dropout for regularization. Attributes: layers (nn.Sequential): The sequence of layers in the network. \"\"\" def __init__(self, input_dim): \"\"\" Initialize the neural network. Args: input_dim (int): The dimension of the input features. \"\"\" super(NeuralNetwork, self).__init__() self.layers = nn.Sequential( # Used to stack layers nn.Linear(input_dim, 256), # A fully connected layer nn.LeakyReLU(0.001), # Activation function used LeakyReLU nn.BatchNorm1d(256), # Batch normalisation for faster and more stable training nn.Dropout(0.4), # To prevent overfitting nn.Linear(256, 128), nn.LeakyReLU(0.001), nn.BatchNorm1d(128), nn.Dropout(0.3), nn.Linear(128, 64), nn.LeakyReLU(0.001), nn.BatchNorm1d(64), nn.Dropout(0.2), nn.Linear(64, 32), nn.LeakyReLU(0.001), nn.BatchNorm1d(32), nn.Dropout(0.2), nn.Linear(32, 16), nn.LeakyReLU(0.001), nn.BatchNorm1d(16), nn.Dropout(0.2), nn.Linear(16, 1), nn.Sigmoid() ) def forward(self, x): \"\"\" Define the forward pass of the network. Args: x (torch.Tensor): The input tensor. Returns: torch.Tensor: The output of the network. \"\"\" return self.layers(x) # Forward method to define how input data flows through network def predict(self, X): \"\"\" Make predictions using the model. Args: X (np.array): The input features. Returns: np.array: The predicted probabilities. \"\"\" self.eval() # Set the model to evaluation mode with torch.no_grad(): # disables gradient calculation for efficiency X_tensor = torch.FloatTensor(X) outputs = self(X_tensor) return outputs.numpy() def evaluate(self, X, y): \"\"\" Evaluate the model's performance. Args: X (np.array): The input features. y (np.array): The true labels. Returns: tuple: A tuple containing the loss and AUC score. \"\"\" self.eval() with torch.no_grad(): X_tensor = torch.FloatTensor(X) y_tensor = torch.FloatTensor(y) outputs = self(X_tensor) loss = nn.BCELoss()(outputs, y_tensor.unsqueeze(1)) # BCE => Binary Cross entropy loss auc = roc_auc_score(y, outputs.numpy()) return loss.item(), auc # returns loss and auc def create_model(input_dim): \"\"\" Create an instance of the NeuralNetwork model. Args: input_dim (int): The dimension of the input features. Returns: NeuralNetwork: An instance of the neural network model. \"\"\" return NeuralNetwork(input_dim) # create instance of NeuralNetwork def train_model(X_train, y_train, X_val, y_val, epochs=10, batch_size=32, seed=None): # handles training process \"\"\" Train the neural network model. Args: X_train (np.array): Training features. y_train (np.array): Training labels. X_val (np.array): Validation features. y_val (np.array): Validation labels. epochs (int, optional): Number of training epochs. Defaults to 10. batch_size (int, optional): Batch size for training. Defaults to 32. seed (int, optional): Random seed for reproducibility. Defaults to None. Returns: tuple: A tuple containing the trained model and training history. \"\"\" if seed is not None: set_seed(seed) print(\"\\nStep 3: Creating and training the model\") input_dim = X_train.shape[1] model = create_model(input_dim) # Convert data to PyTorch tensors X_train_tensor = torch.FloatTensor(X_train) y_train_tensor = torch.FloatTensor(y_train) # Create DataLoader train_dataset = TensorDataset(X_train_tensor, y_train_tensor) train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=torch.Generator().manual_seed(seed) \\ if seed is not None else None) # Define loss function and optimizer criterion = nn.BCELoss() optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.001) # weight_decay applies L2 regularization # Early", "source": "pytorch_A4P2.py"}, {"content": "stopping setup best_val_loss = float('inf') # set to infinity first so any computed val_loss will become best for 1st round patience = 3 counter = 0 # History dictionary history = { 'loss': [], 'val_loss': [], 'val_auc': [] } # Training loop for epoch in range(epochs): model.train() # Switches to training mode epoch_loss = 0 for batch_X, batch_y in train_loader: # For each batch of input , target optimizer.zero_grad() # gradients in pytorch accumulate by default, # so clear gradient from each step before new backward pass outputs = model(batch_X) # forward pass with output shape (batch_size, 1) loss = criterion(outputs, batch_y.unsqueeze(1)) # predicted outputs compared to label for loss calculation. # unsqueeze batch_y to match output. # https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch loss.backward() # backpropagation # (calculates the garidents of the loss with respect to weights) optimizer.step() # updates model parameters using computed gradients # and move towrads minimising loss function epoch_loss += loss.item() # accumulates total loss for epoch # Calculate average epoch loss avg_epoch_loss = epoch_loss / len(train_loader) # average the loss per batch for current epoch history['loss'].append(avg_epoch_loss) # Validation val_loss, val_auc = model.evaluate(X_val, y_val) # compute evaluation metric on validation_data history['val_loss'].append(val_loss) history['val_auc'].append(val_auc) print(f'Epoch [{epoch+1}/{epochs}],Loss: {avg_epoch_loss:.4f},Val Loss: {val_loss:.4f},Val AUC: {val_auc:.4f}') # Early stopping check if val_loss < best_val_loss: best_val_loss = val_loss counter = 0 # rest counter to 0 when val_loss is < best_val loss torch.save(model.state_dict(), 'model/pytorch_A4P2_model.pth') else: counter += 1 if counter >= patience: # when counter > = patience then early stop # and load the last model when counter = 0 print(\"Early stopping\") model.load_state_dict(torch.load('model/pytorch_A4P2_model.pth')) break return model, history def print_final_metrics(history): \"\"\" Print the final training metrics. Args: history (dict): A dictionary containing the training history. \"\"\" print(\"\\nFinal training metrics:\") print(f\"Loss: {history['loss'][-1]:.4f}\") print(f\"Validation Loss: {history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history['val_auc'][-1]:.4f}\") def evaluate_model(model, X_test, y_test): \"\"\" Evaluate the model on the test set. Args: model (NeuralNetwork): The trained neural network model. X_test (np.array): Test features. y_test (np.array): Test labels. Returns: tuple: A tuple containing the test loss and AUC score. \"\"\" test_loss, test_auc = model.evaluate(X_test, y_test) print(f\"Test Loss: {test_loss:.4f}\") print(f\"Test AUC: {test_auc:.4f}\") return test_loss, test_auc", "source": "pytorch_A4P2.py"}, {"content": "import pandas as pd from imblearn.over_sampling import SMOTE from imblearn.under_sampling import RandomUnderSampler import os def load_data(input_file): \"\"\" Load data from a CSV file and split it into features and target. Args: input_file (str): Path to the input CSV file. Returns: tuple: A tuple containing two pandas DataFrames: X (features) and y (target variable). \"\"\" data = pd.read_csv(input_file) X = data.drop('Class', axis=1) y = data['Class'] return X, y def resample_data(X, y): \"\"\" Resample the data using a combination of SMOTE and random undersampling. This function first applies SMOTE to increase the minority class to 10% of the majority class, then applies random undersampling to reduce the majority class until the minority class is 50% of the majority. Args: X (pd.DataFrame): The feature DataFrame. y (pd.Series): The target variable Series. Returns: tuple: A tuple containing two numpy arrays: X_resampled (resampled features) and y_resampled (resampled target). \"\"\" # Create the individual resampling objects smote = SMOTE(sampling_strategy=0.1, random_state=42) # Increase minority class to 10% of majority undersampler = RandomUnderSampler(sampling_strategy=0.5, random_state=42) # Reduce majority class until minority class is 50% of majority # Apply SMOTE first X_smote, y_smote = smote.fit_resample(X, y) # Then apply undersampling X_resampled, y_resampled = undersampler.fit_resample(X_smote, y_smote) return X_resampled, y_resampled def main(): input_file = os.environ.get('INPUT_FILE', 'data/train_data.csv') output_dir = os.environ.get('OUTPUT_DIR', 'data/resampled') X, y = load_data(input_file) X_resampled, y_resampled = resample_data(X, y) save_resampled_data(X_resampled, y_resampled, output_dir) print(f\"Original shape: {X.shape}\") print(f\"Resampled shape: {X_resampled.shape}\") print(f\"Original class distribution: {pd.Series(y).value_counts(normalize=True)}\") print(f\"Resampled class distribution: {pd.Series(y_resampled).value_counts(normalize=True)}\")", "source": "resampling.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, LeakyReLU, BatchNormalization from tensorflow.keras.regularizers import l2 from tensorflow.keras.optimizers import Adam from tensorflow.keras.callbacks import EarlyStopping def create_model(input_dim): \"\"\" Create and compile a neural network model for binary classification. Args: input_dim (int): The number of input features. Returns: tf.keras.models.Sequential: A compiled Keras Sequential model. The model architecture consists of multiple dense layers with LeakyReLU activation, batch normalization, and dropout. The final layer uses sigmoid activation for binary classification. The model is compiled with Adam optimizer and binary cross-entropy loss. \"\"\" model = Sequential([ Dense(256, kernel_regularizer=l2(0.001), input_shape=(input_dim,)), LeakyReLU(alpha=0.001), BatchNormalization(), Dropout(0.4), Dense(128, kernel_regularizer=l2(0.001)), LeakyReLU(alpha=0.001), BatchNormalization(), Dropout(0.3), Dense(64, kernel_regularizer=l2(0.001)), LeakyReLU(alpha=0.001), BatchNormalization(), Dropout(0.2), Dense(32, kernel_regularizer=l2(0.01)), LeakyReLU(alpha=0.001), BatchNormalization(), Dropout(0.2), Dense(16, kernel_regularizer=l2(0.01)), LeakyReLU(alpha=0.001), BatchNormalization(), Dropout(0.2), Dense(1, activation='sigmoid') ]) model.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')]) return model def train_model(X_train, y_train, X_val, y_val, epochs=10, batch_size=32): \"\"\" Create and train the neural network model. Args: X_train (np.array): Training features. y_train (np.array): Training labels. X_val (np.array): Validation features. y_val (np.array): Validation labels. epochs (int, optional): Number of training epochs. Defaults to 10. batch_size (int, optional): Batch size for training. Defaults to 32. Returns: tuple: A tuple containing the trained model and training history. This function creates the model, sets up early stopping, and trains the model on the provided data. It uses the validation data for early stopping. \"\"\" print(\"\\nStep 3: Creating and training the model\") input_dim = X_train.shape[1] model = create_model(input_dim) # Early Stopping Callback early_stopping = EarlyStopping( monitor='val_loss', # Monitor validation loss patience=3, # Number of epochs with no improvement after which training will be stopped restore_best_weights=True # Restores model weights from the epoch with the best validation loss ) # Train the model with Early Stopping history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping] # Add Early Stopping callback ) return model, history # You can add a function to print the final metrics if you want def print_final_metrics(history): \"\"\" Print the final training metrics. Args: history (tf.keras.callbacks.History): The history object returned by model.fit(). This function prints the final loss and AUC values for both training and validation data. \"\"\" print(\"\\nFinal training metrics:\") print(f\"Loss: {history.history['loss'][-1]:.4f}\") print(f\"AUC: {history.history['auc'][-1]:.4f}\") print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")", "source": "train_A4P1.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, BatchNormalization from tensorflow.keras.optimizers import Adam from tensorflow.keras.callbacks import EarlyStopping from tensorflow.keras.initializers import GlorotUniform, Zeros def swish(x): return x * tf.nn.sigmoid(x) def swiglu(x): dim = x.shape[-1] // 2 x, gate = x[..., :dim], x[..., dim:] return swish(gate) * x class SwiGLU(tf.keras.layers.Layer): def __init__(self, units, **kwargs): super(SwiGLU, self).__init__(**kwargs) self.units = units self.dense = Dense(units * 2, kernel_initializer=GlorotUniform(), bias_initializer=Zeros()) def call(self, inputs): x = self.dense(inputs) return swiglu(x) def create_model(input_dim): model = Sequential([ SwiGLU(128, input_shape=(input_dim,)), BatchNormalization(), Dropout(0.3), SwiGLU(64), BatchNormalization(), Dropout(0.2), SwiGLU(32), BatchNormalization(), Dropout(0.1), Dense(1, activation='sigmoid', kernel_initializer=GlorotUniform(), bias_initializer=Zeros()) ]) optimizer = Adam(learning_rate=0.001, clipvalue=1.0) model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')]) return model def train_model(X_train, y_train, X_val, y_val, epochs=20, batch_size=128): print(\"\\nStep 3: Creating and training the model with SwiGLU\") input_dim = X_train.shape[1] model = create_model(input_dim) early_stopping = EarlyStopping( monitor='val_auc', mode='max', patience=10, restore_best_weights=True ) history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping] ) return model, history def print_final_metrics(history): print(\"\\nFinal training metrics:\") print(f\"Loss: {history.history['loss'][-1]:.4f}\") print(f\"AUC: {history.history['auc'][-1]:.4f}\") print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\") def predict_with_threshold(model, X, threshold=0.5): y_pred_proba = model.predict(X) return (y_pred_proba > threshold).astype(int)", "source": "train_A4P1_SwiGLU.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, BatchNormalization from tensorflow.keras.regularizers import l2 from tensorflow.keras.optimizers import Adam def create_model(input_dim): \"\"\" Create and compile a simple neural network model for binary classification. Args: input_dim (int): The number of input features. Returns: tf.keras.models.Sequential: A compiled Keras Sequential model. The model architecture consists of multiple dense layers with ReLU activation, batch normalization, and dropout. The final layer uses sigmoid activation for binary classification. The model is compiled with Adam optimizer and binary cross-entropy loss. \"\"\" model = Sequential([ Dense(128, activation='relu', input_shape=(input_dim,), kernel_regularizer=l2(0.01)), BatchNormalization(), Dropout(0.2), Dense(64, activation='relu', kernel_regularizer=l2(0.01)), BatchNormalization(), Dropout(0.2), Dense(32, activation='relu', kernel_regularizer=l2(0.01)), BatchNormalization(), Dropout(0.2), Dense(16, activation='relu', kernel_regularizer=l2(0.01)), BatchNormalization(), Dropout(0.2), Dense(1, activation='sigmoid') ]) model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')]) return model def train_model(X_train, y_train, X_val, y_val, epochs=10, batch_size=32): \"\"\" Create and train the neural network model. Args: X_train (np.array): Training features. y_train (np.array): Training labels. X_val (np.array): Validation features. y_val (np.array): Validation labels. epochs (int, optional): Number of training epochs. Defaults to 10. batch_size (int, optional): Batch size for training. Defaults to 32. Returns: tuple: A tuple containing the trained model and training history. This function creates the model and trains it on the provided data. It uses the validation data to monitor the model's performance during training. \"\"\" print(\"\\nStep 3: Creating and training the model\") input_dim = X_train.shape[1] model = create_model(input_dim) # Train the model history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size ) return model, history # You can add a function to print the final metrics if you want def print_final_metrics(history): \"\"\" Print the final training metrics. Args: history (tf.keras.callbacks.History): The history object returned by model.fit(). This function prints the final loss and AUC values for both training and validation data. \"\"\" print(\"\\nFinal training metrics:\") print(f\"Loss: {history.history['loss'][-1]:.4f}\") print(f\"AUC: {history.history['auc'][-1]:.4f}\") print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")", "source": "train_batchnorm.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense, Dropout, LeakyReLU from tensorflow.keras.regularizers import l2 from tensorflow.keras.optimizers import Adam def create_model(input_dim): \"\"\" Create and compile a neural network model for binary classification. Args: input_dim (int): The number of input features. Returns: tf.keras.models.Sequential: A compiled Keras Sequential model. The model architecture consists of multiple dense layers with LeakyReLU activation and dropout. The final layer uses sigmoid activation for binary classification. The model is compiled with Adam optimizer and binary cross-entropy loss. \"\"\" model = Sequential([ Dense(128, kernel_regularizer=l2(0.01), input_shape=(input_dim,)), LeakyReLU(alpha=0.1), Dropout(0.3), Dense(64, kernel_regularizer=l2(0.01)), LeakyReLU(alpha=0.1), Dropout(0.3), Dense(32, kernel_regularizer=l2(0.01)), LeakyReLU(alpha=0.1), Dropout(0.3), Dense(16, kernel_regularizer=l2(0.01)), LeakyReLU(alpha=0.1), Dropout(0.3), Dense(1, activation='sigmoid') ]) model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(name='auc')]) return model def train_model(X_train, y_train, X_val, y_val, epochs=10, batch_size=32): \"\"\" Create and train the neural network model. Args: X_train (np.array): Training features. y_train (np.array): Training labels. X_val (np.array): Validation features. y_val (np.array): Validation labels. epochs (int, optional): Number of training epochs. Defaults to 10. batch_size (int, optional): Batch size for training. Defaults to 32. Returns: tuple: A tuple containing the trained model and training history. This function creates the model and trains it on the provided data. It uses the validation data to monitor the model's performance during training. \"\"\" print(\"\\nStep 3: Creating and training the model\") input_dim = X_train.shape[1] model = create_model(input_dim) # Train the model history = model.fit( X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size ) return model, history # You can add a function to print the final metrics if you want def print_final_metrics(history): \"\"\" Print the final training metrics. Args: history (tf.keras.callbacks.History): The history object returned by model.fit(). This function prints the final loss and AUC values for both training and validation data. \"\"\" print(\"\\nFinal training metrics:\") print(f\"Loss: {history.history['loss'][-1]:.4f}\") print(f\"AUC: {history.history['auc'][-1]:.4f}\") print(f\"Validation Loss: {history.history['val_loss'][-1]:.4f}\") print(f\"Validation AUC: {history.history['val_auc'][-1]:.4f}\")", "source": "train_leakyrelu.py"}, {"content": "\"\"\" This script demonstrates the training of a custom Multi-Layer Perceptron (MLP) model and uses TensorWatch for visualizing the training progress. It loads data, preprocesses it using a custom pipeline, initializes and trains the MLP model, and logs the training loss using TensorWatch. \"\"\" # pylint: disable=import-error import time import pandas as pd import tensorwatch as tw from mlp import MLPTwoLayers as MLP from mlp_datapipeline import Datapipeline def load_and_preprocess_data(file_path): \"\"\" Load data from a CSV file and preprocess it using a custom pipeline. Args: file_path (str): Path to the CSV file containing the raw data. Returns: tuple: A tuple containing features (X) and labels (y). \"\"\" df = pd.read_csv(file_path) pipeline = Datapipeline() X, y = pipeline.transform(df) return X, y def initialize_model(input_size, hidden_size, output_size): \"\"\" Initialize the MLP model with specified layer sizes. Args: input_size (int): Number of input features. hidden_size (int): Number of neurons in the hidden layer. output_size (int): Number of output classes. Returns: MLP: An instance of the MLPTwoLayers model. \"\"\" return MLP(input_size=input_size, hidden_size=hidden_size, output_size=output_size) def train_model(model, X, y, epochs, stream): \"\"\" Train the MLP model and log the training loss using TensorWatch. Args: model (MLP): The MLP model to train. X (np.array): Input features. y (np.array): Target labels. epochs (int): Number of training epochs. stream (tw.Stream): TensorWatch stream for logging the training loss. This function performs the training loop, computes the loss for each epoch, and logs the average loss to the TensorWatch stream. \"\"\" for epoch in range(epochs): epoch_loss = 0 for i in range(len(X)): # Forward pass predictions = model.forward(X[i]) # Compute loss loss = model.loss(predictions, y[i]) epoch_loss += loss # Backward pass model.backward() # Calculate and log the average loss for this epoch avg_loss = epoch_loss / len(X) stream.write((epoch, float(avg_loss))) # Ensure avg_loss is a float # Uncomment the following line to print the loss for each epoch # print(f\"Epoch {epoch}, Loss: {avg_loss}\") time.sleep(1) def main(): \"\"\" Main function to orchestrate the data loading, model training, and visualization process. \"\"\" # Load and preprocess data X, y = load_and_preprocess_data('data/mlp_raw.csv') # Initialize model model = initialize_model(input_size=4, hidden_size=16, output_size=3) # Create a TensorWatch stream w = tw.Watcher() stream = w.create_stream('training_loss') # Train the model train_model(model, X, y, epochs=10, stream=stream) if __name__ == \"__main__\": main()", "source": "tw_training.py"}, {"content": "import matplotlib.pyplot as plt from sklearn.metrics import precision_recall_curve, average_precision_score def plot_precision_recall_curve(y_true, y_scores): \"\"\" Plot the Precision-Recall curve. Parameters: y_true : array-like of shape (n_samples,) True binary labels. y_scores : array-like of shape (n_samples,) Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions. \"\"\" precision, recall, _ = precision_recall_curve(y_true, y_scores) average_precision = average_precision_score(y_true, y_scores) plt.figure(figsize=(10, 6)) plt.step(recall, precision, color='b', alpha=0.2, where='post') plt.fill_between(recall, precision, step='post', alpha=0.2, color='b') plt.xlabel('Recall') plt.ylabel('Precision') plt.ylim([0.0, 1.05]) plt.xlim([0.0, 1.0]) plt.title(f'Precision-Recall curve: AP={average_precision:0.2f}') plt.show() # Example usage: # Assuming you have y_true (true labels) and y_scores (predicted probabilities) # plot_precision_recall_curve(y_true, y_scores)", "source": "utils.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A4.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A4.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss # model.backward() if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [-618, 573, -126], [265, 391, -100] ] # Replace below with your response matrix_2 = [ [-128, -562, -200, -6, -24], [480, 80, -685, -1028, -122], [-127, -618, 573, -126, 28], [924, 265, 391, -100, -235], [384, 280, 218, 279, 59] ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [384, 218, 59] ]", "source": "convolved_matrices.py"}, {"content": "import tensorflow as tf def one_hot_encode(image, label, num_classes): \"\"\"One-hot encodes the label.\"\"\" label = tf.one_hot(label, num_classes) return image, label def prepare_data(data_dir='data/extracted_images/img-classification/tensorfood', img_size=(224, 224), batch_size=32, validation_split=0.2): \"\"\" Prepares the image dataset for training and validation by loading images from a specified directory, splitting the data into training and validation sets, and configuring the dataset for performance optimization. Args: data_dir (str): The directory containing the extracted images. Default is 'data/extracted_images'. img_size (tuple): The target size to resize the images to. Default is (224, 224). batch_size (int): The size of the batches of data. Default is 32. validation_split (float): The fraction of data to reserve for validation. Default is 0.2. Returns: train_ds (tf.data.Dataset): The dataset for training images. val_ds (tf.data.Dataset): The dataset for validation images. num_classes (int): The number of unique classes in the dataset. \"\"\" # Set random seed for reproducibility tf.random.set_seed(42) # Load and split the dataset train_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=validation_split, subset=\"training\", seed=42, image_size=img_size, batch_size=batch_size ) val_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=validation_split, subset=\"validation\", seed=42, image_size=img_size, batch_size=batch_size ) # Get class names and number of classes class_names = train_ds.class_names num_classes = len(class_names) # One-hot encode the labels for both training and validation datasets train_ds = train_ds.map(lambda image, label: one_hot_encode(image, label, num_classes)) val_ds = val_ds.map(lambda image, label: one_hot_encode(image, label, num_classes)) # Configure dataset for performance AUTOTUNE = tf.data.experimental.AUTOTUNE train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE) val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE) return train_ds, val_ds, num_classes if __name__ == \"__main__\": train_ds, val_ds, num_classes = prepare_data() print(f\"Number of classes: {num_classes}\") print(f\"Training dataset: {train_ds}\") print(f\"Validation dataset: {val_ds}\")", "source": "data_preparation.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf import tensorflow_addons as tfa def one_hot_encode(image, label, num_classes): \"\"\"One-hot encodes the label.\"\"\" label = tf.one_hot(label, num_classes) return image, label def maintain_aspect_ratio(image, target_aspect_ratio=1.5): \"\"\"Maintains the aspect ratio of the image.\"\"\" shape = tf.shape(image) height, width = shape[0], shape[1] current_aspect_ratio = tf.cast(width, tf.float32) / tf.cast(height, tf.float32) if current_aspect_ratio > target_aspect_ratio: new_width = tf.cast(tf.cast(height, tf.float32) * target_aspect_ratio, tf.int32) image = tf.image.resize(image, [height, new_width]) else: new_height = tf.cast(tf.cast(width, tf.float32) / target_aspect_ratio, tf.int32) image = tf.image.resize(image, [new_height, width]) return image def random_resize_and_crop(image, min_scale=0.8, max_scale=1.2, target_size=(224, 224)): \"\"\"Randomly resizes and crops the image.\"\"\" original_size = tf.shape(image)[:2] random_scale = tf.random.uniform([], min_scale, max_scale) scaled_size = tf.cast(tf.cast(original_size, tf.float32) * random_scale, tf.int32) image = tf.image.resize(image, scaled_size) image = tf.image.random_crop(image, [target_size[0], target_size[1], 3]) return image def random_translation(image, max_translation=0.2): \"\"\"Applies random translation to the image.\"\"\" return tfa.image.translate(image, [max_translation * tf.random.uniform([2], -1, 1)]) def random_jpeg_quality(image, min_quality=50, max_quality=100): \"\"\"Applies random JPEG compression to the image.\"\"\" quality = tf.random.uniform([], min_quality, max_quality, dtype=tf.int32) image = tf.image.random_jpeg_quality(image, quality, quality) return image def augment_image(image, label, target_size=(224, 224)): \"\"\"Applies various augmentations to the image.\"\"\" image = maintain_aspect_ratio(image) image = random_resize_and_crop(image, target_size=target_size) image = random_translation(image) image = random_jpeg_quality(image) image = tf.image.random_flip_left_right(image) image = tf.image.random_brightness(image, 0.2) image = tf.image.random_contrast(image, 0.8, 1.2) return image, label def prepare_data(data_dir='data/extracted_images/img-classification/tensorfood', img_size=(224, 224), batch_size=32, validation_split=0.2): \"\"\" Prepares the image dataset for training and validation with improved augmentation and balancing techniques. Args: data_dir (str): The directory containing the extracted images. img_size (tuple): The target size to resize the images to. batch_size (int): The size of the batches of data. validation_split (float): The fraction of data to reserve for validation. Returns: train_ds (tf.data.Dataset): The dataset for training images. val_ds (tf.data.Dataset): The dataset for validation images. num_classes (int): The number of unique classes in the dataset. \"\"\" tf.random.set_seed(42) train_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=validation_split, subset=\"training\", seed=42, image_size=img_size, batch_size=None # We'll batch later after balancing ) val_ds = tf.keras.preprocessing.image_dataset_from_directory( data_dir, validation_split=validation_split, subset=\"validation\", seed=42, image_size=img_size, batch_size=batch_size ) class_names = train_ds.class_names num_classes = len(class_names) # Balance the dataset class_counts = [len(list(train_ds.filter(lambda x, y: y == i))) for i in range(num_classes)] max_size = max(class_counts) balanced_ds = [] for i in range(num_classes): class_ds = train_ds.filter(lambda x, y: y == i) class_ds = class_ds.repeat() class_ds = class_ds.take(max_size) balanced_ds.append(class_ds) train_ds = tf.data.experimental.sample_from_datasets(balanced_ds) # Apply augmentations and prepare datasets train_ds = train_ds.map(lambda x, y: augment_image(x, y, img_size), num_parallel_calls=tf.data.AUTOTUNE) train_ds = train_ds.map(lambda x, y: one_hot_encode(x, y, num_classes), num_parallel_calls=tf.data.AUTOTUNE) train_ds = train_ds.batch(batch_size).prefetch(tf.data.AUTOTUNE) val_ds = val_ds.map(lambda x, y: one_hot_encode(x, y, num_classes), num_parallel_calls=tf.data.AUTOTUNE) val_ds = val_ds.prefetch(tf.data.AUTOTUNE) return train_ds, val_ds, num_classes if __name__ == \"__main__\": train_ds, val_ds, num_classes = prepare_data() print(f\"Number of classes: {num_classes}\") print(f\"Training dataset: {train_ds}\") print(f\"Validation dataset: {val_ds}\")", "source": "data_preparation_new.py"}, {"content": "# pylint: disable=import-error import argparse from data_preparation import prepare_data from model_builder import build_model from model_trainer import train_model import mlflow import os def main(): \"\"\" Runs the entire image classification pipeline, which includes the following steps: 1. Prepare the training and validation data. 2. Build the CNN model using a specified architecture. 3. Train the model on the prepared datasets. 4. Log parameters and metrics to MLflow for tracking. 5. Save the trained model and evaluate its performance on the validation set. Command-line arguments: --data_dir_path (str): Path to the data directory containing training data. --setup_mlflow (str): Whether to setup MLflow tracking (default is \"false\"). --mlflow_tracking_uri (str): URI for MLflow tracking server. --mlflow_exp_name (str): Name of the MLflow experiment to log. --model_checkpoint_dir_path (str): Directory to save model checkpoints. --epochs (int): Number of training epochs (default is 50). \"\"\" parser = argparse.ArgumentParser(description=\"Run the entire image classification pipeline\") parser.add_argument(\"--data_dir_path\", type=str, required=True, help=\"Path to the data directory\") parser.add_argument(\"--setup_mlflow\", type=str, default=\"false\", help=\"Whether to setup MLflow\") parser.add_argument(\"--mlflow_tracking_uri\", type=str, help=\"MLflow tracking URI\") parser.add_argument(\"--mlflow_exp_name\", type=str, help=\"MLflow experiment name\") parser.add_argument(\"--model_checkpoint_dir_path\", type=str, required=True, help=\"Path to save model checkpoints\") parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs for training\") args = parser.parse_args() # Setup MLflow if requested if args.setup_mlflow.lower() == \"true\": mlflow.set_tracking_uri(args.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow_exp_name) # Start MLflow run with mlflow.start_run(): # Step 1: Prepare data print(\"Step 1: Preparing data...\") train_ds, val_ds, num_classes = prepare_data(data_dir=args.data_dir_path) # Step 2: Build model print(\"Step 2: Building model...\") model = build_model(input_shape=(224, 224, 3), num_classes=num_classes) # Step 3: Train model print(\"Step 3: Training model...\") trained_model, history = train_model(model, train_ds, val_ds, num_classes, args.epochs, args.model_checkpoint_dir_path) # Log parameters mlflow.log_param(\"epochs\", args.epochs) mlflow.log_param(\"num_classes\", num_classes) # Log metrics for epoch in range(args.epochs): mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch) mlflow.log_metric(\"train_loss\", history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch) # Save the final model final_model_path = os.path.join(args.model_checkpoint_dir_path, 'tensorfood.keras') trained_model.save(final_model_path) # Log the model to MLflow mlflow.keras.log_model(trained_model, \"model\") # Evaluate the model on the validation set val_loss, val_accuracy = trained_model.evaluate(val_ds) mlflow.log_metric(\"final_val_loss\", val_loss) mlflow.log_metric(\"final_val_accuracy\", val_accuracy) print(f\"Final Validation Loss: {val_loss:.4f}\") print(f\"Final Validation Accuracy: {val_accuracy:.4f}\") print(\"Pipeline completed. Model and metrics logged to MLflow.\") if __name__ == \"__main__\": main()", "source": "main.py"}, {"content": "# pylint: disable=import-error import argparse from data_preparation_new import prepare_data from model_builder import build_model from model_trainer import train_model import mlflow import os def main(): \"\"\" Runs the entire image classification pipeline, which includes the following steps: 1. Prepare the training and validation data using the new data preparation script. 2. Build the CNN model using a specified architecture. 3. Train the model on the prepared datasets. 4. Log parameters and metrics to MLflow for tracking. 5. Save the trained model and evaluate its performance on the validation set. Command-line arguments: --data_dir_path (str): Path to the data directory containing training data. --setup_mlflow (str): Whether to setup MLflow tracking (default is \"false\"). --mlflow_tracking_uri (str): URI for MLflow tracking server. --mlflow_exp_name (str): Name of the MLflow experiment to log. --model_checkpoint_dir_path (str): Directory to save model checkpoints. --epochs (int): Number of training epochs (default is 50). --batch_size (int): Batch size for training (default is 32). --img_size (int): Size of the input images (default is 224). \"\"\" parser = argparse.ArgumentParser(description=\"Run the entire image classification pipeline\") parser.add_argument(\"--data_dir_path\", type=str, required=True, help=\"Path to the data directory\") parser.add_argument(\"--setup_mlflow\", type=str, default=\"false\", help=\"Whether to setup MLflow\") parser.add_argument(\"--mlflow_tracking_uri\", type=str, help=\"MLflow tracking URI\") parser.add_argument(\"--mlflow_exp_name\", type=str, help=\"MLflow experiment name\") parser.add_argument(\"--model_checkpoint_dir_path\", type=str, required=True, help=\"Path to save model checkpoints\") parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs for training\") parser.add_argument(\"--batch_size\", type=int, default=32, help=\"Batch size for training\") parser.add_argument(\"--img_size\", type=int, default=224, help=\"Size of the input images\") args = parser.parse_args() # Setup MLflow if requested if args.setup_mlflow.lower() == \"true\": mlflow.set_tracking_uri(args.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow_exp_name) # Start MLflow run with mlflow.start_run(): # Step 1: Prepare data print(\"Step 1: Preparing data...\") train_ds, val_ds, num_classes = prepare_data( data_dir=args.data_dir_path, img_size=(args.img_size, args.img_size), batch_size=args.batch_size ) # Step 2: Build model print(\"Step 2: Building model...\") model = build_model(input_shape=(args.img_size, args.img_size, 3), num_classes=num_classes) # Step 3: Train model print(\"Step 3: Training model...\") trained_model, history = train_model(model, train_ds, val_ds, num_classes, args.epochs, args.model_checkpoint_dir_path) # Log parameters mlflow.log_param(\"epochs\", args.epochs) mlflow.log_param(\"num_classes\", num_classes) mlflow.log_param(\"batch_size\", args.batch_size) mlflow.log_param(\"img_size\", args.img_size) # Log metrics for epoch in range(args.epochs): mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch) mlflow.log_metric(\"train_loss\", history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch) # Save the final model final_model_path = os.path.join(args.model_checkpoint_dir_path, 'tensorfood_new.keras') trained_model.save(final_model_path) # Log the model to MLflow mlflow.keras.log_model(trained_model, \"model\") # Evaluate the model on the validation set val_loss, val_accuracy = trained_model.evaluate(val_ds) mlflow.log_metric(\"final_val_loss\", val_loss) mlflow.log_metric(\"final_val_accuracy\", val_accuracy) print(f\"Final Validation Loss: {val_loss:.4f}\") print(f\"Final Validation Accuracy: {val_accuracy:.4f}\") print(\"Pipeline completed. Model and metrics logged to MLflow.\") if __name__ == \"__main__\": main()", "source": "main_new.py"}, {"content": "# pylint: disable=import-error import tensorflow as tf from tensorflow.keras.applications import ResNet50 from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout from tensorflow.keras.models import Model def build_model(input_shape=(224, 224, 3), num_classes=12): \"\"\" Builds a convolutional neural network model using a pre-trained ResNet50 base with custom top layers for classification tasks. The ResNet50 base is frozen to retain its pre-trained weights and is followed by custom layers for fine-tuning and classification. Args: input_shape (tuple): Shape of the input images, default is (224, 224, 3). num_classes (int): Number of output classes for the final dense layer, default is 12. Returns: model (tf.keras.Model): A Keras Model object ready for training and evaluation. \"\"\" # Load the pre-trained ResNet50 model without the top layers base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape) # Freeze the base model layers for layer in base_model.layers: layer.trainable = False # Add custom layers on top of the base model x = base_model.output x = GlobalAveragePooling2D()(x) x = Dense(1024, activation='relu')(x) x = Dropout(0.5)(x) x = Dense(512, activation='relu')(x) x = Dropout(0.3)(x) outputs = Dense(num_classes, activation='softmax')(x) # Create the final model model = Model(inputs=base_model.input, outputs=outputs) return model if __name__ == \"__main__\": # Example usage model = build_model() model.summary() # Save the model architecture as a PNG file tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True) print(\"Model built successfully. Architecture saved as 'model_architecture.png'.\")", "source": "model_builder.py"}, {"content": "# pylint: disable=import-error from tensorflow.keras.optimizers import Adam from tensorflow.keras.losses import CategoricalCrossentropy from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau from data_preparation import prepare_data import mlflow import mlflow.keras import argparse import os def train_model(model, train_ds, val_ds, num_classes, epochs, model_checkpoint_dir_path='/pvc-data/workspaces/yeo-york-yong/all-assignments/assignment5/models'): \"\"\" Trains a CNN model using the provided training and validation datasets, with model checkpoints and early stopping. The model is compiled with the Adam optimizer and categorical crossentropy loss function. Args: model (tf.keras.Model): The CNN model to be trained. train_ds (tf.data.Dataset): The training dataset. val_ds (tf.data.Dataset): The validation dataset. num_classes (int): The number of output classes for the model. epochs (int): The number of epochs to train the model. model_checkpoint_dir_path (str): The directory path to save the best model checkpoints. Returns: model (tf.keras.Model): The trained model. history (History): Training history containing metrics values during training. \"\"\" # Compile the model model.compile( optimizer=Adam(learning_rate=0.0001), loss=CategoricalCrossentropy(), metrics=['accuracy'] ) # Define callbacks callbacks = [ ModelCheckpoint(os.path.join(model_checkpoint_dir_path, 'best_model.keras'), save_best_only=True, monitor='val_accuracy'), EarlyStopping(patience=3, restore_best_weights=True), ReduceLROnPlateau(factor=0.1, patience=5) ] # Train the model history = model.fit( train_ds, validation_data=val_ds, epochs=epochs, callbacks=callbacks ) return model, history if __name__ == \"__main__\": parser = argparse.ArgumentParser(description=\"Train CNN model with MLflow logging\") parser.add_argument(\"--data_dir_path\", type=str, required=True, help=\"Path to the data directory\") parser.add_argument(\"--setup_mlflow\", type=str, default=\"false\", help=\"Whether to setup MLflow\") parser.add_argument(\"--mlflow_tracking_uri\", type=str, help=\"MLflow tracking URI\") parser.add_argument(\"--mlflow_exp_name\", type=str, help=\"MLflow experiment name\") parser.add_argument(\"--model_checkpoint_dir_path\", type=str, required=True, help=\"Path to save model checkpoints\") parser.add_argument(\"--epochs\", type=int, default=50, help=\"Number of epochs for training\") args = parser.parse_args() # Setup MLflow if requested if args.setup_mlflow.lower() == \"true\": mlflow.set_tracking_uri(args.mlflow_tracking_uri) mlflow.set_experiment(args.mlflow_exp_name) # Prepare data train_ds, val_ds, num_classes = prepare_data(data_dir=args.data_dir_path) # Start MLflow run with mlflow.start_run(): # Log parameters mlflow.log_param(\"epochs\", args.epochs) mlflow.log_param(\"num_classes\", num_classes) # Train the model model, history = train_model(train_ds, val_ds, num_classes, args.epochs, args.model_checkpoint_dir_path) # Log metrics for epoch in range(args.epochs): mlflow.log_metric(\"train_accuracy\", history.history['accuracy'][epoch], step=epoch) mlflow.log_metric(\"train_loss\", history.history['loss'][epoch], step=epoch) mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][epoch], step=epoch) mlflow.log_metric(\"val_loss\", history.history['val_loss'][epoch], step=epoch) # Save the final model final_model_path = os.path.join(args.model_checkpoint_dir_path, 'tensorfood.keras') model.save(final_model_path) # Log the model to MLflow mlflow.keras.log_model(model, \"model\") # Evaluate the model on the validation set val_loss, val_accuracy = model.evaluate(val_ds) mlflow.log_metric(\"final_val_loss\", val_loss) mlflow.log_metric(\"final_val_accuracy\", val_accuracy) print(f\"Final Validation Loss: {val_loss:.4f}\") print(f\"Final Validation Accuracy: {val_accuracy:.4f}\") print(\"Training completed. Model and metrics logged to MLflow.\")", "source": "model_trainer.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "# pylint: disable=import-error import torch import torch.nn as nn import torch.optim as optim import numpy as np class CNN2DModel(nn.Module): def __init__(self, num_features, sequence_length, output_size, num_filters=64, kernel_size=3, dropout_rate=0.2): super(CNN2DModel, self).__init__() self.conv1 = nn.Conv2d(1, num_filters, kernel_size, padding=kernel_size//2) self.conv2 = nn.Conv2d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2) self.pool = nn.MaxPool2d(2) self.dropout = nn.Dropout(dropout_rate) # Calculate the size of the flattened features after convolutions and pooling self.flatten_size = self._get_flatten_size(sequence_length, num_features) self.fc1 = nn.Linear(self.flatten_size, 50) self.fc2 = nn.Linear(50, output_size) self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters()) def _get_flatten_size(self, sequence_length, num_features): x = torch.randn(1, 1, sequence_length, num_features) x = self.conv1(x) x = self.pool(x) x = self.conv2(x) x = self.pool(x) return x.view(1, -1).size(1) def forward(self, x): # x shape: (batch_size, sequence_length, num_features) x = x.unsqueeze(1) # Add channel dimension: (batch_size, 1, sequence_length, num_features) # The added channel allows time series data for 2D convolutions x = torch.relu(self.conv1(x)) x = self.pool(x) x = self.dropout(x) x = torch.relu(self.conv2(x)) x = self.pool(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def fit(self, dataloader, num_epochs=100): self.train() for epoch in range(num_epochs): total_loss = 0 for inputs, targets in dataloader: self.optimizer.zero_grad() outputs = self(inputs) loss = self.criterion(outputs, targets) loss.backward() self.optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\") def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for inputs, _ in dataloader: outputs = self(inputs) predictions.extend(outputs.numpy()) return np.array(predictions) def evaluate(self, train_dataloader, test_dataloader): self.eval() def compute_metrics(dataloader): total_mse = 0 total_samples = 0 with torch.no_grad(): for inputs, targets in dataloader: outputs = self(inputs) mse = self.criterion(outputs, targets) total_mse += mse.item() * inputs.size(0) total_samples += inputs.size(0) avg_mse = total_mse / total_samples if total_samples > 0 else float('inf') avg_rmse = np.sqrt(avg_mse) return avg_mse, avg_rmse train_mse, train_rmse = compute_metrics(train_dataloader) test_mse, test_rmse = compute_metrics(test_dataloader) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse }", "source": "cnn2d_model.py"}, {"content": "# pylint: disable=import-error import torch import torch.nn as nn import torch.optim as optim import numpy as np class CNNModel(nn.Module): def __init__(self, input_channels, sequence_length, output_size, num_filters=64, kernel_size=3, dropout_rate=0.2): super(CNNModel, self).__init__() self.conv1 = nn.Conv1d(input_channels, num_filters, kernel_size, padding=kernel_size//2) self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size, padding=kernel_size//2) self.pool = nn.MaxPool1d(2) self.dropout = nn.Dropout(dropout_rate) # Calculate the size of the flattened features after convolutions and pooling self.flatten_size = self._get_flatten_size(input_channels, sequence_length) self.fc1 = nn.Linear(self.flatten_size, 50) self.fc2 = nn.Linear(50, output_size) self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters()) def _get_flatten_size(self, input_channels, sequence_length): x = torch.randn(1, input_channels, sequence_length) #random tensor to simulate batch of single sample x = self.conv1(x) #Pass through first convolution layer if x.size(2) > 1: #check if length of output tensor is greater than 1, if yes, apply max pooling x = self.pool(x) x = self.conv2(x) if x.size(2) > 1: x = self.pool(x) return x.view(1, -1).size(1) #flatten to create 1D tensor with all features to connect with fully conneceted llayer def forward(self, x): # x shape: (batch_size, sequence_length, num_features) x = x.permute(0, 2, 1) # Change to (batch_size, num_features, sequence_length) x = torch.relu(self.conv1(x)) if x.size(2) > 1: x = self.pool(x) x = self.dropout(x) x = torch.relu(self.conv2(x)) if x.size(2) > 1: x = self.pool(x) x = self.dropout(x) x = x.view(x.size(0), -1) # Flatten x = torch.relu(self.fc1(x)) x = self.fc2(x) return x def fit(self, dataloader, num_epochs=100): self.train() for epoch in range(num_epochs): total_loss = 0 for inputs, targets in dataloader: self.optimizer.zero_grad() outputs = self(inputs) loss = self.criterion(outputs, targets) loss.backward() self.optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}\") def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for inputs, _ in dataloader: outputs = self(inputs) predictions.extend(outputs.numpy()) return np.array(predictions) def evaluate(self, train_dataloader, test_dataloader): self.eval() def compute_metrics(dataloader): total_mse = 0 total_samples = 0 with torch.no_grad(): for inputs, targets in dataloader: outputs = self(inputs) mse = self.criterion(outputs, targets) total_mse += mse.item() * inputs.size(0) total_samples += inputs.size(0) avg_mse = total_mse / total_samples if total_samples > 0 else float('inf') avg_rmse = np.sqrt(avg_mse) return avg_mse, avg_rmse train_mse, train_rmse = compute_metrics(train_dataloader) test_mse, test_rmse = compute_metrics(test_dataloader) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse }", "source": "cnn_model.py"}, {"content": "import pandas as pd class DataPipeline: def __init__(self, use_rnn=False): self.features = ['PRES', 'TEMP', 'Iws', 'Is', 'Ir', 'DEWP', 'pm2.5'] self.lag_features = { 'pm2.5': 24, 'Iws': 3, 'DEWP': 3, 'PRES': 1, 'TEMP': 1, 'Is': 1, 'Ir': 1 } self.use_rnn = use_rnn def create_target_variable(self, df, horizon): df[f'target_{horizon}'] = df['pm2.5'].shift(-horizon) return df def create_lag_features(self, df): if not self.use_rnn: for feature, max_lag in self.lag_features.items(): for lag in range(1, max_lag + 1): df[f'{feature}_lag_{lag}'] = df[feature].shift(lag) return df def run_data_pipeline(self, csv_path, horizon=1): # Step 1: Ingest the CSV file df = pd.read_csv(csv_path) # Step 2: Combine year, month, day, and hour into a single datetime column df['datetime'] = pd.to_datetime(df[['year', 'month', 'day', 'hour']]) # Step 3: Drop the original date and time columns df = df.drop(['year', 'month', 'day', 'hour', 'Unnamed: 0'], axis=1) # Step 4: Sort the DataFrame by datetime df = df.sort_values('datetime') # Step 5: Fill missing values using backward fill df.fillna(method='bfill', inplace=True) # Step 6: Create lag features (only for non-RNN) df = self.create_lag_features(df) # Step 7: Create target variable (PM2.5 one hour ahead) df['target'] = df['pm2.5'].shift(-1) # Step 8: Drop rows with NaN values (due to lagging) df = df.dropna() # Step 9: Create target variable for the specified horizon df = self.create_target_variable(df, horizon) # Step 10: Select final feature set if self.use_rnn: final_features = ['datetime'] + self.features + [f'target_{horizon}'] else: final_features = ['datetime'] for feature, max_lag in self.lag_features.items(): if feature == 'pm2.5': final_features.extend([f'pm2.5_lag_{i}' for i in range(1, 25)]) elif feature in ['Iws', 'DEWP']: final_features.extend([f'{feature}_lag_{i}' for i in range(1, 4)]) else: final_features.append(f'{feature}_lag_1') final_features.append(f'target_{horizon}') df = df[final_features] # Step 11: Drop rows with NaN values (due to lagging and target creation) df = df.dropna() # Return the processed DataFrame return df", "source": "datapipeline.py"}, {"content": "from src.datapipeline import DataPipeline from src.ml_model import ForecastModel import pandas as pd def run_experiment(data_path, horizons=[1, 3, 6, 12, 24]): pipeline = DataPipeline() metrics_list = [] for horizon in horizons: # Prepare data for the current horizon df = pipeline.run_data_pipeline(data_path, horizon) # Determine the split point (e.g., use the last 20% of the data for testing) split_point = int(len(df) * 0.8) # Split the data train_df = df.iloc[:split_point] test_df = df.iloc[split_point:] # Separate features and target target_col = f'target_{horizon}' feature_cols = [col for col in df.columns if col not in ['datetime', target_col]] X_train, y_train = train_df[feature_cols], train_df[target_col] X_test, y_test = test_df[feature_cols], test_df[target_col] # Initialize and train the model model = ForecastModel() model.fit(X_train, y_train) # Evaluate the model evaluation_metrics = model.evaluate(X_train, y_train, X_test, y_test) metrics_list.append({ 'Horizon': horizon, 'Train RMSE': evaluation_metrics['train_rmse'], 'Test RMSE': evaluation_metrics['test_rmse'], 'Train MSE': evaluation_metrics['train_mse'], 'Test MSE': evaluation_metrics['test_mse'] }) # Create a DataFrame from the metrics metrics_df = pd.DataFrame(metrics_list) return model, metrics_df", "source": "ml_experiment.py"}, {"content": "# pylint: disable=import-error from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import mean_squared_error import numpy as np class ForecastModel: def __init__(self, n_estimators=100, random_state=42): self.model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state) def fit(self, X, y): self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): # Predict on train and test data y_train_pred = self.model.predict(X_train) y_test_pred = self.model.predict(X_test) # Calculate MSE for train and test train_mse = mean_squared_error(y_train, y_train_pred) test_mse = mean_squared_error(y_test, y_test_pred) # Calculate RMSE for train and test (often easier to interpret as it's in the same units as the target variable) train_rmse = np.sqrt(train_mse) test_rmse = np.sqrt(test_mse) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse } def predict(self, X): return self.model.predict(X) def feature_importance(self): # Return feature importances if available if hasattr(self.model, 'feature_importances_'): return self.model.feature_importances_ else: return None", "source": "ml_model.py"}, {"content": "# pylint: disable=import-error import torch import torch.nn as nn import torch.optim as optim import numpy as np class RNNModel(nn.Module): def __init__(self, input_size, num_rnn, num_layers, output_size, lookahead): super(RNNModel, self).__init__() self.rnn = nn.RNN(input_size, num_rnn, num_layers, batch_first=True) self.fc = nn.Linear(num_rnn, output_size) self.lookahead = lookahead self.criterion = nn.MSELoss() self.optimizer = optim.Adam(self.parameters()) def forward(self, x): out, _ = self.rnn(x) out = self.fc(out[:, -self.lookahead:, :]) # Take only the last lookahead time steps return out.squeeze(-1) # Remove the last dimension if it's 1 def fit(self, dataloader, num_epochs=100): self.train() for epoch in range(num_epochs): total_loss = 0 for inputs, targets in dataloader: self.optimizer.zero_grad() outputs = self(inputs) loss = self.criterion(outputs, targets) # targets should now match outputs shape loss.backward() self.optimizer.step() total_loss += loss.item() if (epoch + 1) % 10 == 0: print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/len(dataloader):.4f}') def predict(self, dataloader): self.eval() predictions = [] with torch.no_grad(): for inputs, _ in dataloader: outputs = self(inputs) predictions.append(outputs.numpy()) return np.concatenate(predictions) def evaluate(self, train_dataloader, test_dataloader): self.eval() def compute_metrics(dataloader): total_mse = 0 total_samples = 0 with torch.no_grad(): for inputs, targets in dataloader: outputs = self(inputs) mse = self.criterion(outputs, targets) total_mse += mse.item() * inputs.size(0) total_samples += inputs.size(0) avg_mse = total_mse / total_samples avg_rmse = np.sqrt(avg_mse) return avg_mse, avg_rmse train_mse, train_rmse = compute_metrics(train_dataloader) test_mse, test_rmse = compute_metrics(test_dataloader) return { 'train_mse': train_mse, 'test_mse': test_mse, 'train_rmse': train_rmse, 'test_rmse': test_rmse }", "source": "rnn_model.py"}, {"content": "# pylint: disable=import-error import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data, lookback, lookahead): self.data = torch.FloatTensor(data.values) self.lookback = lookback self.lookahead = lookahead self.length = len(data) - lookback - lookahead + 1 def __len__(self): return self.length def __getitem__(self, idx): window_start = idx window_end = idx + self.lookback + self.lookahead window = self.data[window_start:window_end] # Extract features (input) and labels features = window[:self.lookback] labels = window[self.lookback:, -1] # Assuming the last column is the target # Reshape to match the required dimensions features = features.view(self.lookback, -1) # (lookback, features) labels = labels.view(-1) # (1,) return features, labels", "source": "windowing.py"}, {"content": "def test_always_passes(): assert True > 0 # This will always pass print(\"Test passed\")", "source": "test_pass.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\"", "source": "test_context_vector.py"}, {"content": "import os import streamlit as st import pandas as pd from dotenv import load_dotenv from langchain.vectorstores import FAISS from langchain_google_genai import GoogleGenerativeAIEmbeddings from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage from langchain.chains.conversation.memory import ConversationBufferMemory from langchain.agents.agent_types import AgentType from langchain_experimental.agents import create_pandas_dataframe_agent from langchain.document_loaders import CSVLoader, PDFPlumberLoader # Load environment variables load_dotenv() # Fetch credentials from environment variables endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") model = os.getenv(\"AZURE_OPENAI_MODEL\") api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\") model_version = os.getenv(\"AZURE_OPENAI_MODEL_VERSION\") # Initialize Azure OpenAI LLM llm = AzureChatOpenAI( azure_endpoint=endpoint, openai_api_key=api_key, model_name=model, openai_api_version=api_version, model_version=model_version, temperature=0.5, max_tokens=300, model_kwargs={\"top_p\": 0.5} ) # Function to load CSVs and PDFs for retrieval (RAG) def load_data_for_rag(data_folder): docs = [] csv_dataframes = [] # Load CSV files for file in os.listdir(data_folder): if file.endswith(\".csv\"): csv_path = os.path.join(data_folder, file) loader = CSVLoader(file_path=csv_path) docs.extend(loader.load()) # Load as documents for RAG # Load CSV and add a column to indicate its source (AIAP batch number) df = pd.read_csv(csv_path, header=0, names=[\"Candidate\", \"Assessor\", \"Result\"]) df['AIAP_Batch_Number'] = file # Add source column csv_dataframes.append(df) # Load PDF files for file in os.listdir(data_folder): if file.endswith(\".pdf\"): pdf_path = os.path.join(data_folder, file) loader = PDFPlumberLoader(file_path=pdf_path) docs.extend(loader.load()) # Load as documents for RAG # Merge all CSV dataframes into one with a 'source_file' column merged_csv_df = pd.concat(csv_dataframes, ignore_index=True) return docs, merged_csv_df # Function to embed data (CSV and PDF) and store in FAISS def process_data_with_faiss(docs): embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) split_docs = text_splitter.split_documents(docs) vectors = FAISS.from_documents(split_docs, embeddings) return vectors # Function to set up retrieval and question answering (RAG) def setup_rag_chain(llm, vectors, top_k): memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\") retriever = vectors.as_retriever(search_kwargs={\"k\": top_k}) rag_chain = ConversationalRetrievalChain.from_llm( llm=llm, retriever=retriever, memory=memory, return_source_documents=True ) return rag_chain # Function to create a Pandas Agent for CSV analysis with clear dataset context def setup_pandas_agent_with_context(llm, dataframe): # Define a more explicit system-level prompt system_prompt = f\"\"\" You are working with a merged dataset containing the results of two technical assessments (Batch 13 and Batch 14). The dataset has the following columns: - Candidate: The name of the candidate. - Assessor: The name of the assessor for the assessment. - Result: Whether the candidate passed or failed the assessment. - AIAP_Batch_Number: This column indicates which assessment (Batch 13 or Batch 14) the data belongs to. Important: This single dataframe contains information from both Batch 13 and Batch 14. You do not need to look for a separate dataframe for each batch. When answering questions: 1. Always consider the entire dataframe, which includes data from both batches. 2. Use the 'AIAP_Batch_Number' column to distinguish between the two batches when necessary. 3. For questions about candidates in both assessments, look for candidates that appear with both Batch 13 and Batch 14 in the 'AIAP_Batch_Number' column. Please perform analysis dynamically based on the columns provided and the instructions above. \"\"\" # Set up the Pandas agent with the provided context in the system prompt pandas_agent = create_pandas_dataframe_agent( llm, dataframe, system_prompt=system_prompt, verbose=True, agent_type=AgentType.OPENAI_FUNCTIONS, handle_parsing_errors=True, allow_dangerous_code=True ) return pandas_agent # Streamlit UI elements st.title(\"AIAP Batch Technical Assessment Q&A Platform\") # Initialize session state for chat history if it doesn't exist if 'chat_history'", "source": "app.py"}, {"content": "not in st.session_state: st.session_state.chat_history = [] # Automatically read data from the 'data' folder data_folder = \"data\" st.sidebar.write(f\"Loading data from: {data_folder}\") # Load CSV and PDF data for retrieval (RAG) docs, merged_csv_df = load_data_for_rag(data_folder) # Display CSV data in the sidebar for viewing purposes only st.sidebar.subheader(\"Merged CSV Data\") st.sidebar.dataframe(merged_csv_df) # Sidebar: Top-k parameter for RAG retrieval top_k = st.sidebar.slider(\"Top-k retrieved documents\", min_value=1, max_value=10, value=5) # Embed documents and store them in FAISS vectors = process_data_with_faiss(docs) rag_chain = setup_rag_chain(llm, vectors, top_k) # Setup Pandas Agent for the merged CSV file with context pandas_agent = setup_pandas_agent_with_context(llm, merged_csv_df) # Function to let LLM determine whether to use Pandas Agent or RAG for a question def determine_approach(user_question, pandas_agent, rag_chain): # Ask LLM to determine the best approach reasoning_prompt = f\"\"\" You are an AI assistant. The provided dataset is merged from two technical assessments, with the 'AIAP_Batch_Number' column indicating the batch. Your task is to decide whether the user's question should be answered using RAG (for PDF documents) or Pandas Agent (for CSV analysis). If the question is about data analysis, statistics, or specific candidate results from both batches, Pandas Agent should be used. If the question is about general context or comparing information from both batches, use the RAG retrieval system. Here is the user's question: \"{user_question}\" Which approach should be used: Pandas Agent or RAG? Provide reasoning. \"\"\" # Send the reasoning prompt to the LLM response = llm([HumanMessage(content=reasoning_prompt)]) # Extract reasoning from LLM response reasoning = response.content # Extract reasoning from the response return reasoning # Modified function to handle the user's question def handle_question(user_question, pandas_agent, rag_chain): reasoning = determine_approach(user_question, pandas_agent, rag_chain) if \"pandas agent\" in reasoning.lower(): full_question = f\"\"\" Remember, you are working with a single merged dataframe that contains data from both Batch 13 and Batch 14. The 'AIAP_Batch_Number' column indicates which batch each row belongs to. User question: {user_question} Please analyze the data and provide a clear, concise answer. \"\"\" result = pandas_agent.run(full_question) # Capture the AgentExecutor output and return it for inspection chain_output = f\"AgentExecutor chain output:\\n{result}\" return result, [], reasoning + \"\\n\" + chain_output else: response = rag_chain({\"question\": user_question}) return response['answer'], response[\"source_documents\"], reasoning # Display chat history st.subheader(\"Chat History\") for message in st.session_state.chat_history: with st.chat_message(message[\"role\"]): st.write(message[\"content\"]) if \"reasoning\" in message: with st.expander(\"View Reasoning\"): st.write(message[\"reasoning\"]) # User input user_question = st.chat_input(\"Ask your question here:\") if user_question: st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question}) with st.chat_message(\"user\"): st.write(user_question) with st.chat_message(\"assistant\"): with st.spinner(\"Retrieving answer...\"): answer, retrieved_docs, reasoning = handle_question(user_question, pandas_agent, rag_chain) st.write(answer) with st.expander(\"View Reasoning and Code Execution\"): st.write(reasoning) if \"AgentExecutor chain output\" in reasoning: st.write(f\"Agent execution output: {answer}\") # Print Pandas agent output for inspection if retrieved_docs: with st.expander(\"Retrieved Documents\"): for doc in retrieved_docs: st.write(f\"**Document Source:** {doc.metadata['source']}\") st.write(f\"{doc.page_content[:500]}\") st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"reasoning\": reasoning}) # Display commonly asked questions st.subheader(\"Commonly Asked Questions\") common_questions = [ \"When the dateline for AIAP Batch 13?\", \"What is the difference between AIAP Batch 13 and 14?\", \"The total number of candidates who pass the two technical assessments?\", \"Which candidate are present in both technical assessments?\", \"Who is the assessor of Shuckle?\" ] for question in common_questions: if st.button(question):", "source": "app.py"}, {"content": "st.session_state.chat_history.append({\"role\": \"user\", \"content\": question}) with st.chat_message(\"user\"): st.write(question) with st.chat_message(\"assistant\"): with st.spinner(\"Retrieving answer...\"): answer, retrieved_docs, reasoning = handle_question(question, pandas_agent, rag_chain) st.write(answer) with st.expander(\"View Reasoning\"): st.write(reasoning) if retrieved_docs: with st.expander(\"Retrieved Documents\"): for doc in retrieved_docs: st.write(f\"**Document Source:** {doc.metadata['source']}\") st.write(f\"{doc.page_content[:500]}\") st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"reasoning\": reasoning})", "source": "app.py"}, {"content": "import os import streamlit as st import pandas as pd from dotenv import load_dotenv from langchain.vectorstores import FAISS from langchain_google_genai import GoogleGenerativeAIEmbeddings from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain from langchain.chat_models import AzureChatOpenAI from langchain.schema import HumanMessage from langchain.chains.conversation.memory import ConversationBufferMemory from langchain.agents.agent_types import AgentType from langchain_experimental.agents import create_pandas_dataframe_agent from langchain.document_loaders import CSVLoader, PDFPlumberLoader from langchain.callbacks.streamlit import StreamlitCallbackHandler # Load environment variables load_dotenv() # Fetch credentials from environment variables endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\") api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") model = os.getenv(\"AZURE_OPENAI_MODEL\") api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\") model_version = os.getenv(\"AZURE_OPENAI_MODEL_VERSION\") # Initialize Azure OpenAI LLM llm = AzureChatOpenAI( azure_endpoint=endpoint, openai_api_key=api_key, model_name=model, openai_api_version=api_version, model_version=model_version, temperature=0.7, max_tokens=300, model_kwargs={\"top_p\": 0.5} ) # Function to load CSVs and PDFs for retrieval (RAG) def load_data_for_rag(data_folder): \"\"\" Loads and processes CSV and PDF files from the specified folder for use in a Retrieval-Augmented Generation (RAG) system. This function reads all CSV and PDF files in the provided folder. CSV files are loaded both as documents (for retrieval) and as dataframes for further analysis. Each CSV dataframe is augmented with a column indicating the source file (AIAP batch number), and all dataframes are merged into one. PDF files are loaded and converted into documents for retrieval as part of the RAG process. Args: data_folder (str): The path to the folder containing CSV and PDF files to be loaded. Returns: tuple: A tuple containing: - docs (list): A list of document objects loaded from the CSV and PDF files, to be used in document retrieval for RAG. - merged_csv_df (pd.DataFrame): A single pandas DataFrame containing all CSV data, with an additional 'AIAP_Batch_Number' column indicating the source of each row. \"\"\" docs = [] csv_dataframes = [] # Load CSV files for file in os.listdir(data_folder): if file.endswith(\".csv\"): csv_path = os.path.join(data_folder, file) loader = CSVLoader(file_path=csv_path) docs.extend(loader.load()) # Load as documents for RAG # Load CSV and add a column to indicate its source (AIAP batch number) df = pd.read_csv(csv_path, header=0, names=[\"Candidate\", \"Assessor\", \"Result\"]) df['AIAP_Batch_Number'] = file # Add source column csv_dataframes.append(df) # Load PDF files for file in os.listdir(data_folder): if file.endswith(\".pdf\"): pdf_path = os.path.join(data_folder, file) loader = PDFPlumberLoader(file_path=pdf_path) docs.extend(loader.load()) # Load as documents for RAG # Merge all CSV dataframes into one with a 'source_file' column merged_csv_df = pd.concat(csv_dataframes, ignore_index=True) return docs, merged_csv_df # Function to embed data (CSV and PDF) and store in FAISS def process_data_with_faiss(docs): \"\"\" Embeds documents (from CSV and PDF files) into vector representations and stores them in a FAISS index for efficient similarity search. This function processes a list of documents by splitting them into chunks for better embedding performance. It uses a pre-trained embedding model from Google Generative AI to convert the text into embeddings and stores the resulting vectors in a FAISS (Facebook AI Similarity Search) index, allowing for fast document retrieval based on similarity. Args: docs (list): A list of document objects to be embedded and stored. These documents typically come from CSV and PDF files. Returns: FAISS: A FAISS vector index containing the embedded document chunks, ready for retrieval operations. \"\"\" embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\") text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200) split_docs = text_splitter.split_documents(docs) vectors = FAISS.from_documents(split_docs, embeddings) return vectors #", "source": "app_v2.py"}, {"content": "Function to set up retrieval and question answering (RAG) def setup_rag_chain(llm, vectors, top_k): \"\"\" Sets up a Retrieval-Augmented Generation (RAG) conversational chain using the provided language model, FAISS vector index, and a memory buffer. This function configures a RAG chain to combine document retrieval with conversational memory, enabling context-aware question answering. The chain retrieves the top-k most relevant documents from the FAISS index based on the user's query and uses a language model to generate responses that consider both the retrieved documents and the conversation history. Args: llm: The language model (LLM) used for generating responses in the conversational retrieval chain. vectors: A FAISS vector index containing embedded document chunks, used for document retrieval. top_k (int): The number of top documents to retrieve for each query. Returns: ConversationalRetrievalChain: A RAG chain that retrieves relevant documents and generates responses using the LLM, while maintaining conversation history via a buffer memory. \"\"\" memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True, output_key=\"answer\") retriever = vectors.as_retriever(search_kwargs={\"k\": top_k}) rag_chain = ConversationalRetrievalChain.from_llm( llm=llm, retriever=retriever, memory=memory, return_source_documents=True ) return rag_chain # Function to create a Pandas Agent for CSV analysis with clear dataset context def setup_pandas_agent_with_context(llm, dataframe): \"\"\" Sets up a Pandas dataframe agent with explicit contextual instructions for analyzing a merged dataset containing technical assessment results. This function creates a Pandas agent capable of answering questions dynamically by analyzing a dataframe that contains the results of two AIAP technical assessments (Batch 13 and Batch 14). The agent is given a custom system-level prompt that provides specific instructions on how to interpret the dataframe, including how to distinguish between the two batches using the 'AIAP_Batch_Number' column. Args: llm: The language model (LLM) used for the Pandas agent, which interprets and responds to questions based on the provided dataframe. dataframe (pd.DataFrame): The merged dataframe containing data from Batch 13 and Batch 14, with columns 'Candidate', 'Assessor', 'Result', and 'AIAP_Batch_Number' (indicating the batch source). Returns: Agent: A Pandas dataframe agent configured with a system prompt that provides explicit instructions for how to analyze the merged dataset dynamically. This agent is verbose, meaning it shows reasoning and code, and it can handle parsing errors and execute potentially dangerous code when necessary. \"\"\" # Define a more explicit system-level prompt system_prompt = f\"\"\" You are working with a merged dataset containing the results of two technical assessments (Batch 13 and Batch 14). The dataset has the following columns: - Candidate: The name of the candidate. - Assessor: The name of the assessor for the assessment. - Result: Whether the candidate passed or failed the assessment. - AIAP_Batch_Number: This column indicates which assessment (Batch 13 or Batch 14) the data belongs to. Important: This single dataframe contains information from both Batch 13 and Batch 14. You do not need to look for a separate dataframe for each batch. When answering questions: 1. Always consider the entire dataframe, which includes data from both batches. 2. Use the 'AIAP_Batch_Number' column to distinguish between the two batches when necessary. 3. For questions about candidates in both assessments, look for candidates that appear with both Batch 13 and Batch 14 in the", "source": "app_v2.py"}, {"content": "'AIAP_Batch_Number' column. Please perform analysis dynamically based on the columns provided and the instructions above. \"\"\" # Set up the Pandas agent with the provided context in the system prompt pandas_agent = create_pandas_dataframe_agent( llm, dataframe, system_prompt=system_prompt, verbose=True, # Now shows reasoning and code agent_type=AgentType.OPENAI_FUNCTIONS, handle_parsing_errors=True, allow_dangerous_code=True ) return pandas_agent # Streamlit UI elements st.title(\"AIAP Batch Technical Assessment Q&A Platform\") # Initialize session state for chat history if it doesn't exist if 'chat_history' not in st.session_state: st.session_state.chat_history = [] # Automatically read data from the 'data' folder data_folder = \"data\" st.sidebar.write(f\"Loading data from: {data_folder}\") # Load CSV and PDF data for retrieval (RAG) docs, merged_csv_df = load_data_for_rag(data_folder) # Display CSV data in the sidebar for viewing purposes only st.sidebar.subheader(\"Merged CSV Data\") st.sidebar.dataframe(merged_csv_df) # Sidebar: Top-k parameter for RAG retrieval top_k = st.sidebar.slider(\"Top-k retrieved documents\", min_value=1, max_value=10, value=5) # Embed documents and store them in FAISS vectors = process_data_with_faiss(docs) rag_chain = setup_rag_chain(llm, vectors, top_k) # Setup Pandas Agent for the merged CSV file with context pandas_agent = setup_pandas_agent_with_context(llm, merged_csv_df) # Function to let LLM determine whether to use Pandas Agent or RAG for a question def determine_approach(user_question, pandas_agent, rag_chain): \"\"\" Determines whether to use the Pandas Agent or the Retrieval-Augmented Generation (RAG) system based on the user's question. This function prompts the language model (LLM) to analyze the user's question and decide whether to use the Pandas Agent (for CSV data analysis) or the RAG system (for document retrieval from PDF files). The decision is based on the nature of the question: - If the question involves data analysis, statistics, or specific candidate results from the CSV, the Pandas Agent is used. - If the question is more general or involves comparing information from both assessments, the RAG retrieval system is used. Args: user_question (str): The user's input question to be evaluated. pandas_agent: The agent responsible for analyzing the merged CSV data. rag_chain: The retrieval chain responsible for handling document retrieval from PDF files. Returns: str: The reasoning provided by the LLM, explaining whether the Pandas Agent or RAG system should be used to answer the user's question. \"\"\" # Ask LLM to determine the best approach reasoning_prompt = f\"\"\" You are an AI assistant. The provided dataset is merged from two technical assessments, with the 'AIAP_Batch_Number' column indicating the batch. Your task is to decide whether the user's question should be answered using RAG (for PDF documents) or Pandas Agent (for CSV analysis). If the question is about data analysis, statistics, or specific candidate results from both batches, Pandas Agent should be used. If the question is about general context or comparing information from both batches, use the RAG retrieval system. Here is the user's question: \"{user_question}\" Which approach should be used: Pandas Agent or RAG? Provide reasoning. \"\"\" # Send the reasoning prompt to the LLM response = llm([HumanMessage(content=reasoning_prompt)]) # Extract reasoning from LLM response reasoning = response.content # Extract reasoning from the response return reasoning # Modified function to handle the user's question using StreamlitCallbackHandler for logging def handle_question(user_question, pandas_agent, rag_chain): \"\"\" Handles a user's question by determining whether to use", "source": "app_v2.py"}, {"content": "the Pandas Agent (for CSV data) or RAG system (for document retrieval) and executes the appropriate approach to provide an answer. This function first determines the appropriate approach (Pandas Agent or RAG) using the `determine_approach` function based on the user's question. If the question relates to data analysis from the CSV, the Pandas Agent is used to analyze the merged dataset. The function also sets up Streamlit logging to capture the agent's reasoning and code execution. If the question relates to general document information or comparison, the RAG system retrieves relevant documents and generates a response using the language model. Args: user_question (str): The question provided by the user. pandas_agent: The agent responsible for analyzing the merged CSV data from Batch 13 and Batch 14. rag_chain: The retrieval chain responsible for handling document retrieval from PDF files using the RAG system. Returns: tuple: A tuple containing: - result (str): The answer to the user's question from either the Pandas Agent or RAG system. - retrieved_docs (list): A list of source documents retrieved by the RAG system (empty if Pandas Agent is used). - reasoning (str): The reasoning provided by the LLM on whether the Pandas Agent or RAG system was selected. \"\"\" reasoning = determine_approach(user_question, pandas_agent, rag_chain) if \"pandas agent\" in reasoning.lower(): full_question = f\"\"\" Remember, you are working with a single merged dataframe that contains data from both Batch 13 and Batch 14. The 'AIAP_Batch_Number' column indicates which batch each row belongs to. User question: {user_question} Please analyze the data with the appropriate functions and provide a clear, concise answer. \"\"\" # Create a container in Streamlit for logging log_container = st.container() # Set up StreamlitCallbackHandler to capture the agent's reasoning and code execution streamlit_callback = StreamlitCallbackHandler(parent_container=log_container) # Run the pandas agent and capture the logs using the callback handler result = pandas_agent.run(full_question, callbacks=[streamlit_callback]) # The result from the agent is returned directly, and logs are handled by the Streamlit callback return result, [], reasoning # No need to explicitly capture logs, handled by Streamlit callback else: response = rag_chain({\"question\": user_question}) return response['answer'], response[\"source_documents\"], reasoning # Function to handle commonly asked questions def set_common_question(question): st.session_state.common_question = question # Commonly Asked Questions in the Sidebar st.sidebar.subheader(\"Commonly Asked Questions\") common_questions = [ \"When is the deadline for AIAP Batch 13?\", \"What is the difference between AIAP Batch 13 and 14?\", \"The total number of candidates who passed both technical assessments?\", \"Which candidates are present in both technical assessments?\", \"Who is the assessor of Shuckle?\" ] # Sidebar buttons for commonly asked questions for question in common_questions: if st.sidebar.button(question): set_common_question(question) # Display chat history st.subheader(\"Chat History\") for message in st.session_state.chat_history: with st.chat_message(message[\"role\"]): st.write(message[\"content\"]) if \"reasoning\" in message: with st.expander(\"View Reasoning and Code\"): st.write(message[\"reasoning\"]) # Get the pre-filled question from the session state pre_filled_question = st.session_state.get(\"common_question\", \"\") # User input user_question = st.chat_input(\"Ask your question here:\") # If there's a pre-filled question from common questions, set the input text if pre_filled_question and not user_question: user_question = pre_filled_question st.session_state.common_question = \"\" # Clear after use if user_question: st.session_state.chat_input = \"\" # Clear input box after use", "source": "app_v2.py"}, {"content": "st.session_state.chat_history.append({\"role\": \"user\", \"content\": user_question}) with st.chat_message(\"user\"): st.write(user_question) with st.chat_message(\"assistant\"): with st.spinner(\"Retrieving answer...\"): answer, retrieved_docs, reasoning = handle_question(user_question, pandas_agent, rag_chain) st.write(answer) with st.expander(\"View Reasoning\"): st.write(reasoning) if retrieved_docs: with st.expander(\"Retrieved Documents\"): for doc in retrieved_docs: st.write(f\"**Document Source:** {doc.metadata['source']}\") st.write(f\"{doc.page_content[:500]}\") st.session_state.chat_history.append({\"role\": \"assistant\", \"content\": answer, \"reasoning\": reasoning})", "source": "app_v2.py"}, {"content": "## setup environment import os import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.model_selection import train_test_split from sklearn.compose import ColumnTransformer from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder def transform(data_path: str): \"\"\"Transform raw data to be ready for machine learning. Args: data_path (str): path to raw data .csv file Returns: tuple ((numpy arrays)): X_train, X_test, y_train, y_test \"\"\" # basic data cleaning and feature engineering df_clean = transform_eda(data_path=data_path) # use to define ordinal feature 'education' order education_to_year = { 'Children': 0.0, 'Less than 1st grade': 0.5, '1st 2nd 3rd or 4th grade': 2.5, '5th or 6th grade': 5.5, '7th and 8th grade': 7.5, '9th grade': 9.0, '10th grade': 10.0, '11th grade': 11.0, '12th grade no diploma': 12.0, 'High school graduate': 12.0, 'Some college but no degree': 14.0, 'Associates degree-academic program': 14.0, 'Associates degree-occup /vocational': 14.0, 'Bachelors degree(BA AB BS)': 16.0, 'Masters degree(MA MS MEng MEd MSW MBA)': 18.0, 'Prof school degree (MD DDS DVM LLB JD)': 20.0, 'Doctorate degree(PhD EdD)': 21.0, } # Define X as input features and y as the outcome variable X = df_clean.drop(columns=['income_group']) y = df_clean['income_group'] # Test/train split the data X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y) # Build a preprocessing step for numeric features numeric_features = X_train.select_dtypes(['int', 'float']).columns numeric_transformer = StandardScaler() # Build a preprocessing step for nominal features nominal_features = X_train.drop(columns=['education']).select_dtypes(['object', 'category']).columns nominal_transformer = OneHotEncoder( handle_unknown='ignore', min_frequency=0.05, # throw away small categories feature_name_combiner=lambda x, y: f'{x}.{y}') # combine feature name by '.' for post processing # Build a preprocessing step for ordinal features ordinal_features = ['education'] ordinal_transformer = OrdinalEncoder(categories=[list(education_to_year)]) # If using sklearn's pipelines, use sklearn.compose.ColumnTransformer to combine these pipelines, # taking care to apply each of them to the correct set of columns. Any feature not modified by # any of the pipelines can be passed through to the output dataframe (if needed). preprocessor = ColumnTransformer( transformers=[ ('numeric', numeric_transformer, numeric_features), ('nominal', nominal_transformer, nominal_features), ('ordinal', ordinal_transformer, ordinal_features) ], sparse_threshold=0) X_train_transformed = preprocessor.fit_transform(X_train) X_test_transformed = preprocessor.transform(X_test) # encode y data y_encode_map = {'- 50000.': 0., '50000+.' : 1.} y_train_transformed = y_train.map(y_encode_map).to_numpy() y_test_transformed = y_test.map(y_encode_map).to_numpy() return X_train_transformed, X_test_transformed, y_train_transformed, y_test_transformed def transform_eda(data_path: str): \"\"\"Data cleaning and feature engineering. Args: data_path (str): path to raw data (*.csv) Returns: DataFrame: final dataframe \"\"\" ## read in raw data df_data = pd.read_csv(data_path, index_col='id') ## data cleaning cols_na = ['hispanic_origin'] cols_to_cat = ['own_business_or_self_employed', 'detailed_industry_recode', 'detailed_occupation_recode', 'year', 'veterans_benefits'] df_clean = data_clean(df_data, cols_na, cols_to_cat, 'row') ## feature engineer # years of education education_to_year = { 'Children': 0.0, 'Less than 1st grade': 0.5, '1st 2nd 3rd or 4th grade': 2.5, '5th or 6th grade': 5.5, '7th and 8th grade': 7.5, '9th grade': 9.0, '10th grade': 10.0, '11th grade': 11.0, '12th grade no diploma': 12.0, 'High school graduate': 12.0, 'Some college but no degree': 14.0, 'Associates degree-academic program': 14.0, 'Associates degree-occup /vocational': 14.0, 'Bachelors degree(BA AB BS)': 16.0, 'Masters degree(MA MS MEng MEd MSW MBA)': 18.0, 'Prof school degree (MD DDS DVM LLB JD)': 20.0, 'Doctorate degree(PhD EdD)': 21.0, } df_clean['edu_year'] = df_clean['education'].map(education_to_year) # yearly wage assuming 40hrs work per week df_clean['wage_in_year'] = df_clean['wage_per_hour'] *", "source": "datapipeline.py"}, {"content": "40 * df_clean['weeks_worked_in_year'] / 100 # capital change: gain - loss # `capital_gains` and `capital_losses` can be dropped df_clean['capital_change'] = df_clean['capital_gains'] - df_clean['capital_losses'] df_clean.drop(columns=['capital_gains', 'capital_losses'], inplace=True) ## return final dataframe return df_clean def data_clean(df: pd.DataFrame, cols_na: list, cols_to_cat: list, drop_na = None): \"\"\"Perform data cleaning process on a DataFrame. Args: df (DataFrame): original dataframe cols_na (list): columns to convert `Null` to 'NA' string cols_to_cat (list): columns to convert dtype to 'category' drop_na (None | str, optional): how to drop `Null`. Defaults to None. - None: do nothing. - 'row': drop rows containing `Null`. - 'column': drop columns containing `Null`. Raises: NotImplementedError: drop_na not in [None, 'row', 'column'] Returns: df (DataFrame): cleaned data \"\"\" # convert `Null` to 'NA' string df[cols_na] = df[cols_na].fillna('NA') # drop Null if drop_na is None: # do nothing pass elif drop_na in ['row', 'column']: df = df.dropna(axis = 0 if drop_na=='row' else 1) else: raise NotImplementedError(\"Only support 'row' and 'column' now.\") # convert numeric cols to categorical for col in cols_to_cat: df[col] = df[col].astype('category') return df", "source": "datapipeline.py"}, {"content": "import numpy as np __DEBUG__ = False class DecisionTree: # only apply to binary class labels 0 and 1 def __init__(self, max_depth=None) -> None: \"\"\"Decision tree for binary classification. Args: max_depth (int|None, optional): maximum depth allowed. Defaults to None. \"\"\" # maximum depth allowed self.max_depth = max_depth # root node self.root = None return def gini(self, idx0, idx1): \"\"\"Calculate Gini Impurity. Args: idx0 (list): location mask of label 0 for the datapoints in a node. idx1 (list): location mask of label 1 for the datapoints in a node. Returns: impurity (float): Gini Impurity \"\"\" p0 = np.sum(idx0) / len(idx0) p1 = np.sum(idx1) / len(idx1) impurity = 1 - p0*p0 - p1*p1 return impurity def fit(self, X, y): \"\"\"Fit the decision tree. Args: X (2d-array): shape [n_samples, n_features] y (1d-array): shape [n_samples,] Returns: self (DecisionTree): self object \"\"\" self.root = TreeNode(tree=self) self.root.build(X, y) return self def predict(self, X, threshold=0.5): \"\"\"Predict class labels. Args: X (2d-array): shape [n_samples, n_features] threshold (float, optional): threshold for class 1. Defaults to 0.5. Returns: y_pred (1d-array): shape [n_samples] \"\"\" y_proba = self.predict_proba(X) y_pred = y_proba >= threshold return y_pred def predict_proba(self, X): \"\"\"Predict probability of class 1. Args: X (2d-array): shape [n_samples, n_features] Raises: NotImplementedError: The model is not fitted. Returns: y_proba (1d-array, shape [n_samples,]): probability of class 1. \"\"\" if self.root is None: raise NotImplementedError('Please fit the model first.') y_proba = np.zeros(shape=X.shape[0]) for i, x in enumerate(X): y_proba[i] = self.root.predict_proba(x) return y_proba class TreeNode: # only apply to binary class labels 0 and 1 def __init__(self, tree: DecisionTree, parent=None, depth=0) -> None: \"\"\"Node for DecisionTree. Args: tree (DecisionTree): The tree this node belongs to. parent (TreeNode|None, optional): Parent node. Defaults to None. depth (int, optional): Depth of this node in the tree. Defaults to 0. \"\"\" # pointer to the tree this node belongs to self.tree = tree # predict_proba value, only applicable if the node is a leaf self.value = None # gini impurity of this node self.gini = None # split criteria [feature_id, value], NOT applicable to leaf self.criteria = None # parent node self.parent = parent # left child node, NOT applicable to leaf self.left = None # right child node, NOT applicable to leaf self.right = None # if the node is leaf self.is_leaf = False # number of samples pass to this node during training self.samples = None # depth of current node in tree self.depth = depth return def build(self, X, y): \"\"\"Build current node. Args: X (2d-array): shape [n_samples, n_features] y (1d-array): shape [n_samples,] \"\"\" self.samples = X.shape[0] if __DEBUG__: print(f'Build node: depth = {self.depth}, sample size = {self.samples}') self.gini = self.cal_gini(y) # if reached max_depth, this is a leaf node if (self.tree.max_depth is not None) and (self.depth == self.tree.max_depth): self.build_leaf(X, y) return # if y is pure, this is a leaf node if np.unique(y).shape[0] == 1: self.build_leaf(X, y) return # search criteria best_feature_id = None best_criteria = None best_gini = np.Infinity for feature_id in range(X.shape[1]): feature = X[:, feature_id].flatten() for criteria in np.unique(feature): left_mask = feature < criteria right_mask = feature >= criteria if (np.sum(left_mask)==0) or (np.sum(right_mask)==0): # one", "source": "decision_tree.py"}, {"content": "side is empty continue left_weight = np.sum(left_mask) / self.samples right_weight = 1 - left_weight left_gini = self.cal_gini(y[np.argwhere(left_mask).flatten()]) right_gini = self.cal_gini(y[np.argwhere(right_mask).flatten()]) weighted_gini = left_weight*left_gini + right_weight*right_gini if weighted_gini < best_gini: # find a better candidate best_feature_id = feature_id best_criteria = criteria best_gini = weighted_gini # finalize if best_gini == np.Infinity: # case 1: no candidate self.build_leaf(X, y) else: # case 2: has candidate feature = X[:, best_feature_id].flatten() left_mask = feature < best_criteria right_mask = feature >= best_criteria self.left = TreeNode(tree=self.tree, parent=self, depth=self.depth + 1) self.left.build(X[np.argwhere(left_mask).flatten()], y[np.argwhere(left_mask).flatten()]) self.right = TreeNode(tree=self.tree, parent=self, depth=self.depth + 1) self.right.build(X[np.argwhere(right_mask).flatten()], y[np.argwhere(right_mask).flatten()]) self.criteria = [best_feature_id, best_criteria] return def build_leaf(self, X, y): \"\"\"Build current node as leaf. Args: X (2d-array): shape [n_samples, n_features] y (1d-array): shape [n_samples,] \"\"\" # build current node as leaf self.is_leaf = True self.value = np.average(y) return def cal_gini(self, y): \"\"\"Calculate Gini impurity. Args: y (1d-array): shape [n_samples,] Returns: impurity (float): Gini impurity \"\"\" impurity = DecisionTree().gini(idx0= y == 0, idx1= y == 1) return impurity def predict_proba(self, x): \"\"\"Prediction for a single sample. Args: x (1d-array): shape [n_features,] Returns: pred_proba (float): probability of class 1 \"\"\" # prediction for a single sample if self.is_leaf: return self.value else: # not leaf feature_id = self.criteria[0] value = x[feature_id] criteria = self.criteria[1] if value < criteria: return self.left.predict_proba(x) else: return self.right.predict_proba(x)", "source": "decision_tree.py"}, {"content": "\"\"\"Collection of EDA tool functions by myself.\"\"\" __version__ = '0.24.9.2' __author__ = 'Zhang Zhou' ## setup environment import pyodbc import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.cm as cm import seaborn as sns import pyprind from sklearn.feature_selection import mutual_info_classif, mutual_info_regression from sklearn.metrics import silhouette_samples, silhouette_score ## tool functions def inspect_data(df: pd.DataFrame, key = None): \"\"\"Print dataset inspection report. Args: df (DataFrame): dataset to be inspected key (None | list | str, optional): columns included to check row duplication. Defaults to None. - None: use all columns - 'index': based on df.index - str | list: column name(s) \"\"\" # shape, column names, etc. print(df.info(), '\\n') # number of duplicated rows if key == 'index': num_duplicated = df.index.duplicated().sum() else: num_duplicated = df.duplicated(subset=key).sum() print('Number of duplicated rows:', num_duplicated, '\\n') # null inspection df_null = df.isnull() print(f'Rows with NULL: {(df_null.sum(axis=1) > 0).sum():,d}' f'/{df.shape[0]:,d}') print(f'Columns with NULL: {(df_null.sum(axis=0) > 0).sum():,d}' f'/{df.shape[1]:,d}') print(f'Total NULL cells: {df_null.sum().sum():,d}' f' ({df_null.sum().sum() / (df.shape[0] * df.shape[1]):.2%})') print('\\n') # describe all columns for col in df.columns: print(df[col].describe(), '\\n') return def plot_cat_feature_dist(df_data: pd.DataFrame, cols: None | list = None, return_df: bool = False, figsize: tuple = (8, 6)): \"\"\"Make horizontal stacked barplot showing category feature distribution. Args: df_data (DataFrame): dataset cols (None | list, optional): columns for showing. Defaults to None. - None: show all columns. return_df (bool, optional): Need to return category proportion dataframe. Defaults to False. figsize (tuple, optional): figure size. Defaults to (8, 6). Returns: None|DataFrame: - None: if return_df==False - df (DataFrame): if return_df==True \"\"\" if cols is not None: # select the category columns df_data = df_data[cols] # generate df with columns: ['category', 'proportion', 'cat', 'feature'] dfs = [] for col in df_data.columns: df = df_data[col].value_counts(normalize=True).sort_values(ascending=False).reset_index() df['cat'] = df.index df.rename(columns={col: 'category'}, inplace=True) df['feature'] = col dfs.append(df) df = pd.concat(dfs, axis=0) # pivot table for plot df_plot = df.pivot(index='feature', columns='cat', values='proportion').fillna(0).reset_index() # plot df_plot.plot(x='feature', kind='barh', stacked=True, legend=False, figsize=figsize, edgecolor = \"none\") plt.xlim(0, 1) plt.xlabel('Proportion') plt.title('Category features distribution', fontsize=15) plt.show() # return df if requested if return_df == True: return df return def plot_mutual_info(df: pd.DataFrame, cols_obj: list, cols_cat: list, cols_disc: list, cols_cont: list, return_df: bool = False, figsize: tuple = (12, 12)): \"\"\"Plot mutual info heatmap. Args: df (DataFrame): dataset. cols_obj (list): columns with dtype='object' cols_cat (list): columns with dtype='category' cols_disc (list): columns with numeric data that need to be treated as discrete values. cols_cont (list): columns with numeric data that need to be treated as continuous values. return_df (bool, optional): if need to return mutual info dataframe. Defaults to False. figsize (tuple, optional): figure size. Defaults to (12, 12). Returns: (None | DataFrame): depend on return_df \"\"\" feature_list = cols_obj + cols_cat + cols_disc + cols_cont # mask to identify discrete features discrete_features = [True]*(len(feature_list) - len(cols_cont)) + [False]*len(cols_cont) # generate new datafrome with label encoding df_label = df[feature_list] for col in cols_obj: df_label[col] = df_label[col].astype('category').cat.codes for col in cols_cat: df_label[col] = df_label[col].cat.codes # calculate mutual info mutual_info = [] pbar = pyprind.ProgBar(len(feature_list)) # show progress bar in console for col in cols_obj + cols_cat + cols_disc: # discrete features mutual_info.append( mutual_info_classif(df_label, df_label[col],", "source": "eda_tools.py"}, {"content": "n_jobs=-1, discrete_features=discrete_features) ) pbar.update() for col in cols_cont: # continuous features mutual_info.append( mutual_info_regression(df_label, df_label[col], n_jobs=-1, discrete_features=discrete_features) ) pbar.update() # convert to DataFrame mutual_info = pd.DataFrame(mutual_info, index=feature_list, columns=feature_list) # plot fig, ax = plt.subplots(figsize=figsize) sns.heatmap(np.log10(mutual_info + 1e-6), # add small value to avoid divide zero error vmin=-2, vmax=0, cmap='YlOrBr', ax=ax) plt.title('Mutual information (log10 scale)', fontsize=15) plt.show() if return_df == True: return mutual_info def plot_silhouette_analysis(X, cluster_labels, n_clusters, ax, metric='euclidean'): \"\"\"Plot silhouette analysis Args: X (Array like, shape=(n_samples, n_features)): dataset cluster_labels (Array like, shape=(n_samples,)): cluster labels - Must have same index as `X`. - Labels must be integers 0 to (n-1), where n is the cluster number. n_clusters (int): number of clusters ax (Axe): axis to add the plot metric (str, optional): distance metric. Defaults to 'euclidean'. \"\"\" ax.set_xlim(-0.1, 1) # The (n_clusters+1)*10 is for inserting blank space between silhouette # plots of individual clusters, to demarcate them clearly. ax.set_ylim(0, X.shape[0] + (n_clusters + 1) * 10) # Compute the silhouette scores for each sample sample_silhouette_values = silhouette_samples(X, cluster_labels, metric=metric) y_lower = 10 for i in range(n_clusters): # Aggregate the silhouette scores for samples belonging to # cluster i, and sort them ith_cluster_silhouette_values = \\ sample_silhouette_values[cluster_labels == i] ith_cluster_silhouette_values.sort() size_cluster_i = ith_cluster_silhouette_values.shape[0] y_upper = y_lower + size_cluster_i color = cm.nipy_spectral(float(i) / n_clusters) ax.fill_betweenx( np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7, ) # Label the silhouette plots with their cluster number at the middle ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i)) # Compute the new y_lower for next plot y_lower = y_upper + 10 # gap is 10 silhouette_avg = silhouette_score(X, cluster_labels, metric=metric) ax.axvline(x=silhouette_avg, color='red', ls='--') ax.set_title(f'n_clusters = {n_clusters}, ' f'Silhouette score = {silhouette_avg:.2f}') ax.set_xlabel('The silhouette coefficient values') ax.set_ylabel('Cluster label') ax.set_yticks([]) # Clear the yaxis labels / ticks ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1]) return def query_Azure_to_dataframe(server, database, username, password, driver, query, index_col=None): \"\"\"Query a SQL database from Azure and return the results as a Pandas DataFrame.\"\"\" conn_str = ( f\"DRIVER={driver};\" f\"SERVER={server};\" f\"DATABASE={database};\" f\"UID={username};\" f\"PWD={password};\" \"Encrypt=yes;\" \"TrustServerCertificate=no;\" ) try: conn = pyodbc.connect(conn_str) df = pd.read_sql(query, conn, index_col=index_col) return df except pyodbc.Error as e: print(f\"Error connecting to database: {e}\") except Exception as e: print(f\"Error converting to DataFrame: {e}\") finally: conn.close() # local minimas/maximas def find_local_extrema(array): \"\"\"Find local minima and maxima indices in a 1D array. Args: array (1d-array): The input array Returns: tuple: (minima_indices, maxima_indices) \"\"\" # Handle edge cases if len(array) < 3: return [], [] # Create a boolean mask for local minima and maxima is_minimum = np.logical_and(array[1:-1] < array[:-2], # smaller than left array[1:-1] < array[2:]) # smaller than right is_maximum = np.logical_and(array[1:-1] > array[:-2], # larger than left array[1:-1] > array[2:]) # larger than right # Find indices of local minima and maxima minima_indices = np.where(is_minimum)[0] + 1 maxima_indices = np.where(is_maximum)[0] + 1 return minima_indices, maxima_indices", "source": "eda_tools.py"}, {"content": "from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import f1_score class Model: def __init__(self): # init your model here self.model = None # no fitted model yet return def train(self, params, X_train, y_train): \"\"\"Train model Args: params (dict): hyperparameters X_train (2d-array): shape [n_samples, n_features] y_train (1d-array): shape [n_samples,] Returns: score (float): f1 score on train dataset \"\"\" # Your implementation goes here # For our case, this function should train the initialised model and return the train f1 score # Return a evaluation metric (f1 in this case) as a single float so the caller can make use of it # initiate model self.model = RandomForestClassifier(**params) # fit model self.model.fit(X_train, y_train) # return f1 score score = f1_score(y_train, self.model.predict(X_train), average='weighted') return score def evaluate(self, X_test, y_test): \"\"\"Evaluate model Args: X_test (2d-array): shape [n_samples, n_features] y_test (1d-array): shape [n_samples,] Raises: NotImplementedError: Model is NOT trained. Returns: score (float): f1 score on test dataset \"\"\" # This function should use the trained model to predict the target for the test data and return the test f1 score if self.model is None: raise NotImplementedError('Please train model first.') score = f1_score(y_test, self.model.predict(X_test), average='weighted') return score def get_default_params(self): \"\"\"return the default params to be used for training the model from scratch \"\"\" # This function should return the default parameters to be used for training the model from scratch # The output of this function should be the same as the params parameter in the train function # Before submitting your work to gitlab, edit this function to return the optimized parameters for your model return {'max_depth': None, 'min_samples_leaf': 0.001, 'n_estimators': 500, 'n_jobs': -1, 'random_state': 314}", "source": "model.py"}, {"content": "import numpy as np from .decision_tree import DecisionTree __DEBUG__ = False class RandomForest: def __init__(self, n_trees=5, subsample_size=1.0, sample_with_replacement=True, max_depth=None, feature_proportion=1.0, random_state=None) -> None: \"\"\"Random forest binary classifier. Args: n_trees (int, optional): number of trees. Defaults to 5. subsample_size (float, optional): range (0, 1]. Defaults to 1.0. sample_with_replacement (bool, optional): Defaults to True. max_depth (int | None, optional): maximum tree depth allowed. Defaults to None. feature_proportion (float, optional): proportion of features used in each tree. Defaults to 1.0. random_state (int | None, optional): seed for random number generator. Defaults to None. \"\"\" self.n_trees = n_trees self.subsample_size = subsample_size self.sample_with_replacement = sample_with_replacement self.max_depth = max_depth self.feature_proportion = feature_proportion self.random_state = random_state # random number generator self.__rng__ = np.random.default_rng(random_state) return def fit(self, X, y): \"\"\"Fit RandomForest model. Args: X (2d-array): shape [n_samples, n_features] y (1d-array): shape [n_samples,], class labels 0/1 Returns: self (RandomForest): self object \"\"\" # sample/feature size/subsize self.sample_size_ = X.shape[0] self.subsample_size_ = int(np.ceil(self.sample_size_ * self.subsample_size)) self.feature_size_ = X.shape[1] self.subfeature_size_ = int(np.ceil(self.feature_size_ * self.feature_proportion)) # container for trees self.trees_ = [] # container for idx_list of features used for each tree self.features_ = [] # container for idx_list of samples used for each tree self.samples_ = [] # fit trees for _ in range(self.n_trees): if __DEBUG__: print('Fit tree', _) tree, features, samples = self.fit_tree(X, y) self.trees_.append(tree) self.features_.append(features) self.samples_.append(samples) if __DEBUG__: print(' Done') return self def fit_tree(self, X, y): \"\"\"Fit one DecisionTree. Args: X (2d-array): shape [n_samples, n_features] y (1d-array): shape [n_samples,], class labels 0/1 Returns: tuple: tree, features, samples - tree (DecisionTree): a fitted DecisionTree - features (array): idx of used features - samples (array): idx of used samples \"\"\" # select features features = self.__rng__.choice(self.feature_size_, self.subfeature_size_, replace=False) # select samples samples = self.__rng__.choice(self.sample_size_, self.subsample_size_, replace=self.sample_with_replacement) # get X_sub, y_sub X_sub = X[samples, :][:, features] y_sub = y[samples] # fit a tree tree = DecisionTree(max_depth=self.max_depth) tree.fit(X_sub, y_sub) return tree, features, samples def predict_proba(self, X): \"\"\"Predict probability of class 1. Args: X (2d-array): shape [n_samples, n_features] Raises: NotImplementedError: The model is not fitted. Returns: y_proba (1d-array, shape [n_samples,]): probability of class 1. \"\"\" # container of y_proba from each tree y_probas = [] for i in range(self.n_trees): tree = self.trees_[i] features = self.features_[i] X_feature_selected = X[:, features] y_proba = tree.predict_proba(X_feature_selected) y_probas.append(y_proba.reshape((1, -1))) # average predictions from all trees y_probas = np.concatenate(y_probas) y_proba = y_probas.mean(axis=0) return y_proba def predict(self, X, threshold=0.5): \"\"\"Predict class labels. Args: X (2d-array): shape [n_samples, n_features] threshold (float, optional): threshold for class 1. Defaults to 0.5. Returns: y_pred (1d-array): shape [n_samples] \"\"\" y_proba = self.predict_proba(X) y_pred = y_proba >= threshold return y_pred", "source": "random_forest.py"}, {"content": "from ..src import datapipeline from ..src import model def test_model(): data_path = \"/data/A1.csv\" X_train, X_test, y_train, y_test = datapipeline.transform(data_path) uut = model.Model() params = uut.get_default_params() train_f1 = uut.train(params, X_train, y_train) test_f1 = uut.evaluate(X_test, y_test) assert test_f1 >= 0.78, 'f1 score lower than minimum' print(\"Test passed\")", "source": "test_model.py"}, {"content": "import streamlit as st import pandas as pd from sklearn.impute import SimpleImputer from sklearn.preprocessing import StandardScaler import os import pickle import requests # Define the URL and the destination path url = \"https://storage.googleapis.com/aisg-mlops-pub-data/mlops-dsp/cancer.pkl\" dest_path = os.path.join(\"assets\", \"model.pkl\") # Check if the destination path exists if not os.path.exists(os.path.dirname(dest_path)): # If the directory does not exist, create it os.makedirs(os.path.dirname(dest_path)) # Check if the destination file exists if not os.path.isfile(dest_path): # If the file does not exist, download it msg = st.toast(\"Model doesn't exist. Downloading...\") response = requests.get(url) response.raise_for_status() # Make sure that the request was successful with open(dest_path, \"wb\") as f: f.write(response.content) msg.toast(f\"Model downloaded successfully.\") # Load the model with open(dest_path, 'rb') as file: rf_model = pickle.load(file) mapper = lambda input_dict: lambda ipt: input_dict[ipt] def predict_cancer_risk(): st.title(\"Lung Cancer Occurence Prediction\") # Collect user inputs age = st.slider(\"Age\", 18, 100, 18) smoking_duration = st.slider(\"Smoking Duration (years)\", 0, age) current_weight = st.slider(\"Current Weight (kg)\", 40, 125, 80) last_weight = st.slider(\"Last Known Weight (kg)\", 40, 125, 80) col0 = st.columns(2) air_pollution_exposure = col0[0].select_slider( \"Air Pollution Exposure\", [0, 1, 2], 1, mapper({0: \"Low\", 1: \"Medium\", 2: \"High\"}) ) frequency_of_tiredness = col0[1].select_slider( \"Frequency of Tiredness\", [0, 1, 2], 1, mapper({0: \"None/Low\", 1: \"Medium\", 2: \"High\"}) ) col1 = st.columns(3) gender_female = 1 if col1[0].radio(\"Gender\", (\"Male\", \"Female\")) == 'Male' else 1 copd_history = col1[1].radio(\"COPD History\", (\"No\", \"Yes\")) genetic_markers = col1[2].radio(\"Genetic Markers Present\", (\"No\", \"Yes\")) weight_diff = current_weight - last_weight gender_male = 0 if gender_female == 'Female' else 1 copd_history_no = 1 if copd_history == 'No' else 0 copd_history_yes = 0 if copd_history == 'No' else 1 genetic_markers_present = 1 if genetic_markers == 'Yes' else 0 # Create a DataFrame with user inputs df = pd.DataFrame({ 'age': [float(age)], 'air_pollution_exposure': [float(air_pollution_exposure)], 'current_weight': [float(current_weight)], 'frequency_of_tiredness': [float(frequency_of_tiredness)], 'smoking_duration': [float(smoking_duration)], 'weight_diff': [float(weight_diff)], 'gender_female': [float(gender_female)], 'gender_male': [float(gender_male)], 'copd_history_no': [float(copd_history_no)], 'copd_history_yes': [float(copd_history_yes)], 'genetic_markers': [float(genetic_markers_present)] }) # Make a prediction if st.button(\"Predict\"): prediction = rf_model.predict(df)[0] #xst.text(df.T) if prediction == 1: st.error(f\"Prediction: Lung Cancer Occurrence Likely\") else: st.success(f\"Prediction: Lung Cancer Occurence Not Likely\") # Show the app predict_cancer_risk()", "source": "app.py"}, {"content": "from pydantic.types import PositiveInt from typing import Callable import pandas as pd import numpy as np from sklearn.decomposition import PCA from sklearn.preprocessing import PowerTransformer, QuantileTransformer class Datapipeline(): def __init__(self, n_components: PositiveInt | None = None, random_state: int | None = None, ) -> None: \"\"\"Pipeline for data processing. Args: n_components (PositiveInt | None, optional): argument for PCA transformer. Defaults to None. random_state (int | None, optional): random seed. Defaults to None. \"\"\" # container for trained models/transformers self.is_fitted = False self.models = dict() self.n_conmponents = n_components self.random_state = random_state def transform_train_data(self, train_data_path: str): \"\"\"Transform silver train dataset. NOTE: No oversampling or SMOTE. Args: train_data_path (str): file path. Returns: (X_train, y_train): gold train dataset. \"\"\" # step 0: read in silver data df_0 = pd.read_csv(train_data_path) X_0 = df_0.iloc[:, :-1] y_0 = df_0.iloc[:, -1] # step 1: PCA on fraudulent samples pca = self.train_pca_transformer( # Only on V1~28 features, and fraudulent samples X=X_0[df_0['Class']==True].iloc[:, :28], y=y_0, # ignored n_components=self.n_conmponents, random_state=self.random_state, ) self.models[1] = pca # step 1 transformer df_1 = pca.transform(X_0.iloc[:, :28]) # ndarray df_1 = pd.DataFrame( # C1 and more pca features df_1, columns=[f'C{n+1}' for n in range(df_1.shape[1])], ) # add rest features df_1 = pd.concat([df_1, df_0.iloc[:,-2:]], axis=1) # step 2: convert and FE on 'Amount' X_2 = df_1.iloc[:, :-1] # remove last col y_2 = df_1['Class'] X_2['Amount'] = np.log10(X_2['Amount'] + 0.5) df_fe = pd.DataFrame() # container for new features df_fe['Amount_low'] = X_2['Amount'] < 0.2 df_fe['Amount_high'] = X_2['Amount'] > 2 # step 3: Transformer on num features X_3, y_3 = X_2.copy(), y_2.copy() # PowerTransformer pt_model = self.train_transformer( X=X_3, y=y_3, Model=PowerTransformer, ) df_pt = pd.DataFrame( pt_model.transform(X_3), columns=[col + '_pt' for col in X_3.columns] ) # QuantileTransformer qt_model = self.train_transformer( X=X_3, y=y_3, Model=QuantileTransformer, output_distribution='normal' ) df_qt = pd.DataFrame( qt_model.transform(X_3), columns=[col + '_qt' for col in X_3.columns] ) # save step 3 models self.models[3] = (pt_model, qt_model) # step -1: finalize X_train = pd.concat([df_pt, df_qt, df_fe], axis=1) y_train = y_3 self.is_fitted = True return X_train, y_train def transform_test_data(self, test_data_path: str): \"\"\"Transform silver test dataset. Args: test_data_path (str): file path. Raises: NotImplementedError: Must transform train data first. Returns: (X_test, y_test): gold test dataset. \"\"\" # pipeline fitting status check if self.is_fitted is False: raise NotImplementedError(\"Please transform train data first.\") # step 0: read in silver data df_0 = pd.read_csv(test_data_path) X_0 = df_0.iloc[:, :-1] y_0 = df_0.iloc[:, -1] # step 1: PCA transform pca = self.models[1] df_1 = pca.transform(X_0.iloc[:, :28]) # ndarray df_1 = pd.DataFrame( # C1 and more pca features df_1, columns=[f'C{n+1}' for n in range(df_1.shape[1])], ) # add rest features df_1 = pd.concat([df_1, df_0.iloc[:,-2:]], axis=1) # step 2: convert and FE on 'Amount' X_2 = df_1.iloc[:, :-1] # remove last col y_2 = df_1['Class'] X_2['Amount'] = np.log10(X_2['Amount'] + 0.5) df_fe = pd.DataFrame() # container for new features df_fe['Amount_low'] = X_2['Amount'] < 0.2 df_fe['Amount_high'] = X_2['Amount'] > 2 # step 3: Transformer on num features X_3, y_3 = X_2.copy(), y_2.copy() pt_model, qt_model = self.models[3] # PowerTransformer df_pt = pd.DataFrame( pt_model.transform(X_3), columns=[col + '_pt' for col in X_3.columns] ) # QuantileTransformer df_qt = pd.DataFrame( qt_model.transform(X_3), columns=[col + '_qt' for col in X_3.columns] ) # step -1: finalize X_test =", "source": "datapipeline.py"}, {"content": "pd.concat([df_pt, df_qt, df_fe], axis=1) y_test = y_3 return X_test, y_test @staticmethod def train_pca_transformer(X: pd.DataFrame, y=None, n_components: PositiveInt | None = None, random_state: int | None = None, **kwargs ) -> PCA: \"\"\"Train a PCA transformer. Args: X (DataFrame): Input dataframe, shape [n_samples, n_features] y (Any, optional): Ignored. Defaults to None. n_components (PositiveInt | None, optional): Number of PCA components. Defaults to None. random_state (int | None, optional): random number seed. Defaults to None. **kwargs: Pass to PCA model. Returns: PCA: An instance of trained PCA transformer. \"\"\" pca = PCA(n_components=n_components, random_state=random_state, **kwargs) pca.fit(X=X) return pca @staticmethod def train_transformer(X: pd.DataFrame, y: pd.DataFrame | pd.Series, Model: Callable, **kwargs): \"\"\"Train a data transformer. NOTE: This transformer must NOT change data shape. Args: X (DataFrame): Input X data. y (DataFrame | Series): Input y data. Model (Class): Transformer class, NOT instance. Returns: model: trained instance of the transformer. \"\"\" model = Model(**kwargs) model.fit(X, y) return model", "source": "datapipeline.py"}, {"content": "from typing import TypeVar import numpy as np class Activation: def __init__(self) -> None: pass def forward(self, X: np.ndarray, train_mode: bool) -> np.ndarray: pass def derivative(self, X: np.ndarray) -> np.ndarray: pass ActivationClass = TypeVar(\"ActivationClass\", bound=Activation) class MLPTwoLayers: # DO NOT adjust the constructor params def __init__(self, input_size=3072, hidden_size=100, output_size=10): self.rng = np.random.default_rng(314) # reproducibility self.lr = 0.0003 # learning rate self.train_mode = True # build 2 layers # no need to specify input_size self.layers = [ DenseLayer(hidden_size, self.lr, rng=self.rng, activation=SigmoidActivate()), DenseLayer(output_size, self.lr, rng=self.rng, activation=SigmoidActivate()) ] return def train(self): \"\"\"Set to train mode\"\"\" self.train_mode = True def eval(self): \"\"\"Set to eval mode\"\"\" self.train_mode = False def forward(self, features: np.ndarray) -> np.ndarray: \"\"\" Takes in the features returns the prediction \"\"\" X = features for layer in self.layers: y = layer.forward(X, self.train_mode) X = y return y def loss(self, predictions: np.ndarray, label: np.ndarray) -> float: \"\"\" Takes in the predictions and label returns the training loss \"\"\" # Binary cross entropy according to all label classes # sum everything term1 = -label * (np.log(predictions)) term2 = (1. - label) * np.log(1. - predictions) cost = np.sum(term1 - term2) # store sigma_y for backward self.sigma_y_ = predictions - label return cost def accuracy(self, predictions: np.ndarray, label: np.ndarray) -> float: \"\"\" Takes in the predictions and label returns the accuracy score \"\"\" label_pred = np.argmax(predictions, axis=1) return np.average(label_pred == np.argmax(label, axis=1)) def fit(self, X: np.ndarray, y: np.ndarray): \"\"\"Fit the model for 1 epoch. Args: X (ndarray): shape [n_samples, n_features] or [n_features,] y (ndarray): shape [n_samples, n_labels] Returns: self: \"\"\" if len(y.shape) == 1: y = y.reshape((1, -1)) # record last mode last_mode = self.train_mode self.train_mode = True predictions = self.forward(X) self.sigma_y_ = predictions - y self.backward() # restore last mode self.train_mode = last_mode return self def backward(self): \"\"\" Adjusts the internal weights/biases \"\"\" sigma_a = self.sigma_y_ for layer in self.layers[::-1]: # reverse order sigma_x = layer.backward(sigma_a) sigma_a = sigma_x return class DenseLayer: def __init__(self, size: int, learn_rate: float, activation: ActivationClass, rng: np.random.Generator) -> None: self.size = size self.lr = learn_rate self.w = None # weights self.b = None # biases self.feature_size = None self.activation = activation # ActivationClass self.rng = rng # random number generator return def backward(self, sigma_a: np.ndarray) -> np.ndarray: \"\"\"Compute backward propagation step. Args: sigma_a (ndarray): difference between activated output and expected output. Returns: sigma_x (ndarray): difference between input and expetected input. \"\"\" # sigma of logits output # [n_samples, n_hidden] # print(f'sigma_a {sigma_a.shape}, a_derivative {self.a_derivative.shape}') sigma_z = sigma_a * self.a_derivative # sigma of inputs # [n_samples, n_hidden] dot [n_hidden, n_features] # -> [n_samples, n_features] sigma_x = np.dot(sigma_z, self.w.T) # print(f'backward:\\n sigma_z {sigma_z.shape}, w.T {self.w.T.shape}, sigma_x {sigma_x.shape}') # gradient of weights # [n_hidden, n_samples] dot [n_samples, n_feature] # -> [n_hidden, n_feature] grad_w = np.dot(sigma_z.T, self.X).T # print(f' sigma_z.T {sigma_z.T.shape}, X {self.X.shape}') # gradient of biases # [n_hidden,] grad_b = np.sum(sigma_z, axis=0) # update weights and biases # print(f' w {self.w.shape}, grad_w {grad_w.shape}') self.w -= self.lr * grad_w self.b -= self.lr * grad_b return sigma_x def forward(self, X: np.ndarray, train_mode=False) -> np.ndarray: \"\"\"Compute forward propagation step Args: X (ndarray): Input in below shapes. -", "source": "mlp.py"}, {"content": "[n_samples, n_features] for multiple samples - [n_features,] for single sample is acceptable train_mode (bool): Defaults to False. Returns: (ndarray): activated output. \"\"\" if len(X.shape) == 1: # only 1 sample is passed in a flat form X = X.reshape((1, -1)) # shape = [n_samples, n_features] if self.feature_size is None: # initiate weights self.feature_size = X.shape[1] self.init_weights() # step 1: net input of layer # [n_samples, n_features] dot [n_features, n_hidden] # -> [n_samples, n_hidden] z = np.dot(X, self.w) + self.b # step 2: activation of layer a = self.activation.forward(z, train_mode) if train_mode: # record status # [n_samples, n_hidden] self.z = z self.a = a self.X = X # derivative of activation function # [n_samples, n_hidden] self.a_derivative = self.activation.derivative(X) # print(f'forward:\\n X {X.shape}, w {self.w.shape}, a {a.shape}') return a def init_weights(self) -> None: self.b = np.zeros(self.size) # bias -> 0 # Xavier normal initialization std = np.sqrt(2 / (self.feature_size + self.size)) self.w = self.rng.normal( loc=0.0, scale=std, size=(self.feature_size, self.size) ) return class SigmoidActivate(Activation): def __init__(self) -> None: super().__init__() self.activition = None def forward(self, X: np.ndarray, train_mode=True): \"\"\"Compute logistic function (sigmoid)\"\"\" activation = 1. / (1. + np.exp(-np.clip(X, -250, 250))) if train_mode: self.activition = activation return activation def derivative(self, X: np.ndarray): \"\"\"Compute derivative\"\"\" if self.activition is None: _ = self.forward(X) return self.activition * (1. - self.activition) class MLPTwoLayers_AccLoss(MLPTwoLayers): def __init__(self, input_size=3072, hidden_size=100, output_size=10, threshold=0.5): super().__init__(input_size, hidden_size, output_size) self.threshold = threshold def fit(self, X: np.ndarray, y: np.ndarray): \"\"\"Fit the model for 1 epoch. Args: X (ndarray): shape [n_samples, n_features] or [n_features,] y (ndarray): shape [n_samples, n_labels] Returns: self: \"\"\" if len(y.shape) == 1: y = y.reshape((1, -1)) # record last mode last_mode = self.train_mode self.train_mode = True predictions = self.forward(X) self.sigma_y_ = (predictions > self.threshold) - y self.backward() # restore last mode self.train_mode = last_mode return self", "source": "mlp.py"}, {"content": "import numpy as np import pandas as pd class Datapipeline(): def __init__(self,) -> None: return def transform(self, data_path: str) -> tuple[np.ndarray, np.ndarray]: \"\"\"Transform raw data to gold dataset. Args: data_path (str): file path. Returns: tuple[ndarray, ndarray]: X_array[n_sample, n_features], y_array[n_samples,] \"\"\" df = pd.read_csv(data_path) X = df.iloc[:, 1:-1] y = pd.DataFrame() for label in range(3): # one-hot encoding y[f'{label}'] = df['y'] == label return X.to_numpy('float32'), y.to_numpy('float32')", "source": "mlp_datapipeline.py"}, {"content": "import logging from ..src.mlp_datapipeline import Datapipeline from ..src.mlp import MLPTwoLayers as MLP import numpy as np def test_mlp_datapipeline(): data_path = \"/data/A3.csv\" dpl = Datapipeline() X_array, y_array = dpl.transform(data_path) assert isinstance(X_array, np.ndarray), 'X_array is not a numpy array' assert isinstance(y_array, np.ndarray), 'y_array is not a numpy array' def test_mlp(): data_path = \"/data/A3.csv\" dpl = Datapipeline() x, y = dpl.transform(data_path) model = MLP(input_size=4, hidden_size=16, output_size=3) total_loss = 0 total_inputs = 150 all_losses = [] for i in range(total_inputs): predictions = model.forward(x[i]) loss = model.loss(predictions, y[i]) all_losses.append(loss) total_loss += loss if i % 30 == 0: logging.info(f\"Average loss {total_loss/(i+1)}\") model.backward() # actual baseline loss to be adjusted #assert (total_loss/total_inputs) < 14 # Check that loss really decreased from start to end assert (all_losses[0] - all_losses[len(all_losses)-1]) > 0, 'loss did not decrease'", "source": "test_mlp.py"}, {"content": "class ConvolvedMatrices: # If your final matrix 1 is # 1 1 1 # 1 1 1 # 1 1 1 # then write your answer as a list of lists of integers: # matrix_1 = [ # [ 1, 1, 1], # [ 1, 1, 1], # [ 1, 1, 1], # ] # Do not change the variable names # Replace below with your response matrix_1 = [ [ 80, -685, -1028], [ -618, 573, -126], [ 265, 391, -100], ] # Replace below with your response matrix_2 = [ [ -128, -562, -200, -6, -24], [ 480, 80, -685, -1028, -122], [ -127, -618, 573, -126, 28], [ 924, 265, 391, -100, -235], [ 384, 280, 218, 279, 59], ] # Replace below with your response matrix_3 = [ [-128, -200, -24], [-127, 573, 28], [ 384, 218, 59], ]", "source": "convolved_matrices.py"}, {"content": "# test import logging if __name__ == '__main__': logger = logging.getLogger(__name__) logger.info('log test') print('print test')", "source": "train.py"}, {"content": "import hashlib import inspect from ..src import convolved_matrices as Module from ..src.convolved_matrices import ConvolvedMatrices as Response members = inspect.getmembers(Module) module_names = [member[-1].__name__ for member in members if inspect.ismodule(member[-1])] def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(Response.matrix_1) == 'e334b13d27c250667ac4b722cb3c2d23e79c53489a7c02473cb913c408205859', \"Wrong answer\" def test_matrix_2(): assert generate_hash(Response.matrix_2) == '713b0c1d8224bfe1bd0cd8eea3b301789281b661e0308d65fa936a5877848e4e', \"Wrong answer\" def test_matrix_3(): assert generate_hash(Response.matrix_3) == '664126d84de414cd6eacb8d3bc2b4587a1b3093dc9549e79f7ec884a19b9f9a7', \"Wrong answer\" def test_dependencies(): for module_name in module_names: has_no_deps = not (\"numpy\" in module_name or \"tensorflow\" in module_name or \"torch\" in module_name) assert has_no_deps, \"A tensor manipulation library was used\"", "source": "test_convolved_matrices.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim class CNNModel(nn.Module): def __init__(self, input_size, num_filters, kernel_size, output_size, lookback): super().__init__() self.conv1d = nn.Conv1d(input_size, num_filters, kernel_size, padding='same') self.relu = nn.ReLU() self.pool = nn.MaxPool1d(kernel_size) self.flatten = nn.Flatten() self.fc = nn.Linear(num_filters * lookback // kernel_size, output_size) self.metrics = None def forward(self, x): x = self.conv1d(x) x = self.relu(x) x = self.pool(x) x = self.flatten(x) x = self.fc(x) return x def fit(self, train_dataloader, test_dataloader, epochs, learning_rate): \"\"\"Return dataframe with record of metrics\"\"\" optimizer = optim.Adam(self.parameters(), lr=learning_rate) criterion = nn.MSELoss() total_batch = len(train_dataloader) if self.metrics is None: self.metrics = {'epoch': [], 'train_loss': [], 'test_loss': []} for epoch in range(epochs): print(f\"Epoch {epoch+1}/{epochs}\") if len(self.metrics['epoch']) == 0: self.metrics['epoch'].append(1) else: self.metrics['epoch'].append(self.metrics['epoch'][-1] + 1) self.train() # start training for one epoch for batch_idx, (inputs, labels) in enumerate(train_dataloader): optimizer.zero_grad() outputs = self.forward(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\" Batch {batch_idx+1}/{total_batch}, \" f\"Loss (mse): {loss.item():.4f}\") # eval after each epoch self.eval() # eval mode train_loss = self.evaluate(train_dataloader) test_loss = self.evaluate(test_dataloader) self.metrics['train_loss'].append(train_loss) self.metrics['test_loss'].append(test_loss) print(f\" Epoch finish: \" f\"train loss (rmse) = {train_loss:.4f}, \" f\"test loss (rmse) = {test_loss:.4f}\") def predict(self, dataloader): predictions = [] self.eval() with torch.no_grad(): for batch in dataloader: inputs, _ = batch outputs = self.forward(inputs) predictions.append(outputs) # concate predictions from all batches predictions = torch.cat(predictions, dim=0) return predictions def evaluate(self, dataloader): pred, actual = [], [] self.eval() with torch.no_grad(): for batch in dataloader: inputs, labels = batch outputs = self.forward(inputs) pred.append(outputs) actual.append(labels) # concate pred & actual from all batches pred = torch.cat(pred, dim=0) actual = torch.cat(actual, dim=0) # calculate metric mse = nn.MSELoss()(pred, actual) rmse = torch.sqrt(mse) return rmse.item()", "source": "cnn_model.py"}, {"content": "import pandas as pd import numpy as np from sklearn.preprocessing import OneHotEncoder, StandardScaler class DataPipeline: def __init__(self, test_size=0.3) -> None: \"\"\"Data process pipeline. Args: test_size (float, optional): size ratio of test dataset. Defaults to 0.3. \"\"\" self.test_size = test_size def run_data_pipeline(self, csv_path: str) \\ -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\"Data cleaning process. Args: csv_path (str): csv file path. Returns: Tuple[DataFrame, DataFrame]: df_train, df_test \"\"\" # read in csv df_raw = self.read_csv_file(csv_path=csv_path) # data clean df_clean = self.clean_data(df_raw=df_raw) # feature engineer df_train, df_test = self.feature_engineer(df_clean=df_clean) return df_train, df_test def run_NN_data_pipeline(self, csv_path: str) \\ -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\"Data cleaning process for NN model. Args: csv_path (str): csv file path. Returns: Tuple[DataFrame, DataFrame]: df_train, df_test \"\"\" # read in csv df_raw = self.read_csv_file(csv_path=csv_path) # data clean df_clean = self.clean_data(df_raw=df_raw) # feature engineer df_train, df_test = self.feature_engineer_NN(df_clean=df_clean) return df_train, df_test def read_csv_file(self, csv_path: str) -> pd.DataFrame: \"\"\"Read raw csv file as dataframe. Args: csv_path (str): csv file path. Returns: DataFrame: Raw dataframe \"\"\" df_raw = pd.read_csv(csv_path) return df_raw def clean_data(self, df_raw: pd.DataFrame) -> pd.DataFrame: \"\"\"Clean data. Args: df_raw (DataFrame): Raw dataframe Returns: DataFrame: Clean dataframe \"\"\" # select useful features df_clean = df_raw[['pm2.5', 'DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir']] # generate datetime column df_clean['datetime'] = pd.to_datetime( df_raw[['year', 'month', 'day', 'hour']]) # fill na df_clean['pm2.5'] = self.fillna_yesterday(df_clean['pm2.5']) # drop na in case null appear at start of time series df_clean.dropna(inplace=True) return df_clean def fillna_yesterday(self, data: pd.Series) -> pd.Series: \"\"\"Fill Null value with yesterday's data. NOTE: Null appear at the data beginning will NOT be filled. Args: data (Series): column to be filled. Returns: Series: data with Null filled. \"\"\" filled_data = data.copy() for i in range(len(data)): if not filled_data.isna().iloc[i]: continue # found null if i < 24: continue filled_data.iloc[i] = filled_data.iloc[i - 24] # yesterday return filled_data def feature_engineer(self, df_clean: pd.DataFrame) \\ -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\"Feature engineer until ready for modeling. Args: df_clean (DataFrame): output from clean_data method. Returns: Tuple[DataFrame, DataFrame]: df_train, df_test \"\"\" # set datetime to index df_clean.set_index('datetime', inplace=True) df_clean.index.freq = 'H' # target dataframe df_target = df_clean[['pm2.5']] df_target['pm2.5_diff'] = df_target['pm2.5'].diff() # feature dataframe df_feature = pd.DataFrame() df_feature['pm2.5_L1'] = df_target['pm2.5'].shift(1) df_feature['pm2.5_diff_L1'] = df_target['pm2.5_diff'].shift(1) df_feature['pm2.5_diff_L12'] = df_target['pm2.5_diff'].shift(12) df_feature['pm2.5_diff_L24'] = df_target['pm2.5_diff'].shift(24) for feat in ['Iws', 'Is', 'Ir']: df_feature[f'{feat}_L1'] = df_clean[feat].shift(1) df_feature[f'{feat}_diff_L1'] = df_clean[feat].diff().shift(1) for lag in [1, 2]: df_feature[f'DEWP_diff_L{lag}'] = df_clean['DEWP'].diff().shift(lag) for lag in [1, 2, 11, 24]: df_feature[f'TEMP_diff_L{lag}'] = df_clean['TEMP'].diff().shift(lag) for lag in [7, 23]: df_feature[f'PRES_diff_L{lag}'] = df_clean['PRES'].diff().shift(lag) # one hot encode 'cbwd' self.cbwd_1hot_ = OneHotEncoder(handle_unknown='ignore', sparse_output=False) self.cbwd_1hot_.fit(df_clean[['cbwd']]) df_1hot = pd.DataFrame(self.cbwd_1hot_.transform(df_clean[['cbwd']]), columns=self.cbwd_1hot_.get_feature_names_out(), index=df_clean.index) # Lag 1 to avoid look ahead for feat in df_1hot.columns: df_feature[f'{feat}_L1'] = df_1hot[feat].shift(1) df_clean['datetime'] = df_clean.index df_feature['is_weekend'] = df_clean['datetime'].dt.day_name()\\ .isin(['Saturday', 'Sunday']) # combine target and feature dataframe df_final = pd.concat([df_target[['pm2.5_diff']], df_feature], axis=1) # drop na df_final = df_final.dropna() # train/test split split_idx = df_final.shape[0] * (1 - self.test_size) split_idx = int(split_idx) df_train = df_final.iloc[:split_idx, :] df_test = df_final.iloc[split_idx:, :] # standard scaler scaler = StandardScaler() df_train.iloc[:, :-5] = scaler.fit_transform(df_train.iloc[:, :-5]) df_test.iloc[:, :-5] = scaler.transform(df_test.iloc[:, :-5]) # reserve scaler for inverse transform self.scaler_ = scaler return df_train, df_test def feature_engineer_NN(self, df_clean: pd.DataFrame) \\ -> tuple[pd.DataFrame, pd.DataFrame]: \"\"\"Feature engineer until ready for NN modeling. Args:", "source": "datapipeline.py"}, {"content": "df_clean (DataFrame): output from clean_data method. Returns: Tuple[DataFrame, DataFrame]: df_train, df_test \"\"\" # container for FE df_fe = df_clean.copy() # sin & cos for periods step = pd.Series(data=df_fe.index, index=df_fe.index) period = {'6H': 6, '12H': 12, 'Day': 24, 'Year': 24 * 365, } for p in period: df_fe[f'{p}_sin'] = np.sin(step * (2 * np.pi / period[p])) df_fe[f'{p}_cos'] = np.cos(step * (2 * np.pi / period[p])) # one hot encode 'cbwd' self.cbwd_1hot_ = OneHotEncoder(handle_unknown='ignore', sparse_output=False) self.cbwd_1hot_.fit(df_fe[['cbwd']]) df_1hot = pd.DataFrame(self.cbwd_1hot_.transform(df_fe[['cbwd']]), columns=self.cbwd_1hot_.get_feature_names_out(), index=df_fe.index) for col in df_1hot.columns: df_fe[col] = df_1hot[col] # drop 'cbwd' and 'datetime' df_fe.drop(columns=['cbwd', 'datetime'], inplace=True) # insert target to the 1st col df_fe.insert(0, 'pm2.5_diff', df_fe['pm2.5'].diff()) # train/test split split_idx = df_fe.shape[0] * (1 - self.test_size) split_idx = int(split_idx) df_train = df_fe.iloc[:split_idx, :] df_test = df_fe.iloc[split_idx:, :] # normalize the left 8 cols # others (categorical, sin/cos) should be intact self.train_mean_ = df_train.iloc[:, :8].mean() self.train_std_ = df_train.iloc[:, :8].std() df_train.iloc[:, :8] = \\ (df_train.iloc[:, :8] - self.train_mean_) / self.train_std_ df_test.iloc[:, :8] = \\ (df_test.iloc[:, :8] - self.train_mean_) / self.train_std_ return df_train, df_test", "source": "datapipeline.py"}, {"content": "# from src.data_pipeline import Datapipeline # from src.evaluation import evaluate # from src.model import Model # def run_experiment(data_path, lags=[]): # # Read data # # Perform data split # for lag in lags: # # Fit and evaluate over for each lag value # metrics_dict[f\"lag_{lag}\"] = { # \"train_error\": train_error # \"test_error\": test_error # } # return model, metrics_dict", "source": "ml_experiment.py"}, {"content": "import pandas as pd from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import root_mean_squared_error class ForecastModel: def __init__(self): self.model = RandomForestRegressor(n_jobs=-1, random_state=314) def fit(self, X, y): \"\"\"Fit model.\"\"\" self.model.fit(X, y) def evaluate(self, X_train, y_train, X_test, y_test): \"\"\"Evaluate metric (MSE).\"\"\" y_train_pred = self.model.predict(X_train) train_error = root_mean_squared_error(y_train, y_train_pred) y_test_pred = self.model.predict(X_test) test_error = root_mean_squared_error(y_test, y_test_pred) return train_error, test_error def predict(self, X): \"\"\"Make prediction.\"\"\" return self.model.predict(X) def data_transform(self, data: pd.DataFrame): \"\"\"Split dataset into X, y for model input Args: data (DataFrame): output from datapipeline. Returns: Tuple[DataFrame, Series]: X, y \"\"\" X = data.iloc[:, 1:] y = data.iloc[:, 0] return X, y", "source": "ml_model.py"}, {"content": "import torch import torch.nn as nn import torch.optim as optim class RNNModel(nn.Module): def __init__(self, input_size, num_lstm, num_layers, output_size): super(RNNModel, self).__init__() self.lstm = nn.LSTM(input_size, num_lstm, num_layers, batch_first=True) self.fc = nn.Linear(num_lstm, output_size) self.metrics = None def forward(self, x): _, (hidden, _) = self.lstm(x) output = self.fc(hidden[-1]) return output def fit(self, train_dataloader, test_dataloader, epochs, learning_rate): \"\"\"Return dataframe with record of metrics\"\"\" optimizer = optim.Adam(self.parameters(), lr=learning_rate) criterion = nn.MSELoss() total_batch = len(train_dataloader) if self.metrics is None: self.metrics = {'epoch': [], 'train_loss': [], 'test_loss': []} for epoch in range(epochs): print(f\"Epoch {epoch+1}/{epochs}\") if len(self.metrics['epoch']) == 0: self.metrics['epoch'].append(1) else: self.metrics['epoch'].append(self.metrics['epoch'][-1] + 1) self.train() # start training for one epoch for batch_idx, (inputs, labels) in enumerate(train_dataloader): optimizer.zero_grad() outputs = self.forward(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() print(f\" Batch {batch_idx+1}/{total_batch}, \" f\"Loss (mse): {loss.item():.4f}\") # eval after each epoch self.eval() # eval mode train_loss = self.evaluate(train_dataloader) test_loss = self.evaluate(test_dataloader) self.metrics['train_loss'].append(train_loss) self.metrics['test_loss'].append(test_loss) print(f\" Epoch finish: \" f\"train loss (rmse) = {train_loss:.4f}, \" f\"test loss (rmse) = {test_loss:.4f}\") def predict(self, dataloader): predictions = [] self.eval() with torch.no_grad(): for batch in dataloader: inputs, _ = batch outputs = self.forward(inputs) predictions.append(outputs) # concate predictions from all batches predictions = torch.cat(predictions, dim=0) return predictions def evaluate(self, dataloader): pred, actual = [], [] self.eval() with torch.no_grad(): for batch in dataloader: inputs, labels = batch outputs = self.forward(inputs) pred.append(outputs) actual.append(labels) # concate pred & actual from all batches pred = torch.cat(pred, dim=0) actual = torch.cat(actual, dim=0) # calculate metric mse = nn.MSELoss()(pred, actual) rmse = torch.sqrt(mse) return rmse.item()", "source": "rnn_model.py"}, {"content": "import numpy as np import pandas as pd import torch from torch.utils.data import Dataset class WindowGenerator(Dataset): def __init__(self, data: pd.DataFrame, lookback: int, lookahead: int): self.df = data.copy() # avoid editing original data self.lookback = lookback self.lookahead = lookahead # Calculate the length of the dataset based on lookback and lookahead self.length = len(self.df) - self.lookback - self.lookahead + 1 # Calculate pm2.5_diff based on number of lookahead self.df['pm2.5_diff'] = self.df['pm2.5'] - self.df['pm2.5'].shift(lookahead) def __len__(self): return self.length def __getitem__(self, idx: int): # Get the starting index for the window start_idx = idx # Extract features from the dataframe (col 1 onwards) features = self.df.iloc[start_idx: start_idx + self.lookback, 1:] # Extract label at the lookahead position (col 0) label = self.df.iloc[start_idx + self.lookback + self.lookahead - 1, 0] # Convert features and label to tensors features = torch.tensor(features.values, dtype=torch.float) # Add dimension for single value label = torch.tensor(label, dtype=torch.float).unsqueeze(0) return features, label class WindowGenerator_cnn1d(Dataset): def __init__(self, data: pd.DataFrame, lookback: int, lookahead: int): self.df = data.copy() # avoid editing original data self.lookback = lookback self.lookahead = lookahead # Calculate the length of the dataset based on lookback and lookahead self.length = len(self.df) - self.lookback - self.lookahead + 1 # Calculate pm2.5_diff based on number of lookahead self.df['pm2.5_diff'] = self.df['pm2.5'] - self.df['pm2.5'].shift(lookahead) def __len__(self): return self.length def __getitem__(self, idx: int): # Get the starting index for the window start_idx = idx # Extract features from the dataframe (col 1 onwards) features = self.df.iloc[start_idx: start_idx + self.lookback, 1:] # Extract label at the lookahead position (col 0) label = self.df.iloc[start_idx + self.lookback + self.lookahead - 1, 0] # Convert features and label to tensors features = torch.tensor(features.values, dtype=torch.float) # Add dimension for single value label = torch.tensor(label, dtype=torch.float).unsqueeze(0) return torch.transpose(features, 0, 1), label", "source": "windowing.py"}, {"content": "import pandas as pd import numpy as np from src.datapipeline import DataPipeline def test_clean_data(): \"\"\"Tests if run_data_pipeline cleans data correctly with no missing values\"\"\" # Create a sample DataFrame with no missing values data = {'pm2.5': range(10), 'DEWP': [10] * 10, 'TEMP': [20] * 10, 'PRES': [30] * 10, 'cbwd': ['N'] * 10, 'Iws': [5] * 10, 'Is': [0] * 10, 'Ir': [0] * 10, 'year': [2023] * 10, 'month': [10] * 10, 'day': range(1, 11), 'hour': range(1, 11)} df_raw = pd.DataFrame(data) # Create the DataPipeline object pipeline = DataPipeline() # test clean_data df_clean = pipeline.clean_data(df_raw) # Assert expected columns are selected expected_columns = ['pm2.5', 'DEWP', 'TEMP', 'PRES', 'cbwd', 'Iws', 'Is', 'Ir', 'datetime'] assert list(df_clean.columns) == expected_columns # Assert no missing values assert not df_clean.isnull().values.any() def test_data_clean_missing_values(): \"\"\"Tests if run_data_pipeline fills missing values with yesterday's data\"\"\" # Create a sample DataFrame with missing values data = {'pm2.5': [np.nan]*24 + list(range(100)) + [np.nan]*24, 'DEWP': [10] * 148, 'TEMP': [20] * 148, 'PRES': [30] * 148, 'cbwd': ['N'] * 148, 'Iws': [5] * 148, 'Is': [0] * 148, 'Ir': [0] * 148, 'year': [2023] * 148, 'month': [10] * 148, 'day': [10] * 148, 'hour': [0] * 148} df_raw = pd.DataFrame(data) # Create the DataPipeline object pipeline = DataPipeline() # Run the data pipeline clean_series = pipeline.fillna_yesterday(df_raw['pm2.5']) # Assert missing values are filled except for the beginning assert clean_series.iloc[24:].isnull().sum() == 0 assert pd.isna(clean_series.iloc[0]) assert pd.isna(clean_series.iloc[1]) # Assert filling uses yesterday's data assert clean_series.iloc[-1] == clean_series.iloc[-1 - 24]", "source": "test_datapipeline.py"}, {"content": "class ContextVector: # If your final matrix 1 is # 0.2345 0.3678 0.1234 0.9876 # then round your answer to 2 decimal places # and write as a list: # matrix_1 = [ # [ 0.23, 0.37, 0.12, 0.99] # ] # Do not change the variable names # Replace below with your response vector = [ [ 0.53, 0.23, 0.68, 0.45] ]", "source": "context_vector.py"}, {"content": "import hashlib from ..src.context_vector import ContextVector def generate_hash(ans): '''Converts answer to hash''' ans_str = str(ans) m = hashlib.sha256() m.update(ans_str.encode()) ans_hash = m.hexdigest() return ans_hash def test_matrix_1(): assert generate_hash(ContextVector.vector) == 'f06bc9a074f78d542d34c40b2c9c511166dd6a0a03d04363ff0cc927d96ddcf4', \"Wrong answer\" def test_pass_ci(): assert (1 - 1) == 0", "source": "test_context_vector.py"}]