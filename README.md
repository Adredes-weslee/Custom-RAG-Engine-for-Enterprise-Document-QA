# ğŸ§  Custom RAG Engine for Enterprise Document QA

> A production-ready, local-first Retrieval-Augmented Generation (RAG) system for enterprise document analysis. Features GPU acceleration with CPU fallback, hybrid embeddings for code and text understanding, and powerful local LLM inference via Ollama.

---

## ğŸ¯ Project Overview

This RAG system is designed for **enterprise document analysis** with a focus on **GitLab repository data** and **code understanding**. Built with a local-first architecture that leverages your hardware resources while maintaining complete data privacy.

### ğŸ—ï¸ Architecture Highlights
- **Local-First Design**: All processing happens on your hardware - no external API dependencies
- **GPU-Accelerated Processing**: Leverages CUDA for 10-50x faster data processing 
- **Specialized Embeddings**: Code-aware embeddings for Python files, semantic embeddings for notebooks
- **Privacy-First**: Complete data privacy - no information sent to external services
- **Production-Ready**: Comprehensive testing and deployment scripts

---

## ğŸš€ Key Features

| Feature | Local Development | Sharing Options |
|---------|------------------|-----------------|
| **Model Selection** | `llama3.2:3b`, `llama3.1:8b`, `llama3.2:1b` | Same (via ngrok/VPS) |
| **Processing** | GPU-accelerated data ingestion | Pre-built indices |
| **Performance** | Maximum quality enhancement | Full local performance |
| **Privacy** | Complete local processing | Complete local processing |
| **Deployment** | Local Streamlit app | Tunneling/Self-hosting |

### âš ï¸ **Important: Streamlit Cloud Limitations**

**Streamlit Community Cloud does NOT support this application** due to:
- âŒ Cannot install or run Ollama server
- âŒ Cannot access local model servers (localhost:11434)
- âŒ Missing system dependencies (cmake, swig, pkg-config)
- âŒ Insufficient resources for local model inference

**âœ… This application is designed for LOCAL deployment only.**

---

## ğŸ“‚ Complete Project Structure

```
Custom-RAG-Engine-for-Enterprise-Document-QA/
â”œâ”€â”€ ğŸ“„ README.md                              # This comprehensive guide
â”œâ”€â”€ ğŸ“„ requirements.txt                       # Local dependencies (GPU/CPU)
â”œâ”€â”€ ğŸ“„ setup_models.py                       # Ollama model downloader
â”œâ”€â”€ ğŸ“„ run_data_ingestion.py                 # Data processing launcher
â”œâ”€â”€ ğŸ“„ check.py                              # System compatibility checker
â”œâ”€â”€ ğŸ—‚ï¸ data/                                 
â”‚   â””â”€â”€ aiap17-gitlab-data/                  # GitLab repository data (23 people)
â”œâ”€â”€ ğŸ—‚ï¸ src/                                  # Main application code
â”‚   â”œâ”€â”€ ğŸ“„ main.py                           # ğŸ¯ Streamlit app entry point
â”‚   â””â”€â”€ ğŸ—‚ï¸ rag_engine/                      # Core RAG engine
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ data_processing/             # Data ingestion pipeline
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_enhancement.py       # LLM-powered text enhancement
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ data_ingestion.py         # Main data processing pipeline
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ file_retrieval.py         # GitLab file discovery
â”‚       â”‚   â””â”€â”€ ğŸ“„ text_extraction.py        # Multi-format text extraction
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ embeddings/                  # Embedding generation & indexing
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ embedding_generation.py   # Hybrid embeddings (code + text)
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ faiss_index.py           # GPU-accelerated FAISS operations
â”‚       â”‚   â””â”€â”€ ğŸ“„ model_loader.py          # Model loading with GPU detection
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ evaluation/                  # Response quality evaluation
â”‚       â”‚   â””â”€â”€ ğŸ“„ evaluation_agent.py       # Judge model evaluation
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ models/                      # LLM integration
â”‚       â”‚   â””â”€â”€ ğŸ“„ ollama_model.py          # Ollama client wrapper
â”‚       â”œâ”€â”€ ğŸ—‚ï¸ retrieval/                   # RAG pipeline components
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ document_store.py         # Document metadata management
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ question_handler.py       # Query routing & processing
â”‚       â”‚   â””â”€â”€ ğŸ“„ rag_chain.py             # LangChain pipeline orchestration
â”‚       â””â”€â”€ ğŸ—‚ï¸ ui/                          # User interface
â”‚           â””â”€â”€ ğŸ“„ streamlit_ui.py          # Streamlit UI components
â”œâ”€â”€ ğŸ—‚ï¸ tests/                               # Comprehensive test suite
â”‚   â”œâ”€â”€ ğŸ“„ run_tests.py                     # Main test runner
â”‚   â”œâ”€â”€ ğŸ“„ test_requirements.py             # Dependency verification
â”‚   â”œâ”€â”€ ğŸ“„ test_data_processing.py          # Data pipeline tests
â”‚   â”œâ”€â”€ ğŸ“„ test_embeddings_comprehensive.py # Embedding & GPU tests
â”‚   â””â”€â”€ ğŸ“„ test_rag_retrieval.py           # End-to-end RAG tests
â”œâ”€â”€ ğŸ—‚ï¸ utils/                               # Shared utilities
â”‚   â”œâ”€â”€ ğŸ“„ logging_setup.py                 # Comprehensive logging
â”‚   â””â”€â”€ ğŸ“„ model_config.py                  # Environment-aware configuration
â””â”€â”€ ğŸ—‚ï¸ Generated Files/ (Created by data ingestion)
    â”œâ”€â”€ ğŸ“„ faiss_code_index.bin             # Code embeddings index
    â”œâ”€â”€ ğŸ“„ faiss_non_code_index.bin         # Notebook embeddings index
    â”œâ”€â”€ ğŸ“„ code_docstore.json               # Enhanced Python file content
    â””â”€â”€ ğŸ“„ non_code_docstore.json           # Enhanced notebook content
```

---

## ğŸ› ï¸ Installation & Setup

### ğŸ”§ Prerequisites
- **Python 3.9+**
- **CUDA 12.6+** (recommended for GPU acceleration)
- **16GB+ RAM** (for model processing)
- **Ollama** (for local LLM inference)
- **Git LFS** (for large model files)

### ğŸš€ Quick Start

#### 1. Environment Setup
```bash
# Clone repository
git clone <your-repository-url>
cd Custom-RAG-Engine-for-Enterprise-Document-QA

# Install dependencies
pip install -r requirements.txt

# Verify system compatibility
python check.py
```

#### 2. Install Ollama
```bash
# Install Ollama (choose your platform)
# Linux/macOS:
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: Download from https://ollama.ai/download
# OR using winget:
winget install Ollama.Ollama
```

#### 3. Setup Models
```bash
# Download required models
python setup_models.py

# Expected output:
# ğŸ  Detected local development environment
# ğŸ“¥ Downloading llama3.2:3b for data enhancement...
# ğŸ“¥ Downloading llama3.1:8b for evaluation...
# ğŸ“¥ Downloading llama3.2:1b for fallback...
# âœ… All models ready for local development!
```

#### 4. Start Ollama Server
```bash
# Start Ollama server (runs on localhost:11434)
ollama serve

# Verify models are available
ollama list
```

#### 5. Data Processing (GPU Recommended)
```bash
# Quick test (3 people, ~30 files)
python run_data_ingestion.py --test

# Custom limits
python run_data_ingestion.py --limit-people 5 --limit-files 20

# Full processing (all 23 people, ~1000+ files)
python run_data_ingestion.py

# Expected outputs:
# faiss_code_index.bin        (~10-50MB)
# faiss_non_code_index.bin    (~20-100MB)  
# code_docstore.json          (~50-200MB)
# non_code_docstore.json      (~100-500MB)
```

#### 6. Run Application
```bash
# Launch Streamlit app (in new terminal)
streamlit run src/main.py

# Application will automatically:
# âœ… Connect to local Ollama server
# âœ… Load pre-built indices
# âœ… Initialize RAG interface
# ğŸŒ Available at: http://localhost:8501
```

---

## ğŸ¯ Deployment Options

### ğŸ  Local Development (Primary Method) âœ…

**Best for**: All use cases - this is the ONLY officially supported deployment

```bash
# Complete local setup
ollama serve                               # Start model server
streamlit run src/main.py                 # Start web interface

# Capabilities:
âœ… GPU-accelerated processing (10-50x faster)
âœ… High-quality LLM enhancement
âœ… Complete model selection
âœ… Maximum performance
âœ… Complete privacy
âœ… No API costs
âœ… Uses your hardware (GPU/CPU)
```

### ğŸŒ Sharing Your App

#### **Option 1: Ngrok Tunnel (Recommended for Sharing)**
```bash
# Install ngrok
npm install -g ngrok
# OR download from: https://ngrok.com/download

# Run your app locally
streamlit run src/main.py

# In another terminal, create public tunnel
ngrok http 8501

# Share the public URL: https://abc123.ngrok.io
# âœ… Full functionality maintained
# âœ… Uses your local GPU/CPU
# âœ… Complete privacy (data stays on your machine)
```

#### **Option 2: VPS/Server Deployment**
```bash
# Deploy on DigitalOcean, AWS, Linode, etc.
# 1. Provision server with GPU (optional)
# 2. Install Docker + Ollama
# 3. Clone repository
# 4. Run setup scripts
# 5. Configure firewall (port 8501)
# 6. Access via server IP
```

#### **Option 3: Docker Container**
```dockerfile
# Create containerized deployment
FROM nvidia/cuda:12.6-runtime-ubuntu22.04
# Install Ollama + dependencies
# Copy application
# Expose port 8501
# Can deploy anywhere that supports Docker
```

### âŒ Streamlit Community Cloud (NOT SUPPORTED)

**Why it doesn't work:**
```bash
âŒ Cannot install Ollama server
âŒ Cannot run local model servers (localhost:11434)
âŒ Missing system dependencies (cmake, swig, pkg-config)
âŒ Insufficient compute resources for large models
âŒ Cannot access your local GPU hardware
âŒ Architecture incompatibility with local LLM servers

# Attempting to deploy will result in:
# - Build failures due to missing dependencies
# - Runtime errors when trying to connect to Ollama
# - Import errors for compiled packages
```

---

## ğŸ¤– Local Model System

### ğŸ“‹ Model Configuration

| Environment | Primary Model | Judge Model | Enhancement Model | Use Case |
|------------|---------------|-------------|-------------------|----------|
| **Local (High Memory)** | `llama3.2:3b` | `llama3.1:8b` | `llama3.2:3b` | âœ… Maximum quality |
| **Local (Standard)** | `llama3.2:3b` | `llama3.2:3b` | `llama3.2:3b` | âœ… Good performance |
| **Local (Minimal)** | `llama3.2:1b` | `llama3.2:1b` | `llama3.2:1b` | âœ… Resource constrained |

### ğŸ”„ Automatic Detection
```python
# Environment detection (utils/model_config.py)
def detect_environment():
    if not is_ollama_available():
        raise RuntimeError("âŒ Ollama server not available - run 'ollama serve'")
    elif has_high_memory() and has_gpu():
        return "local_high"
    elif has_sufficient_memory():
        return "local_standard"
    else:
        return "local_minimal"
```

---

## ğŸ“Š Data Processing Pipeline

### ğŸ” Data Sources
- **GitLab Repository**: 23 AI apprentice projects
- **File Types**: Python scripts (`.py`), Jupyter notebooks (`.ipynb`)
- **Content**: Machine learning assignments, data science projects
- **Structure**: `/person/assignment/files`

### âš¡ Processing Steps

1. **File Discovery**: Recursive GitLab repository scanning
2. **Text Extraction**: Code-aware chunking with metadata preservation
3. **LLM Enhancement**: Adds comments, docstrings, explanations via Ollama
4. **Hybrid Embeddings**: 
   - Code files â†’ `microsoft/graphcodebert-base` â†’ 768D vectors
   - Notebooks â†’ `all-MiniLM-L6-v2` â†’ 384D vectors
   - Dimensionality alignment â†’ 384D common space
5. **FAISS Indexing**: GPU-accelerated similarity search indices
6. **Document Storage**: Enhanced content with source metadata

### ğŸ“ˆ Processing Performance

| Mode | Files | Local GPU Time | Local CPU Time | Quality |
|------|-------|----------------|----------------|---------|
| **Test** | ~30 | 5 min | 15 min | Good |
| **Custom** | ~100 | 15 min | 45 min | Good |
| **Full** | 1000+ | 45 min | 4+ hours | Excellent |

---

## ğŸ¯ Usage Examples

### ğŸ” Asking Questions

```python
# Code-specific questions
"How do I implement a decision tree in Python?"
"Show me examples of data preprocessing pipelines"
"What are common machine learning evaluation metrics?"

# Project-specific questions  
"How did students approach assignment 1?"
"What libraries are commonly used for data science?"
"Show me examples of model evaluation code"

# Conceptual questions
"Explain the difference between supervised and unsupervised learning"
"What are best practices for data validation?"
```

### ğŸ“Š Response Structure
```json
{
  "answer": "Enhanced LLM response with context from local models",
  "sources": ["file1.py", "notebook2.ipynb"],
  "evaluation": {
    "relevance": 0.95,
    "accuracy": 0.88,
    "completeness": 0.92
  },
  "reasoning": "Judge model assessment",
  "model_used": "llama3.2:3b"
}
```

---

## ğŸ§ª Testing & Validation

### ğŸ” Run Comprehensive Tests
```bash
# Ensure Ollama is running first
ollama serve

# Full test suite
python tests/run_tests.py

# Individual components
python tests/test_requirements.py         # Dependencies âœ…
python tests/test_data_processing.py     # File processing âœ…
python tests/test_embeddings_comprehensive.py  # GPU/CPU embeddings âœ…
python tests/test_rag_retrieval.py      # End-to-end RAG âœ…
```

### ğŸ“Š Expected Test Results
```
âœ… ALL DEPENDENCIES INSTALLED!
âœ… Ollama server accessible at localhost:11434
âœ… ALL TESTS COMPLETED SUCCESSFULLY!
âœ… GPU available - using CUDA acceleration
âœ… Environment detection: local_high
âœ… Model loading: llama3.2:3b, llama3.1:8b
âœ… Data processing: WORKING
âœ… RAG pipeline: WORKING
ğŸš€ Ready for local deployment!
```

---

## ğŸ”§ Configuration & Customization

### ğŸ›ï¸ Model Configuration (utils/model_config.py)
```python
# Customize models for your hardware
LOCAL_MODELS = {
    "high": {
        "primary": "llama3.2:3b",      # High-quality responses
        "judge": "llama3.1:8b",        # Thorough evaluation  
        "fallback": "llama3.2:1b"      # Resource constraints
    },
    "standard": {
        "primary": "llama3.2:3b",      # Good responses
        "judge": "llama3.2:3b",        # Same model evaluation
        "fallback": "llama3.2:1b"      # Resource constraints
    },
    "minimal": {
        "primary": "llama3.2:1b",      # Faster responses
        "judge": "llama3.2:1b",        # Quick evaluation
        "fallback": "llama3.2:1b"      # Consistent
    }
}
```

### âš™ï¸ Data Processing Options
```python
# Quick testing (development)
python run_data_ingestion.py --test

# Custom limits (testing)
python run_data_ingestion.py --limit-people 5 --limit-files 20

# Skip enhancement for speed (optional)
python run_data_ingestion.py --no-enhancement

# Full production processing
python run_data_ingestion.py
```

---

## ğŸš€ Production Deployment Guide

### ğŸ“‹ Pre-deployment Checklist
- [ ] Run full test suite locally
- [ ] Process data with GPU enhancement
- [ ] Verify index file generation
- [ ] Test Streamlit app locally
- [ ] Ensure Ollama models are downloaded
- [ ] Configure sharing method (ngrok/VPS)

### ğŸŒ Sharing Setup Options

#### **Option 1: Ngrok Tunnel (Easiest)**
```bash
# 1. Install ngrok
npm install -g ngrok

# 2. Get free account at ngrok.com for persistent URLs
ngrok config add-authtoken YOUR_TOKEN

# 3. Start your app
streamlit run src/main.py

# 4. Create tunnel (in new terminal)
ngrok http 8501

# 5. Share URL: https://abc123.ngrok.app
```

#### **Option 2: VPS Deployment**
```bash
# 1. Provision server (DigitalOcean, Linode, etc.)
# 2. Install dependencies:
apt update && apt install -y python3 python3-pip git
curl -fsSL https://ollama.ai/install.sh | sh

# 3. Clone and setup:
git clone <your-repo>
cd Custom-RAG-Engine-for-Enterprise-Document-QA
pip install -r requirements.txt
python setup_models.py

# 4. Start services:
ollama serve &
streamlit run src/main.py --server.address=0.0.0.0

# 5. Configure firewall for port 8501
# 6. Access via: http://YOUR_SERVER_IP:8501
```

---

## ğŸ“Š Performance & Benchmarks

### ğŸ”¥ GPU vs CPU Performance

| Operation | Local GPU | Local CPU | Speedup |
|-----------|-----------|-----------|---------|
| **Data Ingestion** | 45 min | 4+ hours | 5-6x |
| **Embedding Generation** | 2.3s | 45.7s | 20x |
| **FAISS Index Creation** | 0.8s | 12.4s | 15x |
| **RAG Query Response** | 1.2s | 3.8s | 3x |
| **Model Loading** | 8s | 25s | 3x |

### ğŸ’¾ File Sizes (23 People, Full Dataset)

| File | Size | Description |
|------|------|-------------|
| `faiss_code_index.bin` | ~15MB | Code vector index |
| `faiss_non_code_index.bin` | ~35MB | Notebook vector index |
| `code_docstore.json` | ~120MB | Enhanced Python files |
| `non_code_docstore.json` | ~280MB | Enhanced notebooks |
| **Total** | **~450MB** | Complete RAG system |

### ğŸ–¥ï¸ System Requirements

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|---------|
| **RAM** | 8GB | 16GB | 32GB+ |
| **GPU** | None (CPU) | GTX 1660 | RTX 3080+ |
| **Storage** | 10GB | 50GB | 100GB+ |
| **CPU** | 4 cores | 8 cores | 16+ cores |

---

## ğŸ”§ Troubleshooting

### âŒ Common Issues & Solutions

#### **Issue**: Ollama connection failed
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# If not running, start it:
ollama serve

# Verify models are downloaded:
ollama list
```

#### **Issue**: Models not found
```bash
# Download required models:
python setup_models.py

# OR manually:
ollama pull llama3.2:3b
ollama pull llama3.1:8b
ollama pull llama3.2:1b
```

#### **Issue**: GPU not detected
```bash
# Check CUDA installation
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"

# Reinstall PyTorch with CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
```

#### **Issue**: Import errors
```bash
# Install missing dependencies
pip install -r requirements.txt

# For compilation issues, install build tools:
# Ubuntu/Debian:
apt install build-essential cmake pkg-config

# Windows: Install Visual Studio Build Tools
```

#### **Issue**: "I want to deploy to Streamlit Cloud"
```bash
# âŒ NOT POSSIBLE with current architecture
# âœ… Solutions:
# 1. Use ngrok tunnel for sharing local app
# 2. Deploy on VPS with Docker
# 3. Use cloud APIs (requires major code rewrite)
```

---

## ğŸ¯ Customization for Your Data

### ğŸ—‚ï¸ Using Your Own Data

1. **Replace Data Source**:
   ```bash
   # Replace data/aiap17-gitlab-data/ with your repository
   cp -r /path/to/your/data data/your-project-data
   
   # Update configuration
   python run_data_ingestion.py --root-directory data/your-project-data
   ```

2. **Modify File Types**:
   ```python
   # Edit file_retrieval.py to support your file types
   SUPPORTED_EXTENSIONS = ['.py', '.ipynb', '.md', '.txt', '.java', '.cpp']
   ```

3. **Customize Enhancement**:
   ```python
   # Edit data_enhancement.py for domain-specific improvements
   ENHANCEMENT_PROMPT = """
   Improve this {file_type} for better searchability:
   - Add domain-specific comments
   - Explain business logic  
   - Add relevant keywords for {your_domain}
   """
   ```

4. **Adjust Models**:
   ```python
   # Modify utils/model_config.py for different models
   # Available Ollama models: ollama.ai/library
   LOCAL_MODELS = {
       "primary": "codellama:7b",     # For code-heavy datasets
       "judge": "llama3.1:8b",        # For evaluation
       "fallback": "gemma2:2b"        # Lightweight option
   }
   ```

---

## ğŸ¤ Contributing

1. **Fork & Clone**
   ```bash
   git clone https://github.com/your-username/Custom-RAG-Engine-for-Enterprise-Document-QA.git
   ```

2. **Create Feature Branch**
   ```bash
   git checkout -b feature/amazing-enhancement
   ```

3. **Test Your Changes**
   ```bash
   # Ensure Ollama is running
   ollama serve

   # Run tests
   python tests/run_tests.py
   ```

4. **Submit Pull Request**
   - Ensure all tests pass
   - Include performance benchmarks
   - Document new features
   - Test on both GPU and CPU environments

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **AI Singapore AIAP Batch 17** - Training program and dataset
- **LangChain Community** - RAG framework foundation
- **Ollama Team** - Local LLM inference platform
- **FAISS & HuggingFace** - Vector search and embedding models
- **Streamlit** - Rapid UI development platform

---

## ğŸ“š Key Technical Papers & References

1. [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
2. [LangChain Documentation](https://python.langchain.com/docs/)
3. [FAISS: A Library for Efficient Similarity Search](https://github.com/facebookresearch/faiss)
4. [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
5. [GraphCodeBERT: Pre-training Code Representations with Data Flow](https://arxiv.org/abs/2009.08366)
6. [Ollama Documentation](https://ollama.ai/docs)

---

## ğŸš¨ **Important Deployment Notice**

**This RAG system is designed exclusively for LOCAL deployment.** 

- âœ… **Works perfectly**: Local Streamlit + Ollama
- âœ… **For sharing**: Use ngrok tunneling or VPS deployment  
- âŒ **Does NOT work**: Streamlit Community Cloud (architectural limitations)

**For sharing your app with others, use the ngrok tunnel method or deploy on your own server infrastructure.**

---

**â­ Star this repository if you find it useful!**

**ğŸ› Issues**: [GitHub Issues](https://github.com/Adredes-weslee/Custom-RAG-Engine-for-Enterprise-Document-QA/issues)  
**ğŸ’¬ Discussions**: [GitHub Discussions](https://github.com/Adredes-weslee/Custom-RAG-Engine-for-Enterprise-Document-QA/discussions)  
**ğŸ“§ Contact**: weslee.qb@gmail.com

---

*This project represents a complete enterprise RAG solution optimized for local deployment with powerful hardware utilization. The local-first architecture ensures maximum performance, complete privacy, and full control over your data and models.*